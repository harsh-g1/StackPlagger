Questions,SO Answers
"I need some help guys!! I am a self-taught, newbie in encryption, and after reading, testing, and error for more than two weeks on how to solve this, and finding very little crowd knowledge and almost no documentation from Google.
I am trying to read the integrity verdict, that I have managed to get it IntegrityTokenRequest doing
    String nonce = Base64.encodeToString(&quot;this_is_my_nonce&quot;.getBytes(), Base64.URL_SAFE | Base64.NO_WRAP | Base64.NO_PADDING);
    IntegrityManager myIntegrityManager =   IntegrityManagerFactory
          .create(getApplicationContext());
    // Request the integrity token by providing a nonce.
    Task&lt;IntegrityTokenResponse&gt; myIntegrityTokenResponse = myIntegrityManager
          .requestIntegrityToken(IntegrityTokenRequest
          .builder()
          .setNonce(nonce)
          .build());

    myIntegrityTokenResponse.addOnSuccessListener(new OnSuccessListener&lt;IntegrityTokenResponse&gt;() {
        @Override
        public void onSuccess(IntegrityTokenResponse myIntegrityTokenResponse) {
            String token = myIntegrityTokenResponse.token();
            // so here I have my Integrity token.
            // now how do I read it??
        }
    }

As per the documentation, it's all set up in the Play Console, and created the Google Cloud project accordingly. Now here comes the big hole in the documentation:
a) The JWT has 4 dots that divide the JWT into 5 sections, not in 3 sections as described here https://jwt.io/
b) Developer.Android.com recommends to Decrypt and Verify on Google Servers

I have no idea on how or were to execute this command... :-(
c) if I choose to decrypt and verify the returned token it's more complicated as I don't have my own secure server environment, only my App and the Google Play Console.
d) I found in the Google Clound Platform OAuth 2.0 Client IDs &quot;Android client for com.company.project&quot; JSON file that I have downloaded, but no clue (again) on how to use it in my App for getting the veredict from the Integrity Token.
{&quot;installed&quot;:
    {&quot;client_id&quot;:&quot;123456789012-abcdefghijklmnopqrstuvwxyza0g2ahk.apps.googleusercontent.com&quot;,
        &quot;project_id&quot;:&quot;myproject-360d3&quot;,
        &quot;auth_uri&quot;:&quot;https://accounts.google.com/o/oauth2/auth&quot;,
        &quot;token_uri&quot;:&quot;https://oauth2.googleapis.com/token&quot;,
        &quot;auth_provider_x509_cert_url&quot;:https://www.googleapis.com/oauth2/v1/certs
    }
}

I'm sure I am missing a lot, please help
","Using a cloud server to decode and verify the token is better.
For example, if you going with Java service then the below code will send the integrity token to the google server hence you can verify the response.
Enable PlayIntegrity API in Google Cloud Platform against the app and download the JSON file and configure in the code.
Similarly, you should enable PlayIntegrity API in Google PlayConsole against the app
Add Google Play Integrity Client Library to your project
Maven Dependency
&lt;project&gt;
 &lt;dependencies&gt;
   &lt;dependency&gt;
     &lt;groupId&gt;com.google.apis&lt;/groupId&gt;
     &lt;artifactId&gt;google-api-services-playintegrity&lt;/artifactId&gt;
     &lt;version&gt;v1-rev20220211-1.32.1&lt;/version&gt;
   &lt;/dependency&gt;
 &lt;/dependencies&gt;

   
Gradle
repositories {
   mavenCentral()
}
dependencies {
   implementation 'com.google.apis:google-api-services-playintegrity:v1-rev20220211-1.32.1'
}

Token decode
DecodeIntegrityTokenRequest requestObj = new DecodeIntegrityTokenRequest();
requestObj.setIntegrityToken(request.getJws());
//Configure downloaded Json file
GoogleCredentials credentials = GoogleCredentials.fromStream(new FileInputStream(&quot;&lt;Path of JSON file&gt;\\file.json&quot;));
HttpRequestInitializer requestInitializer = new HttpCredentialsAdapter(credentials);

 HttpTransport HTTP_TRANSPORT = new NetHttpTransport();
 JsonFactory JSON_FACTORY = new JacksonFactory();
 GoogleClientRequestInitializer initialiser = new PlayIntegrityRequestInitializer();
 
 
Builder playIntegrity = new PlayIntegrity.Builder(HTTP_TRANSPORT, JSON_FACTORY, requestInitializer).setApplicationName(&quot;testapp&quot;)
        .setGoogleClientRequestInitializer(initialiser);
             PlayIntegrity play = playIntegrity.build();
    
DecodeIntegrityTokenResponse response = play.v1().decodeIntegrityToken(&quot;com.test.android.integritysample&quot;, requestObj).execute();

Then the response will be as follows
{
&quot;tokenPayloadExternal&quot;: {
    &quot;accountDetails&quot;: {
        &quot;appLicensingVerdict&quot;: &quot;LICENSED&quot;
    },
    &quot;appIntegrity&quot;: {
        &quot;appRecognitionVerdict&quot;: &quot;PLAY_RECOGNIZED&quot;,
        &quot;certificateSha256Digest&quot;: [&quot;pnpa8e8eCArtvmaf49bJE1f5iG5-XLSU6w1U9ZvI96g&quot;],
        &quot;packageName&quot;: &quot;com.test.android.integritysample&quot;,
        &quot;versionCode&quot;: &quot;4&quot;
    },
    &quot;deviceIntegrity&quot;: {
        &quot;deviceRecognitionVerdict&quot;: [&quot;MEETS_DEVICE_INTEGRITY&quot;]
    },
    &quot;requestDetails&quot;: {
        &quot;nonce&quot;: &quot;SafetyNetSample1654058651834&quot;,
        &quot;requestPackageName&quot;: &quot;com.test.android.integritysample&quot;,
        &quot;timestampMillis&quot;: &quot;1654058657132&quot;
    }
}
}

Check for License
String licensingVerdict = response.getTokenPayloadExternal().getAccountDetails().getAppLicensingVerdict();
    if(!licensingVerdict.equalsIgnoreCase(&quot;LICENSED&quot;)) {
         throw new Exception(&quot;Licence is not valid.&quot;);
            
    }

Verify App Integrity
public void checkAppIntegrity(DecodeIntegrityTokenResponse response,  String appId) throws Exception {
    AppIntegrity appIntegrity = response.getTokenPayloadExternal().getAppIntegrity();
    
    if(!appIntegrity.getAppRecognitionVerdict().equalsIgnoreCase(&quot;PLAY_RECOGNIZED&quot;)) {
        throw new Exception(&quot;The certificate or package name does not match Google Play records.&quot;);
    }
     if(!appIntegrity.getPackageName().equalsIgnoreCase(appId)) {
         throw new Exception(&quot;App package name mismatch.&quot;);
        
     }
     
     if(appIntegrity.getCertificateSha256Digest()!= null) {
        //If the app is deployed in Google PlayStore then Download the App signing key certificate from Google Play Console (If you are using managed signing key). 
        //otherwise download Upload key certificate and then find checksum of the certificate.
         Certificate cert = getCertificate(&quot;&lt;Path to Signing certificate&gt;\deployment_cert.der&quot;);
         MessageDigest md = MessageDigest.getInstance(&quot;SHA-256&quot;); 

        byte[] der = cert.getEncoded(); 
        md.update(der);
        byte[] sha256 = md.digest();
        
        //String checksum = Base64.getEncoder().encodeToString(sha256);
       String checksum = Base64.getUrlEncoder().encodeToString(sha256);
       /** Sometimes checksum value ends with '=' character, you can avoid this character before perform the match **/
       checksum = checksum.replaceAll(&quot;=&quot;,&quot;&quot;);        
        if(!appIntegrity.getCertificateSha256Digest().get(0).contains(checksum)) {
             throw new Exception(&quot;App certificate mismatch.&quot;);
        }
     }
}
public static Certificate getCertificate(String certificatePath)
        throws Exception {
    CertificateFactory certificateFactory = CertificateFactory
            .getInstance(&quot;X509&quot;);
    FileInputStream in = new FileInputStream(certificatePath);

    Certificate certificate = certificateFactory
            .generateCertificate(in);
    in.close();

    return certificate;
}

Verify Device integrity
//Check Device Integrity
public void deviceIntegrity(DecodeIntegrityTokenResponse response) {
    DeviceIntegrity deviceIntegrity = response.getTokenPayloadExternal().getDeviceIntegrity();
    if(!deviceIntegrity.getDeviceRecognitionVerdict().contains(&quot;MEETS_DEVICE_INTEGRITY&quot;)) {
        throw new Exception(&quot;Does not meet Device Integrity.&quot;);
        
    }
}

Similary you can verify the Nonce and App Package name with previously stored data in server
"
"I am working on several spring-boot applications which have the traditional pattern of thread-per-request. We are using Spring-boot-webflux to acquire WebClient to perform our RESTful integration between the applications. Hence our application design requires that we block the publisher right after receiving a response.
Recently, we've been discussing whether we are unnecessarily spending resources using a reactive module in our otherwise blocking application design. As I've understood it, WebClient makes use of the event loop by assigning a worker thread to perform the reactive actions in the event loop. So using webclient with .block() would sleep the original thread while assigning another thread to perform the http-request. Compared to the alternative RestTemplate, it seems like WebClient would spend additional resources by using the event loop.
Is it correct that partially introducing spring-webflux in this way leads to additional spent resources while not yielding any positive contribution to performance, neither single threaded and concurrent? We are not expecting to ever upgrade our current stack to be fully reactive, so the argument of gradually upgrading does not apply.
","In this presentation Rossen Stoyanchev from the Spring team explains some of these points.
WebClient will use a limited number of threads - 2 per core for a total of 12 threads on my local machine - to handle all requests and their responses in the application. So if your application receives 100 requests and makes one request to an external server for each, WebClient will handle all of those using those threads in a non-blocking / asynchronous manner.
Of course, as you mention, once you call block your original thread will block, so it would be 100 threads + 12 threads for a total of 112 threads to handle those requests. But keep in mind that these 12 threads do not grow in size as you make more requests, and that they don't do I/O heavy lifting, so it's not like WebClient is spawning threads to actually perform the requests or keeping them busy on a thread-per-request fashion.
I'm not sure if when the thread is under block it behaves the same as when making a blocking call through RestTemplate - it seems to me that in the former the thread should be inactive waiting for the NIO call to complete, while in the later the thread should be handling I/O work, so maybe there's a difference there.
It gets interesting if you begin using the reactor goodies, for example handling requests that depend on one another, or many requests in parallel. Then WebClient definitely gets an edge as it'll perform all concurrent actions using the same 12 threads, instead of using a thread per request.
As an example, consider this application:
@SpringBootApplication
public class SO72300024 {

    private static final Logger logger = LoggerFactory.getLogger(SO72300024.class);

    public static void main(String[] args) {
        SpringApplication.run(SO72300024.class, args);
    }

    @RestController
    @RequestMapping(&quot;/blocking&quot;)
    static class BlockingController {

        @GetMapping(&quot;/{id}&quot;)
        String blockingEndpoint(@PathVariable String id) throws Exception {
            logger.info(&quot;Got request for {}&quot;, id);
            Thread.sleep(1000);
            return &quot;This is the response for &quot; + id;
        }

        @GetMapping(&quot;/{id}/nested&quot;)
        String nestedBlockingEndpoint(@PathVariable String id) throws Exception {
            logger.info(&quot;Got nested request for {}&quot;, id);
            Thread.sleep(1000);
            return &quot;This is the nested response for &quot; + id;
        }

    }

    @Bean
    ApplicationRunner run() {
        return args -&gt; {
            Flux.just(callApi(), callApi(), callApi())
                    .flatMap(responseMono -&gt; responseMono)
                    .collectList()
                    .block()
                    .stream()
                    .flatMap(Collection::stream)
                    .forEach(logger::info);
            logger.info(&quot;Finished&quot;);
        };
    }

    private Mono&lt;List&lt;String&gt;&gt; callApi() {
        WebClient webClient = WebClient.create(&quot;http://localhost:8080&quot;);
        logger.info(&quot;Starting&quot;);
        return Flux.range(1, 10).flatMap(i -&gt;
                        webClient
                                .get().uri(&quot;/blocking/{id}&quot;, i)
                                .retrieve()
                                .bodyToMono(String.class)
                                .doOnNext(resp -&gt; logger.info(&quot;Received response {} - {}&quot;, I, resp))
                                .flatMap(resp -&gt; webClient.get().uri(&quot;/blocking/{id}/nested&quot;, i)
                                        .retrieve()
                                        .bodyToMono(String.class)
                                        .doOnNext(nestedResp -&gt; logger.info(&quot;Received nested response {} - {}&quot;, I, nestedResp))))
                .collectList();
    }
}

If you run this app, you can see that all 30 requests are handled immediately in parallel by the same 12 (in my computer) threads. Neat! If you think you can benefit from such kind of parallelism in your logic, it's probably worth it giving WebClient a shot.
If not, while I wouldn't actually worry about the &quot;extra resource spending&quot; given the reasons above, I don't think it would be worth it adding the whole reactor/webflux dependency for this - besides the extra baggage, in day to day operations it should be a lot simpler to reason about and debug RestTemplate and the thread-per-request model.
Of course, as others have mentioned, you ought to run load tests to have proper metrics.
"
"The following snippet does not compile on javac, version 17 (Temurin)
class Instanceof {
    static void doesNotWork(Object o) {
        if (o == null) {
            throw new Error();
        } else if (!(o instanceof String s)) {
            throw new Error();
        }   
        System.out.println(s); // error here
    }
}

It generates this error: cannot find symbol
cannot find symbol
symbol:   variable s
location: class Instanceof

However, the following (in my opinion) equivalent variations work:
With an explicit else block:
static void doesWork(Object o) {
    if (o == null) {
        throw new Error();
    } else if (!(o instanceof String s)) {
        throw new Error();
    } else {
        System.out.println(s);
    }
}

Or without an else:
static void doesWork(Object o) {
    if (o == null) {
        throw new Error();
    }
    if (!(o instanceof String s)) {
        throw new Error();
    }
    System.out.println(s);
}

Or with a single if:
static void doesWork(Object o) {
    if (o == null || !(o instanceof String s)) {
        throw new Error();
    }
    System.out.println(s);
}

Is this a bug in javac?
If yes, should I report this, but where exactly?
","The doesNotWork case is equivalent to this:
static void doesNotWork(Object o) {
    if (o == null) {
        throw new Error();
    } else {
        if (!(o instanceof String s)) {
            throw new Error();
        }
    }
    System.out.println(s); // error here
}

This makes it more obvious that String s is inside a block bounded by curly brackets and is therefore out of scope in the same way that this doesn't work either:
static void doesNotWork(Object o) {
    {
        if (!(o instanceof String s)) {
            throw new Error();
        }
    }
    System.out.println(s); // error here
}

In the case where it does work, with the println inside the else, it's equivalent to this:
if (o == null) {
    throw new Error();
} else {
    if (!(o instanceof String s)) {
        throw new Error();
    } else {
        System.out.println(s);
    }
}

Which shows the println being in scope.
"
"I have recently upgraded Android Studio to Flamingo and also upgraded Gradle from 7.4.2 to 8.0.0. All things working fine in version 7.4.2.
When I generate a signed APK using Gradle 8.0.0, it's giving me a runtime error of java.lang.ClassCastException.
I have tried many solutions like adding Proguard rules for Retrofit, Okio, OkHttp, etc., but it still give me an error like this.
Note: When I downgraded from 8.0.0 to 7.4.2, it's working.
So anyone can help me to find out problem with AGP 8.0.0.
build.gradle(app)
plugins {
    id 'com.android.application'
    id 'org.jetbrains.kotlin.android'
    id 'kotlin-kapt'
    id 'kotlin-parcelize'
    id 'com.google.dagger.hilt.android'
    id 'com.google.gms.google-services'
    id 'com.google.firebase.crashlytics'
}


android {
    
    compileSdk 33

    defaultConfig {
        
        minSdk 24
        targetSdk 33
        versionCode 22
        versionName &quot;1.0.16&quot;
        multiDexEnabled true

        testInstrumentationRunner &quot;androidx.test.runner.AndroidJUnitRunner&quot;
        /*vectorDrawables {
            useSupportLibrary true
        }*/

        def localPropertiesFile = rootProject.file(&quot;local.properties&quot;)
        def localProperties = new Properties()
        localProperties.load(new FileInputStream(localPropertiesFile))
        buildConfigField &quot;String&quot;, &quot;API_KEY&quot;, localProperties['API_KEY']



    }

    
    buildTypes {
        release {
            minifyEnabled true
            shrinkResources true
            signingConfig signingConfigs.release
            proguardFiles getDefaultProguardFile('proguard-android-optimize.txt'), 'proguard-rules.pro'
        }
    }
    compileOptions {
        sourceCompatibility JavaVersion.VERSION_17
        targetCompatibility JavaVersion.VERSION_17
    }
    kotlinOptions {
        jvmTarget = '17'
    }
    buildFeatures {
        compose true
        viewBinding = true
    }
    composeOptions {
        kotlinCompilerExtensionVersion '1.4.2'
    }
    packagingOptions {
        resources {
            excludes += '/META-INF/{AL2.0,LGPL2.1}'
        }
    }
    bundle {
        language {
            enableSplit = false
        }
    }
}

dependencies {

    implementation 'androidx.core:core-ktx:1.10.0'
    implementation 'androidx.appcompat:appcompat:1.6.1'
    implementation 'com.google.android.material:material:1.8.0'
    implementation 'androidx.constraintlayout:constraintlayout:2.1.4'
    implementation 'androidx.multidex:multidex:2.0.1'

    
    implementation 'com.google.accompanist:accompanist-permissions:0.24.11-rc'
    implementation 'com.google.accompanist:accompanist-webview:0.24.11-rc'
    implementation 'com.google.accompanist:accompanist-pager:0.24.13-rc'
    implementation &quot;com.google.accompanist:accompanist-pager-indicators:0.24.13-rc&quot;
    implementation &quot;com.google.accompanist:accompanist-drawablepainter:0.25.1&quot;
    implementation &quot;com.google.accompanist:accompanist-flowlayout:0.31.0-alpha&quot;


   
    implementation 'androidx.activity:activity-compose:1.7.1'
    implementation platform('androidx.compose:compose-bom:2022.10.00')
    implementation 'androidx.compose.ui:ui'
    implementation 'androidx.compose.ui:ui-graphics'
    implementation 'androidx.compose.ui:ui-tooling-preview'
    implementation 'androidx.compose.material:material'
//    implementation 'androidx.compose.material3:material3'
    implementation &quot;androidx.navigation:navigation-compose:2.5.3&quot;
    implementation 'com.google.firebase:protolite-well-known-types:18.0.0'
    implementation &quot;androidx.compose.ui:ui-viewbinding&quot;
    implementation project(path: ':pdfviewer')

  
    testImplementation 'junit:junit:4.13.2'
    androidTestImplementation 'androidx.test.ext:junit:1.1.5'
    androidTestImplementation 'androidx.test.espresso:espresso-core:3.5.1'
    androidTestImplementation &quot;androidx.compose.ui:ui-test-junit4&quot;

    
    implementation &quot;com.google.dagger:hilt-android:2.45&quot;
    debugImplementation &quot;androidx.compose.ui:ui-test-manifest&quot;
    kapt &quot;com.google.dagger:hilt-compiler:2.45&quot;
    kapt &quot;androidx.hilt:hilt-compiler:1.0.0&quot;
    implementation 'androidx.hilt:hilt-navigation-compose:1.0.0'

  
    implementation &quot;androidx.activity:activity-ktx:1.7.1&quot;

  
    implementation &quot;androidx.lifecycle:lifecycle-extensions:2.2.0&quot;
    implementation &quot;androidx.lifecycle:lifecycle-livedata-ktx:2.6.1&quot;
    implementation &quot;androidx.lifecycle:lifecycle-runtime-ktx:2.6.1&quot;
    implementation &quot;androidx.lifecycle:lifecycle-viewmodel-ktx:2.6.1&quot;
    implementation &quot;androidx.lifecycle:lifecycle-process:2.6.1&quot;
    kapt &quot;androidx.lifecycle:lifecycle-compiler:2.6.1&quot;

    /* *****************************************************
       **** Retrofit2
       ****************************************************** */
    implementation 'com.squareup.retrofit2:retrofit:2.9.0'
    implementation 'com.squareup.retrofit2:converter-gson:2.9.0'
    implementation &quot;com.squareup.okhttp3:okhttp:4.9.0&quot;
    implementation &quot;com.squareup.okhttp3:logging-interceptor:4.9.0&quot;
    implementation 'com.squareup.retrofit2:converter-moshi:2.9.0'

   
    implementation 'org.jetbrains.kotlinx:kotlinx-coroutines-core:1.6.4'
    implementation 'org.jetbrains.kotlinx:kotlinx-coroutines-android:1.6.4'

   
    implementation &quot;androidx.room:room-runtime:2.5.1&quot;
    kapt &quot;androidx.room:room-compiler:2.5.1&quot;

    
    implementation &quot;androidx.room:room-ktx:2.5.1&quot;

   
    implementation 'androidx.core:core-splashscreen:1.0.1'

    
    def billing_version = &quot;5.2.0&quot;
    implementation &quot;com.android.billingclient:billing:$billing_version&quot;
    implementation &quot;com.android.billingclient:billing-ktx:$billing_version&quot;
    implementation 'com.google.firebase:firebase-crashlytics-buildtools:2.9.5'

   
    implementation platform('com.google.firebase:firebase-bom:31.1.0')
    implementation 'com.google.firebase:firebase-config-ktx'
    implementation 'com.google.firebase:firebase-analytics-ktx'
    implementation 'com.google.firebase:firebase-crashlytics-ktx'
    implementation 'com.google.firebase:firebase-messaging-ktx'
    implementation 'com.google.android.gms:play-services-ads:22.0.0'

   
    implementation 'com.airbnb.android:lottie-compose:4.0.0'


}

kapt {
    correctErrorTypes true
}

Project Gradle File
buildscript {
    ext {
        compose_ui_version = '1.5.0-alpha02'
        kotlin_version = '1.8.10'
    }
        dependencies {
            // Add this line
            classpath 'com.google.gms:google-services:4.3.15'
            classpath 'com.google.firebase:firebase-crashlytics-gradle:2.9.5'
            classpath &quot;org.jetbrains.kotlin:kotlin-gradle-plugin:$kotlin_version&quot;
        }
    repositories {
        mavenCentral()
    }
}// Top-level build file where you can add configuration options common to all sub-projects/modules.
plugins {
    id 'com.android.application' version '8.0.0' apply false
    id 'com.android.library' version '8.0.0' apply false
    id 'org.jetbrains.kotlin.android' version '1.8.10' apply false
    id 'com.google.dagger.hilt.android' version '2.44' apply false
}

","I was facing this issue because of the Parcelize plugin. I'm using the Parcelize plugin to pass Custom Object ArrayList between activities.
After a lot of trials finally found the solution of changing proguard-rules.pro files.
Here is my content of proguard-rules.pro file
# Add project specific ProGuard rules here.
# You can control the set of applied configuration files using the
# proguardFiles setting in build.gradle.
#
# For more details, see
#   http://developer.android.com/guide/developing/tools/proguard.html

# If your project uses WebView with JS, uncomment the following
# and specify the fully qualified class name to the JavaScript interface
# class:
#-keepclassmembers class fqcn.of.javascript.interface.for.webview {
#   public *;
#}

# Uncomment this to preserve the line number information for
# debugging stack traces.
#-keepattributes SourceFile,LineNumberTable

# If you keep the line number information, uncomment this to
# hide the original source file name.
#-renamesourcefileattribute SourceFile

-dontwarn rx.**

-dontwarn okio.**

-dontwarn com.squareup.okhttp.**
-keep class com.squareup.okhttp.** { *; }
-keep interface com.squareup.okhttp.** { *; }

-dontwarn retrofit.**
-keep class retrofit.** { *; }
-keepclasseswithmembers class * {
    @retrofit.http.* &lt;methods&gt;;
}

-keepattributes Signature
-keepattributes *Annotation*


-dontwarn javax.annotation.**

# A resource is loaded with a relative path so the package of this class must be preserved.
-adaptresourcefilenames okhttp3/internal/publicsuffix/PublicSuffixDatabase.gz

# Animal Sniffer compileOnly dependency to ensure APIs are compatible with older versions of Java.
-dontwarn org.codehaus.mojo.animal_sniffer.*

# OkHttp platform used only on JVM and when Conscrypt and other security providers are available.
-dontwarn okhttp3.internal.platform.**
-dontwarn org.conscrypt.**
-dontwarn org.bouncycastle.**
-dontwarn org.openjsse.**

-repackageclasses
-ignorewarnings

# Retrofit does reflection on generic parameters. InnerClasses is required to use Signature and
# EnclosingMethod is required to use InnerClasses.
-keepattributes Signature, InnerClasses, EnclosingMethod

# Retrofit does reflection on method and parameter annotations.
-keepattributes RuntimeVisibleAnnotations, RuntimeVisibleParameterAnnotations

# Keep annotation default values (e.g., retrofit2.http.Field.encoded).
-keepattributes AnnotationDefault

# Retain service method parameters when optimizing.
-keepclassmembers,allowshrinking,allowobfuscation interface * {
    @retrofit2.http.* &lt;methods&gt;;
}

# Ignore JSR 305 annotations for embedding nullability information.
-dontwarn javax.annotation.**

# Guarded by a NoClassDefFoundError try/catch and only used when on the classpath.
-dontwarn kotlin.Unit

# Top-level functions that can only be used by Kotlin.
-dontwarn retrofit2.KotlinExtensions
-dontwarn retrofit2.KotlinExtensions$*

# With R8 full mode, it sees no subtypes of Retrofit interfaces since they are created with a Proxy
# and replaces all potential values with null. Explicitly keeping the interfaces prevents this.
-if interface * { @retrofit2.http.* &lt;methods&gt;; }
-keep,allowobfuscation interface &lt;1&gt;

# Keep inherited services.
-if interface * { @retrofit2.http.* &lt;methods&gt;; }
-keep,allowobfuscation interface * extends &lt;1&gt;

# Keep generic signature of Call, Response (R8 full mode strips signatures from non-kept items).
-keep,allowobfuscation,allowshrinking interface retrofit2.Call
-keep,allowobfuscation,allowshrinking class retrofit2.Response

# With R8 full mode generic signatures are stripped for classes that are not
# kept. Suspend functions are wrapped in continuations where the type argument
# is used.
-keep,allowobfuscation,allowshrinking class kotlin.coroutines.Continuation


# Animal Sniffer compileOnly dependency to ensure APIs are compatible with older versions of Java.
-dontwarn org.codehaus.mojo.animal_sniffer.*

#-keep public class com.itextpdf.**
-keep class com.itextpdf.** { *; }
-dontwarn com.itextpdf.*

-renamesourcefileattribute SourceFile

-dontwarn retrofit2.**
-keep class retrofit2.** {*;}

##---------------Begin: proguard configuration for Gson  ----------
# Gson uses generic type information stored in a class file when working with fields. Proguard
# removes such information by default, so configure it to keep all of it.
-keepattributes Signature

# For using GSON @Expose annotation
-keepattributes *Annotation*

# Gson specific classes
-dontwarn sun.misc.**
#-keep class com.google.gson.stream.** { *; }

# Application classes that will be serialized/deserialized over Gson
-keep class com.google.gson.examples.android.model.** { &lt;fields&gt;; }

# Prevent proguard from stripping interface information from TypeAdapter, TypeAdapterFactory,
# JsonSerializer, JsonDeserializer instances (so they can be used in @JsonAdapter)
-keep class * extends com.google.gson.TypeAdapter
-keep class * implements com.google.gson.TypeAdapterFactory
-keep class * implements com.google.gson.JsonSerializer
-keep class * implements com.google.gson.JsonDeserializer

# Prevent R8 from leaving Data object members always null
-keepclassmembers,allowobfuscation class * {
  @com.google.gson.annotations.SerializedName &lt;fields&gt;;
}


-keep class com.google.gson.reflect.TypeToken
-keep class * extends com.google.gson.reflect.TypeToken
-keep public class * implements java.lang.reflect.Type


# Retain generic signatures of TypeToken and its subclasses with R8 version 3.0 and higher.
-keep,allowobfuscation,allowshrinking class com.google.gson.reflect.TypeToken
-keep,allowobfuscation,allowshrinking class * extends com.google.gson.reflect.TypeToken

##---------------End: proguard configuration for Gson  ----------
-dontwarn org.slf4j.**

-keepdirectories src/main/res/font/*



-dontusemixedcaseclassnames
-verbose
-keepattributes *Annotation*

# For native methods, see http://proguard.sourceforge.net/manual/examples.html#native
-keepclasseswithmembernames class * {
    native &lt;methods&gt;;
}

# keep setters in Views so that animations can still work.
# see http://proguard.sourceforge.net/manual/examples.html#beans
-keepclassmembers public class * extends android.view.View {
   void set*(***);
   *** get*();
}

# We want to keep methods in Activity that could be used in the XML attribute onClick
-keepclassmembers class * extends android.app.Activity {
   public void *(android.view.View);
}

# For enumeration classes, see http://proguard.sourceforge.net/manual/examples.html#enumerations
-keepclassmembers enum * {
    public static **[] values();
    public static ** valueOf(java.lang.String);
}

-keep class * implements android.os.Parcelable {
  public static final android.os.Parcelable$Creator *;
}

-keepclassmembers class **.R$* {
    public static &lt;fields&gt;;
}

# Firebase
-keep class com.google.android.gms.** { *; }
-keep class com.google.firebase.** { *; }

# in order to provide the most meaningful crash reports, add the following line:
-keepattributes SourceFile,LineNumberTable
# If you are using custom exceptions, add this line so that custom exception types are skipped during obfuscation:
-keep public class * extends java.lang.Exception

-keep class com.crashlytics.** { *; }
-dontwarn com.crashlytics.**

# Jackson
-keep @com.fasterxml.jackson.annotation.JsonIgnoreProperties class * { *; }
-keep class com.fasterxml.** { *; }
-keep class org.codehaus.** { *; }
-keepnames class com.fasterxml.jackson.** { *; }
-keepclassmembers public final enum com.fasterxml.jackson.annotation.JsonAutoDetect$Visibility {
    public static final com.fasterxml.jackson.annotation.JsonAutoDetect$Visibility *;
}

# General
-keepattributes SourceFile,LineNumberTable,*Annotation*,EnclosingMethod,Signature,Exceptions,InnerClasses

This code is for parcelize plugin
# For enumeration classes, see http://proguard.sourceforge.net/manual/examples.html#enumerations
    -keepclassmembers enum * {
        public static **[] values();
        public static ** valueOf(java.lang.String);
    }
    
    -keep class * implements android.os.Parcelable {
      public static final android.os.Parcelable$Creator *;
    }
    
    -keepclassmembers class **.R$* {
        public static &lt;fields&gt;;
    }

"
"Say I have this abstract class:
package test.one;

public abstract class One {
  
  protected abstract void whatever();

  public void run() {
    whatever();
  }
  
}

And use it like this:
package test.two;

import test.one.One;

public class Three {

  public static void main(String[] args) {
    One one = new One() {
      @Override
      protected void whatever() {
        System.out.println(&quot;Do whatever..&quot;);
      }
    };
    one.whatever();
  }
}

This code fails on compilation which is pretty much expected.
test/two/Three.java:14: error: whatever() has protected access in One
    one.whatever();
       ^
1 error

But the below code compiles successfully which seems surprisingly:
package test.two;

import test.one.One;

public class Two {

  public static void main(String[] args) {
    new One() {
      @Override
      protected void whatever() {
        System.out.println(&quot;Do whatever..&quot;);
      }
    }.whatever();
  }
}

The difference is that in the latter case I'm accessing the method without a named reference. Why does the compiler allow such access?
","
The difference is that in the latter case I'm accessing the method without a named reference. Why does the compiler allow such access?

No, the difference is that in the latter case you're accessing the method on the anonymous class rather than on a reference of type One.
Leaving aside the oddities around protected access, you can see the difference very easily by just creating an anonymous class with a public method:
class Test {
    public static void main(String[] args) {
        // This is fine...
        new Object() {
            public void method() {
                System.out.println(&quot;Called&quot;);
            }
        }.method();
        
        // This is not, because Object doesn't contain a method called &quot;method&quot;.
        Object o = new Object() {
            public void method() {
                System.out.println(&quot;Called&quot;);
            }
        };
        o.method();        
    }
}

As noted in comments, another way to see the same effect is to use var, so that the compile-time type of the variable is the anonymous class.
Even private members within anonymous classes can be accessed within the containing scope, just as if they were normal nested classes.
"
"In Spring Boot 3, they have changed so that trailing slashes, by default, are no longer ignored. For example, if I have a GET resource, /users, and I navigate to /users/ then Spring Boot webflux will now respond with 404.
You can change this by implementing a WebFluxConfigurer and overriding the configurePathMatching method:
@Override
public void configurePathMatching(PathMatchConfigurer configurer) {
     configurer.setUseTrailingSlashMatch();
}

However, setUseTrailingSlashMatch is deprecated, and the docs says to use PathPatternParser.setMatchOptionalTrailingSeparator(boolean) instead. However, I don't understand how/where you actually configure this.
So the question is, how do I set PathPatternParser.setMatchOptionalTrailingSeparator(boolean)?
","As @joe-clay has mentioned in his comment, PathPatternParser.setMatchOptionalTrailingSeparator(boolean) is deprecated as well in favour of explicit redirects. So you have 3 options:

Declare both routes explicitly in the controller handler @GetMapping({&quot;/users&quot;, &quot;/users/&quot;}). The downside is that you need to do this for every controller, but can be used as a stop-gap solution.

Implement org.springframework.web.server.WebFilter interface to explicitly redirect to the desired url. Something along these lines:


@Override
public Mono&lt;Void&gt; filter(ServerWebExchange exchange, WebFilterChain chain) {

    URI originalUri = exchange.getRequest().getURI();

    if (/* check condition for trailing slash using originalUri getPath(), getQuery() etc. */) {
        String originalPath = originalUri.getPath();
        String newPath = originalPath.substring(0, originalPath.length() - 1); // ignore trailing slash
        try {
            URI newUri = new URI(originalUri.getScheme(),
                    originalUri.getUserInfo(),
                    originalUri.getHost(),
                    originalUri.getPort(),
                    newPath,
                    originalUri.getQuery(),
                    originalUri.getFragment());

            ServerHttpResponse response = exchange.getResponse();
            response.setStatusCode(HttpStatus.MOVED_PERMANENTLY);    // optional
            response.getHeaders().setLocation(mutatedUri);

            return Mono.empty();

        } catch (URISyntaxException e) {
            throw new IllegalStateException(e.getMessage(), e);
        }
    }
    return chain.filter(exchange);
}


Explicitly rewrite the incoming url in the proxy (for example using rewrite rules in nginx) to match the expected url.

In options 2 and 3, you may choose to return an HTTP 301 response as well.
"
"I am working on a project where I generate an EC private key using Java and then import it in the browser using JavaScript. The key imports successfully in Chrome, but it fails in Safari.Hereâ€™s my JavaScript code for importing private key:
[Try running this html file in browser]
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
  &lt;title&gt;ECDH Key Pair Generation&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt; 
  &lt;script&gt;

//Utils
function _extractRawKeyMaterial(pem, type) {
  const pemHeader = `-----BEGIN ${type} KEY-----`;
  const pemFooter = `-----END ${type} KEY-----`;

  const endingIndex = pem.indexOf(pemFooter);
  const startingIndex = pem.indexOf(pemHeader) + pemHeader.length;

  const pemContents = pem.substring(startingIndex, endingIndex);
  var return_object = convertBase64StringToArrayBuffer(pemContents.trim());
  return return_object;
}

 const convertBase64StringToArrayBuffer = base64String =&gt; {
  const text = window.atob(base64String);
  return convertStringToArrayBuffer(text);
};

 const convertStringToArrayBuffer = str =&gt; {
  const buf = new ArrayBuffer(str.length);
  const bufView = new Uint8Array(buf);
  for (let i = 0, strLen = str.length; i &lt; strLen; i++) {
    bufView[i] = str.charCodeAt(i);
  }
  return buf;
};


// private key
var privateKeyGenerated = `-----BEGIN PRIVATE KEY-----
ME4CAQAwEAYHKoZIzj0CAQYFK4EEACIENzA1AgEBBDAMvyd7HU0FwJxgs5N87NVw
MPOR60umJXnhPjdtn0O0RHgx2J0sVnvw7B6ue1Wb5uQ=
-----END PRIVATE KEY-----`

// Pass the loaded private key to your function
_loadEccPrivateKey(privateKeyGenerated);

// Code working in chrome but fails in safari with an error : Data provided to an operation does not meet requirements
 async function _loadEccPrivateKey(pemKey) {
  try {
     const rawKey = _extractRawKeyMaterial(pemKey.trim(), &quot;PRIVATE&quot;);

    //console.log(rawKey)
    const key = await window.crypto.subtle.importKey(
      &quot;pkcs8&quot;, // Format for private keys
      rawKey,
      {
        name: &quot;ECDH&quot;,
        namedCurve: &quot;P-384&quot;,
      },
      true,
      [&quot;deriveBits&quot;, &quot;deriveKey&quot;] // Key usages
    );

    console.log('Imported Private Key:', key);
    return key;
  } catch (e) {
    console.error('Error importing private key:', e);
    throw e;
  }
}

&lt;/script&gt; 
&lt;/body&gt;
&lt;/html&gt;

The code works perfectly in Chrome but throws an error in Safari. The error message is
&quot;DATA PROVIDED TO AN OPERATION DOES NOT MEET REQUIREMENTS&quot;
Here is my JAVA CODE for more information:

import org.bouncycastle.jce.provider.BouncyCastleProvider;

import java.io.FileOutputStream;
import java.io.IOException;
import java.security.*;
import java.security.spec.ECGenParameterSpec;
import java.util.Base64;

public class TestApplication {

    private static final String CURVE = &quot;secp384r1&quot;; // P-384 curve

    public static void main(String[] args) {
        try {
            // Add BouncyCastle Provider
            Security.addProvider(new BouncyCastleProvider());

            // Generate EC key pair
            ECGenParameterSpec parameterSpec = new ECGenParameterSpec(CURVE);
            KeyPairGenerator keyPairGenerator = KeyPairGenerator.getInstance(&quot;EC&quot;, &quot;BC&quot;);
            keyPairGenerator.initialize(parameterSpec, new SecureRandom());
            KeyPair keyPair = keyPairGenerator.generateKeyPair();

            // Extract and print private key
            PrivateKey privateKey = keyPair.getPrivate();
            String privateKeyPem = convertToPem(privateKey);
            System.out.println(&quot;Private Key in PEM format:\n&quot; + privateKeyPem);

            // Save the private key in binary format to a file (optional)
            String privateKeyFilePath = &quot;private_key.bin&quot;;
            saveKeyToBinaryFile(privateKey, privateKeyFilePath);

        } catch (Exception e) {
            e.printStackTrace();
        }
    }

    // Convert private key to PEM format
    private static String convertToPem(PrivateKey privateKey) {
        String base64Key = Base64.getEncoder().encodeToString(privateKey.getEncoded());
        return &quot;-----BEGIN PRIVATE KEY-----\n&quot; +
                base64Key +
                &quot;\n-----END PRIVATE KEY-----&quot;;
    }

    // Save the private key in binary format
    private static void saveKeyToBinaryFile(PrivateKey privateKey, String filePath) {
        try (FileOutputStream fos = new FileOutputStream(filePath)) {
            fos.write(privateKey.getEncoded());
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}



If you want to try it yourself, just run this Java POC: https://github.com/ChetanTailor/JavaPrivateKeyPOC
","This is a known Safari and Firefox bug where importKey requires EC keys to include the public component as well as the private.
Here's a working P-384 private key (generated with openssl ecparam -genkey -name prime256v1 -noout and ASCII armor tweaked to match the expected header):
var privateKeyGenerated = `-----BEGIN PRIVATE KEY-----
MIG2AgEAMBAGByqGSM49AgEGBSuBBAAiBIGeMIGbAgEBBDBoZCuF4gA0MozAQFtE
lm+zCPikEs5JeMFyZRVPpXEHYsQQFZc71KYFNdAA0uazYHWhZANiAAQkQ/kYHu/y
F9Ec2QPkQxtqRWKgi8U2ZIqo6SeJfgs/4g7P3EaFgx/T2BAGw1HIrwfO1kiAJi/f
tkdHqte8uf88Oo8vq1YSniBNV8E4kC4VbsrHNrYcBPk0XfyL1B4pJ8M=
-----END PRIVATE KEY-----`

You can compare the ASN.1 parsing of this key:
PrivateKeyInfo SEQUENCE (3 elem)

    version Version INTEGER 0
    privateKeyAlgorithm AlgorithmIdentifier SEQUENCE (2 elem)
        algorithm OBJECT IDENTIFIER 1.2.840.10045.2.1 ecPublicKey (ANSI X9.62 public key type)
        parameters ANY OBJECT IDENTIFIER 1.3.132.0.34 secp384r1 (SECG (Certicom) named elliptic curve)
    privateKey PrivateKey OCTET STRING (158 byte) 30819B020101043068642B85E20034328CC0405B44966FB308F8A412CE4978C172651…
        SEQUENCE (3 elem)
            INTEGER 1
            OCTET STRING (48 byte) 68642B85E20034328CC0405B44966FB308F8A412CE4978C17265154FA5710762C41015…
            [1] (1 elem)
                BIT STRING (776 bit) 0000010000100100010000111111100100011000000111101110111111110010000101…

with the key you provided:
PrivateKeyInfo SEQUENCE (3 elem)

    version Version INTEGER 0
    privateKeyAlgorithm AlgorithmIdentifier SEQUENCE (2 elem)
        algorithm OBJECT IDENTIFIER 1.2.840.10045.2.1 ecPublicKey (ANSI X9.62 public key type)
        parameters ANY OBJECT IDENTIFIER 1.3.132.0.34 secp384r1 (SECG (Certicom) named elliptic curve)
    privateKey PrivateKey OCTET STRING (55 byte) 303502010104300CBF277B1D4D05C09C60B3937CECD57030F391EB4BA62579E13E376D…
        SEQUENCE (2 elem)
            INTEGER 1
            OCTET STRING (48 byte) 0CBF277B1D4D05C09C60B3937CECD57030F391EB4BA62579E13E376D9F43B4447831D8…

Note that the second one is missing an element at the end, representing the public key.

You can fix your Java code by passing the private key through PrivateKeyInfo, which is the ASN.1 structure expected by browsers. Unfortunately BouncyCastle's implementation introduces new, unsupported structures, like a public key identifier, so you must manually re-encode it with only the parts you want.
This way you can create an encoded key that exactly matches the OpenSSL structures:
PrivateKeyInfo originalKeyInfo = PrivateKeyInfo.getInstance(keyPair.getPrivate().getEncoded());

ASN1Sequence oldPrivateKeySequence = DERSequence
        .getInstance(originalKeyInfo.getPrivateKey().getOctets());
DERSequence newPrivateKeySequence = new DERSequence(new ASN1Encodable[] {
        // Version (1).
        oldPrivateKeySequence.getObjectAt(0),

        // Private key bytes.
        oldPrivateKeySequence.getObjectAt(1),

        // Public key algorithm. Accepted by Firefox but not Safari, so must be skipped.
        // oldPrivateKeySequence.getObjectAt(2),

        // Public key bytes, tagged [1].
        oldPrivateKeySequence.getObjectAt(3),
});

// Re-create PrivateKeyInfo with only the structures we want.
ASN1EncodableVector v = new ASN1EncodableVector();

// Version fixed to zero.
v.add(new ASN1Integer(BigIntegers.ZERO));
v.add(originalKeyInfo.getPrivateKeyAlgorithm());
v.add(new DEROctetString(newPrivateKeySequence));

byte[] keyPairEncoded = new DERSequence(v).getEncoded();

Here's the full source code:
import org.bouncycastle.asn1.ASN1Encodable;
import org.bouncycastle.asn1.ASN1EncodableVector;
import org.bouncycastle.asn1.ASN1Integer;
import org.bouncycastle.asn1.ASN1Primitive;
import org.bouncycastle.asn1.ASN1Sequence;
import org.bouncycastle.asn1.ASN1Set;
import org.bouncycastle.asn1.DEROctetString;
import org.bouncycastle.asn1.DERSequence;
import org.bouncycastle.asn1.DERTaggedObject;
import org.bouncycastle.asn1.pkcs.PrivateKeyInfo;
import org.bouncycastle.asn1.x509.AlgorithmIdentifier;
import org.bouncycastle.jce.provider.BouncyCastleProvider;
import org.bouncycastle.util.BigIntegers;

import java.io.FileOutputStream;
import java.io.IOException;
import java.security.*;
import java.security.spec.ECGenParameterSpec;
import java.util.Base64;

public class TestApplication {

    private static final String CURVE = &quot;secp384r1&quot;; // P-384 curve

    public static void main(String[] args) {
        try {
            // Add BouncyCastle Provider
            Security.addProvider(new BouncyCastleProvider());

            // Generate EC key pair
            ECGenParameterSpec parameterSpec = new ECGenParameterSpec(CURVE);
            KeyPairGenerator keyPairGenerator = KeyPairGenerator.getInstance(&quot;EC&quot;, &quot;BC&quot;);
            keyPairGenerator.initialize(parameterSpec, new SecureRandom());
            KeyPair keyPair = keyPairGenerator.generateKeyPair();

            // Encode with Safari-compatible ASN.1 structure.
            byte[] keyPairBytes = encodeKeyPair(keyPair);

            // Extract and print key pair
            String privateKeyPem = convertToPem(keyPairBytes);
            System.out.println(&quot;Private Key in PEM format:\n&quot; + privateKeyPem);

            // Save the key pair in binary format to a file (optional)
            String privateKeyFilePath = &quot;private_key.bin&quot;;
            saveKeyToBinaryFile(keyPairBytes, privateKeyFilePath);

        } catch (Exception e) {
            e.printStackTrace();
        }
    }

    // Convert a KeyPair into ASN.1 encoded PrivateKeyInfo compatible with Safari.
    private static byte[] encodeKeyPair(KeyPair keyPair) throws IOException {
        PrivateKeyInfo originalKeyInfo = PrivateKeyInfo.getInstance(keyPair.getPrivate().getEncoded());

        ASN1Sequence oldPrivateKeySequence = DERSequence
                .getInstance(originalKeyInfo.getPrivateKey().getOctets());
        DERSequence newPrivateKeySequence = new DERSequence(new ASN1Encodable[] {
                // Version (1).
                oldPrivateKeySequence.getObjectAt(0),

                // Private key bytes.
                oldPrivateKeySequence.getObjectAt(1),

                // Public key algorithm. Accepted by Firefox but not Safari, so must be skipped.
                // oldPrivateKeySequence.getObjectAt(2),

                // Public key bytes, tagged [1].
                oldPrivateKeySequence.getObjectAt(3),
        });

        // Re-create PrivateKeyInfo with only the structures we want.
        ASN1EncodableVector v = new ASN1EncodableVector();

        // Version fixed to zero.
        v.add(new ASN1Integer(BigIntegers.ZERO));
        v.add(originalKeyInfo.getPrivateKeyAlgorithm());
        v.add(new DEROctetString(newPrivateKeySequence));

        return new DERSequence(v).getEncoded();
    }

    // Convert private key to PEM format
    private static String convertToPem(byte[] privateKey) {
        String base64Key = Base64.getEncoder().encodeToString(privateKey);
        return &quot;-----BEGIN PRIVATE KEY-----\n&quot; +
                base64Key +
                &quot;\n-----END PRIVATE KEY-----&quot;;
    }

    // Save the private key in binary format
    private static void saveKeyToBinaryFile(byte[] privateKey, String filePath) {
        try (FileOutputStream fos = new FileOutputStream(filePath)) {
            fos.write(privateKey);
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}


Finally, here's an example of the encoded keypair that this code generates:
-----BEGIN PRIVATE KEY-----
MIG2AgEAMBAGByqGSM49AgEGBSuBBAAiBIGeMIGbAgEBBDBzsru70B3wapVJZsFj4hUHxAGO4B5fJypfAvGyKEyRc2ZdjaVWIOd+vfhgfKFIqe6hZANiAAR7f1ZbUKI2lLAgZ4dnHVHGTQ7D9E2yMxwT5gYiGKdc8+AHGBzoYauI4YTOMVBYHwNrqYT1oO0ruH2sI53U+iy1KnbUAPAP9z0lHi8HONJZ8D+FbKTQa5LWihLTJLihFJw=
-----END PRIVATE KEY-----

You can see how it's parsed with the same structure as the first OpenSSL key:
PrivateKeyInfo SEQUENCE (3 elem)

    version Version INTEGER 0
    privateKeyAlgorithm AlgorithmIdentifier SEQUENCE (2 elem)
        algorithm OBJECT IDENTIFIER 1.2.840.10045.2.1 ecPublicKey (ANSI X9.62 public key type)
        parameters ANY OBJECT IDENTIFIER 1.3.132.0.34 secp384r1 (SECG (Certicom) named elliptic curve)
    privateKey PrivateKey OCTET STRING (158 byte) 30819B020101043073B2BBBBD01DF06A954966C163E21507C4018EE01E5F272A5F02F…
        SEQUENCE (3 elem)
            INTEGER 1
            OCTET STRING (48 byte) 73B2BBBBD01DF06A954966C163E21507C4018EE01E5F272A5F02F1B2284C9173665D8D…
            [1] (1 elem)
                BIT STRING (776 bit) 0000010001111011011111110101011001011011010100001010001000110110100101…

"
"I struggle to find resources on this, and yet, so many of my classes are running into this error when I compile my code on the latest Java (21).
Here is a code example.
public class ThisEscapeExample
{

        public Object o;

        public ThisEscapeExample()
        {

                this.overridableMethod();

        }

        public void overridableMethod()
        {

                this.o = new Object();

        }

}

And here is my compilation command.
javac -Xlint:all ThisEscapeExample.java
ThisEscapeExample.java:9: warning: [this-escape] possible 'this' escape before subclass is fully initialized
                this.overridableMethod();
                                      ^
1 warning

","Here is the JDK Bug System entry that introduces this new warning - https://bugs.openjdk.org/browse/JDK-8299995
Long story short, the this-escape warning is to warn you when a subclass may be able to @Override a method that is also called in the superclass' constructor.
This is dangerous because overriding a method that is used in the constructor allows subclass' to unintentionally introduce a bug during a subclass' initialization. What if that method depends on state that has not yet been created because we are still in the super constructor? After all, you cannot do anything in the subclass' constructor before calling the super constructor (for now).
There are a few ways to remedy this.

Only use methods in the constructor that cannot be overridden.

static methods.

final methods.

MUST BE IN THE SAME CLASS AS YOUR CONSTRUCTOR, YOU CANNOT USE PARENT FINAL INSTANCE METHODS IN THE CHILD CLASS' CONSTRUCTOR


private methods.



Make the class itself final.

Don't pass in/use this to begin with - instead, pass in the particular component of this that you needed.

Basically, quit being lazy and be explicit with what you need. Don't just pass in your God object -- pass in only the specific attributes you need.



Please note - these rules apply recursively. Meaning, when you call a method in the constructor, not only does that method have to be &quot;not-overridable&quot;, but the methods that that method passes this into must ALSO match one of the rules above. If your top-level method is not overridable, but one of the methods inside of it is, and that method has this in its scope, then you will receive a this-escape error upon compilation. Here is an example.
import javax.swing.*;

public class GUI
{

   private final JFrame frame;

   public GUI()
   {
   
      this.frame = new JFrame();
   
      this.frame.add(this.createBottomPanel());
   
   }

   //final! Does that mean we are safe?
   final JPanel createBottomPanel()
   {
   
      final JButton save = new JButton();
   
      save
         .addActionListener
         (
            /*
             * No. We get the warning here at the start of this lambda.
             * The reason is because we have given this lambda the
             * ability to call this.toString(), and we don't know when
             * it will do that. Maybe now, maybe later. But if it does
             * it now, then we could end up doing things before the
             * object is fully created. And if that were to happen, then
             * that would be a this-escape. So, the warning occurs here,
             * to let you know that it is possible.
             */
            actionEvent -&gt;
            {
           
               this.toString();
           
            }
           
         )
         ;
   
      return null;
   
   }

}

This question also has some useful examples, in case mine above does not make much sense -- Why does &#39;this-escape&#39; warning trigger when calling final methods from the superclass
Now, if none of the solutions above are an option for you, consider the tactic of lazy loading your data. Lazy loading is when you load your data only as needed -- meaning, NOT in your constructor. For example, if your class needs a database connection, don't make the connection happen in the constructor, do it in the getter call. Like this.
public class SomeClass
{

    private DbConnection connection = null;

    //More fields here.

    public SomeClass()
    {

        //Don't set the db connection here.

    }

    public DbConnection getConnection()
    {

        if (this.connection == null)
        {

            this.connection = createAConnection(this);

        }

        return this.connection;

    }

}


And finally, if none of this works, or there is just some entirely unescapable situation, there are 2 very hacky, undesirable ways to just silence the error. You really should NOT depend on this, but if you are CERTAIN that it can't hurt you, here are 2 ways to silence it.

Supress the warning using @SuppressWarnings(&quot;this-escape&quot;).

But please note, if you do this, then your class will not be allowed to become a Value class. Value classes are going to give you a MASSIVE performance increase. But in order to turn your class into a value class, you have to have no this-escape. That warning turns into an error for value classes, so if you have any this-escape in your value class, then your value class will not compile.


Turn off ALL reports of a &quot;this-escape&quot; warning! DANGEROUS! DO NOT DO UNLESS ABSOLUTELY NECESSARY!

This is the nuclear option. The big red button. Please self-reflect before doing this. Doing this will turn off ALL reports of a this-escape for ALL SOURCE CODE THAT YOU ARE COMPILING. So that means that you should really only use this if a whole bunch of classes are getting this warning, and you know for certain that none of them are a problem (or that, any problems that exist, you are willing to pay the consequences for them). If that's true, then you can type in -Xlint:-this-escape as one of the command line arguments for your compiler command, and that will turn off this warning entirely for all code that you are compiling. I've said enough about how perilous this is, so I won't repeat myself. I will only add that if you decide to do this, make sure you only do it on the specific files/projects/workspaces you want, and don't leave this setting on for others. You can look up your IDE's specific settings to figure out how to set this on the PROJECT-SPECIFIC level and not the ALL-PROJECTS level.



"
"Good morning.
I have been fighting with this issue for the past two days so I decided to post a question about it.
Basically I have a Spring Boot project which executes basic CRUD operations through a React JS front-end.
Everything seemed to work fine until I added Spring Security to the project. Since then whenever I make a request (using axios) from the front-end I get the following error:
Access to XMLHttpRequest at 'http://localhost:8080/calciatore/list' from origin 'http://localhost:3000' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource.

Before implementing Spring Security everything worked perfectly just using @CrossOrigin(origins = &quot;*&quot;) in my back-end controllers, but now I always get that error even if the URL is configured not to be protected through login by Spring Security.
In the meanwhile, I have no problems making any request (POST for login or GET for data fetching) from Postman.
I tried looking for a solution all around the internet but still didn't find one.
If you need me to show a portion of code just ask.
Thanks in advance.
","Try using the global CORS config as shown in below code to allow CORS for all endpoints.
import org.springframework.context.annotation.Bean;
import org.springframework.stereotype.Component;
import org.springframework.web.cors.CorsConfiguration;
import org.springframework.web.servlet.config.annotation.CorsRegistry;
import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;

@Component
public class CorsConfig {

    @Bean
    public WebMvcConfigurer corsConfigurer() {

        return new WebMvcConfigurer() {
            @Override
            public void addCorsMappings(CorsRegistry registry) {
                registry
                        .addMapping(&quot;/**&quot;)
                        .allowedMethods(CorsConfiguration.ALL)
                        .allowedHeaders(CorsConfiguration.ALL)
                        .allowedOriginPatterns(CorsConfiguration.ALL);
            }
        };
    }
}

Since spring boot 2.4 you are supposed to use allowedOriginPatterns instead of allowedOrigins. Also you cannot use wildcard '*' along with credentials : true
"
"I try to get all spans created in the following chain associated to the same trace context/traceId by context propagation:
service1 -&gt; aws sqs queue -&gt; service2
Auto. context propagation is not working with aws sqs and aws sdk v2 atm (https://github.com/open-telemetry/opentelemetry-java-instrumentation/issues/3684), even though the AwsTraceHeader is actually set in the sqs message, I have to take care for it explicitly by

service1: Writing traceId in sqs message user attribute
traceId=Span.current().getSpanContext().getTraceId()
service2: Reading traceId from sqs message user attribute traceId and overwriting current span.traceId / essentially creating Context of service1

However, it is now unclear how to actually overwrite span.traceId in the span that service2 created which is confusing because for example with Golang it seems to be straightforward: How to create opentelemetry span from a string traceid
I see only getters e.g. Span.current().getSpanContext().getTraceId()
but no setters or builder methods.
Update:
Even by creating a new span and making it current (not sure if this goes in the right direction)  the tracer.spanBuilder does no offer setters for traceId AFAIU)
@Inject
io.opentelemetry.api.trace.Tracer tracer;

Span consumeMessageSpan = tracer.spanBuilder(&quot;consumeMessage&quot;).startSpan();

consumeMessage.makeCurrent();

Update 2
This snippet from otel official docs looks promising

To link spans from remote processes, it is sufficient to set
theÂ Remote ContextÂ as parent.

Span childRemoteParent = tracer.spanBuilder(&quot;Child&quot;).setParent(remoteContext).startSpan(); 

However, also no examples or ideas how to create remoteContext and setting traceId to the one extracted from the sqs message
Any hints how to do that?
","I've done the following for a child JVM
(that is running using the OTel auto-instrumentation agent):
    public static void main(String[] args) {
        Span span = createSpanLinkedToParent();
        try (Scope scope = span.makeCurrent()) {
            // do stuff
        } finally {
            span.end();
        }
    }

    private static Span createSpanLinkedToParent() {
        // Fetch the trace and span IDs from wherever you've stored them
        String traceIdHex = System.getProperty(&quot;otel.traceid&quot;);
        String spanIdHex = System.getProperty(&quot;otel.spanid&quot;);

        SpanContext remoteContext = SpanContext.createFromRemoteParent(
                traceIdHex,
                spanIdHex,
                TraceFlags.getSampled(),
                TraceState.getDefault());

        return GlobalOpenTelemetry.getTracer(&quot;&quot;)
                .spanBuilder(&quot;root span name&quot;)
                .setParent(Context.current().with(Span.wrap(remoteContext)))
                .startSpan();
    }

The next improvement I plan to make it to serialise the flags and state, perhaps using code here in Context Propagation https://opentelemetry.io/docs/instrumentation/java/manual/#context-propagation but the above works for now.
"
"I have a list of integers as input, the order of items in input is not important.
I need to form a new list having size n with the below features.
Here i, j represents the index position of the output list such that i &lt; j &lt; n
Items from 0 to i should be in increasing order strictly

Items from i to j should be in decreasing order strictly

Items from j to n should be in increasing order strictly

The new list must satisfy the above properties, and it need not have all the elements from the original input list.
Example 1:
input  [2, 1, 3, 3, 1, 2, 1, 2, 3]
valid output sequence with max selected items is [1,2,3,2,1,2,3]
size of this output sequence is 7, so return the value 7

Explanation:
increasing from position 0 to 2 =&gt; [1,2,3]
decreasing from position 2 to 4 =&gt; [3,2,1]
again increasing from position 4 to last index =&gt; [1,2,3]

Example 2:
input  [5, 5, 2, 1, 3, 4, 5]
valid output sequence with max selected items is [1, 3, 5, 4, 2, 5]
size of this output sequence is 6, so return the value 6

Explanation:
increasing from position 0 to 2 =&gt; [1,3,5]
decreasing from position 2 to 4 =&gt; [5,4,2]
again increasing from position 4 to last index =&gt; [2,5]

Example 3:
input  [1, 3, 5, 4, 2, 6, 8, 7, 9]

Output: 9 

Example 4:
input = [1,100]

for this input we can get the updated sequence as [100, 1]

a) increasing part = [100], here i = 0
b) decreasing part = [100, 1], here i=0, j=1
c) increasing part = [1], here j to end, j = 1

Observations: The last item in increasing part is same as first item of decreasing part in above discussion (i.e (a) and (b) groups), similarly the last item of decreasing part is same as first item of increasing part (i.e. (b) and (c) groups)
Constraints:
2 &lt;= input size &lt;= 105
1 &lt;= input element &lt;= 109
input contains at least 2 unique elements.
The program should return the size of the output sequence.
I tried to solve this using a TreeMap:
public static int solve(List&lt;Integer&gt; list) {
        int n = list.size();
        TreeMap&lt;Integer, Integer&gt; map = new TreeMap&lt;&gt;();
        int min = Integer.MAX_VALUE;
        for(int e : list) {
            min = Math.min(min, e);
            map.put(e, map.getOrDefault(e, 0)+1);
        }
        int result = 1;
        map.put(min, map.getOrDefault(min,0)-1);
        if(map.get(min) &lt;=0) map.remove(min);
        while(true) {
            Integer key = map.higherKey(min);
            if(key == null) break;
            map.put(key, map.getOrDefault(key,0)-1);
            if(map.get(key) &lt;=0) map.remove(key);
            min = key;
            result++;
        }
        int max = min;
        while(true) {
            Integer key = map.lowerKey(max);
            if(key == null) break;
            map.put(key, map.getOrDefault(key,0)-1);
            if(map.get(key) &lt;=0) map.remove(key);
            max = key;
            result++;
        }
        
        min = max;
        while(true) {
            Integer key = map.higherKey(min);
            if(key == null) break;
            map.remove(key);
            min = key;
            result++;
        }
        return result;
    }

    public static void main(String[] args) {
        System.out.println(solve(List.of(1, 3, 5, 4, 2, 6, 8, 7, 9))); // Expected output: 9

        System.out.println(solve(List.of(5, 5, 2, 1, 3, 4, 5))); // Expected output: 6

        System.out.println(solve(List.of(1, 100))); // Expected output: 2
        
        System.out.println(solve(List.of(2, 1, 3, 3, 1, 2, 1, 2, 3))); // Expected output: 7
    }

The code fails for input 5, 5, 2, 1, 3, 4, 5, it returns 5 as output instead of 6. This is due to my generated array becoming [1,2,3,4,5] with 5 items also not following increasing-decreasing-increasing pattern
So I am using wrong approach to solve this problem, what is the correct approach to solve this problem.
","Observations
We need at least two distinct values to have a solution -- if this is not the case we cannot satisfy the constraints, namely that &quot;Items from i to j should be in decreasing order strictly&quot; and &quot;i &lt; j&quot;.
The maximum value is a good value to use at index 𝑖, and the minimum value at index 𝑗. All other distinct values can be used once between those two indices, forming the middle, descending section. If there are copies of these &quot;other&quot; values, they can be used once more in the first, ascending section, and again in the third, ascending section, meaning that we can use up to three occurrences of the same value. This is not so for the minimum and maximum value. The maximum (it index 𝑖) can only be used again as the ending value of the third section, while the minimum (at index 𝑗) can only be used again as the starting value of the first section, meaning those two values can be used up to twice only.
It is not needed to actually build the new list as we only need to return the size of that list.
Algorithm
With these observations we get to this algorithm:

Count all occurrences (frequencies) of each distinct value.
Identify the minimum value: it can be used at most twice in  result.
Similarly for the maximum value.
All other values can be used at most 3 times.
Sum all the frequencies taking those limits into account.

Suggested code
    public static int solve(List&lt;Integer&gt; list) {
        Map.Entry&lt;Integer, Integer&gt; min, max;

        // Get all frequencies
        Map&lt;Integer, Integer&gt; counts = new HashMap&lt;&gt;();
        list.forEach(s -&gt; counts.merge(s, 1, Math::addExact));

        if (counts.size() &lt; 2) return 0; // No solution
        
        // Get minimum and its frequency, and use at most 2 copies
        min = Collections.min(counts.entrySet(), Map.Entry.comparingByKey());
        counts.remove(min.getKey());
        int result = Math.min(2, min.getValue());

        // Similar approach for the maximum:
        max = Collections.max(counts.entrySet(), Map.Entry.comparingByKey());
        counts.remove(max.getKey());
        result += Math.min(2, max.getValue());

        // All other values can be used at most 3 times
        for (int count : counts.values()) {
            result += Math.min(3, count);
        }        
        return result;
    }

Time Complexity
I've not used a (sorted) TreeMap here, as processing all input values into a TreeMap would lead to a O(𝑛log𝑛) complexity. Using a HashMap we can achieve a linear time complexity for this process.
Finding both minimum and maximum again represents O(𝑛) time complexity (in the worst case where all 𝑛 input values are distinct), and also the final loop to accumulate the result has a linear time complexity.
This brings the total time complexity to O(𝑛).
NB: in practice the overhead of the repeated iteration over the values may make the benefit over using a sorted container only apparent for very large inputs.
"
"I use Spring Boot v2.7.0, installed from &quot;start.spring.io&quot; and from there I installed Thymeleaf, and as I searched in the parent-pom I found out that:
thymeleaf-spring5 (v3.0.15.RELEASE), thymeleaf-extras-java8time (v3.0.4.RELEASE)
Lately, I needed to apply the pattern &lt;form th:method=&quot;put/delete&quot;.../&gt;.
After googling in verious places, I found the solution, which was reffered in the book as well:
&quot;Taming Thymeleaf Practical Guide to building a web application with Spring Boot and Thymeleaf - Wim Deblauwe&quot;
which is the top/excellent books of Thymeleaf, and from which I learn Thymeleaf.
Acoording to these, I did:
Step 1:
Added this property in application.properties:
spring.mvc.hiddenmethod.filter.enabled=true

and I tried it in the application.yaml (as a 2nd solution, because the previous did not work), like this way:
spring:
  mvc:
    hiddenmethod:
      filter:
        enabled: true

Step 2:
I used:
&lt;form th:method=&quot;put&quot;.../&gt;
&lt;form th:method=&quot;delete&quot;.../&gt;


Step 3:
Finally I used the: &quot;@PutMapping, @DeleteMapping&quot; in my controller handler methods.
The result was the error message:
There was an unexpected error (type=Method Not Allowed, status=405).
Request method 'POST' not supported
org.springframework.web.HttpRequestMethodNotSupportedException: Request method 'POST' not supported
    at org.springframework.web.servlet.mvc.method.RequestMappingInfoHandlerMapping.handleNoMatch(RequestMappingInfoHandlerMapping.java:253)
    at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.lookupHandlerMethod(AbstractHandlerMethodMapping.java:442)

After googling I found this solution, to add the needed bean by myself with the following way, which DID WORKED:
@Bean
public FilterRegistrationBean&lt;HiddenHttpMethodFilter&gt; hiddenHttpMethodFilter() {
    FilterRegistrationBean&lt;HiddenHttpMethodFilter&gt; filterRegistrationBean = new FilterRegistrationBean&lt;&gt;(new HiddenHttpMethodFilter());
    filterRegistrationBean.setUrlPatterns(Arrays.asList(&quot;/*&quot;));
    return filterRegistrationBean;
}

I wonder why this configuration &quot;spring.mvc.hiddenmethod.filter.enabled=true&quot;, does not add the needed bean in my case, and I have to add it by myself.
Anyone can help me on this, please?
Thanks a lot in advance
","I just did a test myself and this worked fine.

Create a new project on start.spring.io selecting Spring Boot 2.7.1 with dependencies Spring Web and Thymeleaf
Create a controller:

import org.springframework.stereotype.Controller;
import org.springframework.ui.Model;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.ModelAttribute;
import org.springframework.web.bind.annotation.PutMapping;
import org.springframework.web.bind.annotation.RequestMapping;

@RequestMapping
@Controller
public class TestController {
    @GetMapping
    public String index(Model model) {
        model.addAttribute(&quot;formData&quot;, new TestFormData());
        return &quot;index&quot;;
    }

    @PutMapping
    public String doPut(@ModelAttribute(&quot;formData&quot;) TestFormData formData) {
        System.out.println(&quot;formData.getSomeString() = &quot; + formData.getSomeString());
        return &quot;redirect:/&quot;;
    }
}

With this form data class:
public class TestFormData {
    private String someString;

    public String getSomeString() {
        return someString;
    }

    public void setSomeString(String someString) {
        this.someString = someString;
    }
}


Create an index.html file in src/main/resources/templates:

&lt;!DOCTYPE html&gt;
&lt;html lang=&quot;en&quot;
      xmlns:th=&quot;http://www.thymeleaf.org&quot;&gt;
&lt;head&gt;
    &lt;meta charset=&quot;UTF-8&quot;&gt;
    &lt;title&gt;Title&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;form th:action=&quot;@{/}&quot; th:method=&quot;put&quot; th:object=&quot;${formData}&quot;&gt;
    &lt;input th:field=&quot;*{someString}&quot;&gt;
    &lt;button&gt;Save&lt;/button&gt;
&lt;/form&gt;
&lt;/body&gt;
&lt;/html&gt;


Update application.properties to contain:

spring.mvc.hiddenmethod.filter.enabled=true


Start the application and go to http://localhost:8080 and enter something in the input field. When pressing save, it is printed to the console, showing that the @PutMapping works.

"
"I have just upgraded our Spring Boot applications to Java 21. As a part of that, I have also done changes to use virtual threads. Both when serving API requests and when doing async operations internally using executors.
For one use-case, it seems like an Executor powered by virtual threads is performing worse than a ForkJoinPool powered by OS threads. This use-case is setting some MDC values and calling an external system through HTTP.
This is my pseudo-ish-code:
List&lt;...&gt; ... = executorService.submit(
                () -&gt; IntStream.rangeClosed(-from, to)
                        .mapToObj(i -&gt; ...)
                        .parallel()
                        .map(... -&gt; {
                            try {
                                service.setSomeThreadLocalString(...);
                                MDC.put(..., ...);
                                MDC.put(..., ...);

                                return service.call(...);
                            } finally {
                                service.removeSomeThreadLocalString(...);
                                MDC.remove(...);
                                MDC.remove(...);
                            }
                        })
                        .toList())
        .get();

Where ExecutorService is either:

new ForkJoinPool(30)
Executors.newVirtualThreadPerTaskExecutor()

It looks like option 1 is performing a lot better than 2. Sometimes it is 100% faster than option 2. I have done this test in a Java 21 environment. I am testing with 10 parallel executions. Where option 1 takes 800-1000ms normally, option 2 takes 1500-2000 ms.
If it makes any difference, have this property enabled in Spring Boot:
spring:
  threads:
    virtual:
      enabled: true

Any ideas why this is happening?
","You are assuming that submitting a parallel stream operation as a job to another executor service will make the Stream implementation use that executor service. This is not the case.
There is an undocumented trick to make a parallel stream operation use a different Fork/Join pool by initiating it from a worker thread of that pool. But the executor service producing virtual threads is not a Fork/Join pool.
So when you initiate the parallel stream operation from a virtual thread, the parallel stream will use the common pool for the operation. In other words, you are still using platform threads except for the one initiating virtual thread, as the Stream implementation also performs work in the caller thread.
So when I use the following program
public class ParallelStreamInsideVirtualThread {
    public static void main(String[] args) throws Exception {
        var executorService = Executors.newVirtualThreadPerTaskExecutor();
        var job = executorService.submit(
            () -&gt; {
              Thread init = Thread.currentThread();
              return IntStream.rangeClosed(0, 10).parallel()
                 .peek(x -&gt; printThread(init))
                 .mapToObj(String::valueOf)
                 .toList();
            });
        job.get();
    }
  
    static void printThread(Thread initial) {
        Thread t = Thread.currentThread();
        System.out.println((t.isVirtual()? &quot;Virtual  &quot;: &quot;Platform &quot;)
            + (t == initial? &quot;(initiator)&quot;: t.getName()));
    }
}

it will print something like
Virtual  (initiator)
Virtual  (initiator)
Platform ForkJoinPool.commonPool-worker-1
Platform ForkJoinPool.commonPool-worker-3
Platform ForkJoinPool.commonPool-worker-2
Platform ForkJoinPool.commonPool-worker-4
Virtual  (initiator)
Platform ForkJoinPool.commonPool-worker-1
Platform ForkJoinPool.commonPool-worker-3
Platform ForkJoinPool.commonPool-worker-5
Platform ForkJoinPool.commonPool-worker-2

In short, you are not measuring the performance of virtual threads at all.
"
"is it possible to send UTF-8 character from a okhttp3 client ?
For the following string :
String fileName = &quot;3$ MÃ¹ F'RANÃ§Ã©_33902_Country_5_202105&quot;;
String contentDisposition = &quot;attachment;filename=&quot; + &quot;\&quot;&quot; +  fileName + &quot;\&quot;&quot;;

I've tried (for the contentDisposition header) :
Headers headers = new Headers.Builder()
                       .addUnsafeNonAscii(&quot;Content-Disposition&quot;, contentDisposition)
                       .add(&quot;Authorization&quot;, bearer)
                       .add(&quot;Content-type&quot;, &quot;application/octet-stream&quot;)
                       .build();
             Request request = new Request.Builder()
                     .headers(headers)
                     .post(requestBody) 
                     .url(urlAddress)
                     .build();

But the server receive : 3$ MÃƒÂ¹ F'RANÃƒÂ§ÃƒÂ©_33902_Country_5_202105
This request is send to a firm partner, so I have no access to the back-end.
application/octet-stream is needed by the back-end.
Body is created like this :
byte[] data = FileUtils.readFileToByteArray(file);
RequestBody requestBody = RequestBody.create(data);

It works perfectly fine with Postman.
Full MVCE (cannot be complete with file and back-end informations but it crashes before, anyway, so you can just start this exact code and it should throws the error) :
public class App 
{
    public static void main( String[] args ) throws IOException
    {
                OkHttpClient client = new OkHttpClient().newBuilder()
                    .build();
                MediaType mediaType = MediaType.parse(&quot;application/octet-stream&quot;);
                RequestBody body = RequestBody.create(mediaType, &quot;&quot;);
                Request request = new Request.Builder()
                  .url(&quot;xxxx&quot;)
                  .method(&quot;POST&quot;, body)
                  .addHeader(&quot;Content-Type&quot;, &quot;application/octet-stream&quot;)
                  .addHeader(&quot;content-disposition&quot;, &quot;attachment;filename=\&quot;3$ MÃ¹ F'RANÃ§Ã©_33902_Country_5_202105.csv\&quot;&quot;)
                  .addHeader(&quot;Authorization&quot;, &quot;Bearer xxxxx&quot;)
                  .addHeader(&quot;Cookie&quot;, &quot;xxxxxx&quot;)
                  .build();
                Response response = client.newCall(request).execute();
    }
}

Error received : java.lang.IllegalArgumentException: Unexpected char 0xf9 at 25 in content-disposition value: attachment;filename=&quot;3$ MÃ¹ F'RANÃ§Ã©_33902_Country_5_202105.csv&quot;
okhttp version : 5.0.0-alpha.2
Did I miss something ?
Thanks
","The default character set for HTTP headers is ISO-8859-1. There is however RFC 6266, describing how you can encode the file name in a Content-Disposition header. Basically, you specify the character set name and then percent-encode the UTF-8 characters. Instead of fileName=&quot;my-simple-filename&quot; you use a parameter starting with filename*=utf-8'' like
import java.net.URLEncoder;

// ...

String fileName = &quot;3$ Mù F'RANçé_33902_Country_5_202105&quot;;
String contentDisposition = &quot;attachment;filename*=utf-8''&quot; + encodeFileName(fileName);

// ...

private static String encodeFileName(String fileName) throws UnsupportedEncodingException {
  return URLEncoder.encode(fileName, &quot;UTF-8&quot;).replace(&quot;+&quot;, &quot;%20&quot;);
}

Using the URL encoder and then modifying the result for &quot;+&quot; is a cheap trick I found here, if you want to avoid using Guava, Spring's ContentDisposition class or any other library and simply work with JRE classes.

Update: Here is a full MCVE, showing how to send an UTF-8 string both as a POST body and as a content disposition file name. The demo server shows how to decode that header manually - usually HTTP servers should do that automatically.
Maven POM showing used dependencies:
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

  &lt;groupId&gt;org.example&lt;/groupId&gt;
  &lt;artifactId&gt;SO_Java_OkHttp3SendUtf8_70804280&lt;/artifactId&gt;
  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

  &lt;properties&gt;
    &lt;maven.compiler.source&gt;11&lt;/maven.compiler.source&gt;
    &lt;maven.compiler.target&gt;11&lt;/maven.compiler.target&gt;
  &lt;/properties&gt;

  &lt;dependencies&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;com.squareup.okhttp3&lt;/groupId&gt;
      &lt;artifactId&gt;okhttp&lt;/artifactId&gt;
      &lt;version&gt;4.9.3&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.nanohttpd&lt;/groupId&gt;
      &lt;artifactId&gt;nanohttpd&lt;/artifactId&gt;
      &lt;version&gt;2.3.1&lt;/version&gt;
    &lt;/dependency&gt;
  &lt;/dependencies&gt;

&lt;/project&gt;

OkHttp demo client:
import okhttp3.Headers;
import okhttp3.OkHttpClient;
import okhttp3.Request;
import okhttp3.RequestBody;
import okhttp3.Response;

import java.io.IOException;
import java.net.URL;
import java.net.URLEncoder;
import java.nio.charset.StandardCharsets;
import java.util.Objects;

public class Client {
  public static void main(String[] args) throws IOException {
    String fileName = &quot;3$ Mù F'RANçé_33902_Country_5_202105&quot;;
    String contentDisposition = &quot;attachment;filename*=utf-8''&quot; + encodeFileName(fileName);
    RequestBody requestBody = RequestBody.create(fileName.getBytes(StandardCharsets.UTF_8));
    Headers headers = new Headers.Builder()
      .add(&quot;Content-Disposition&quot;, contentDisposition)
      .add(&quot;Content-type&quot;, &quot;application/octet-stream; charset=utf-8&quot;)
      .build();
    Request request = new Request.Builder()
      .headers(headers)
      .post(requestBody)
      .url(new URL(&quot;http://localhost:8080/&quot;))
      .build();
    OkHttpClient client = new OkHttpClient();
    Response response = client.newCall(request).execute();
    System.out.println(Objects.requireNonNull(response.body()).string());
  }

  private static String encodeFileName(String fileName) {
    return URLEncoder.encode(fileName, StandardCharsets.UTF_8).replace(&quot;+&quot;, &quot;%20&quot;);
  }
}

NanoHTTPD demo server:
import fi.iki.elonen.NanoHTTPD;

import java.io.IOException;
import java.net.URLDecoder;
import java.nio.charset.StandardCharsets;
import java.util.HashMap;
import java.util.Map;

public class Server extends NanoHTTPD {

  public Server() throws IOException {
    super(8080);
    start(NanoHTTPD.SOCKET_READ_TIMEOUT, false);
    System.out.println(&quot;\nRunning! Point your browsers to http://localhost:8080/ \n&quot;);
  }

  public static void main(String[] args) throws IOException {
    new Server();
  }

  private static final String UTF_8_FILE_NAME_PREFIX = &quot;;filename*=utf-8''&quot;;
  private static final int UTF_8_FILE_NAME_PREFIX_LENGTH = UTF_8_FILE_NAME_PREFIX.length();

  @Override
  public Response serve(IHTTPSession session) {
    try {
      Map&lt;String, String&gt; files = new HashMap&lt;&gt;();
      session.parseBody(files);
      String postBody = files.get(&quot;postData&quot;);
      String contentDisposition = session.getHeaders().get(&quot;content-disposition&quot;);
      String fileName = decodeFileName(
        contentDisposition.substring(
          contentDisposition.indexOf(UTF_8_FILE_NAME_PREFIX) + UTF_8_FILE_NAME_PREFIX_LENGTH
        )
      );
      System.out.println(&quot;POST body:           &quot; + postBody);
      System.out.println(&quot;Content disposition: &quot; + contentDisposition);
      System.out.println(&quot;UTF-8 file name:     &quot; + fileName);
      return newFixedLengthResponse(postBody + &quot;\n&quot; + fileName);
    }
    catch (IOException | ResponseException e) {
      e.printStackTrace();
      return newFixedLengthResponse(e.toString());
    }
  }

  private static String decodeFileName(String fileName) {
    return URLDecoder.decode(fileName.replace(&quot;%20&quot;, &quot;+&quot;), StandardCharsets.UTF_8);
  }

}

If first you run the server and then the client, you will see this on the server console:
Running! Point your browsers to http://localhost:8080/ 

POST body:           3$ Mù F'RANçé_33902_Country_5_202105
Content disposition: attachment;filename*=utf-8''3%24%20M%C3%B9%20F%27RAN%C3%A7%C3%A9_33902_Country_5_202105
UTF-8 file name:     3$ Mù F'RANçé_33902_Country_5_202105

On the client console, you see:
3$ Mù F'RANçé_33902_Country_5_202105
3$ Mù F'RANçé_33902_Country_5_202105

"
"Today, while working on a project for a college â€œDesign Patternsâ€ course (Java 11 required), I discovered a problem with the access restriction of the access modifier that can be bypassed by declaring var. I know how var is used, it's just a syntactic sugar that leaves the type inference to the compiler.
I can't figure out what type of alias the var is actually here:

is it &quot;Child.InnerChild&quot;? Wouldn't that be a type mismatch?
&quot;InnerParent&quot;? Doesn't this bypass the protected access restrictor?

Here is my simplified code:
public abstract class Parent {
    protected abstract static class InnerParent {
        public InnerParent self() {
            return this;
        }
    }
}

public class Child extends Parent {
    public static class InnerChild extends InnerParent {}
}

import anotherpackage.Child;

/**
 * Compiling with Java 11:
 */
public class Main {
    public static void main(String[] args) {
        // As we expected a compilation error: The returned static type does not match the expected type
        // Child.InnerChild innerChild = new Child.InnerChild().self();

        // As we expected a compilation error: Parent.InnerParent is package visible (protected)
        // Parent.InnerParent innerChild = new Child.InnerChild().self();

        // Why does it compile and run correctly here?
        // var is just syntactic sugar for the compiler type, it should be a Parent.InnerParent alias here,
        // why is var allowed to transgress the protected access restriction?
        var innerChild = new Child.InnerChild().self(); // perhce' non da' errore ? var e' un alias di cosa ?
        System.out.println(innerChild);
        System.out.println(innerChild.getClass().getName());
    }
}

I've also asked ChatGPT, but it's not responding as well as I'd like, and I'm not sure it's correct:

Why var Works

Inferred Type: The inferred type for var innerChild is Parent.InnerParent.
Access Rules: Since the type is inferred and not explicitly written in the code, the compiler doesn't enforce access restrictions for the declared variable.


I found a new problem: why can't I access getClass()?

However it is possible to compile this way.
System.out.println(((Object) innerChild).getClass().getName());
// OUTPUT: com.github.lorenzoyang.anotherpackage.Child$InnerChild

","The answer is: var can represent anything the compiler can reason about as a type, even non-denotable ones: It lets you kick the can down the road. The error you ended up with has existed since before the introduction of var which proves this isn't a problem with var specifically; it's an intended (but somewhat odd) effect that matches the java lang spec.
Let's get into the specifics and break it down. But first:
Reminder – GPT is useless
GPT is a tool that produces an authoritative answer. As in, it'll pretty much keep trying to answer your question until something rolls out that looks authoritative. It can be a complete load of hogwash and in this case, indeed it is. Generally asking GPT for objective answers is a really bad idea: Sure, for simple questions it seems magically amazing, but then - it was a simple question. There are many ways to get the answer to a simple question. It's not what you should optimize for - that would be answers to complex questions. And GPT is very bad at that. In the sense that you can't tell. It'll give you a great sounding answer.
Remember this: Asking a GPT about language esoterica questions is ridiculous. Don't ever do it.
I'm not going to address the GPT answers any further. They are useless (possibly wrong, possibly not wrong. Any hint it gives might be relevant, or not. Hence, useless).
Non-denotable types
It's a language/compiler question so we refer to the source, the JLS:
JLS21 §14.4.1:

If the LocalVariableType is var, then let T be the type of the initializer expression when treated as if it did not appear in an assignment context, and were thus a standalone expression (§15.2). The type of the local variable is the upward projection of T with respect to all synthetic type variables mentioned by T (§4.10.5).

And also note this clarification from the example box:

Note that some variables declared with var cannot be declared with an explicit type, because the type of the variable is not denotable.

That last part is implicit in the actual definition (in that the definition does not at any point state that the type that the compiler infers for var has to be denotable, you must therefore infer that it does not have to be; the note in the example box calls this out).
And it's key to understanding what's going on here. Here's an example of undenotable type use that's easier to follow than what this question has found:
class Example {
  public static void main(String[] args) {
    Object o = new Object() {
      void test() {
        System.out.println(&quot;HELLO!&quot;);
      }
    };
    o.test(); // compiler error.

    var p = new Object() {
      void test() {
        System.out.println(&quot;HELLO!&quot;);
      }
    };

    p.test(); // works!!
  }

  void asAReminder() {
    new Object() {
      void test() {
        System.out.println(&quot;This has always worked&quot;); 
      }
    }.test();
  }
}

Paste the above into a file, compile it - error on the line that says 'compiler error'. Remark it out, compile again, run it - works fine.
Which is bizarre in the sense that there is nothing you could possibly replace var with that makes this work. What's happening is that p's type is inferred to be the type of the anonymous local class and that type does have a test() method. This has always been part of java (well, since 1.2 or so, decades ago at this point), it's just that var now lets you declare local variables with that type. The asAReminder() method shows this; that code 'works' (compiles, and runs as you'd expect) in any java version of the past 2 decades.
Hence, var can denote InnerParent just fine here. It's not 'denotable' - if you literally replace var in your example with InnerParent it would fail to compile because InnerParent isn't accessible, but var merely represents the type of the expression the way the compiler treats it. And the compiler has to support it. After all, this is perfectly legal:
Object o = new Child.InnerChild().self();

The compiler is going to have to deal with the fact that the expression being assigned to o here is InnerParent (the def of self() says so, after all), and will therefore have to figure out in context that this is just fine.
We can prove that too!
Proof that this has nothing to do with var
Try to replace your main definition with the following:
public class Main {
    public static void main(String[] args) {
        new Child.InnerChild().self().getClass();
    }
}

That code will fail to compile, with the exact same error:

Object.getClass() is defined in an inaccessible class or interface


In other words, in java 8 (which doesn't support var at all - var as a feature was added in java 10) we can have the same concept: An expression which is, itself, legal, and whose type isn't accessible in the context we wrote it. It 'works' but the type is extremely poisonous: If you 'touch' it, almost anything you care to do to it will be a compiler error; even invoking final methods inherited directly from the obviously accessible java.lang.Object. Pretty much the only thing you can do with them is treat them as an accessible type first (by casting or assigning to a variable) or pass them to another method verbatim.
var merely lets you kick the can down the road.
"
"I'm trying get more experience with the new Foreign Function &amp; Memory API in Java 22+. The best way how to learn a new API is by using it in a project.
My project's goal is to report on the taskbar the progress of some long-running task. As far as I know, there is no &quot;native&quot; support of this in JavaFX. There are some libraries like
FXTaskbarProgressBar which serves the purpose, but only for Windows OS. And it is using the &quot;old&quot; Java Native Interface (JNI).
After a short research, I found a simple Go library
taskbar. This library inspired me to try porting to Java for JavaFX.
First I used jextract to get java bindings to native library calls:
jextract --output target/generated-sources/jextract -t &quot;taskbar_test.gen&quot; --include-function &quot;XOpenDisplay&quot; --include-function &quot;XChangeProperty&quot; --include-function &quot;XFlush&quot; --include-function &quot;XCloseDisplay&quot; /usr/include/X11/Xlib.h

Then I created a simple application to simulate long running process
where I try to update progress on taskbar by calling method
&quot;XChangeProperty&quot; which I found in documentation of X11:
https://www.x.org/releases/X11R7.7/doc/libX11/libX11/libX11.html#XChangeProperty
Unfortunately this does not work. The program does not crash,
task is running on background, but no update on taskbar is happening.
Here is the code I created:
package taskbar_test;

import com.sun.glass.ui.Window;
import javafx.application.Application;
import javafx.concurrent.Task;
import javafx.scene.Scene;
import javafx.scene.control.Button;
import javafx.scene.layout.VBox;
import javafx.stage.Stage;
import taskbar_test.gen.Xlib_h;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;

public class AppLinuxXlib extends Application {

    @Override
    public void start(Stage primaryStage) {
        Button startButton = new Button(&quot;Start Long Running Task&quot;);

        startButton.setOnAction(event -&gt; {
            final long rawHandle = Window.getWindows().getFirst().getRawHandle();
            System.out.println(rawHandle);
            // Create a long-running task
            Task&lt;Void&gt; longTask = new Task&lt;&gt;() {
                @Override
                protected Void call() throws Exception {
                    System.out.println(&quot;Started&quot;);

                    try (var arena = Arena.ofConfined()) {
                        var NET_WM_XAPP_PROGRESS = arena.allocateFrom(&quot;NET_WM_XAPP_PROGRESS&quot;);
//                        var NET_WM_XAPP_PROGRESS_PULSE = arena.allocateFrom(&quot;NET_WM_XAPP_PROGRESS_PULSE&quot;);

                        MemorySegment x11Session = Xlib_h.XOpenDisplay(MemorySegment.NULL);
                        System.out.println(x11Session);

                        // Prepare the progress data
                        MemorySegment initData = arena.allocateFrom(ValueLayout.JAVA_INT, 0);
                        Xlib_h.XChangeProperty(x11Session,                    // display
                                MemorySegment.ofAddress(rawHandle).address(), // window
                                NET_WM_XAPP_PROGRESS.address(),               // property
                                6,                                            // type
                                32,                                           // format
                                0,                                            // mode PropModeReplace=0
                                initData,                                     // data
                                1);                                           // nelements
                        Xlib_h.XFlush(x11Session);

                        System.out.println(&quot;Countdown started&quot;);

                        // Set the taskbar progress
                        for (int i = 0; i &lt;= 100; i+=20) {
                            // Simulate work
                            Thread.sleep(500);
                            System.out.println(i);
                            MemorySegment progressData = arena.allocateFrom(ValueLayout.JAVA_INT, i);
                            // Update taskbar progress
                            // https://www.x.org/releases/X11R7.7/doc/libX11/libX11/libX11.html#XChangeProperty
                            Xlib_h.XChangeProperty(x11Session,                    // display
                                    MemorySegment.ofAddress(rawHandle).address(), // window
                                    NET_WM_XAPP_PROGRESS.address(),               // property
                                    6,                                            // type
                                    32,                                           // format
                                    0,                                            // mode PropModeReplace=0
                                    progressData,                                 // data
                                    1);                                           // nelements
                            Xlib_h.XFlush(x11Session);
                        }
                        System.out.println(&quot;Finished&quot;);
                        Xlib_h.XCloseDisplay(x11Session);

                    } catch (Throwable ex) {
                        ex.printStackTrace();
                    }
                    return null;
                }
            };

            // Start the task in a new thread
            new Thread(longTask).start();
        });

        VBox vbox = new VBox(10, startButton);
        Scene scene = new Scene(vbox, 300, 200);
        primaryStage.setScene(scene);
        primaryStage.setTitle(&quot;Taskbar Progress Example Linux&quot;);
        primaryStage.show();
    }

    public static void main(String[] args) {
        launch(args);
    }
}

What am I doing wrong and what is the correct direction with how to make the implementation work?
","I wasn’t able to solve this, but I suspect it’s because I am running Gnome 43.9 Classic, and I’m not sure it supports showing progress in taskbar buttons.  But I can provide some advice, at least:

The Window type in X is not a pointer or address.  It is an XID, which is usually a 32-bit integer.
The property argument is an Atom, which is also an XID, not an address or pointer.  An Atom must be obtained by passing the atom’s string name to XInternAtom.
The name of the Atom, from what I can gather, starts with an underscore:  _NET_WM_XAPP_PROGRESS  (I don’t understand why there is so little documentation about this property on the web.)

With those things in mind, I changed your code to this.  (I replaced the taskbar_test.gen.Xlib_h package with explicit calls to java.lang.foreign.)
import com.sun.glass.ui.Window;

import javafx.application.Application;
import javafx.concurrent.Task;
import javafx.scene.Scene;
import javafx.scene.control.Button;
import javafx.scene.layout.VBox;
import javafx.stage.Stage;

import java.lang.invoke.MethodHandle;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.lang.foreign.Linker;
import java.lang.foreign.SymbolLookup;
import java.lang.foreign.FunctionDescriptor;

public class AppLinuxXlib extends Application {
    private static final int None = 0;              // from X.h
    private static final int False = 0;             // from Xlib.h
    private static final int PropModeReplace = 0;   // from X.h

    private static final int XA_CARDINAL = 6;       // from Xatom.h

    private MethodHandle XOpenDisplay;
    private MethodHandle XCloseDisplay;
    private MethodHandle XInternAtom;
    private MethodHandle XChangeProperty;
    private MethodHandle XFlush;

    @Override
    public void init()
    throws Exception {
        Linker linker = Linker.nativeLinker();
        SymbolLookup lookup = SymbolLookup.loaderLookup();

        XOpenDisplay = linker.downcallHandle(
            lookup.findOrThrow(&quot;XOpenDisplay&quot;),
            FunctionDescriptor.of(
                ValueLayout.ADDRESS,                            // returns Display *
                ValueLayout.ADDRESS.withName(&quot;display_name&quot;)    // char *display_name
            ));
        XCloseDisplay = linker.downcallHandle(
            lookup.findOrThrow(&quot;XCloseDisplay&quot;),
            FunctionDescriptor.of(
                ValueLayout.JAVA_INT,                           // returns int
                ValueLayout.ADDRESS.withName(&quot;display&quot;)         // Display *display
            ));
        XInternAtom = linker.downcallHandle(
            lookup.findOrThrow(&quot;XInternAtom&quot;),
            FunctionDescriptor.of(
                ValueLayout.JAVA_INT,                            // returns Atom
                ValueLayout.ADDRESS.withName(&quot;display&quot;),         // Display *display
                ValueLayout.ADDRESS.withName(&quot;atom_name&quot;),       // char *atom_name
                ValueLayout.JAVA_INT.withName(&quot;only_if_exists&quot;)  // Bool only_if_exists
            ));
        XChangeProperty = linker.downcallHandle(
            lookup.findOrThrow(&quot;XChangeProperty&quot;),
            FunctionDescriptor.of(
                ValueLayout.JAVA_INT,                       // returns int
                ValueLayout.ADDRESS.withName(&quot;display&quot;),    // Display *display
                ValueLayout.JAVA_INT.withName(&quot;w&quot;),         // Window w
                ValueLayout.JAVA_INT.withName(&quot;property&quot;),  // Atom property
                ValueLayout.JAVA_INT.withName(&quot;type&quot;),      // Atom type
                ValueLayout.JAVA_INT.withName(&quot;format&quot;),    // int format
                ValueLayout.JAVA_INT.withName(&quot;mode&quot;),      // int mode
                ValueLayout.ADDRESS.withName(&quot;data&quot;),       // char *data
                ValueLayout.JAVA_INT.withName(&quot;nelements&quot;)  // int nelements
            ));
        XFlush = linker.downcallHandle(
            lookup.findOrThrow(&quot;XFlush&quot;),
            FunctionDescriptor.of(
                ValueLayout.JAVA_INT,                   // returns int
                ValueLayout.ADDRESS.withName(&quot;display&quot;) // Display *display
            ));
    }

    private MemorySegment XOpenDisplay(MemorySegment display)
    throws Throwable {
        return (MemorySegment) XOpenDisplay.invokeExact(display);
    }

    private int XCloseDisplay(MemorySegment display)
    throws Throwable {
        return (int) XCloseDisplay.invokeExact(display);
    }

    private int XInternAtom(MemorySegment display,
                            MemorySegment atomName,
                            int onlyIfExists)
    throws Throwable {
        return (int) XInternAtom.invokeExact(display, atomName, onlyIfExists);
    }

    private int XFlush(MemorySegment display)
    throws Throwable {
        return (int) XFlush.invokeExact(display);
    }

    private int XChangeProperty(MemorySegment display,
                                int window,
                                int property,
                                int type,
                                int format,
                                int mode,
                                MemorySegment data,
                                int dataLen)
    throws Throwable {
        return (int) XChangeProperty.invokeExact(display,
            window, property, type, format, mode, data, dataLen);
    }

    @Override
    public void start(Stage primaryStage) {
        Button startButton = new Button(&quot;Start Long Running Task&quot;);

        startButton.setOnAction(event -&gt; {
            final int window = (int) Window.getWindows().getFirst().getNativeWindow();
            System.out.printf(&quot;Window=%#x%n&quot;, window);
            // Create a long-running task
            Task&lt;Void&gt; longTask = new Task&lt;&gt;() {
                @Override
                protected Void call() throws Exception {
                    System.out.println(&quot;Started&quot;);

                    try (var arena = Arena.ofConfined()) {
                        MemorySegment x11Session = XOpenDisplay(MemorySegment.NULL);
                        System.out.println(&quot;display=&quot; + x11Session);

                        var name = arena.allocateFrom(&quot;_NET_WM_XAPP_PROGRESS&quot;);
                        var NET_WM_XAPP_PROGRESS = XInternAtom(x11Session, name, 0);
                        if (NET_WM_XAPP_PROGRESS == None) {
                            throw new RuntimeException(&quot;XInternAtom failed.&quot;);
                        }
//                        var NET_WM_XAPP_PROGRESS_PULSE = arena.allocateFrom(&quot;NET_WM_XAPP_PROGRESS_PULSE&quot;);

                        // Prepare the progress data
                        MemorySegment progressData = arena.allocateFrom(ValueLayout.JAVA_INT, 0);
                        int status = XChangeProperty(
                                x11Session,                                   // display
                                window,                                       // window
                                NET_WM_XAPP_PROGRESS,                         // property
                                XA_CARDINAL,                                  // type
                                32,                                           // format
                                PropModeReplace,                              // mode
                                progressData,                                 // data
                                1);                                           // nelements
                        if (status == False) {
                            throw new RuntimeException(&quot;XChangeProperty returned &quot; + status);
                                    
                        }

                        status = XFlush(x11Session);
                        if (status == False) {
                            throw new RuntimeException(&quot;XFlush returned &quot; + status);
                        }

                        System.out.println(&quot;Countdown started&quot;);

                        // Set the taskbar progress
                        for (int i = 0; i &lt;= 100; i+=20) {
                            // Simulate work
                            Thread.sleep(500);
                            System.out.println(i);
                            progressData.set(ValueLayout.JAVA_INT, 0, i);
                            // Update taskbar progress
                            // https://www.x.org/releases/X11R7.7/doc/libX11/libX11/libX11.html#XChangeProperty
                            status = XChangeProperty(
                                    x11Session,                                   // display
                                    window,                                       // window
                                    NET_WM_XAPP_PROGRESS,                         // property
                                    XA_CARDINAL,                                  // type
                                    32,                                           // format
                                    PropModeReplace,                              // mode
                                    progressData,                                 // data
                                    1);                                           // nelements
                            if (status == False) {
                                throw new RuntimeException(&quot;XChangeProperty returned &quot; + status);
                            }

                            status = XFlush(x11Session);
                            if (status == False) {
                                throw new RuntimeException(&quot;XFlush returned &quot; + status);
                            }
                        }
                        System.out.println(&quot;Finished&quot;);
                        XCloseDisplay(x11Session);

                    } catch (Throwable ex) {
                        ex.printStackTrace();
                    }
                    return null;
                }
            };

            // Start the task in a new thread
            new Thread(longTask).start();
        });

        VBox vbox = new VBox(10, startButton);
        Scene scene = new Scene(vbox, 300, 200);
        primaryStage.setScene(scene);
        primaryStage.setTitle(&quot;Taskbar Progress Example Linux&quot;);
        primaryStage.show();
    }

    public static void main(String[] args) {
        launch(args);
    }
}

This runs, but has no visual effect on my system.  I don’t know if that’s because my version of Gnome doesn’t support it.  I also tried using the xprop program to set the same window property, and that too had no effect.
"
"I am upgrading my project from springboot 2.7.7 to springboot 3.1.1 and java 11 to 17
I have spring-boot-starter-mail included as a dependency and I try to send mail as follows
@Service
public class MailerService {

    @Autowired
    public JavaMailSender javaMailSender;

    public void sendEmail(String toAddress, String subject, String body, boolean error) throws MessagingException {

        MimeMessagePreparator preparator = mimeMessage -&gt; {
            final Address recipient = new InternetAddress(toAddress);
            mimeMessage.setFrom(new InternetAddress(fromAddress));
            mimeMessage.setRecipient(Message.RecipientType.TO, recipient);
            mimeMessage.setSentDate(new Date());
            mimeMessage.setSubject(subject);
            mimeMessage.setText(body);
        };

        // Send the e-mail
        javaMailSender.send(preparator);

... other code...


At javaMailSender.send I get the following exception:
java.lang.IllegalStateException: Not provider of jakarta.mail.util.StreamProvider was found
0 = {StackTraceElement@19049} &quot;org.springframework.mail.javamail.JavaMailSenderImpl.send(JavaMailSenderImpl.java:387)&quot;
1 = {StackTraceElement@19050} &quot;org.springframework.mail.javamail.JavaMailSenderImpl.send(JavaMailSenderImpl.java:366)&quot;
2 = {StackTraceElement@19051} &quot;nz.co.niwa.bjs.service.MailerService.sendEmail(MailerService.java:44)&quot;
3 = {StackTraceElement@19052} &quot;nz.co.niwa.bjs.service.MailerService.sendDataPointEmail(MailerService.java:54)&quot;
4 = {StackTraceElement@19053} &quot;nz.co.niwa.bjs.service.BulkPointDataFetchService.uploadCSVAndSendEmail(BulkPointDataFetchService.java:421)&quot;
5 = {StackTraceElement@19054} &quot;nz.co.niwa.bjs.service.BulkPointDataFetchService.lambda$retrieveForecastData$6(BulkPointDataFetchService.java:351)&quot;
6 = {StackTraceElement@19055} &quot;java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)&quot;
7 = {StackTraceElement@19056} &quot;java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)&quot;
8 = {StackTraceElement@19057} &quot;java.base/java.util.concurrent.CompletableFuture$Completion.exec(CompletableFuture.java:483)&quot;
9 = {StackTraceElement@19058} &quot;java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)&quot;
10 = {StackTraceElement@19059} &quot;java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)&quot;
11 = {StackTraceElement@19060} &quot;java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)&quot;
12 = {StackTraceElement@19061} &quot;java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)&quot;
13 = {StackTraceElement@19062} &quot;java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)&quot;

How can I fix this? Any help is appreciated.
Thank you
EDIT: mvn:dependency tree
[INFO] --- maven-dependency-plugin:3.5.0:tree (default-cli) @ mintaka-bulk-task-service ---
[INFO] nz.co.niwa.bjs:mintaka-bulk-task-service:jar:1.6.0-SNAPSHOT
[INFO] +- org.springframework.boot:spring-boot-starter-actuator:jar:3.1.1:compile
[INFO] |  +- org.springframework.boot:spring-boot-actuator-autoconfigure:jar:3.1.1:compile
[INFO] |  |  \- org.springframework.boot:spring-boot-actuator:jar:3.1.1:compile
[INFO] |  +- io.micrometer:micrometer-observation:jar:1.11.1:compile
[INFO] |  |  \- io.micrometer:micrometer-commons:jar:1.11.1:compile
[INFO] |  \- io.micrometer:micrometer-core:jar:1.11.1:compile
[INFO] |     +- org.hdrhistogram:HdrHistogram:jar:2.1.12:runtime
[INFO] |     \- org.latencyutils:LatencyUtils:jar:2.0.3:runtime
[INFO] +- org.springframework.boot:spring-boot-starter-web:jar:3.1.1:compile
[INFO] |  +- org.springframework.boot:spring-boot-starter-json:jar:3.1.1:compile
[INFO] |  |  \- com.fasterxml.jackson.module:jackson-module-parameter-names:jar:2.15.2:compile
[INFO] |  +- org.springframework.boot:spring-boot-starter-tomcat:jar:3.1.1:compile
[INFO] |  |  +- org.apache.tomcat.embed:tomcat-embed-core:jar:10.1.10:compile
[INFO] |  |  \- org.apache.tomcat.embed:tomcat-embed-websocket:jar:10.1.10:compile
[INFO] |  +- org.springframework:spring-web:jar:6.0.10:compile
[INFO] |  |  \- org.springframework:spring-beans:jar:6.0.10:compile
[INFO] |  \- org.springframework:spring-webmvc:jar:6.0.10:compile
[INFO] |     +- org.springframework:spring-aop:jar:6.0.10:compile
[INFO] |     +- org.springframework:spring-context:jar:6.0.10:compile
[INFO] |     \- org.springframework:spring-expression:jar:6.0.10:compile
[INFO] +- org.springframework.boot:spring-boot-starter-webflux:jar:3.1.1:compile
[INFO] |  +- org.springframework.boot:spring-boot-starter-reactor-netty:jar:3.1.1:compile
[INFO] |  |  \- io.projectreactor.netty:reactor-netty-http:jar:1.1.8:compile
[INFO] |  |     +- io.netty:netty-codec-http:jar:4.1.94.Final:compile
[INFO] |  |     |  +- io.netty:netty-common:jar:4.1.94.Final:compile
[INFO] |  |     |  +- io.netty:netty-buffer:jar:4.1.94.Final:compile
[INFO] |  |     |  +- io.netty:netty-transport:jar:4.1.94.Final:compile
[INFO] |  |     |  +- io.netty:netty-codec:jar:4.1.94.Final:compile
[INFO] |  |     |  \- io.netty:netty-handler:jar:4.1.94.Final:compile
[INFO] |  |     +- io.netty:netty-codec-http2:jar:4.1.94.Final:compile
[INFO] |  |     +- io.netty:netty-resolver-dns:jar:4.1.94.Final:compile
[INFO] |  |     |  +- io.netty:netty-resolver:jar:4.1.94.Final:compile
[INFO] |  |     |  \- io.netty:netty-codec-dns:jar:4.1.94.Final:compile
[INFO] |  |     +- io.netty:netty-resolver-dns-native-macos:jar:osx-x86_64:4.1.94.Final:compile
[INFO] |  |     |  \- io.netty:netty-resolver-dns-classes-macos:jar:4.1.94.Final:compile
[INFO] |  |     +- io.netty:netty-transport-native-epoll:jar:linux-x86_64:4.1.94.Final:compile
[INFO] |  |     |  +- io.netty:netty-transport-native-unix-common:jar:4.1.94.Final:compile
[INFO] |  |     |  \- io.netty:netty-transport-classes-epoll:jar:4.1.94.Final:compile
[INFO] |  |     \- io.projectreactor.netty:reactor-netty-core:jar:1.1.8:compile
[INFO] |  |        \- io.netty:netty-handler-proxy:jar:4.1.94.Final:compile
[INFO] |  |           \- io.netty:netty-codec-socks:jar:4.1.94.Final:compile
[INFO] |  \- org.springframework:spring-webflux:jar:6.0.10:compile
[INFO] |     \- io.projectreactor:reactor-core:jar:3.5.7:compile
[INFO] |        \- org.reactivestreams:reactive-streams:jar:1.0.4:compile
[INFO] +- org.springframework.boot:spring-boot-starter-test:jar:3.1.1:compile
[INFO] |  +- org.springframework.boot:spring-boot-test:jar:3.1.1:compile
[INFO] |  +- org.springframework.boot:spring-boot-test-autoconfigure:jar:3.1.1:compile
[INFO] |  +- com.jayway.jsonpath:json-path:jar:2.8.0:compile
[INFO] |  +- jakarta.xml.bind:jakarta.xml.bind-api:jar:4.0.0:compile
[INFO] |  |  \- jakarta.activation:jakarta.activation-api:jar:2.1.2:compile
[INFO] |  +- net.minidev:json-smart:jar:2.4.11:compile
[INFO] |  |  \- net.minidev:accessors-smart:jar:2.4.11:compile
[INFO] |  |     \- org.ow2.asm:asm:jar:9.3:compile
[INFO] |  +- org.assertj:assertj-core:jar:3.24.2:compile
[INFO] |  +- org.hamcrest:hamcrest:jar:2.2:compile
[INFO] |  +- org.junit.jupiter:junit-jupiter:jar:5.9.3:compile
[INFO] |  |  +- org.junit.jupiter:junit-jupiter-api:jar:5.9.3:compile
[INFO] |  |  |  +- org.opentest4j:opentest4j:jar:1.2.0:compile
[INFO] |  |  |  +- org.junit.platform:junit-platform-commons:jar:1.9.3:compile
[INFO] |  |  |  \- org.apiguardian:apiguardian-api:jar:1.1.2:compile
[INFO] |  |  +- org.junit.jupiter:junit-jupiter-params:jar:5.9.3:compile
[INFO] |  |  \- org.junit.jupiter:junit-jupiter-engine:jar:5.9.3:runtime
[INFO] |  |     \- org.junit.platform:junit-platform-engine:jar:1.9.3:runtime
[INFO] |  +- org.mockito:mockito-core:jar:5.3.1:compile
[INFO] |  |  +- net.bytebuddy:byte-buddy-agent:jar:1.14.5:compile
[INFO] |  |  \- org.objenesis:objenesis:jar:3.3:runtime
[INFO] |  +- org.mockito:mockito-junit-jupiter:jar:5.3.1:compile
[INFO] |  +- org.skyscreamer:jsonassert:jar:1.5.1:compile
[INFO] |  |  \- com.vaadin.external.google:android-json:jar:0.0.20131108.vaadin1:compile
[INFO] |  +- org.springframework:spring-core:jar:6.0.10:compile
[INFO] |  |  \- org.springframework:spring-jcl:jar:6.0.10:compile
[INFO] |  +- org.springframework:spring-test:jar:6.0.10:compile
[INFO] |  \- org.xmlunit:xmlunit-core:jar:2.9.1:compile
[INFO] +- io.awspring.cloud:spring-cloud-aws-starter:jar:3.0.1:compile
[INFO] |  +- io.awspring.cloud:spring-cloud-aws-autoconfigure:jar:3.0.1:compile
[INFO] |  +- io.awspring.cloud:spring-cloud-aws-core:jar:3.0.1:compile
[INFO] |  |  +- software.amazon.awssdk:regions:jar:2.20.63:compile
[INFO] |  |  |  +- software.amazon.awssdk:annotations:jar:2.20.63:compile
[INFO] |  |  |  +- software.amazon.awssdk:utils:jar:2.20.63:compile
[INFO] |  |  |  +- software.amazon.awssdk:sdk-core:jar:2.20.63:compile
[INFO] |  |  |  +- software.amazon.awssdk:profiles:jar:2.20.63:compile
[INFO] |  |  |  \- software.amazon.awssdk:json-utils:jar:2.20.63:compile
[INFO] |  |  |     \- software.amazon.awssdk:third-party-jackson-core:jar:2.20.63:compile
[INFO] |  |  \- software.amazon.awssdk:auth:jar:2.20.63:compile
[INFO] |  |     +- software.amazon.awssdk:http-client-spi:jar:2.20.63:compile
[INFO] |  |     \- software.amazon.eventstream:eventstream:jar:1.0.1:compile
[INFO] |  \- org.slf4j:slf4j-api:jar:2.0.7:compile
[INFO] +- io.awspring.cloud:spring-cloud-aws-starter-sqs:jar:3.0.1:compile
[INFO] |  \- io.awspring.cloud:spring-cloud-aws-sqs:jar:3.0.1:compile
[INFO] |     +- software.amazon.awssdk:sqs:jar:2.20.63:compile
[INFO] |     |  +- software.amazon.awssdk:aws-query-protocol:jar:2.20.63:compile
[INFO] |     |  +- software.amazon.awssdk:protocol-core:jar:2.20.63:compile
[INFO] |     |  +- software.amazon.awssdk:aws-core:jar:2.20.63:compile
[INFO] |     |  +- software.amazon.awssdk:metrics-spi:jar:2.20.63:compile
[INFO] |     |  +- software.amazon.awssdk:endpoints-spi:jar:2.20.63:compile
[INFO] |     |  +- software.amazon.awssdk:apache-client:jar:2.20.63:runtime
[INFO] |     |  \- software.amazon.awssdk:netty-nio-client:jar:2.20.63:runtime
[INFO] |     +- software.amazon.awssdk:arns:jar:2.20.63:compile
[INFO] |     \- org.springframework:spring-messaging:jar:6.0.10:compile
[INFO] +- io.awspring.cloud:spring-cloud-aws-starter-s3:jar:3.0.1:compile
[INFO] |  +- io.awspring.cloud:spring-cloud-aws-s3:jar:3.0.1:compile
[INFO] |  |  \- software.amazon.awssdk:s3:jar:2.20.63:compile
[INFO] |  |     +- software.amazon.awssdk:aws-xml-protocol:jar:2.20.63:compile
[INFO] |  |     \- software.amazon.awssdk:crt-core:jar:2.20.63:compile
[INFO] |  \- io.awspring.cloud:spring-cloud-aws-s3-cross-region-client:jar:3.0.1:compile
[INFO] +- org.springframework.boot:spring-boot-starter-data-jpa:jar:3.1.1:compile
[INFO] |  +- org.springframework.boot:spring-boot-starter-aop:jar:3.1.1:compile
[INFO] |  |  \- org.aspectj:aspectjweaver:jar:1.9.19:compile
[INFO] |  +- org.springframework.boot:spring-boot-starter-jdbc:jar:3.1.1:compile
[INFO] |  |  +- com.zaxxer:HikariCP:jar:5.0.1:compile
[INFO] |  |  \- org.springframework:spring-jdbc:jar:6.0.10:compile
[INFO] |  +- org.springframework.data:spring-data-jpa:jar:3.1.1:compile
[INFO] |  |  +- org.springframework.data:spring-data-commons:jar:3.1.1:compile
[INFO] |  |  +- org.springframework:spring-orm:jar:6.0.10:compile
[INFO] |  |  \- org.springframework:spring-tx:jar:6.0.10:compile
[INFO] |  \- org.springframework:spring-aspects:jar:6.0.10:compile
[INFO] +- org.springframework.boot:spring-boot-starter-mail:jar:3.1.1:compile
[INFO] |  +- org.springframework:spring-context-support:jar:6.0.10:compile
[INFO] |  \- org.eclipse.angus:jakarta.mail:jar:1.1.0:compile
[INFO] |     \- org.eclipse.angus:angus-activation:jar:2.0.1:runtime
[INFO] +- org.springframework.boot:spring-boot-starter-validation:jar:3.1.1:compile
[INFO] |  +- org.apache.tomcat.embed:tomcat-embed-el:jar:10.1.10:compile
[INFO] |  \- org.hibernate.validator:hibernate-validator:jar:8.0.0.Final:compile
[INFO] |     \- jakarta.validation:jakarta.validation-api:jar:3.0.2:compile
[INFO] +- org.springframework.boot:spring-boot-starter:jar:3.1.1:compile
[INFO] |  +- org.springframework.boot:spring-boot:jar:3.1.1:compile
[INFO] |  +- org.springframework.boot:spring-boot-autoconfigure:jar:3.1.1:compile
[INFO] |  +- jakarta.annotation:jakarta.annotation-api:jar:2.1.1:compile
[INFO] |  \- org.yaml:snakeyaml:jar:1.33:compile
[INFO] +- org.springframework.boot:spring-boot-starter-log4j2:jar:3.1.1:compile
[INFO] |  +- org.apache.logging.log4j:log4j-slf4j2-impl:jar:2.20.0:compile
[INFO] |  |  \- org.apache.logging.log4j:log4j-api:jar:2.20.0:compile
[INFO] |  +- org.apache.logging.log4j:log4j-core:jar:2.20.0:compile
[INFO] |  \- org.apache.logging.log4j:log4j-jul:jar:2.20.0:compile
[INFO] +- org.apache.logging.log4j:log4j-layout-template-json:jar:2.20.0:compile
[INFO] +- com.fasterxml.jackson.datatype:jackson-datatype-jdk8:jar:2.15.2:compile
[INFO] |  \- com.fasterxml.jackson.core:jackson-core:jar:2.15.2:compile
[INFO] +- com.fasterxml.jackson.datatype:jackson-datatype-jsr310:jar:2.15.2:compile
[INFO] |  \- com.fasterxml.jackson.core:jackson-annotations:jar:2.15.2:compile
[INFO] +- com.fasterxml.jackson.core:jackson-databind:jar:2.15.2:compile
[INFO] +- com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.15.2:compile
[INFO] +- nz.co.niwa:arcgis:jar:1.3.2:compile
[INFO] |  +- org.apache.httpcomponents:httpclient:jar:4.5.13:compile
[INFO] |  |  +- org.apache.httpcomponents:httpcore:jar:4.4.16:compile
[INFO] |  |  \- commons-logging:commons-logging:jar:1.2:compile
[INFO] |  \- org.apache.commons:commons-lang3:jar:3.12.0:compile
[INFO] +- nz.co.niwa:clidb:jar:1.6.8:compile
[INFO] |  +- junit:junit:jar:4.13.2:compile
[INFO] |  +- com.oracle.jdbc:ojdbc7:jar:12.1.0.2:compile
[INFO] |  |  +- com.oracle.jdbc:xdb6:jar:12.1.0.2:compile
[INFO] |  |  +- com.oracle.jdbc:orai18n:jar:12.1.0.2:compile
[INFO] |  |  +- com.oracle.jdbc:xmlparserv2:jar:12.1.0.2:compile
[INFO] |  |  +- com.oracle.jdbc:oraclepki:jar:12.1.0.2:compile
[INFO] |  |  +- com.oracle.jdbc:osdt_cert:jar:12.1.0.2:compile
[INFO] |  |  \- com.oracle.jdbc:osdt_core:jar:12.1.0.2:compile
[INFO] |  \- com.google.guava:guava:jar:30.0-jre:compile
[INFO] |     +- com.google.guava:failureaccess:jar:1.0.1:compile
[INFO] |     +- com.google.guava:listenablefuture:jar:9999.0-empty-to-avoid-conflict-with-guava:compile
[INFO] |     +- com.google.code.findbugs:jsr305:jar:3.0.2:compile
[INFO] |     +- org.checkerframework:checker-qual:jar:3.5.0:compile
[INFO] |     +- com.google.errorprone:error_prone_annotations:jar:2.3.4:compile
[INFO] |     \- com.google.j2objc:j2objc-annotations:jar:1.3:compile
[INFO] +- nz.co.niwa:aquarius:jar:2.0.8:compile
[INFO] |  +- commons-codec:commons-codec:jar:1.15:compile
[INFO] |  \- org.bouncycastle:bcpkix-jdk15on:jar:1.70:compile
[INFO] |     +- org.bouncycastle:bcprov-jdk15on:jar:1.70:compile
[INFO] |     \- org.bouncycastle:bcutil-jdk15on:jar:1.70:compile
[INFO] +- org.mapstruct:mapstruct:jar:1.4.2.Final:compile
[INFO] +- commons-collections:commons-collections:jar:3.2.2:compile
[INFO] +- commons-io:commons-io:jar:2.11.0:compile
[INFO] +- org.hibernate.orm:hibernate-core:jar:6.2.2.Final:compile
[INFO] |  +- jakarta.persistence:jakarta.persistence-api:jar:3.1.0:compile
[INFO] |  +- jakarta.transaction:jakarta.transaction-api:jar:2.0.1:compile
[INFO] |  +- org.jboss.logging:jboss-logging:jar:3.5.1.Final:compile
[INFO] |  +- org.hibernate.common:hibernate-commons-annotations:jar:6.0.6.Final:runtime
[INFO] |  +- io.smallrye:jandex:jar:3.0.5:runtime
[INFO] |  +- com.fasterxml:classmate:jar:1.5.1:compile
[INFO] |  +- net.bytebuddy:byte-buddy:jar:1.14.5:compile
[INFO] |  +- org.glassfish.jaxb:jaxb-runtime:jar:4.0.3:runtime
[INFO] |  |  \- org.glassfish.jaxb:jaxb-core:jar:4.0.3:runtime
[INFO] |  |     +- org.glassfish.jaxb:txw2:jar:4.0.3:runtime
[INFO] |  |     \- com.sun.istack:istack-commons-runtime:jar:4.1.2:runtime
[INFO] |  +- jakarta.inject:jakarta.inject-api:jar:2.0.1:runtime
[INFO] |  \- org.antlr:antlr4-runtime:jar:4.10.1:compile
[INFO] +- com.oracle.weblogic:ojdbc7:jar:12.1.3-0-0:provided
[INFO] +- javax.measure:unit-api:jar:2.2:compile
[INFO] +- tec.units:unit-ri:jar:1.0.3:compile
[INFO] |  \- tec.uom.lib:uom-lib-common:jar:1.0.2:compile
[INFO] +- com.h2database:h2:jar:2.1.214:test
[INFO] +- com.squareup.okhttp3:okhttp:jar:4.0.1:test
[INFO] |  +- com.squareup.okio:okio:jar:2.2.2:test
[INFO] |  \- org.jetbrains.kotlin:kotlin-stdlib:jar:1.8.22:test
[INFO] |     +- org.jetbrains.kotlin:kotlin-stdlib-common:jar:1.8.22:test
[INFO] |     \- org.jetbrains:annotations:jar:13.0:test
[INFO] \- com.squareup.okhttp3:mockwebserver:jar:4.0.1:test
[INFO] --------------------------------------------------------------

","This issue is fixed in Jakarta Mail 2.1.3 Final Release. Upgrade Jakarta Mail to 2.1.3 and Jakarta Activation to 2.1.3.
Otherwise, use the workaround described in: 665 - Jakarta Mail erroneously assumes that classes can be loaded from Thread#getContextClassLoader which is to manipulate the context class loader during your call.
@Service
public class MailerService {

@Autowired
public JavaMailSender javaMailSender;

public void sendEmail(String toAddress, String subject, String body, boolean error) throws MessagingException {

    MimeMessagePreparator preparator = mimeMessage -&gt; {
        final Address recipient = new InternetAddress(toAddress);
        mimeMessage.setFrom(new InternetAddress(fromAddress));
        mimeMessage.setRecipient(Message.RecipientType.TO, recipient);
        mimeMessage.setSentDate(new Date());
        mimeMessage.setSubject(subject);
        mimeMessage.setText(body);
    };

    // Send the e-mail
    Thread t = Thread.currentThread();
    ClassLoader orig = t.getContextClassLoader();
    t.setContextClassLoader(InternetAddress.class.getClassLoader());
    try {
        javaMailSender.send(preparator);
    } finally {
        t.setContextClassLoader(orig);
    }

I'm assuming that the InternetAddress.class is in the same classloader as the SMTP provider classes.  If that is not the case they you have to try other classloaders.
"
"I am currently working on an OAuth 2.0 login / user management system using Spring Security. Since I'm writing my own authorization server (based upon docs like here) using the spring-security-oauth2-authorization-server module, I am also implementing a user management / admin dashboard.
Naturally, the endpoints for the user management are on the auth server. So the auth server acts as authorization server and (somewhat like a) resource server. To authorize a user to use the admin dashboard, they will need to log-in of course, so first they are redirected to the auth servers /authorize endpoint which then redirects them to the login menu. The authorization code grant flow is then followed normally. But every step is being done on the same server (i.e. authentication and accessing of the protected admin endpoints)!
I am struggeling to configure our auth server to act as an auth server AND resource server because of the following issues:
The authorization server saves the securityContext to the session. The session ID (JSESSIONID) is then left in the users browser as a cookie. The problem is that when the user tries to access a secured endpoint on the auth server such as the {...}/admin/users endpoint, the cookie alone is enough to authorize them to make a request to that endpoint. This means that the entire authorization flow can be circumvented, when a bearer token should be requested first to access the protected endpoint. We would like the secured endpoints to be accessible ONLY with a bearer token and just a bearer token, not a session (or a combination of both).
Here is a shortened version of the current security config:
@Bean
@Order(1)
public CorsFilter corsFilter(CorsConfigurationSource corsConfigurationSource) {
    logger.info(&quot;Creating corsFilter bean&quot;);
    return new CorsFilter(corsConfigurationSource);
}


/**
 * Configures the authorization server endpoints.
 */
@Bean
@Order(2)
public SecurityFilterChain authorizationServerSecurityFilterChain(HttpSecurity http, RegisteredClientRepository clientRepository) throws Exception {

    OAuth2AuthorizationServerConfiguration.applyDefaultSecurity(http);

    http.getConfigurer(OAuth2AuthorizationServerConfigurer.class)
            .registeredClientRepository(clientRepository) // autowired from ClientConfig.java
            .oidc(Customizer.withDefaults());

    http.exceptionHandling((exceptions) -&gt; exceptions
            .defaultAuthenticationEntryPointFor(
                    new LoginUrlAuthenticationEntryPoint(&quot;/login&quot;),
                    new MediaTypeRequestMatcher(MediaType.TEXT_HTML)
            )
    );

    http.oauth2ResourceServer((resourceServer) -&gt; resourceServer
            .jwt(Customizer.withDefaults()));

    http.csrf(AbstractHttpConfigurer::disable);

    return http.build();
}

@Bean
@Order(3)
public SecurityFilterChain defaultSecurityFilterChain(HttpSecurity http) throws Exception {
    http.securityMatcher(new NegatedRequestMatcher(new AntPathRequestMatcher(&quot;/admin/**&quot;)));

    http.authorizeHttpRequests((authorize) -&gt;
            authorize
                    .requestMatchers(new AntPathRequestMatcher(&quot;/register&quot;)).permitAll()
                    .requestMatchers(new AntPathRequestMatcher(&quot;/recover&quot;)).permitAll()
                    .requestMatchers(new AntPathRequestMatcher(&quot;/error/**&quot;)).permitAll()
                    .requestMatchers(new AntPathRequestMatcher(&quot;/css/**&quot;)).permitAll()
                    .requestMatchers(new AntPathRequestMatcher(&quot;/js/**&quot;)).permitAll()
                    .requestMatchers(new AntPathRequestMatcher(&quot;/favicon.ico&quot;)).permitAll()
                    .anyRequest().authenticated());

    http.oauth2ResourceServer((resourceServer) -&gt; resourceServer
            .jwt(Customizer.withDefaults()));

    // set custom login form
    http.formLogin(form -&gt; {
        form.loginPage(&quot;/login&quot;);
        form.permitAll();
    });

    http.logout(conf -&gt; {
        // default logout url
        conf.logoutSuccessHandler(logoutSuccessHandler());
    });

    http.csrf(AbstractHttpConfigurer::disable);
    http.cors(AbstractHttpConfigurer::disable);

    return http.build();
}

@Bean
@Order(4)
public SecurityFilterChain adminResourceFilterChain(HttpSecurity http) throws Exception {

    // handle out custom endpoints in this filter chain
    http.authorizeHttpRequests((authorize) -&gt;
            authorize
                    .requestMatchers(new AntPathRequestMatcher(&quot;/admin/**&quot;)).hasRole(&quot;ADMIN&quot;)
                    .anyRequest().authenticated());

    http.sessionManagement(conf -&gt; conf.sessionCreationPolicy(SessionCreationPolicy.STATELESS));

    http.oauth2ResourceServer((resourceServer) -&gt; resourceServer
            .jwt(Customizer.withDefaults()));

    http.csrf(AbstractHttpConfigurer::disable);
    http.cors(AbstractHttpConfigurer::disable);

    return http.build();
}

How to configure the authorization server so the admin endpoints are secured independently from the security context from the session?
For disclosure purposes, I was debugging a lot and tried basically the whole basics! I also tried a few other things:
According to Spring Security Documentation, it's possible to set the .sessionManagement to STATELESS in the security config. I had hoped that this would fix the issue, but setting this in the resource server filter chain showed to cause another issue: With the session management flag set to STATELESS, the  login isnâ€™t processed properly. After the POST request from the login form, instead of redirecting to the â€œredirect_urlâ€ from the /authorize request, the auth server redirects to â€œ/â€â€¦? I think this is because the auth server module relies on the security context saved to the session for some of its filters.
I also had some issues with CORS and thought this might cause this.Considering that the docs say:

CORS must be processed before Spring Security because the pre-flight request will not contain any cookies (i.e. the JSESSIONID). If the request does not contain any cookies and Spring Security is first, the request will determine the user is not authenticated (since there are no cookies in the request) and reject it.

This would explain that thefront-end Vue.js application doesn't work properly, but not the debugging calls via Postman. Now I deactivated CORS to not deal with those issues.
","The following configuration sets up an extra filter chain for a set of admin endpoints, ordered to be between the auth server endpoints and the rest of the application (user authentication endpoints).
@Configuration
@EnableWebSecurity
public class SecurityConfig {

    @Bean
    @Order(1)
    public SecurityFilterChain authorizationServerSecurityFilterChain(HttpSecurity http)
            throws Exception {
        OAuth2AuthorizationServerConfiguration.applyDefaultSecurity(http);
        http.getConfigurer(OAuth2AuthorizationServerConfigurer.class)
            .oidc(Customizer.withDefaults());
        http
            .exceptionHandling((exceptions) -&gt; exceptions
                .defaultAuthenticationEntryPointFor(
                    new LoginUrlAuthenticationEntryPoint(&quot;/login&quot;),
                    new MediaTypeRequestMatcher(MediaType.TEXT_HTML)
                )
            )
            .oauth2ResourceServer((oauth2) -&gt; oauth2
                .jwt(Customizer.withDefaults())
            );

        return http.build();
    }

    @Bean
    @Order(2)
    public SecurityFilterChain adminSecurityFilterChain(HttpSecurity http)
            throws Exception {
        http
            .securityMatcher(&quot;/admin/**&quot;)
            .authorizeHttpRequests((authorize) -&gt; authorize
                .requestMatchers(HttpMethod.GET).hasAuthority(&quot;SCOPE_admin:read&quot;)
                .anyRequest().hasAuthority(&quot;SCOPE_admin:write&quot;)
            )
            .oauth2ResourceServer((oauth2) -&gt; oauth2.jwt(Customizer.withDefaults()));

        return http.build();
    }

    @Bean
    @Order(3)
    public SecurityFilterChain defaultSecurityFilterChain(HttpSecurity http)
            throws Exception {
        http
            .authorizeHttpRequests((authorize) -&gt; authorize
                .anyRequest().authenticated()
            )
            .formLogin(Customizer.withDefaults);

        return http.build();
    }

}

You can simply create a RegisteredClient that provides admin:read and/or admin:write scopes. You should use a normal OAuth2 flow (such as authorization_code or client_credentials) to obtain access tokens with that scope. You don't want to attempt to customize the adminSecurityFilterChain to handle authenticating a user, that is already handled by the defaultSecurityFilterChain.
If you have a SPA (e.g. Vue.js), it becomes an OAuth2 Client to initiate a redirect for the authorization request (GET /oauth2/authorize), or better still should interact with a BFF using sessions and not worry about access tokens at all. Either way, the admin API is accessible via bearer tokens, though this pattern should used rarely. If building an authorization server product (i.e. keycloak-like) it might make sense, but in a microservices architecture I would suggest moving the admin endpoints to a separate microservice, which just keeps things simpler.
"
"I have a yaml file, for example:
# this is the part I don't care about
config:
  key-1: val-1
other-config:
  lang: en
  year: 1906
# below is the only part I care about
interesting-setup:
  port: 1234
  validation: false
  parts:
    - on-start: backup
      on-stop: say-goodbye

Also I have a POJO class that is suitable for the interesting-setup part
public class InterestingSetup {
    int port;
    boolean validation;
    List&lt;Map&lt;String, String&gt;&gt; parts;
}

I want to load just the interesting-setup part (similarly as @ConfigurationProperties(&quot;interesting-setup&quot;) in Spring)
Currently I'm doing it like this:
Map&lt;String, Object&gt; yamlConfig = yaml.load(yamlFile);            # loading the whole file to Map with Object values
Object interestingObject = yamlConfig.get(&quot;interesting-setup&quot;);  # loading 'interesting-setup' part as an object
Map&lt;String, Object&gt; interestingMap = (Map&lt;String, Object&gt;);      # Casting object to Map&lt;String, Object&gt;
String yamlDumped = yaml.dump(interestingMap);                   # Serialization to String
InterestingSetup finalObject = yaml.load(yamlDumped);            # Getting final object from String

The crucial part is when I have an Object (Map&lt;String, Object&gt;) and want to cast it to my final class.
To do that - I need to serialize it to String, so the process looks like this:
File -&gt; Map&lt;String, Object&gt; -&gt; Object -&gt; Map&lt;String, Object&gt; -&gt; String -&gt; FinalClass
and I'd like to avoid deserialization and again serialization of the same data.
So can I somehow use Yaml to map the Map&lt;String, Object&gt; to another class? I cannot see this in an API?
","AFAIK, the SnakeYAML library doesn't provide a straight way to do that.
You may try tweaking it and defining a container base class with only the fields you required to support. Consider for example the following POJO:
public class Container {
  private InterestingSetup interestingSetup;

  public InterestingSetup getInterestingSetup() {
    return interestingSetup;
  }

  public void setInterestingSetup(InterestingSetup interestingSetup) {
    this.interestingSetup = interestingSetup;
  }

  @Override
  public String toString() {
    return &quot;Container{&quot; +
        &quot;interestingSetup=&quot; + interestingSetup +
        '}';
  }
}

Where, InterestingSetup is your own class:
import java.util.List;
import java.util.Map;

public class InterestingSetup {
  private int port;
  private boolean validation;
  private List&lt;Map&lt;String, String&gt;&gt; parts;

  public int getPort() {
    return port;
  }

  public void setPort(int port) {
    this.port = port;
  }

  public boolean isValidation() {
    return validation;
  }

  public void setValidation(boolean validation) {
    this.validation = validation;
  }

  public List&lt;Map&lt;String, String&gt;&gt; getParts() {
    return parts;
  }

  public void setParts(List&lt;Map&lt;String, String&gt;&gt; parts) {
    this.parts = parts;
  }

  @Override
  public String toString() {
    return &quot;InterestingSetup{&quot; +
        &quot;port=&quot; + port +
        &quot;, validation=&quot; + validation +
        &quot;, parts=&quot; + parts +
        '}';
  }
}

With those beans in place, the following code would work as you required:
import java.io.ByteArrayInputStream;
import java.io.InputStream;
import java.io.UnsupportedEncodingException;
import java.nio.charset.StandardCharsets;

import org.yaml.snakeyaml.TypeDescription;
import org.yaml.snakeyaml.Yaml;
import org.yaml.snakeyaml.constructor.Constructor;
import org.yaml.snakeyaml.representer.Representer;

public class Main {
  public static void main(String... args) throws UnsupportedEncodingException {
    String yamlString =
        &quot;# this is the part I don't care about\n&quot; +
        &quot;config:\n&quot; +
        &quot;  key-1: val-1\n&quot; +
        &quot;other-config:\n&quot; +
        &quot;  lang: en\n&quot; +
        &quot;  year: 1906\n&quot; +
        &quot;# below is the only part I care about\n&quot; +
        &quot;interesting-setup:\n&quot; +
        &quot;  port: 1234\n&quot; +
        &quot;  validation: false\n&quot; +
        &quot;  parts:\n&quot; +
        &quot;    - on-start: backup\n&quot; +
        &quot;      on-stop: say-goodbye&quot;;

    // Skip unknown properties
    Representer representer = new Representer();
    representer.getPropertyUtils().setSkipMissingProperties(true);

    // Define the target object type
    Constructor constructor = new Constructor(Container.class);
    TypeDescription containerTypeDescription = new TypeDescription(Container.class);

    // Define how the interesting-setup property should be processed
    containerTypeDescription.substituteProperty(&quot;interesting-setup&quot;, InterestingSetup.class,
        &quot;getInterestingSetup&quot;, &quot;setInterestingSetup&quot;);
    constructor.addTypeDescription(containerTypeDescription);

    // Finally, parse the YAML
    Yaml yaml = new Yaml(constructor, representer);
    InputStream inputStream = new ByteArrayInputStream(yamlString.getBytes(StandardCharsets.UTF_8));;
    Container container = yaml.load(inputStream);
    System.out.println(container.getInterestingSetup());
  }
}

Perhaps, a more simple solution will consists on using some method that allows you, given a bunch of fields and their corresponding values, to set the appropriate information in the InterestedSetup bean. You can use the Reflection API for that. The populate method in the BeansUtils class from Apache Commons can also be handy as well:
Map&lt;String, Object&gt; yamlConfig = yaml.load(yamlFile); 
Object interestingObject = yamlConfig.get(&quot;interesting-setup&quot;);
Map&lt;String, Object&gt; interestingMap = (Map&lt;String, Object&gt;);
InterestingSetup finalObject = BeanUtils.populate(interestingMap);

As an alternate approach, you can use Jackson to process the YAML file. The code will be similar to this:
ObjectMapper mapper = new ObjectMapper(new YAMLFactory());
// As the helper object Container doesn't contain all the properties
// it is necessary to indicate that fact to the library to avoid
// errors
mapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);
Container container = mapper.readValue(yamlString, Container.class);
System.out.println(container.getInterestingSetup());

The Container class is the same presented above with the addition of a @JsonProperty annotation in order to successfully handle the interesting-setup field:
import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import com.fasterxml.jackson.annotation.JsonProperty;

// Instead of mapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false)
// you can annotate the class with @JsonIgnoreProperties(ignoreUnknown = true)
// to avoid errors related to unknown properties
public class Container {

  @JsonProperty(&quot;interesting-setup&quot;)
  private InterestingSetup interestingSetup;

  public InterestingSetup getInterestingSetup() {
    return interestingSetup;
  }

  public void setInterestingSetup(InterestingSetup interestingSetup) {
    this.interestingSetup = interestingSetup;
  }

  @Override
  public String toString() {
    return &quot;Container{&quot; +
        &quot;interestingSetup=&quot; + interestingSetup +
        '}';
  }
}

The required artifacts can be downloaded from Maven as the following dependency:
&lt;dependency&gt;
    &lt;groupId&gt;com.fasterxml.jackson.dataformat&lt;/groupId&gt;
    &lt;artifactId&gt;jackson-dataformat-yaml&lt;/artifactId&gt;
    &lt;version&gt;2.13.1&lt;/version&gt;
&lt;/dependency&gt;

"
"Is there any way to hide Schema from the Responses and Request body parts? We only need to show Example Value. We use OpenAPI 3.
Dependency:
&lt;dependency&gt;
   &lt;groupId&gt;org.springdoc&lt;/groupId&gt;
   &lt;artifactId&gt;springdoc-openapi-ui&lt;/artifactId&gt;
   &lt;version&gt;1.6.9&lt;/version&gt;
&lt;/dependency&gt;

We can hide listed schema part by using springdoc.swagger-ui.defaultModelsExpandDepth=-1 in application.properties file.

but we want to remove the API schema part from Request Body and Responses.

I tried content= @Content(schema = @Schema(hidden = true )) but it hides whole request body/Response.

Code for Response:
@ApiResponses({
            @ApiResponse(responseCode = &quot;200&quot;, content = @Content(schema = @Schema(name = &quot;Success response&quot;, example = &quot;JsonResponse...&quot;),
                    mediaType = MediaType.APPLICATION_JSON_VALUE)),
            @ApiResponse(responseCode = &quot;400&quot;, description = &quot;BAD REQUEST&quot;, content = @Content(schema = @Schema(hidden = true))) 
    })

Code for Request Body:
@io.swagger.v3.oas.annotations.parameters.RequestBody(
            content= @Content(schema = @Schema(example=&quot;JsonRequestBody...&quot;)))

Can anyone please suggest how we can do that?
UPDATE:
We can hide the Schema part from the response like below.
@ApiResponse(responseCode = IConstants.R_str_200, content = @Content(examples=
@ExampleObject(name=&quot;SUCCESS RESPONSE&quot;,value=&quot;Json response...&quot;),
                mediaType = IConstants.MEDIA_JSONVALUE))


but still can't able to hide Schema part from Request Body.
","I don't think this can be solved using annotations.
You can predefine swagger css to hide the element you want.
To achieve that, first check which version of swagger-ui are you using.
In my case it's 3.25.0.
You can check which version you are using by going to External Libraries folder (if you use InteliJ) and you should find it there ( see picture below)

Then, write controller class like this :
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;

import java.io.BufferedReader;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.nio.charset.StandardCharsets;
import java.util.stream.Collectors;

@RestController
@RequestMapping(path = &quot;/swagger-ui&quot;)
public class SwaggerController {
    @GetMapping(path = &quot;/swagger-ui.css&quot;, produces = &quot;text/css&quot;)
    public String getCss() {
        String orig = toText(getClass().getResourceAsStream(&quot;/META-INF/resources/webjars/swagger-ui/3.25.0/swagger-ui.css&quot;));
        String customCss = &quot;li.tabitem.active {\n&quot; +
                &quot;    display:block !important;\n&quot; +
                &quot;}\n&quot; +
                &quot;li.tabitem {\n&quot; +
                &quot;    display:none !important;\n&quot; +
                &quot;}}&quot;;
        return  orig+customCss;
    }


    static String toText(InputStream in) {
        return new BufferedReader(new InputStreamReader(in, StandardCharsets.UTF_8))
                .lines().collect(Collectors.joining(&quot;\n&quot;));
    }
}

The endpoint of this controller will be called when the css is loaded.
In essence, the loading css is intercepted here and a custom css is added to hide the element you want.
With this change, when you start the application and go to the endpoint to view the swagger documentation you should see UI as in the picture below :

"
"Here's JBoss JSTL implementation for the EscapeXML tag
public class EscapeXML {

    private static final String[] ESCAPES;

    static {
        int size = '&gt;' + 1; // '&gt;' is the largest escaped value
        ESCAPES = new String[size];
        ESCAPES['&lt;'] = &quot;&amp;lt;&quot;;
        ESCAPES['&gt;'] = &quot;&amp;gt;&quot;;
        ESCAPES['&amp;'] = &quot;&amp;amp;&quot;;
        ESCAPES['\''] = &quot;&amp;#039;&quot;;
        ESCAPES['&quot;'] = &quot;&amp;#034;&quot;;
    }
  //omitted
}

Why is ESCAPES a 61 elements array? What are the implication of using a Map&lt;Character,String&gt; instead?
","I think the main reason is performance. Each map query needs to get the hashcode, and then calculate the position of the array in the map, and the array can be obtained directly. The following is a simple test, querying 10,000 times separately, the array is about 10 times faster than the map.
array query result: cost time= 184041
map query result: cost time= 1677042

import org.junit.Before;
import org.junit.Test;

import java.util.Arrays;
import java.util.HashMap;
import java.util.Map;
import java.util.Random;

/**
 * @author jahe
 * @date 2022/1/9
 * @note
 */
public class ArrayMapTest {
    private char[] chars = {'&lt;', '&gt;', '&amp;', '\'', '&quot;'};
    private char[] charsForQuery = new char[10000];
    @Before
    public void init(){
        Random random = new Random(5);
        random.nextInt(5);
        for (int i = 0; i &lt; charsForQuery.length; i++) {
            charsForQuery[i] = chars[random.nextInt(5)];
        }
        System.out.println(Arrays.toString(charsForQuery));
    }
    @Test
    public void test() {
        int size = '&gt;' + 1;
        String[] ESCAPES = new String[size];
        ESCAPES['&lt;'] = &quot;&amp;lt;&quot;;
        ESCAPES['&gt;'] = &quot;&amp;gt;&quot;;
        ESCAPES['&amp;'] = &quot;&amp;amp;&quot;;
        ESCAPES['\''] = &quot;&amp;#039;&quot;;
        ESCAPES['&quot;'] = &quot;&amp;#034;&quot;;
        long start = System.nanoTime();
        doTestForArray(ESCAPES);
        long end = System.nanoTime();
        System.out.println(&quot;array query result: cost time= &quot; + (end - start));

        Map&lt;Character, String&gt; map = new HashMap&lt;&gt;();
        map.put('&lt;', &quot;&amp;lt;&quot;);
        map.put('&gt;', &quot;&amp;gt;&quot;);
        map.put('&amp;', &quot;&amp;amp;&quot;);
        map.put('\'', &quot;&amp;#039;&quot;);
        map.put('&quot;', &quot;&amp;#034;&quot;);
        start = System.nanoTime();
        doTestForMap(map);
        end = System.nanoTime();
        System.out.println(&quot;map query result: cost time= &quot; + (end - start));

    }
    private void doTestForArray(String[] ESCAPES){
        for (char c : charsForQuery) {
            String str = ESCAPES[c];
        }
    }
    private void doTestForMap(Map&lt;Character, String&gt; map){
        for (char c : charsForQuery) {
            String s = map.get(c);
        }
    }
}

"
"I have a class for spring security, validating token from the user. I got the code from Auth0 website and modified antMatcher part for my configuration. Here is the code:
@EnableWebSecurity
public class SecurityConfig {

    @Value(&quot;${auth0.audience}&quot;)
    private String audience;

    @Value(&quot;${spring.security.oauth2.resourceserver.jwt.issuer-uri}&quot;)
    private String issuer;

    @Bean
    public SecurityFilterChain filterChain(HttpSecurity http) throws Exception {
        /*
        This is where we configure the security required for our endpoints and setup our app to serve as
        an OAuth2 Resource Server, using JWT validation.
        */
        http
            .csrf().disable()
            .authorizeRequests()
            .antMatchers(HttpMethod.GET, &quot;/data/actuator/**&quot;).permitAll()
            .antMatchers(HttpMethod.PUT, &quot;/data/**&quot;).hasAuthority(&quot;SCOPE_data:write&quot;)
            .anyRequest().authenticated()
            .and().cors()
            .and().oauth2ResourceServer().jwt();
        return http.build();
    }

    @Bean
    JwtDecoder jwtDecoder() {
        /*
        By default, Spring Security does not validate the &quot;aud&quot; claim of the token, to ensure that this token is
        indeed intended for our app. Adding our own validator is easy to do:
        */
        NimbusJwtDecoder jwtDecoder = (NimbusJwtDecoder)
                JwtDecoders.fromOidcIssuerLocation(issuer);
        OAuth2TokenValidator&lt;Jwt&gt; audienceValidator =
                new com.nuance.pindata.health.importer.security.AudienceValidator(audience);
        OAuth2TokenValidator&lt;Jwt&gt; withIssuer = JwtValidators.createDefaultWithIssuer(issuer);
        OAuth2TokenValidator&lt;Jwt&gt; withAudience = new DelegatingOAuth2TokenValidator&lt;&gt;(withIssuer, audienceValidator);
        jwtDecoder.setJwtValidator(withAudience);
        return jwtDecoder;
    }
}

I am now trying to write unit test, but there is no good way to test it. I can practically test changing method/path, but it is not straight forward how to write this unit test, and it can be done through integration (automation) tests.
From Spring Security HttpSecurity Configuration Testing, he suggests not writing unit test for such security config as well. What is the right approach here? If I should write unit test, how can I achieve this?
","I covered this subject in this Baeldung article.
You can test actuator endpoints access-control in integration tests only (@SpringBootTest). For your own secured @Components, you can do it also in unit-tests (many samples in this repo):

@Controller with @WebMvcTest (@WebfluxTest if you were in a reactive app)
plain JUnit with @ExtendWith(SpringExtension.class), @EnableMethodSecurity and @Import of the tested component (@Service or @Repository with method security like @PreAuthorize expressions) to get an autowired instance instrumented with security

spring-security-test comes with some MockMvc request post-processors (see org.springframework.security.test.web.servlet.request.SecurityMockMvcRequestPostProcessors.jwt in your case) as well as WebTestClient mutators (see org.springframework.security.test.web.reactive.server.SecurityMockServerConfigurers.mockJwt) to configure Authentication of the right type (JwtAuthenticationToken in your case) and set it in test security context, but both have important limitations:

it can be used only with MockMvc and WebTestClient and as so are inefficient when testing something else than a @Controller (it is useless when testing method security on a @Repository or @Service)
it does not use the authentication converter in the security context. It builds a stub Authentication instance based using the Java DSL to set properties, but things like the authorities conversion logic or actual Authentication type to build are not considered.

Sample usage in an integration test (@SpringBootTest) for actuator to be up (but you get the idea for unit-tests):
&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.security&lt;/groupId&gt;
    &lt;artifactId&gt;spring-security-test&lt;/artifactId&gt;
    &lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;

import static org.springframework.security.test.web.servlet.request.SecurityMockMvcRequestPostProcessors.jwt;

@SpringBootTest(webEnvironment = WebEnvironment.MOCK)
@AutoConfigureMockMvc
class ApplicationIntegrationTest {

    @Autowired
    MockMvc api;

    @Test
    void givenUserIsAnonymous_whenGetLiveness_thenOk() throws Exception {
        api.perform(get(&quot;/data/actuator/health/liveness&quot;))
            .andExpect(status().isOk());
    }

    @Test
    void givenUserIsAnonymous_whenGetMachin_thenUnauthorized() throws Exception {
        api.perform(get(&quot;/data/machin&quot;))
            .andExpect(status().isUnauthorized());
    }

    @Test
    void givenUserIsGrantedWithDataWrite_whenGetMachin_thenOk() throws Exception {
        api.perform(get(&quot;/data/machin&quot;)
                .with(jwt().jwt(jwt -&gt; jwt.authorities(List.of(new SimpleGrantedAuthority(&quot;SCOPE_data:write&quot;))))))
            .andExpect(status().isOk());
    }

    @Test
    void givenUserIsAuthenticatedButNotGrantedWithDataWrite_whenGetMachin_thenForbidden() throws Exception {
        api.perform(get(&quot;/data/machin&quot;)
                .with(jwt().jwt(jwt -&gt; jwt.authorities(List.of(new SimpleGrantedAuthority(&quot;SCOPE_openid&quot;))))))
            .andExpect(status().isForbidden());
    }
}

You might also use @WithJwt from this libs I maintain. As opposed to post-processors and mutators in spring-security-test, it:

works when testing any kind of @Component
uses the authentication converter if it is exposed as a @Bean: it builds a stub org.springframework.security.oauth2.jwt.Jwt from a JSON payload in test resources and calls the actual authentication converter with it.

Above Sample becomes:
&lt;dependency&gt;
    &lt;groupId&gt;com.c4-soft.springaddons&lt;/groupId&gt;
    &lt;artifactId&gt;spring-addons-oauth2-test&lt;/artifactId&gt;
    &lt;version&gt;8.0.0&lt;/version&gt;
    &lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;

@SpringBootTest(webEnvironment = WebEnvironment.MOCK)
@AutoConfigureMockMvc
class ApplicationIntegrationTest {

    @Autowired
    MockMvc api;

    @Test
    @WithAnonymousUser
    void givenUserIsAnonymous_whenGetLiveness_thenOk() throws Exception {
        api.perform(get(&quot;/data/actuator/health/liveness&quot;))
            .andExpect(status().isOk());
    }

    @Test
    @WithAnonymousUser
    void givenUserIsAnonymous_whenGetMachin_thenUnauthorized() throws Exception {
        api.perform(get(&quot;/data/machin&quot;))
            .andExpect(status().isUnauthorized());
    }

    @Test
    @WithJwt(&quot;ch4mp.json&quot;)
    void givenUserIsCh4mp_whenGetMachin_thenOk() throws Exception {
        api.perform(get(&quot;/data/machin&quot;))
            .andExpect(status().isOk());
    }

    @Test
    @WithJwt(&quot;tonton-pirate.json&quot;)
    void givenUserIsTontonPirate_whenGetMachin_thenForbidden() throws Exception {
        api.perform(get(&quot;/data/machin&quot;))
            .andExpect(status().isForbidden());
    }
}

"
"explanation of the question:
you must write a multithreaded program that finds all
integers in the range [1, n] that are divisible by 3, 5, or 7. Return the
sum of all unique integers as your answer.
Note that an integer such as 15 (which is a multiple of 3 and 5) is only
counted once.
The Positive integer n &gt; 0 is given to you as input. Create as many threads as
you need to solve the problem. You can use a Thread Pool for bonus points.
Example:
Input: n = 10
Output: sum = 40
Explanation: Numbers in the range [1, 10] that are divisible by 3, 5, or 7 are:
3, 5, 6, 7, 9, 10. The sum of these numbers is 40.

My solution and problem that I faced:
in this program I created three threads each for finding the integers that are divided by 3,5 and 7 separately then it will store them all in the dividends array list and by the following code it will remove the repeated ones in the array list:
Set&lt;Integer&gt; set = new HashSet&lt;&gt;(dividends);
    dividends.clear();
    dividends.addAll(set);

I used some test cases that were provided by our teacher and the problem is that in the testcases that n=1000 and n=76293 sum won't show the amount that was expected:
n=1000
expected sum:272066
actual sum:247377

and the other problem is that the actual sum keeps changing every time I run the testcase.
can someone tell me what the problem of my code is and how I can fix it
my code:
import java.util.*;
public class FindMultiples
{

public static ArrayList&lt;Integer&gt; dividends = new ArrayList&lt;&gt;();
public static int temp = 0;
public static synchronized void increment(){
    dividends.add(temp);
}
public static class thread implements Runnable{

    public int divisor;
    public int n;

    public thread(int n , int divisor){
        this.n=n;
        this.divisor=divisor;
    }

    @Override
    public void run() {

        for (int i=1 ; i&lt;=n ; i++){
            if (i%divisor==0){
                temp=i;
                increment();
            }
        }
    }
}

public int getSum(int n) {
    int sum = 0;
    Thread thread1 = new Thread(new thread(n,3));
    Thread thread2 = new Thread(new thread(n,7));
    Thread thread3 = new Thread(new thread(n,5));
    
    thread3.start();
    thread2.start();
    thread1.start();
    try {
        thread3.join();
        thread2.join();
        thread1.join();
    }catch (InterruptedException e){

    }
    Set&lt;Integer&gt; set = new HashSet&lt;&gt;(dividends);
    dividends.clear();
    dividends.addAll(set);

    for (int i : dividends){
        sum+=i;
    }

    return sum;
}

public static void main(String[] args) {
}
}

","Most likely your main problem is that you are synchronizing the increment() method, and not synchronizing the temp variable.
While one thread is trying to execute the increment() method, the second thread is changing the value of the temp variable. Run your code in debug and check it out.
It's better to send the value directly to increment() than store it in temp.
See my edited code:
import java.util.*;
public class FindMultiples
{

  public static ArrayList&lt;Integer&gt; dividends = new ArrayList&lt;&gt;();
  public static synchronized void increment(int temp){
     dividends.add(temp);
  }
  
  public static class MyThread implements Runnable{

    public int divisor;
    public int n;

    public MyThread(int n , int divisor){
        this.n=n;
        this.divisor=divisor;
    }

    @Override
    public void run() {

        for (int i=1 ; i&lt;=n ; i++){
            if (i%divisor==0){
                increment(i);
            }
        }
    }
  }

  public int getSum(int n) {
    int sum = 0;
    Thread thread1 = new Thread(new MyThread(n,3));
    Thread thread2 = new Thread(new MyThread(n,7));
    Thread thread3 = new Thread(new MyThread(n,5));

    thread3.start();
    thread2.start();
    thread1.start();
    try {
        thread3.join();
        thread2.join();
        thread1.join();
    }catch (InterruptedException e){
        System.out.println(e.getMessage());
    }
    Set&lt;Integer&gt; set = new HashSet&lt;&gt;(dividends);
    dividends.clear();
    dividends.addAll(set);

    for (int i : dividends){
        sum+=i;
    }

    return sum;
  }

  public static void main(String[] args) {
    FindMultiples findMultiples = new FindMultiples();
    System.out.println(findMultiples.getSum(1000));
  }
}

"
"I have a Java record with one field only:
public record AggregateId(UUID id) {}

And a class with the AggregateId field (other fields removed for readability)
public class Aggregate {

    public final AggregateId aggregateId;

    @JsonCreator
    public Aggregate(
            @JsonProperty(&quot;aggregateId&quot;) AggregateId aggregateId
    ) {
        this.aggregateId = aggregateId;
    }
}

The implementation above serialize and deserialize JSON with given example:
ObjectMapper objectMapper = new ObjectMapper();
String content = &quot;&quot;&quot;
        {
           &quot;aggregateId&quot;: {
                &quot;id&quot;: &quot;3f61aede-83dd-4049-a6ff-337887b6b807&quot;
            }
        }
        &quot;&quot;&quot;;
Aggregate aggregate = objectMapper.readValue(content, Aggregate.class);
System.out.println(objectMapper.writeValueAsString(aggregate));

How could I change Jackson config to replace JSON by that:
{
    &quot;aggregateId&quot;: &quot;3f61aede-83dd-4049-a6ff-337887b6b807&quot;
}

without giving up a separate class for AggregateId and access through fields, without getters?
I tried @JsonUnwrapper annotation, but this caused throws
Exception in thread &quot;X&quot; com.fasterxml.jackson.databind.exc.InvalidDefinitionException: 
    Invalid type definition for type `X`: 
        Cannot define Creator parameter as `@JsonUnwrapped`: combination not yet supported at [Source: (String)&quot;{
            &quot;aggregateId&quot;: &quot;3f61aede-83dd-4049-a6ff-337887b6b807&quot;
        }&quot;

or
Exception in thread &quot;X&quot; com.fasterxml.jackson.databind.exc.InvalidDefinitionException: 
    Cannot define Creator property &quot;aggregateId&quot; as `@JsonUnwrapped`: 
        combination not yet supported at [Source: (String)&quot;{
            &quot;aggregateId&quot;: &quot;3f61aede-83dd-4049-a6ff-337887b6b807&quot;
        }&quot;

Jackson version: 2.13.1
dependencies {
    compile &quot;com.fasterxml.jackson.core:jackson-annotations:2.13.1&quot;
    compile &quot;com.fasterxml.jackson.core:jackson-databind:2.13.1&quot;
}

Of course, it's possible with a custom serializer/deserializer, but I'm looking for an easier solution because I have many different classes with a similar issue.
","The combination of @JsonUnwrapped and @JsonCreator is not supported yet, so we can generate a solution like this:
import com.fasterxml.jackson.annotation.JsonCreator;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.annotation.JsonUnwrapped;
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.SerializationFeature;

import java.util.UUID;

public class AggregateTest {

    static record AggregateId(@JsonProperty(&quot;aggregateId&quot;) UUID id) {}

    static class Aggregate {

        @JsonUnwrapped
        @JsonProperty(access = JsonProperty.Access.READ_ONLY)
        public final AggregateId _aggregateId;
        public final String otherField;

        @JsonCreator
        public Aggregate(@JsonProperty(&quot;aggregateId&quot;) UUID aggregateId,
                         @JsonProperty(&quot;otherField&quot;) String otherField) {
            this._aggregateId = new AggregateId(aggregateId);
            this.otherField = otherField;
        }
    }

    public static void main(String[] args) throws JsonProcessingException {
        String rawJson =
            &quot;{\&quot;aggregateId\&quot;: \&quot;1f61aede-83dd-4049-a6ff-337887b6b807\&quot;,&quot; +
                    &quot;\&quot;otherField\&quot;: \&quot;İsmail Y.\&quot;}&quot;;
        ObjectMapper objectMapper = new ObjectMapper();
        objectMapper.configure(SerializationFeature.FAIL_ON_EMPTY_BEANS, false);
        Aggregate aggregate = objectMapper
                .readValue(rawJson, Aggregate.class);
        System.out.println(objectMapper
                .writeValueAsString(aggregate));
    }
}

Here we briefly get rid of the @JsonUnwrapped field.
We get the UUID with the name aggregateId and create an AggregateId record.
Detailed explanations about it:

https://github.com/FasterXML/jackson-databind/issues/1467
https://github.com/FasterXML/jackson-databind/issues/1497

"
"in spring test, I know I can mock static method(usually static util methods: generate id, get value from Redis) using Mockito like:
try (MockedStatic) {
}

but having to do this in every test method is ugly and cumbersome, is there any way to do it all(i am ok to have a single mocked behavior)
I am thinking maybe a junit5 extension, or Mockito extension, this seems like a common problem, I wonder if anyone tries something with any success.
","try this
public class StaticClassTest {

    MockedStatic&lt;YourStatic&gt; mockedStatic;

    @Before
    public void setup() {
        mockedStatic = Mockito.mockStatic(YourStatic.class);

        // if you want the same behavior all along.
        mockedStatic.when(() -&gt; YourStatic.doSomething(anyString())).thenReturn(&quot;TEST&quot;);
    }
    
    @Test
    public void test_static() {
        // write your test here
    }


    @After
    public void teardown() {
        mockedStatic.close();
    }
}

"
"I've got week data in ISO 8601 format. E.g.:
weekA = '2012-W48'
weekB = '2013-W03'

Is there a class in Java that can represent those weeks and supports basic temporal operations? I tried LocalDate.parse(&quot;2012-W48&quot;,DateTimeFormatter.ISO_WEEK_DATE); but this throws an error because this is a week, not an actual date (i.e. the day in the week is missing). Similar to the LocalDate class, I'd like to be able to do some basic temporal operations such as:

weekA.isBefore(weekB) returns true if weekA is before weekB
weeksBetween(weekA,weekB) returns the number of weeks between the two week dates, i.e. weekB-weekA in weeks.

Ideally I'd only use standard Java classes (Java &gt;= 11).
","Using standard library
The original solution (check below) was using an external library. The credit for this solution goes to 
user85421. The idea is to parse the given string into a LocalDate by defaulting the day of the week to day-1.
public class Main {
    public static void main(String[] args) {
        String strWeekA = &quot;2012-W48&quot;;
        String strWeekB = &quot;2013-W03&quot;;

        DateTimeFormatter dtf = new DateTimeFormatterBuilder()
                .appendPattern(&quot;YYYY-'W'ww&quot;)
                .parseDefaulting(ChronoField.DAY_OF_WEEK, 1)
                .toFormatter();

        LocalDate date1 = LocalDate.parse(strWeekA, dtf);
        LocalDate date2 = LocalDate.parse(strWeekB, dtf);

        System.out.println(WEEKS.between(date1, date2));
    }
}

Output:
7

Online Demo
Using an external library (original solution)
You can use the ThreeTen-Extra library for your requirements.
You can use YearWeek, and its  isBefore and isAfter methods.
You can use java.time.temporal.ChronoUnit#between to calculate the amount of time between two YearWeek objects. Alternatively, you can use YearWeek#until to get the same result.
Given below is the Maven dependency for it:
&lt;dependency&gt;
  &lt;groupId&gt;org.threeten&lt;/groupId&gt;
  &lt;artifactId&gt;threeten-extra&lt;/artifactId&gt;
  &lt;version&gt;1.7.2&lt;/version&gt;
&lt;/dependency&gt;

Demo:
import org.threeten.extra.YearWeek;
import static java.time.temporal.ChronoUnit.WEEKS;

public class Main {
    public static void main(String[] args) {
        String strWeekA = &quot;2012-W48&quot;;
        String strWeekB = &quot;2013-W03&quot;;
        YearWeek weekA = YearWeek.parse(strWeekA);
        YearWeek weekB = YearWeek.parse(strWeekB);
        System.out.println(weekA.isBefore(weekB));
        System.out.println(WEEKS.between(weekA, weekB));
        System.out.println(weekA.until(weekB, WEEKS));
    }
}

Output:
true
7
7

"
"I'm reading Effective Java by Joshua Bloch. In ITEM 8: AVOID FINALIZERS AND CLEANERS of CHAPTER 2 he states:

Finalizers have a serious security problem: they open your class up to
finalizer attacks.The idea behind a finalizer attack is simple: If an
exception is thrown from a constructor or its  serialization
equivalentsâ€”the readObject and readResolve methods (Chapter 12)â€”the
finalizer of a malicious subclass can run on the partially constructed
object that should have â€œdied on the vine.â€ This finalizer can record
a reference to the object in a static field, preventing it from being
garbage collected. Once the malformed object has been recorded, it is
a simple matter to invoke arbitrary methods on this object that should
never have been allowed to exist in the first place. Throwing an
exception from a constructor should be sufficient to prevent an object
from coming into existence; in the presence of finalizers, it is not.
Such attacks can have dire consequences. Final classes are immune to
finalizer attacks because no one can write a malicious subclass of a
final class.

Firstly, I know finalizers have been deprecated since Java 18. Nevertheless, I think it's important to understand the reason behind this decision. My understanding of the excerpt above is as follows:

Finalizers are non-deterministic.
A malicious subclass can run its finalizer method on a partially constructed corrupt superclass object.
Moving the corrupt object's reference to a static field doesnâ€™t let the JVM garbage collect.
The attacker can use this object that should've â€œdied on the vineâ€ and do as they will. Thus, the security flaw.

And secondly, I hope my conceptual understanding of the issue is correct. However, Bloch hasn't demonstrated this issue in a tangible code example. Perhaps because he doesn't want us to mess around with the finalize mechanism in Object.
Could you please demonstrate this to me in code?
For instance, if I have a superclass:
/** Superclass */
public class DemoSecurityProblem {

}

And then the subclass either by inheritance or composition:
public class MaliciousSubClass extends DemoSecurityProblem {
    DemoSecurityProblem demoSecurityProblem = new DemoSecurityProblem();
}

How can an attacker exploit this via the finalize mechanism?
Thanks a lot!
","Your description is basically correct, but overcomplicating things. There is no need to store something in a static variable; as soon as the finalize() method is invoked, the object is already resurrected, as invoking a method on an object implies invoking code with access to the object.
Storing the object reference in a variable is a way to expand the lifetime beyond the execution of the finalize() method but this is not a necessary thing for the attack. Also, instead of using a static variable, the attacker could also make the subclass an inner class and store the reference in the still reachable outer object.
So the following program is already enough to demonstrate the issue
public class FinalizerAttackExample {
    public static void main(String[] args) throws InterruptedException {
      try {
          new MaliciousSubclass();
      } catch(SecurityException ex) {
          System.out.println(&quot;wouldn't get hands on a ResourceClass instance&quot;);
      }
      System.gc();
      Thread.sleep(2000);
    }

    static class ResourceClass {
        ResourceClass() {
            if(!checkCaller()) throw new SecurityException();
        }
        public void criticalAction() {
            System.out.println(&quot;ResourceClass.criticalAction()&quot;);
        }
    }

    /** For our demonstration, all callers are invalid */
    static boolean checkCaller() {
        return false;
    }

    static class MaliciousSubclass extends ResourceClass {
        @Override
        protected void finalize() {
            System.out.println(&quot;see, I got hands on &quot; + this);
            criticalAction();
        }
    }
}

While garbage collection is non-deterministic and the execution of finalizers not guaranteed in general, this example will print
wouldn't get hands on a ResourceClass instance
see, I got hands on FinalizerAttackExample$MaliciousSubclass@7ad74083
ResourceClass.criticalAction()

on a lot of implementations, demonstrating that criticalAction() could be invoked on an object that shouldn’t exist as the constructor threw an exception.
"
"I have a hypothetical rest end point.
  @GetMapping(value = &quot;/happy/{happyId}&quot;,
            produces = MediaType.APPLICATION_JSON_VALUE)
    public Response&lt;?&gt; getHappy(@PathVariable Long happyId) {
        Response response = new Response();
        response.update(happyService.getById(happyId));
        return response;
    }

In this code, happyService could throw UnhappyException if id does not exist, and this code is tested in another place, eg) HappyServiceTest.
Now, let say if I want to test my rest controller, should I also be testing the exception flow? Or is this unnecessary?
eg)
    HappyRestControlerTest.java
    @Test
    void testUnHappy() {
      ...
       assertThrows(UnhappyException.class () -&gt; {happyService.getById(-1L)});
    }
    Is this unnecessary test since I tested the behaviour of happyService in HappyServiceTest?

","For this layer of application there is a specific type of testing. It is called MVC testing and for this you mock your service with some specific response for some specific input and you verify with a test that the Controller behaves as expected.
See the following example
@SpringBootTest
@AutoConfigureMockMvc
public class TestingWebApplicationTest {

    @Autowired
    private MockMvc mockMvc;

    @MockBean
    private HappyService service;

    @Test
    public void shouldReturnMessage() throws Exception {
        when(service.getById(1)).thenReturn(&quot;Happy Response 1&quot;);

        this.mockMvc.perform(get(&quot;/happy/1&quot;))
                    .andExpect(status().isOk())
                    .andExpect(content()
                  .string(containsString(&quot;Happy Response 1&quot;)));
    }
}

This is a type of test that simulates, what the client will receive from controller http status code, http headers, http body content etc...
Spring Boot already includes support for this MockMvc testing via the dependency
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;

"
"Consider the following article in JLS Â§18.1.3 - Bounds
Here when we try to identify the set of bounds on the inference variables - we have one of the following situations:

...

throws Î±: The inference variable Î± appears in a throws clause.

...
A bound of the form throws Î± is purely informational: it directs resolution to
optimize the instantiation of Î± so that, if possible, it is not a checked exception type.

I think this statement is incorrect:

this is because ideally the throws clause is mentioned to take care of checked exceptions which can happen during the course of execution of the code.
Then why still the JLS preventing Î± to be a Checked Exception?
Ideally the inference variable Î± must be bounded to be an exception of Checked type rather than being an Unchecked variant.

Is my understanding correct here or am I missing something?
","I think your interpretation/understanding of this statement is slightly misguided:

A bound of the form throws α is purely informational: it directs resolution to optimize the instantiation of α so that, if possible, it is not a checked exception type.

That line is referring to the resolution, which, as I understand it, is not about where throws α is, but about where α is inferred, conceivably the invocation of the method.
Consider this class:
static class MyClass {

    public static void main(String[] args) {
        MyClass.&lt;RuntimeException&gt;something(0); // same as MyClass.something(1);

        try {
            MyClass.&lt;IOException&gt;something(2);
        } catch (IOException ex) {
            // checked exception
        }
    }

    /**
     * Will throw IOException if argument is 2, a RuntimeException otherwise
     */
    static &lt;T extends Exception&gt; void something(int a) throws T {
        if (a == 2) {
            throw (T) new IOException(); //of course it's a bad cast
        } else {
            throw (T) new Exception();
        }
    }
}

After analyzing the two something method, focus on the invocation in the main method:
The call MyClass.&lt;IOException&gt;something(0) expects an IOException. The caller knows it (assume fully documented contract rather than tightly coupled code), and handles the exception.
This already tells you that the variable can be a checked exception, contrary to what you think.
On the contrary, the call MyClass.&lt;RuntimeException&gt;something(0) expects a runtime exception on similar grounds.
How α (T in the above example) is inferred allows the compiler to skip forcing the caller to catch/handle the exception (if it's to look at the bound, which it'd otherwise have to)
Now about the &quot;optimization&quot;: The type variable being bounded as extends Exception can reasonably be expected to resolve to a checked exception. But, if the caller knows that it shall be a runtime exception, it can &quot;inform&quot; the compiler that it's going to be a runtime exception. This is what I did by specifying RuntimeException in the type witness (RuntimeException is also the resolved type when no type argument is given explicitly).
We can spend days to interpret &quot;optimization&quot;, but at least I as a caller did not have to try/catch the invocation, and I still didn't upset the compiler (first invocation).
"
"I'm trying to write a method that would Return true if it is possible to divide all the members of an array into two different groups of equal size so that the sum of the members of the two groups is equal. If this is not possible, the method Return false.
The conditions are:

The method should be recursive with no use of loops at all, So are all the auxiliary methods
Can not contain loops.
The array is neither null nor empty.
Do not modify the contents of the array (not even temporarily), and do not use an auxiliary array.

public static boolean equalSplit (int[] arr){
    if(arr.length % 2 != 0) // if array length is not equal both sides
        return false;
    return equalSplit (arr, arr[0],(0 + arr.length-1) / 2 , arr.length-1);
} 

public static boolean equalSplit (int[] arr, int start, int mid, int end){
       
}

I got stuck here and i have no clue what to do next.
","something like this should solve your problem and handle all cases.
    public static boolean canBeDividedEqually(int[] arr) {
        if (arr.length % 2 != 0) {
            return false;
        }
        int sum = getSum(arr);
        if (sum % 2 != 0) {
            return false;
        }
        return canBeDividedEqually(arr, sum);

    }

    public static int getSum(int[] arr) {
        return getSum(arr, 0, 0);
    }

    private static int getSum(int[] arr, int sum, int index) {
        if (index &gt;= arr.length) {
            return sum;
        }
        return getSum(arr, sum + arr[index], index + 1);
    }

    private static boolean canBeDividedEqually(int[] arr, int sum) {
        // this can be optimized by canBeDividedEqually(arr, sum/2, arr[0], arr.length/2, 1, 1) because first element should always belong to first group, so we can start search from second element
        return canBeDividedEqually(arr, sum/2, 0, arr.length/2, 0, 0);
//        return canBeDividedEqually(arr, sum/2, arr[0], arr.length/2, 1, 1);
    }

    private static boolean canBeDividedEqually (int[] arr, int searchSum, int currentSum, int searchQuantity, int currentQuantity, int nextIndex) {
        if(searchSum == currentSum &amp;&amp; searchQuantity == currentQuantity) {
            return true;
        }
        if(searchSum &lt;= currentSum || searchQuantity &lt;= currentQuantity) {
            // we have too big sum or we take to much elements
            return false;
        }
        if(nextIndex + (searchQuantity - currentQuantity) &gt; arr.length) {
            // we need to take more elements than we have still available
            return false;
        }
        // add current element into account and search further
        if(canBeDividedEqually(arr, searchSum, currentSum + arr[nextIndex], searchQuantity, currentQuantity + 1, nextIndex + 1)) {
            System.out.println(&quot;true&quot;);
            return true;
        }
        // if above &quot;if&quot; statement is not true, then skip current element and try to search further
        return canBeDividedEqually(arr, searchSum, currentSum, searchQuantity, currentQuantity, nextIndex + 1);
    }

"
"Goal: To make ToolTip always show in the bottom-right position of the node
Problem: Regardless my efforts, I'm unable to override or adapt ToolTip behavior. It always shows up based on the mouse position.
MRE:
import javafx.application.Application;
import javafx.geometry.Point2D;
import javafx.scene.Scene;
import javafx.scene.control.Label;
import javafx.scene.control.Tooltip;
import javafx.scene.layout.AnchorPane;
import javafx.stage.Stage;
import javafx.util.Duration;

public class HelloApplication extends Application {

    @Override
    public void start(Stage stage) {
        Label label = new Label(&quot;TEST\nTEST\nTEST&quot;);
        label.setStyle(&quot;-fx-background-color: green;&quot;);

        Tooltip tooltip = new Tooltip(&quot;TOOLTIP&quot;);
        tooltip.setShowDelay(Duration.seconds(0.5));
        label.setTooltip(tooltip);

        Scene scene = new Scene(new AnchorPane(label));
        stage.setScene(scene);
        
        stage.show();

        Point2D p = label.localToScene(0.0, 0.0);
        label.getTooltip().show(label,
                p.getX() + label.getScene().getX() + label.getScene().getWindow().getX(),
                p.getY() + label.getScene().getY() + label.getScene().getWindow().getY());
    }

    public static void main(String[] args) {
        launch();
    }
}

To make it clearer, this is what I'm looking for:

Instead, ToolTip always shows on top of the node:

","+1 for what @jewelsea suggested.
You can create a custom Tooltip to set this functionality across multiple nodes.
The idea is to set the anchor every time just before showing the Tooltip so that it can work well even when the window is moved across the screen.
You can check the idea in the below demo:

import javafx.application.Application;
import javafx.geometry.Bounds;
import javafx.geometry.Insets;
import javafx.geometry.Point2D;
import javafx.geometry.Pos;
import javafx.scene.Node;
import javafx.scene.Scene;
import javafx.scene.control.Label;
import javafx.scene.control.Tooltip;
import javafx.scene.layout.HBox;
import javafx.stage.Stage;
import javafx.util.Duration;

public class TooltipAnchoringDemo extends Application {

    @Override
    public void start(Stage stage) {
        Label label1 = new Label(&quot;TEST1\nTEST1\nTEST1&quot;);
        label1.setStyle(&quot;-fx-background-color: red;&quot;);
        Label label2 = new Label(&quot;TEST2\nTEST2\nTEST2&quot;);
        label2.setStyle(&quot;-fx-background-color: green;&quot;);
        Label label3 = new Label(&quot;TEST3\nTEST3\nTEST3&quot;);
        label3.setStyle(&quot;-fx-background-color: blue;&quot;);

        CustomTooltip tooltip = CustomTooltip.install(&quot;TOOLTIP 1&quot;, label1);
        tooltip.setShowDelay(Duration.seconds(0.5));

        CustomTooltip.install(&quot;TOOLTIP 2&quot;, label2);
        CustomTooltip.install(&quot;TOOLTIP 3&quot;, label3);

        HBox root = new HBox(15, label1, label2, label3);
        root.setPadding(new Insets(10));
        root.setAlignment(Pos.TOP_LEFT);

        Scene scene = new Scene(root, 300, 180);
        stage.setScene(scene);
        stage.setTitle(&quot;Tooltip Demo&quot;);

        stage.show();
    }

    public static void main(String[] args) {
        launch();
    }
}

class CustomTooltip extends Tooltip {
    /**
     * Pixel offset from the node in the y-axis to display the tooltip.
     * You can keep it to 0. Normally I don't want to keep it very near to the node.
     */
    private static final int Y_OFFSET = 2;

    private Node node;

    public CustomTooltip(final String s) {
        super(s);
    }

    public static CustomTooltip install(String msg, Node node) {
        CustomTooltip tooltip = new CustomTooltip(msg);
        tooltip.node = node;
        install(node, tooltip);
        return tooltip;
    }

    @Override
    protected void show() {
        /* Set the new position before showing. */
        Point2D anchor = anchor(node);
        setAnchorX(anchor.getX());
        setAnchorY(anchor.getY());
        
        super.show();
    }

    /**
     * Calculates the anchor point for the tooltip.
     *
     * @param n the node
     * @return the anchor position
     */
    private Point2D anchor(final Node n) {
        /* Get the node bounds on the screen. */
        final Bounds bounds = n.localToScreen(n.getBoundsInLocal());

        /* Calculate the opening position based on node bounds . */
        final Point2D openPosition = new Point2D(bounds.getMaxX(), bounds.getMaxY() + Y_OFFSET);

        return openPosition;
    }
}

"
"Hey all I am needing a hand with the following:
I am trying to add the &quot;On Action&quot; to my custom control I create in Scene Builder 2.0.

I will have a couple of these in my scene so I am wanting to be able to have only 1 handler for all those toggle buttons. Problem being is that my custom control does not have a &quot;On Action&quot; section in the Code: section like other controls do?

Most built in controls look like this for their Code: section:

How do I add this function to my custom control?
My switch button code:
public final ObjectProperty&lt;EventHandler&lt;ActionEvent&gt;&gt; onActionProperty() { return onAction; }
    public final void setOnAction(EventHandler&lt;ActionEvent&gt; value) { onActionProperty().set(value); }
    public final EventHandler&lt;ActionEvent&gt; getOnAction() { return onActionProperty().get(); }
    private ObjectProperty&lt;EventHandler&lt;ActionEvent&gt;&gt; onAction = new ObjectPropertyBase&lt;EventHandler&lt;ActionEvent&gt;&gt;() {
        @Override protected void invalidated() {
            setEventHandler(ActionEvent.ACTION, get());
        }

        @Override
        public Object getBean() {
            return SliderSwitch.this;
        }

        @Override
        public String getName() {
            return &quot;onAction&quot;;
        }
    };

Loading it up in Scene Builder 2.0 I still do not see any action option under the Code tab.
","Custom components don't automatically come with an &quot;on action&quot; property. You have to actually implement an onAction property in the code1. Take a look at implementations of bulit-in controls that provide such a property for examples. Typically, the implementation of the property looks something like this:
// assumes 'this' is some subtype of 'javafx.scene.Node'
private final ObjectProperty&lt;EventHandler&lt;ActionEvent&gt;&gt; onAction =
    new SimpleObjectProperty&lt;&gt;(this, &quot;onAction&quot;) {
      @Override
      protected void invalidated() {
        setEventHandler(ActionEvent.ACTION, get());
      }  
    };
public final void setOnAction(EventHandler&lt;ActionEvent&gt; onAction) { this.onAction.set(onAction); }
public final EventHandler&lt;ActionEvent&gt; getOnAction() { return onAction.get(); }
public final ObjectProperty&lt;EventHandler&lt;ActionEvent&gt;&gt; onActionProperty() { return onAction; }

But note that is not enough. The custom component also has to fire an ActionEvent whenever it's appropriate. When is it appropriate? Well, that's up to the custom component.
And finally, Scene Builder unfortunately does not put the onAction property of a custom component in the &quot;Code&quot; accordion. It is placed in the &quot;Properties&quot; accordion under a section named &quot;Custom&quot; at the top (see screenshot at end of example below). I'm not aware of a way to change this.
Couple of side notes:

You can actually add change listeners to properties via FXML. Though I'm not aware of a way to do that with Scene Builder.

Scene Builder 2.0 is a very outdated version2. Consider using the latest version from Gluon, which is version 22.0.0 at the time of this answer.



1. In response to a (since deleted) comment I made before posting this answer, you've updated your question to show your custom component now has an onAction property.
2. In a comment you've pointed out that Scene Builder 2.0 does not show the &quot;Custom&quot; section, which means updating Scene Builder is part of the solution.

Example
Here is an example of a custom &quot;switch&quot; control that provides an onAction property. This example has the custom control actually extend Control, which means there's also a &quot;skin&quot; class and a &quot;behavior&quot; class to keep things separate.
There is a screenshot of Scene Builder at the end of the answer.
Source Code
Compiled and tested with Java 22.0.2 and JavaFX 22.0.2.
Switch.java
package com.example.control;

import javafx.beans.property.BooleanProperty;
import javafx.beans.property.ObjectProperty;
import javafx.beans.property.SimpleBooleanProperty;
import javafx.beans.property.SimpleObjectProperty;
import javafx.css.PseudoClass;
import javafx.event.ActionEvent;
import javafx.event.EventHandler;
import javafx.scene.control.Control;
import javafx.scene.control.Skin;

public class Switch extends Control {

  public Switch() {
    getStyleClass().add(DEFAULT_STYLE_CLASS);
  }

  public Switch(boolean selected) {
    this();
    setSelected(selected);
  }

  public void toggle() {
    if (!isDisabled() &amp;&amp; !selected.isBound()) {
      setSelected(!isSelected());
    }
  }

  @Override
  protected Skin&lt;?&gt; createDefaultSkin() {
    return new SwitchSkin(this);
  }

  /* **************************************************************************
   *                                                                          *
   * Properties                                                               *
   *                                                                          *
   ****************************************************************************/

  // -- selected property

  private final BooleanProperty selected = new SimpleBooleanProperty(this, &quot;selected&quot;) {

    private boolean wasSelected;

    @Override
    protected void invalidated() {
      boolean isSelected = get();
      if (wasSelected != isSelected) {
        pseudoClassStateChanged(SELECTED, isSelected);
        fireEvent(new ActionEvent());
        wasSelected = isSelected;
      }
    }
  };

  public final void setSelected(boolean selected) {
    this.selected.set(selected);
  }

  public final boolean isSelected() {
    return selected.get();
  }

  public final BooleanProperty selectedProperty() {
    return selected;
  }

  // -- onAction property

  private ObjectProperty&lt;EventHandler&lt;? super ActionEvent&gt;&gt; onAction;

  public final void setOnAction(EventHandler&lt;? super ActionEvent&gt; onAction) {
    if (this.onAction != null || onAction != null) {
      onActionProperty().set(onAction);
    }
  }

  public final EventHandler&lt;? super ActionEvent&gt; getOnAction() {
    return onAction == null ? null : onAction.get();
  }

  public final ObjectProperty&lt;EventHandler&lt;? super ActionEvent&gt;&gt; onActionProperty() {
    if (onAction == null) {
      onAction = new SimpleObjectProperty&lt;&gt;(this, &quot;onAction&quot;) {
        @Override
        protected void invalidated() {
          setEventHandler(ActionEvent.ACTION, get());
        }
      };
    }
    return onAction;
  }

  /* **************************************************************************
   *                                                                          *
   * CSS                                                                      *
   *                                                                          *
   ****************************************************************************/

  private static final String DEFAULT_STYLE_CLASS = &quot;switch&quot;;
  private static final PseudoClass SELECTED = PseudoClass.getPseudoClass(&quot;selected&quot;);
}

SwitchSkin.java
package com.example.control;

import javafx.animation.Animation;
import javafx.animation.FillTransition;
import javafx.animation.ParallelTransition;
import javafx.animation.TranslateTransition;
import javafx.geometry.HPos;
import javafx.geometry.Insets;
import javafx.geometry.VPos;
import javafx.scene.control.SkinBase;
import javafx.scene.layout.Background;
import javafx.scene.layout.BackgroundFill;
import javafx.scene.layout.CornerRadii;
import javafx.scene.layout.Region;
import javafx.scene.paint.Color;
import javafx.scene.shape.Circle;
import javafx.util.Duration;

class SwitchSkin extends SkinBase&lt;Switch&gt; {

  private static final Duration ANIMATION_DURATION = Duration.millis(100);

  private final Circle thumb = new Circle(10);

  private final ParallelTransition animation;
  private final TranslateTransition translateAnimation;

  private SwitchBehavior behavior;

  SwitchSkin(Switch control) {
    super(control);

    var fillAnimation = new FillTransition(ANIMATION_DURATION);
    fillAnimation.setFromValue(Color.FIREBRICK);
    fillAnimation.setToValue(Color.FORESTGREEN);
    thumb.setFill(fillAnimation.getFromValue());

    translateAnimation = new TranslateTransition(ANIMATION_DURATION);
    translateAnimation.setFromX(0);

    animation = new ParallelTransition(thumb, fillAnimation, translateAnimation);
  }

  @Override
  public void install() {
    var control = getSkinnable();

    var bgFill = new BackgroundFill(Color.GRAY, new CornerRadii(10), new Insets(2));
    control.setBackground(new Background(bgFill));

    control.setMinSize(Region.USE_PREF_SIZE, Region.USE_PREF_SIZE);
    control.setMaxSize(Region.USE_PREF_SIZE, Region.USE_PREF_SIZE);
    getChildren().add(thumb);

    registerChangeListener(control.selectedProperty(), _ -&gt; selectedChanged());

    behavior = new SwitchBehavior(control);
  }

  @Override
  public void dispose() {
    super.dispose();
    if (behavior != null) {
      behavior.dispose();
      behavior = null;
    }
  }

  private void selectedChanged() {
    animation.setRate(isSelected() ? 1 : -1);
    animation.play();
  }

  private boolean isSelected() {
    return getSkinnable().isSelected();
  }

  private boolean animationNotRunning() {
    return animation.getStatus() != Animation.Status.RUNNING;
  }

  @Override
  protected void layoutChildren(
      double contentX, double contentY, double contentWidth, double contentHeight) {
    positionInArea(
        thumb, contentX, contentY, contentWidth, contentHeight, -1, HPos.LEFT, VPos.CENTER);

    double toX = contentX + contentWidth - thumb.getLayoutBounds().getWidth();
    translateAnimation.setToX(toX);
    if (isSelected() &amp;&amp; animationNotRunning() &amp;&amp; thumb.getTranslateX() != toX) {
      animation.setRate(1);
      animation.playFromStart();
    } else if (!isSelected() &amp;&amp; animationNotRunning() &amp;&amp; thumb.getTranslateX() != 0) {
      animation.setRate(-1);
      animation.playFrom(ANIMATION_DURATION);
    }
  }

  @Override
  protected double computePrefWidth(
      double height, double topInset, double rightInset, double bottomInset, double leftInset) {
    return leftInset + rightInset + (thumb.getRadius() * 4);
  }

  @Override
  protected double computePrefHeight(
      double width, double topInset, double rightInset, double bottomInset, double leftInset) {
    return topInset + bottomInset + (thumb.getRadius() * 2);
  }
}

SwitchBehavior.java
package com.example.control;

import java.util.Objects;
import javafx.event.EventHandler;
import javafx.event.WeakEventHandler;
import javafx.scene.input.MouseButton;
import javafx.scene.input.MouseEvent;

class SwitchBehavior {

  private final EventHandler&lt;MouseEvent&gt; onClick = this::handleMouseClicked;
  private final WeakEventHandler&lt;MouseEvent&gt; weakOnClick = new WeakEventHandler&lt;&gt;(onClick);

  private final Switch node;

  SwitchBehavior(Switch node) {
    this.node = Objects.requireNonNull(node);
    node.addEventHandler(MouseEvent.MOUSE_CLICKED, weakOnClick);
  }

  private void handleMouseClicked(MouseEvent event) {
    if (event.getButton() == MouseButton.PRIMARY) {
      node.toggle();
    }
  }

  void dispose() {
    node.removeEventHandler(MouseEvent.MOUSE_CLICKED, weakOnClick);
  }
}

Scene Builder
Using Scene Builder 22.0.0.

"
"I'm using @MethodSource annotation on my Junit test case in order to receive from another method a Map&lt;String, Object&gt;.
Seems that @MethodSource cannot support &quot;Map&quot; object.
This is the error I received:
org.junit.platform.commons.PreconditionViolationException: Cannot convert instance of java.util.HashMap into a Stream: {1=Obj1, 2=Obj2}
Do you know if there is a way to receive back a &quot;Map&quot; object like in this example?
@ParameterizedTest
@MethodSource(&quot;hashMapProvider&quot;)
void testMyMapObj(Map&lt;String, Object&gt; argument) {
    assertNotNull(argument);
    Object obj1 = argument.get(&quot;1&quot;);
}


static Map&lt;String, Object&gt; hashMapProvider() {
    Map&lt;String, Object&gt; map = new HashMap&lt;&gt;();
    map.put(&quot;1&quot;, &quot;Obj1&quot;);
    map.put(&quot;2&quot;, &quot;Obj2&quot;);
    return map;
 }

","If your argument in test method is Map&lt;String, Object&gt;, use as return value Stream&lt;Map&lt;String, Object&gt;&gt; in source method:
import org.junit.jupiter.params.ParameterizedTest;
import org.junit.jupiter.params.provider.MethodSource;

import java.util.Map;
import java.util.stream.Stream;

import static org.junit.jupiter.api.Assertions.assertNotNull;

class SimpleTest {

    @ParameterizedTest
    @MethodSource(&quot;hashMapProvider&quot;)
    void test(Map&lt;String, Object&gt; argument) {
        System.out.println(argument);
        assertNotNull(argument);
    }

    static Stream&lt;Map&lt;String, Object&gt;&gt; hashMapProvider() {
        return Stream.of(
                Map.of(&quot;1&quot;, &quot;Obj1&quot;, &quot;2&quot;, &quot;Obj2&quot;),
                Map.of(&quot;3&quot;, &quot;Obj3&quot;)
        );
    }
}

"
"I am currently working with Java's DateTimeFormatter to parse ISO 8601 formatted timestamps, particularly those containing fractional seconds. While experimenting with different timestamp formats, I noticed some unexpected behavior regarding how the formatter handles optional fractional seconds.
Specifically, I am curious about the leniency of the parser when it comes to the number of digits in the fractional seconds. My implementation allows for timestamps with 9 digits for fractional seconds, yet the parser successfully handles timestamps with only 8 digits while failing for those with 7 or fewer. This has led me to wonder if there is an underlying reason for this behavior, whether it is part of the design of the DateTimeFormatter, and if it is documented anywhere.
I wrote a test using the following code:
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.time.format.DateTimeParseException;

public class DateTimeExample {
    public static void main(String[] args) {
        String[] timestamps = {
            &quot;2023-10-05T15:14:29.123456789Z&quot;, // 9 digits
            &quot;2023-10-05T15:14:29.12345678Z&quot;,  // 8 digits
            &quot;2023-10-05T15:14:29.1234567Z&quot;,   // 7 digits
            &quot;2023-10-05T15:14:29.123456Z&quot;,    // 6 digits
            &quot;2023-10-05T15:14:29.12345Z&quot;,     // 5 digits
            &quot;2023-10-05T15:14:29.1234Z&quot;,      // 4 digits
            &quot;2023-10-05T15:14:29.123Z&quot;,       // 3 digits
            &quot;2023-10-05T15:14:29.12Z&quot;,        // 2 digits
            &quot;2023-10-05T15:14:29.1Z&quot;,         // 1 digit
            &quot;2023-10-05T15:14:29Z&quot;            // no fractional seconds
        };

        DateTimeFormatter formatter = DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd'T'HH:mm:ss[.SSSSSSSSS]'Z'&quot;);

        for (String timestamp : timestamps) {
            try {
                LocalDateTime dateTime = LocalDateTime.parse(timestamp, formatter);
                System.out.println(&quot;Parsed date: &quot; + dateTime);
            } catch (DateTimeParseException e) {
                System.err.println(&quot;Failed to parse: &quot; + timestamp + &quot; - &quot; + e.getMessage());
            }
        }
    }
}

Observations
When I run this code, this is the output:
Parsed date: 2023-10-05T15:14:29.123456789
Parsed date: 2023-10-05T15:14:29.123456780
Failed to parse: 2023-10-05T15:14:29.1234567Z - Text '2023-10-05T15:14:29.1234567Z' could not be parsed at index 19
Failed to parse: 2023-10-05T15:14:29.123456Z - Text '2023-10-05T15:14:29.123456Z' could not be parsed at index 19
Failed to parse: 2023-10-05T15:14:29.12345Z - Text '2023-10-05T15:14:29.12345Z' could not be parsed at index 19
Failed to parse: 2023-10-05T15:14:29.1234Z - Text '2023-10-05T15:14:29.1234Z' could not be parsed at index 19
Failed to parse: 2023-10-05T15:14:29.123Z - Text '2023-10-05T15:14:29.123Z' could not be parsed at index 19
Failed to parse: 2023-10-05T15:14:29.12Z - Text '2023-10-05T15:14:29.12Z' could not be parsed at index 19
Failed to parse: 2023-10-05T15:14:29.1Z - Text '2023-10-05T15:14:29.1Z' could not be parsed at index 19
Parsed date: 2023-10-05T15:14:29

It successfully parses timestamps with 9 digits for fractional seconds or no fractional part, which is the expected behaviour. But why does it also work with 8 digits for fractional part?
My conclusion from this behaviour is that the DateTimeFormatter is lenient with upto one extra digit in the pattern. Is that correct, if so, are there any relevant documentations that I can refer?
","It looks like it was a bug in older versions of Java. The bug was reproducible with Java 11 in my system and with Java 12 on IdeOne (currently it uses Java 12). I tested it also with Java 17 in my system but the bug did not appear, as also can be seen in the screenshot shared by user85421.
On a side note,
There are two problems with your code:

Your timestamps have strings with a time zone offset of Z i.e., +00:00. Therefore, you should parse it into an OffsetDateTime rather than a LocalDateTime. Since they are all in ISO 8601 format, you do not need to use a DateTimeFormatter explicitly, as shown in the below demo.
Never specify 'Z' in a date-time parsing/formatting pattern because 'Z' is a character literal while Z is a pattern character specifying time zone offset. To parse a string representing a time zone offset, you must use X (or XX or XXX depending on the requirement).

Demo:
public class Main {
    public static void main(String[] args) {
        System.out.println(&quot;Java Version: &quot; + System.getProperty(&quot;java.version&quot;));

        String[] timestamps = {
                &quot;2023-10-05T15:14:29.123456789Z&quot;, // 9 digits
                &quot;2023-10-05T15:14:29.12345678Z&quot;,  // 8 digits
                &quot;2023-10-05T15:14:29.1234567Z&quot;,   // 7 digits
                &quot;2023-10-05T15:14:29.123456Z&quot;,    // 6 digits
                &quot;2023-10-05T15:14:29.12345Z&quot;,     // 5 digits
                &quot;2023-10-05T15:14:29.1234Z&quot;,      // 4 digits
                &quot;2023-10-05T15:14:29.123Z&quot;,       // 3 digits
                &quot;2023-10-05T15:14:29.12Z&quot;,        // 2 digits
                &quot;2023-10-05T15:14:29.1Z&quot;,         // 1 digit
                &quot;2023-10-05T15:14:29Z&quot;             // no fractional seconds
        };

        for (String timestamp : timestamps) {
            try {
                System.out.println(&quot;Parsed date: &quot; + OffsetDateTime.parse(timestamp));
            } catch (DateTimeParseException e) {
                System.err.println(&quot;Failed to parse: &quot; + timestamp + &quot; - &quot; + e.getMessage());
            }
        }
    }
}

Output on my system with Java 17:
Java Version: 17.0.7
Parsed date: 2023-10-05T15:14:29.123456789Z
Parsed date: 2023-10-05T15:14:29.123456780Z
Parsed date: 2023-10-05T15:14:29.123456700Z
Parsed date: 2023-10-05T15:14:29.123456Z
Parsed date: 2023-10-05T15:14:29.123450Z
Parsed date: 2023-10-05T15:14:29.123400Z
Parsed date: 2023-10-05T15:14:29.123Z
Parsed date: 2023-10-05T15:14:29.120Z
Parsed date: 2023-10-05T15:14:29.100Z
Parsed date: 2023-10-05T15:14:29Z

Online Demo
For learners: Learn more about the modern date-time API from Trail: Date Time.
"
"I have an app with string resources for German and English. I defined a separate Fragment for changing the language that you can see here
public class FR_Options extends Fragment implements View.OnClickListener {



    /*
    String specifying the language of the App
     */

    public static final String LANGUAGE_GERMAN = &quot;German&quot;;
    public static final String LANGUAGE_ENGLISH = &quot;English&quot;;
    //Set the default language to GERMAN
    public static String currentLanguageOfTheApp = LANGUAGE_ENGLISH;

    public FR_Options() {
        // Required empty public constructor
    }


    public static FR_Options newInstance(String param1, String param2) {
        FR_Options fragment = new FR_Options();

        return fragment;
    }

    @RequiresApi(api = Build.VERSION_CODES.JELLY_BEAN_MR1)
    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
    }


    private FragmentOptionsBinding binding;

    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container,
                             Bundle savedInstanceState) {
        // Inflate the layout for this fragment
        binding = FragmentOptionsBinding.inflate(inflater, container, false);
        return binding.getRoot();
    }

    @Override
    public void onViewCreated(@NonNull View view, @Nullable Bundle savedInstanceState) {
        super.onViewCreated(view, savedInstanceState);
        binding.imageButtonGermany.setOnClickListener(this);
        binding.imageButtonUK.setOnClickListener(this);
        if(currentLanguageOfTheApp.equals(LANGUAGE_ENGLISH)) {
            binding.textViewCurrentLanguageValue.setText(LANGUAGE_ENGLISH);
            binding.imageButtonGermany.setAlpha(0.5f);
            binding.imageButtonUK.setAlpha(1.0f);
        }
        if(currentLanguageOfTheApp.equals(LANGUAGE_GERMAN)) {
            binding.textViewCurrentLanguageValue.setText(LANGUAGE_GERMAN);
            binding.imageButtonGermany.setAlpha(1.0f);
            binding.imageButtonUK.setAlpha(0.5f);
        }

    }


    public void onDestroyView() {
        super.onDestroyView();
        binding = null;
    }

    @RequiresApi(api = Build.VERSION_CODES.JELLY_BEAN_MR1)
    @Override
    public void onClick(View view) {

        if(view.getId() == R.id.imageButtonGermany) {

             /*
            Set the language to &quot;German&quot; for other fragments and database queries
             */

            this.currentLanguageOfTheApp = LANGUAGE_GERMAN;


            /*
            Set the language to &quot;German&quot; for the XML-layout files
             */



            Locale locale;
            locale = new Locale(&quot;de&quot;, &quot;DE&quot;);

            Configuration config = new Configuration(getActivity().getBaseContext().getResources().getConfiguration());
            Locale.setDefault(locale);
            config.setLocale(locale);
            getActivity().recreate();

            getActivity().getBaseContext().getResources().updateConfiguration(config,
                    getActivity().getBaseContext().getResources().getDisplayMetrics());






        }

        if(view.getId() == R.id.imageButtonUK) {

            /*
            Set the language to &quot;English&quot; for other fragments and database queries
             */

            this.currentLanguageOfTheApp = LANGUAGE_ENGLISH;


            /*
            Set the language to &quot;English&quot; for the XML-layout files
             */


            Locale locale;
            locale = new Locale(&quot;en&quot;, &quot;EN&quot;);

            Configuration config = new Configuration(getActivity().getBaseContext().getResources().getConfiguration());
            Locale.setDefault(locale);
            config.setLocale(locale);
            getActivity().recreate();

            getActivity().getBaseContext().getResources().updateConfiguration(config,
                    getActivity().getBaseContext().getResources().getDisplayMetrics());


        }


    }


}

Now when I navigate to a Test fragment whose Java file looks like this
public class Test extends Fragment  {



    int widthDisplay;
    int heightDisplay;


    private FragmentTestBinding binding;

    private ConstraintLayout constraintLayout;
    ConstraintSet constraintSet ;




    private boolean fragmentViewHasBeenCreated = false;


    int helpUpdateCounterProgressBar = 0;//Just for testing

    boolean animationIsWindBladRotating = false;



    private boolean sunIsShiningForImagewViews = false;

    private boolean helpSolarGameRectangleCorrectlyCaughtPreviously = false;

    public Test() {
        // Required empty public constructor
    }


    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

    }

    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container,
                             Bundle savedInstanceState) {
        binding = FragmentTestBinding.inflate(inflater, container, false);

        WindowManager wm = (WindowManager) getActivity().getWindowManager();
        Display display = wm.getDefaultDisplay();
        Point size = new Point();
        display.getSize(size);
        widthDisplay = size.x;
        heightDisplay = size.y;

        //Test to set the string resources programmatically
        String goalText = getString(R.string.goal);
        String timeText = getString(R.string.time);
        binding.textViewGoal.setText(goalText);
        binding.textView3.setText(timeText);


        container.getContext();
        constraintLayout= binding.constraintLayout;


        fragmentViewHasBeenCreated = true;
        getActivity().setRequestedOrientation(ActivityInfo.SCREEN_ORIENTATION_LANDSCAPE);
        constraintLayout = binding.constraintLayout;
        constraintSet = new ConstraintSet();
        return binding.getRoot();



    }//end onCreateView


    @Override
    public void onDestroyView() {
        super.onDestroyView();

        // Reset your variable to false
        fragmentViewHasBeenCreated = false;

    }
}

with the corrsponding xml layout file
&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;androidx.constraintlayout.widget.ConstraintLayout xmlns:android=&quot;http://schemas.android.com/apk/res/android&quot;
    xmlns:app=&quot;http://schemas.android.com/apk/res-auto&quot;
    xmlns:tools=&quot;http://schemas.android.com/tools&quot;
    android:id=&quot;@+id/constraintLayout&quot;
    android:layout_width=&quot;match_parent&quot;
    android:layout_height=&quot;match_parent&quot;
    android:background=&quot;@color/white&quot;
    tools:context=&quot;.MainActivity&quot;&gt;


    &lt;TextView
        android:id=&quot;@+id/textView_Goal&quot;
        android:layout_width=&quot;wrap_content&quot;
        android:layout_height=&quot;wrap_content&quot;
        android:text=&quot;@string/goal&quot;
        android:textSize=&quot;24dp&quot;
        app:layout_constraintBottom_toBottomOf=&quot;parent&quot;
        app:layout_constraintEnd_toEndOf=&quot;parent&quot;
        app:layout_constraintStart_toStartOf=&quot;parent&quot;
        app:layout_constraintTop_toTopOf=&quot;parent&quot; /&gt;

    &lt;TextView
        android:id=&quot;@+id/textView3&quot;
        android:layout_width=&quot;wrap_content&quot;
        android:layout_height=&quot;wrap_content&quot;
        android:text=&quot;@string/time&quot;
        android:textSize=&quot;24dp&quot;
        app:layout_constraintEnd_toEndOf=&quot;@+id/textView_Goal&quot;
        app:layout_constraintStart_toStartOf=&quot;@+id/textView_Goal&quot;
        app:layout_constraintTop_toBottomOf=&quot;@+id/textView_Goal&quot; /&gt;


&lt;/androidx.constraintlayout.widget.ConstraintLayout&gt;

The languages of the string resources android:text=&quot;@string/time&quot; and android:text=&quot;@string/goal&quot; never change and always remain English which is the default language.
In the folder values/string/strings.xml there are the two entries
&quot;    &lt;string name=&quot;goal&quot;&gt;Goal&lt;/string&gt;
    &lt;string name=&quot;time&quot;&gt;Time&lt;/string&gt;&quot;

while in the folder values/string/strings.mxl (de-rDE) there are the two entries &quot;
    &lt;string name=&quot;goal&quot;&gt;Ziel&lt;/string&gt; 
&lt;string name=&quot;time&quot;&gt;Zeit&lt;/string&gt;&quot;

still the laguage is not changes in the Test class no matter what I do in the FR_Options fragment class.
Update: I found out that when changing the language in the FR_Options class and I navigate back to my FR_Menu class which looks like this
public class FR_Menu extends Fragment implements View.OnClickListener{

    private FragmentMenuBinding binding;



    public FR_Menu() {

    }


    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

    }

    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container,
                             Bundle savedInstanceState) {
        binding = FragmentMenuBinding.inflate(inflater, container, false);
        getActivity().setRequestedOrientation(ActivityInfo.SCREEN_ORIENTATION_PORTRAIT);

        binding.buttonGame.setOnClickListener(this);
        binding.buttonOptions.setOnClickListener(this);
        binding.buttonHighscores.setOnClickListener(this);
        binding.buttonFacts.setOnClickListener(this);
        binding.buttonExit.setOnClickListener(this);
        binding.buttonTest.setOnClickListener(this);

        Log.e(&quot;LogTag_Menu&quot;, &quot;Method onCreateView - this: &quot; + this);
        return binding.getRoot();
    }

    @Override
    public void onClick(View view) {

        if(view.getId() == R.id.button_game) {
            Navigation.findNavController(getView()).navigate(FR_MenuDirections.actionFRMenuToFRGame());
        }

        if(view.getId() == R.id.button_highscores) {
            Navigation.findNavController(getView()).navigate(FR_MenuDirections.actionFRMenuToFRHighScores());
        }

        if(view.getId() == R.id.button_facts) {
            //Navigation.findNavController(getView()).navigate(FR_MenuDirections.actionFRMenuToFRInterestingFacts());
            Navigation.findNavController(getView()).navigate(FR_MenuDirections.actionFRMenuToFRRVLevelSelectionMenu());
        }

        if(view.getId() == R.id.button_options) {
            Navigation.findNavController(getView()).navigate(FR_MenuDirections.actionFRMenuToFROptions());
        }

        if(view.getId() == R.id.button_test) {
            Navigation.findNavController(getView()).navigate(FR_MenuDirections.actionFRMenuToTest());
        }


        if(view.getId() == R.id.button_exit) {
            getActivity().finishAndRemoveTask();
        }

    }
}

the language of the string resources are correctly changed. However, when navigating from the FR_Menu class to another class, the language of the string resources changes back to the default (English) again. Why is this happening?
Reminder: Does anybody have an idea as to why this is happening and how to solve this problem?
","currentLanguageOfTheApp is a local variable to FR_Options; so it's not globally saved in persistent storage.
You need to save it persistently typically in SharedPrefs or DataStore, so when the activity is recreated you can get last saved language by the user.
With SharedPreference, something like
private static final String LANGUAGE = &quot;LANGUAGE&quot;;
private static final String SHARED_PREFS_NAME= &quot;SHARED_PREFS_NAME&quot;;

private static String getLanguage(Context context) {
    SharedPreferences prefs = context.getSharedPreferences(SHARED_PREFS_NAME, Context.MODE_PRIVATE);
    return prefs.getString(LANGUAGE, &quot;en&quot;);
}

private static String setLanguage(Context context, String language) {
    SharedPreferences prefs = context.getSharedPreferences(SHARED_PREFS_NAME, Context.MODE_PRIVATE);
    SharedPreferences.Editor editor = prefs.edit();
    editor.putString(LANGUAGE, language);
    editor.apply();
}   

And when changing the language by the user, set that to the SharedPrefs, and restart the activity:
@Override
public void onClick(View view) {
    
    if(view.getId() == R.id.imageButtonGermany) {
        setLanguage(requireContext(), &quot;de&quot;);        
    }

    if(view.getId() == R.id.imageButtonUK) {
        setLanguage(requireContext(), &quot;en&quot;);    
    }

    // restart the activity
    requireActivity().finish();
    requireActivity().startActivity(new Intent(requireActivity(), MainActivity.class));

}

So far this will just save the new languange in a persistent storage, and restarted the activity, but to apply the new language when the activity is recreated, the new baseContext needs to map the string file of the new language instead of the default language String file. So, you need a context wrapper to wrap the new language in the new configuration of the baseContext when the activity is recreated.
In code, you'd create the below ContextWrapper:
import android.annotation.TargetApi;
import android.content.Context;
import android.content.ContextWrapper;
import android.content.res.Configuration;
import android.os.Build;

import java.util.Locale;

public class LanguageContextWrapper extends ContextWrapper {

    public LanguageContextWrapper(Context base) {
        super(base);
    }

    public static ContextWrapper wrap(Context context, String language) {
        Configuration config = context.getResources().getConfiguration();
        Locale sysLocale;
        if (Build.VERSION.SDK_INT &gt; Build.VERSION_CODES.N) {
            sysLocale = getSystemLocale(config);
        } else {
            sysLocale = getSystemLocaleLegacy(config);
        }
        if (!language.isEmpty() &amp;&amp; !sysLocale.getLanguage().equals(language)) {
            Locale locale = new Locale(language);
            Locale.setDefault(locale);
            if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.N) {
                setSystemLocale(config, locale);
            } else {
                setSystemLocaleLegacy(config, locale);
            }

        }
        if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.N) {
            context = context.createConfigurationContext(config);
        } else {
            context.getResources().updateConfiguration(config, context.getResources().getDisplayMetrics());
        }
        return new LanguageContextWrapper(context);
    }

    public static Locale getSystemLocaleLegacy(Configuration config) {
        return config.locale;
    }

    @TargetApi(Build.VERSION_CODES.N)
    public static Locale getSystemLocale(Configuration config) {
        return config.getLocales().get(0);
    }

    public static void setSystemLocaleLegacy(Configuration config, Locale locale) {
        config.locale = locale;
    }

    @TargetApi(Build.VERSION_CODES.N)
    public static void setSystemLocale(Configuration config, Locale locale) {
        config.setLocale(locale);
    }
}

And use it in activity's attachBaseContext() callback:
class MyActivity extends AppCompatActivity {

    @Override
    protected void attachBaseContext(Context newBase) {
        String language = getLanguage(newBase);
        super.attachBaseContext(LanguageContextWrapper.wrap(newBase, language));
        setLocale(getLanguage(newBase));
    }

    private void setLocale(String language) {
        Locale locale = new Locale(language);
        Resources resources = getBaseContext().getResources();
        Configuration conf = resources.getConfiguration();
        conf.setLocale(locale);
        resources.updateConfiguration(conf, resources.getDisplayMetrics());
    }

// ........ reset of your activity
    
}

"
"I'm new here and still learning. Today I learn find duplicate in string. From https://www.javatpoint.com/program-to-find-the-duplicate-characters-in-a-string, I try to learn complete code from web.
When string = &quot;Great responsibility&quot; the output will be:
 Duplicate characters in a given string: 
r
e
t
s
i

because it has duplicate character r e t s i
And when string is &quot;great&quot; the output is
 Duplicate characters in a given string: 


The output is blank because there are no duplicate characters, so I give a description &quot;no duplicate&quot; to define no character duplicate and the output goes like this
Duplicate characters in a given string: 
no duplicates
no duplicates
no duplicates
no duplicates
no duplicates

This returns too many descriptions.
My code
public class DuplicateCharacters {  
    public static void main(String[] args) {  
        String string1 = &quot;Great&quot;;  
        int count;  
          
        //Converts given string into character array  
        char string[] = string1.toCharArray();  
          
        System.out.println(&quot;Duplicate characters in a given string: &quot;);  
        //Counts each character present in the string  
        for(int i = 0; i &lt;string.length; i++) {  
            count = 1;  
            for(int j = i+1; j &lt;string.length; j++) {  
                if(string[i] == string[j] &amp;&amp; string[i] != ' ') {  
                    count++;  
                    //Set string[j] to 0 to avoid printing visited character  
                    string[j] = '0';  
                }  
            }  
            //A character is considered as duplicate if count is greater than 1  
            if(count &gt; 1 &amp;&amp; string[i] != '0')  
                System.out.println(string[i]);  
            else 
             System.out.println(&quot;no duplicates&quot;); 
        }  
    }  
} 

How can I print only one description without repetition? I tried return 0; but it does not work.
Expected output
Duplicate characters in a given string: 
no duplicates

","Add a flag to your program that indicates whether there are duplicates or not. And after loop check whether this flag is true or false.
This method would look like below. I commented code where I updated it.
public static void main(String[] args) {
    String string1 = &quot;Great&quot;;
    int count;

    //Converts given string into character array
    char string[] = string1.toCharArray();

    // here is flag added
    boolean noDuplicates = true;

    System.out.println(&quot;Duplicate characters in a given string: &quot;);
    //Counts each character present in the string
    for(int i = 0; i &lt;string.length; i++) {
      count = 1;
      for(int j = i+1; j &lt;string.length; j++) {
        if(string[i] == string[j] &amp;&amp; string[i] != ' ') {
          count++;
          //Set string[j] to 0 to avoid printing visited character
          string[j] = '0';
        }
      }
      //A character is considered as duplicate if count is greater than 1
      if(count &gt; 1 &amp;&amp; string[i] != '0') {
        System.out.println(string[i]);

        //here is flag updated if duplicates are found
        noDuplicates = false;
      }
    }

    //here is flag check
    if (noDuplicates) {
      System.out.println(&quot;no duplicates&quot;);
    }
  }

And btw. Your algorithm has O(n^2) time complexity. You can figure out one that is better ;-)
"
"I'm trying to create a circle that displays four colors where each quarter of the wheel has a distinct color and has a button that is used to rotate the colors of the wheel 90 degrees to the right when pressed.
This is the code that I have so far. I have the button and circle showing up fine but can't get it to rotate when the button is pressed.
import javafx.application.Application;
import javafx.scene.Group;
import javafx.scene.Scene;
import javafx.scene.layout.FlowPane;
import javafx.scene.paint.Color;
import javafx.scene.shape.Arc;
import javafx.scene.shape.ArcType;
import javafx.stage.Stage;
import javafx.event.ActionEvent;
import javafx.geometry.Pos;
import javafx.scene.control.Button;
import java.util.concurrent.TimeUnit;

public class four_color_wheel extends Application {
    private Group circle;
    private FlowPane pane;
    private Button rotate;
    private Arc blueArc, greenArc, yellowArc, redArc;



    public void start(Stage primaryStage) {
    

        // Blue Arc
        Arc blueArc = new Arc(200, 200, 150, 150, 0, 90);
        blueArc.setType(ArcType.ROUND);
        blueArc.setStroke(Color.BLUE);
        blueArc.setFill(Color.BLUE);
    
        // Green Arc
        Arc greenArc = new Arc(200, 200, 150, 150, 90, 90);
        greenArc.setType(ArcType.ROUND);
        greenArc.setStroke(Color.GREEN);
        greenArc.setFill(Color.GREEN);
    
        // Yellow Arc
        Arc yellowArc = new Arc(200, 200, 150, 150, 180, 90);
        yellowArc.setType(ArcType.ROUND);
        yellowArc.setStroke(Color.YELLOW);
        yellowArc.setFill(Color.YELLOW);

        // Red Arc
        Arc redArc = new Arc(200, 200, 150, 150, 270, 90);
        redArc.setType(ArcType.ROUND);
        redArc.setStroke(Color.RED);
        redArc.setFill(Color.RED);

        Group circle = new Group(blueArc, greenArc, yellowArc, redArc);

        Button rotate = new Button(&quot;Rotate Right&quot;);
        rotate.setOnAction(this::processButtonPress);
    
        FlowPane pane = new FlowPane(circle, rotate);
        pane.setAlignment(Pos.CENTER);
        pane.setHgap(20);
        pane.setStyle(&quot;-fx-background-color: GRAY&quot;);
    
        Scene scene = new Scene(pane, 300, 100);
        primaryStage.setTitle(&quot;Four-Color Wheel&quot;);
        primaryStage.setScene(scene);
        primaryStage.show();
     
    
    
    } 
    
    
 public void processButtonPress(ActionEvent event)
    {
        Group circle = new Group(blueArc, greenArc, yellowArc, redArc);
        circle.setRotate(90);
    }
}

","1- many local variables are hidding fields
2- please , stay with java naming convention
3- if you want to rotate something you need to add its current rotation to the angle
public class FourColorWheel extends Application {

private Group circle;
private FlowPane pane;
private Button rotate;
private Arc blueArc, greenArc, yellowArc, redArc;

public void start(Stage primaryStage) {

    // Blue Arc
    blueArc = new Arc(200, 200, 150, 150, 0, 90);
    blueArc.setType(ArcType.ROUND);
    blueArc.setStroke(Color.BLUE);
    blueArc.setFill(Color.BLUE);

    // Green Arc
    greenArc = new Arc(200, 200, 150, 150, 90, 90);
    greenArc.setType(ArcType.ROUND);
    greenArc.setStroke(Color.GREEN);
    greenArc.setFill(Color.GREEN);

    // Yellow Arc
    yellowArc = new Arc(200, 200, 150, 150, 180, 90);
    yellowArc.setType(ArcType.ROUND);
    yellowArc.setStroke(Color.YELLOW);
    yellowArc.setFill(Color.YELLOW);

    // Red Arc
    redArc = new Arc(200, 200, 150, 150, 270, 90);
    redArc.setType(ArcType.ROUND);
    redArc.setStroke(Color.RED);
    redArc.setFill(Color.RED);

    circle = new Group(blueArc, greenArc, yellowArc, redArc);

    rotate = new Button(&quot;Rotate Right&quot;);
    rotate.setOnAction(this::processButtonPress);

    pane = new FlowPane(circle, rotate);
    pane.setAlignment(Pos.CENTER);
    pane.setHgap(20);
    pane.setStyle(&quot;-fx-background-color: GRAY&quot;);

    Scene scene = new Scene(pane, 400, 400);
    primaryStage.setTitle(&quot;Four-Color Wheel&quot;);
    primaryStage.setScene(scene);
    primaryStage.show();

}

public void processButtonPress(ActionEvent event) {

    circle.setRotate(circle.getRotate() + 90);
}
}

Result :

"
"I am trying to utilize the Observability API from Spring Boot 3.x in my application for tracing and metrics but I'm confused with the necessary setup on how to get proper traceability and metrics details.
I have created a sample Spring Boot application for testing.
I have added these dependencies in the pom.xml:
&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.micrometer&lt;/groupId&gt;
        &lt;artifactId&gt;micrometer-registry-datadog&lt;/artifactId&gt;
        &lt;scope&gt;runtime&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.micrometer&lt;/groupId&gt;
        &lt;artifactId&gt;micrometer-tracing-bridge-brave&lt;/artifactId&gt;
        &lt;scope&gt;runtime&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.micrometer&lt;/groupId&gt;
        &lt;artifactId&gt;micrometer-tracing&lt;/artifactId&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;

application.yml:
spring:
  application:
    name: datadog-sample

server:
  port: 8090

management:
  metrics:
    distribution:
      percentiles-histogram:
        http:
          server:
            requests: true
  endpoint:
    health:
      cache:
        time-to-live: 6s
      show-details: always
    metrics:
      enabled: true
  endpoints:
    web:
      exposure:
        include: health,info,metrics
  health:
  jmx:
    metrics:
      export:
        enabled: true
        step: 1m
  info:
    env:
      enabled: true
  datadog:
    metrics:
      export:
        apiKey: test
  tracing:
    sampling:
      probability: 1.0
    propagation:
      type: W3C

logging:
  pattern:
    console: .%5p [${spring.application.name:},%X{traceId:-},%X{spanId:-}]
      - %msg%n

TestController:
@RestController
@Slf4j
public class TestController {
    @GetMapping(value = &quot;/method1&quot;)
    public ResponseEntity&lt;String&gt; method1(@RequestParam String input) throws IOException, InterruptedException {
        log.info(&quot;Inside the method1 with data = {}&quot;,input);
        HttpRequest request = HttpRequest.newBuilder().uri(URI.create(&quot;http://localhost:8090/method2&quot;)).build();
        HttpResponse&lt;String&gt; response = HttpClient.newHttpClient().send(request, HttpResponse.BodyHandlers.ofString());
        return ResponseEntity.ok(response.body());
    }

    @GetMapping(value = &quot;/method2&quot;)
    public ResponseEntity&lt;String&gt; method2() {
        log.info(&quot;Inside the method2&quot;);
        return ResponseEntity.ok(&quot;Called method2 successfully&quot;);
    }
}

Problem: When Service 1 is invoked (http://localhost:8090/method1?input=testdata), it internally calls Service 2 and generating Trace Id and Span Id but for each service, it's generating different Trace Ids given below in the log:
. INFO [datadog-sample,652553b7e89ee89b58c1c37b35cb6102,58c1c37b35cb6102] - Inside the method1 with data = testdata
. INFO [datadog-sample,652553b7ec4d43c0f0e090c94225d91c,f0e090c94225d91c] - Inside the method2

Questions:

Shouldn't this be a single Trace Id with multiple Span Id so that the flow can be traced easily?

Should I need to use @Obesrved annotation anywhere so that I don't need to customise any behaviour?

To send metrics/observability details to Datadog:

Do I need to add anything specific in the code/configuration apart from including Datadog specific dependencies in the POM along with running the Datadog agent in the background?


Does micrometer observability works out of the box for spring cloud-kafka-binder application or do it needs any specific configuration? If yes, can someone provide a reference example?


","I found the issue in your code on why you were getting different traceIds.

Shouldn't this be single TraceId with multiple SpanId, so that the
flow can be traced easily. Should I need to use @Observed annotation
anywhere as I don't want to customise any behaviour?

Answer: Tracing Context Propagation with Micrometer Tracing via java.net.http.HttpRequest/java.net.http.HttpResponse is not working as tested.
So, I guess it was not considering the call to method2 as part of the same tracing context propagation although it should have been.

Please use RestTemplate way to send the request and for getting the response. The propagation of trace will work for sure with RestTemplate.
Read the docs here: https://spring.io/blog/2022/10/12/observability-with-spring-boot-3
You will need to add OpenTelemetry (io.micrometer:micrometer-tracing-bridge-otel) dependency for Tracing Context Propagation with Micrometer Tracing (needed for RestTemplate) in the pom.xml.
&lt;dependency&gt;
    &lt;groupId&gt;io.micrometer&lt;/groupId&gt;
    &lt;artifactId&gt;micrometer-tracing-bridge-otel&lt;/artifactId&gt;
&lt;/dependency&gt;

For Micrometer Integration with Datadog, you will need to add io.micrometer:micrometer-registry-datadog dependency in the pom.xml:
    &lt;dependency&gt;
        &lt;groupId&gt;io.micrometer&lt;/groupId&gt;
        &lt;artifactId&gt;micrometer-registry-datadog&lt;/artifactId&gt;
        &lt;scope&gt;runtime&lt;/scope&gt;
    &lt;/dependency&gt;

The pom.xml that I have used:
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
    xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
    xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;3.1.4&lt;/version&gt;
        &lt;relativePath /&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;
    &lt;groupId&gt;com.example&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-datadog&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;spring-boot-datadog&lt;/name&gt;
    &lt;description&gt;Demo project for Spring Boot&lt;/description&gt;
    &lt;properties&gt;
        &lt;java.version&gt;17&lt;/java.version&gt;
    &lt;/properties&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;io.micrometer&lt;/groupId&gt;
            &lt;artifactId&gt;micrometer-tracing-bridge-otel&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
            &lt;optional&gt;true&lt;/optional&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;io.micrometer&lt;/groupId&gt;
            &lt;artifactId&gt;micrometer-registry-datadog&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;
            &lt;artifactId&gt;lombok&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;excludes&gt;
                        &lt;exclude&gt;
                            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;
                            &lt;artifactId&gt;lombok&lt;/artifactId&gt;
                        &lt;/exclude&gt;
                    &lt;/excludes&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;

Custom Configuration class:
import org.springframework.boot.web.client.RestTemplateBuilder;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.web.client.RestTemplate;

import io.micrometer.observation.ObservationRegistry;
import io.micrometer.observation.aop.ObservedAspect;

@Configuration
public class ExampleConfiguration {
    
    @Bean
    ObservedAspect observedAspect(ObservationRegistry observationRegistry) {
        return new ObservedAspect(observationRegistry);
    }

    @Bean
    RestTemplate restTemplate(RestTemplateBuilder builder) {
        return builder.build();
    }
    
}

You have to create ObservedAspect bean to enable Observability via AOP.
Update the TestController to this :
import java.net.URI;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.RestController;
import org.springframework.web.client.RestTemplate;

import io.micrometer.observation.annotation.Observed;
import lombok.extern.slf4j.Slf4j;

@RestController
@Slf4j
public class TestController {
    

    @Autowired
    private RestTemplate restTemplate;
    
    @Observed(name = &quot;method1&quot;, contextualName = &quot;method1&quot;)
    @GetMapping(value = &quot;/method1&quot;)
    public ResponseEntity&lt;String&gt; method1(@RequestParam String input) {
        log.info(&quot;Inside the method1 with data = {}&quot;, input);
        ResponseEntity&lt;String&gt; response = restTemplate.getForEntity(URI.create(&quot;http://localhost:8090/method3&quot;),
                String.class);
        return ResponseEntity.ok(response.getBody());
    }

    @Observed(name = &quot;method2&quot;, contextualName = &quot;method2&quot;)
    @GetMapping(value = &quot;/method3&quot;)
    public ResponseEntity&lt;String&gt; method2() {
        log.info(&quot;Inside the method2&quot;);
        return ResponseEntity.ok(&quot;Called method2 successfully&quot;);
    }
} 

Note: You will need to use @Observed to put data into Datadog metrics.
application.properties:
spring.application.name=datadog-sample
server.port=8090

management.datadog.metrics.export.api-key=2196881cdea46553c0e65ab7aa8af0de
management.datadog.metrics.export.application-key=8df3b2e284a626127f5b05a4f40fde064036fcef
management.datadog.metrics.export.uri=https://us5.datadoghq.com

management.metrics.distribution.percentiles-histogram.http.server.requests=true

management.endpoints.web.exposure.include=health,info,metrics

management.tracing.sampling.probability=1.0
management.tracing.propagation.type=w3c

management.info.env.enabled=true

management.jmx.metrics.export.enabled=true
management.jmx.metrics.export.step=1m

management.endpoint.health.cache.time-to-live=6s
management.endpoint.health.show-details=always
management.endpoint.metrics.enabled=true

logging.pattern.console=.%5p [${spring.application.name:},%X{traceId:-},%X{spanId:-}] - %msg%n

For datadog, you will need to set these properties to start using micrometer-registry-datadog with spring-boot:

management.datadog.metrics.export.api-key
management.datadog.metrics.export.application-key
management.datadog.metrics.export.uri

Steps on how to setup Datadog:

Go to this url -&gt; https://us5.datadoghq.com/
Click on Try free and sign up for 14 days trial with giving your credit card.
Enter the required details and click on sign up.
It will ask you install Data Dog Agent 7 initially so that it will detect the agent and let you proceed. Otherwise, it doesn't let you proceed. So, install the Datadog agent.

For macOS, I was able to install agent 7 via this command:
DD_AGENT_MAJOR_VERSION=7 DD_API_KEY=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX DD_SITE=&quot;us5.datadoghq.com&quot; bash -c &quot;$(curl -L https://s3.amazonaws.com/dd-agent/scripts/install_mac_os.sh)&quot;

This agent runs in the background.
To verify that datadog agent is running. Enter this command -
datadog-agent status


Later on, you don't need to run that agent.
Go to your Datadog Dashboard, on the left-hand side below, there will be a profile icon with email id in it. Click on that -&gt; Click on the Organization Settings.



To get api-key, see the screenshot given below.



Click on copy key to copy the api key.



Same process to get the application-key, see the screenshot given below.



Click on copy key to copy the application key.



For trial, by default, they give the following region server url as https://us5.datadoghq.com/. You can copy from browser address itself.

That's all.

When I hit the request, I was able to get the same trace Id with multiple span ids in the console.
Output:

Spring logs with same traceIds:
  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
[32m :: Spring Boot :: [39m              [2m (v3.1.4)[0;39m

. INFO [datadog-sample,,] - Starting SpringBootDatadogApplication using Java 17.0.1 with PID 19939 (/Users/anish/Downloads/spring-boot-datadog/target/classes started by anish in /Users/anish/Downloads/spring-boot-datadog)
. INFO [datadog-sample,,] - No active profile set, falling back to 1 default profile: &quot;default&quot;
. INFO [datadog-sample,,] - Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
. INFO [datadog-sample,,] - For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
. INFO [datadog-sample,,] - Tomcat initialized with port(s): 8090 (http)
. INFO [datadog-sample,,] - Starting service [Tomcat]
. INFO [datadog-sample,,] - Starting Servlet engine: [Apache Tomcat/10.1.13]
. INFO [datadog-sample,,] - Initializing Spring embedded WebApplicationContext
. INFO [datadog-sample,,] - Root WebApplicationContext: initialization completed in 1448 ms
. INFO [datadog-sample,,] - publishing metrics for DatadogMeterRegistry every 1m
. INFO [datadog-sample,,] - LiveReload server is running on port 35729
. INFO [datadog-sample,,] - Exposing 3 endpoint(s) beneath base path '/actuator'
. INFO [datadog-sample,,] - Tomcat started on port(s): 8090 (http) with context path ''
. INFO [datadog-sample,,] - Started SpringBootDatadogApplication in 2.738 seconds (process running for 3.802)
. INFO [datadog-sample,,] - Initializing Spring DispatcherServlet 'dispatcherServlet'
. INFO [datadog-sample,,] - Initializing Servlet 'dispatcherServlet'
. INFO [datadog-sample,,] - Completed initialization in 1 ms
. INFO [datadog-sample,666c0ee462f732fe199562808d56e40b,75defc5f90043736] - Inside the method1 with data = testdata
. INFO [datadog-sample,666c0ee462f732fe199562808d56e40b,95b4fc1dda6635e9] - Inside the method2

Finally, I was able to make it working. :)
Screenshot to verify that it has picked up the metrics:
Go to Metrics -&gt; Explorer menu on the Datadog dashboard.


Does micrometer observability works out of the box for
springcloud-kafka-binder application or it needs any specific
configuration? Any reference example for this?

Yes. Doc link is given below. Please refer to this:

https://docs.spring.io/spring-kafka/reference/appendix/micrometer.html

"
"I have a string:
String inputDate = &quot;18-FEB-24 10.02.33.578000000 AM&quot;;

I am trying to read it back and format it in YYYY-MM-DD HH:mm:ss.SSSSSS.
I tried the SimpleDateFormat as well as DateTimeFormatter libraries but not able to read it back.
Below code from chatgpt but its getting error &quot;String cannot parse at index 3&quot;
DateTimeFormatter inputFormatter = DateTimeFormatter.ofPattern(&quot;dd-MMM-yy hh.mm.ss.SSSSSSSSS a&quot;, Locale.ENGLISH);
DateTimeFormatter outputFormatter = DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss.SSSSSS&quot;);
LocalDateTime dateTime = LocalDateTime.parse(inputDate, inputFormatter);
String formattedDate = dateTime.format(outputFormatter);

","DateTimeFormatterBuilder().parseCaseInsensitive()
The problem is that FEB isn't the month abbreviation that Java expects - it's Feb.
So you need to parse your string case-insensitively, which you can do by building the DateTimeFormatter using a builder, DateTimeFormatterBuilder.
Here’s a complete example:
import java.util.*;
import java.time.*;
import java.time.format.*;

public class Test {
    public static void main (String[] args) throws Exception {
        String inputDate = &quot;18-FEB-24 10.02.33.578000000 AM&quot;;

        // Build the parsing part in a case-insensitive manner.
        DateTimeFormatter inputFormatter = new DateTimeFormatterBuilder()
            .parseCaseInsensitive()
            .appendPattern(&quot;dd-MMM-yy hh.mm.ss.SSSSSSSSS a&quot;)
            .toFormatter(Locale.ENGLISH);

        DateTimeFormatter outputFormatter = DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss.SSSSSS&quot;);
        LocalDateTime dateTime = LocalDateTime.parse(inputDate, inputFormatter);
        String formattedDate = dateTime.format(outputFormatter);
        System.out.println(formattedDate);
    }
}

"
"3d software allow  user to change draw mode dinamically. It can be implemented on javafx ?
","Changing draw mode with radio buttons

In this approach a Box instance change its DrawMode with radiobuttons.
This is a single class javafx you can try .
App.java
public class App extends Application {

    @Override
    public void start(Stage stage) {

        var perspective = new PerspectiveCamera(true);
        perspective.setNearClip(0.1);
        perspective.setFarClip(500);
        perspective.setTranslateZ(-150);
        
        
        
        Shape3D cube = new Box(50, 50, 50);
        cube.setCullFace(CullFace.NONE);
        cube.setMaterial(new PhongMaterial(Color.CORAL));

        var toggleGroup = new ToggleGroup();
        var solid = new RadioButton(&quot;solid&quot;);
        solid.setToggleGroup(toggleGroup);
        solid.setSelected(true);
        var wire = new RadioButton(&quot;wireframe&quot;);
        wire.setToggleGroup(toggleGroup);

        var hBox = new HBox(solid, wire);

        toggleGroup.selectedToggleProperty().addListener((o) -&gt; {
            Toggle selectedToggle = toggleGroup.getSelectedToggle();

            if (selectedToggle == solid) {
                cube.setDrawMode(DrawMode.FILL);
            }
            if (selectedToggle == wire) {
                cube.setDrawMode(DrawMode.LINE);

            }

        });

        var group3d = new Group(perspective, cube);

        var subscene = new SubScene(group3d, 300, 400, true, SceneAntialiasing.BALANCED);
        subscene.setCamera(perspective);

        var stack = new StackPane(subscene, hBox);

        stage.setScene(new Scene(stack, 300, 400));
        stage.show();
    }

    public static void main(String[] args) {
        launch();
    }

}

"
"So I have this code excerpt (a minimal reproduction, from a much larger project, and blocking issue). It leverages Java 22 (preview)'s structured task scope in  combo with virtual threads:
playground.java
void main() throws InterruptedException {
  final var NAME = ScopedValue.&lt;String&gt;newInstance();
  try (var ts = new StructuredTaskScope&lt;&gt;()) {
    ScopedValue.runWhere(NAME, &quot;haha&quot;, () -&gt; {
      ts.fork(() -&gt; {
   // ^^^
   // java.util.concurrent.StructureViolationException: Scoped value bindings have changed
        return null;
      });
    });
    ts.join();
  }
}

Using Java 22, you can run it with java --enable-preview --source 22 playground.java.
","You are using the structured task scope wrong.
The JavaDoc for StructuredTaskScope.fork() states:

StructureViolationExceptionPREVIEW - if the current scoped value bindings are not the same as when the task scope was created

Your code changes the scoped value bindings by calling ScopedValue.runWhere(NAME, &quot;haha&quot;, () -&gt; {}); after the task scope was created.
You should probably structure your code like this:
void main() throws InterruptedException {
  final var NAME = ScopedValue.&lt;String&gt;newInstance();
  ScopedValue.runWhere(NAME, &quot;haha&quot;, () -&gt; {
    try (var ts = new StructuredTaskScope&lt;&gt;()) {
      ts.fork(() -&gt; {
        return null;
      });
    });
    ts.join();
  }
}

Or you can create a new task scope after changing the scoped value:
void main() throws InterruptedException {
  final var NAME = ScopedValue.&lt;String&gt;newInstance();
  try (var ts = new StructuredTaskScope&lt;&gt;()) {
    ScopedValue.runWhere(NAME, &quot;haha&quot;, () -&gt; {
      try (var ts2 = new StructuredTaskScope&lt;&gt;()) {
        ts2.fork(() -&gt; {
          return null;
        });
        ts2.join();
      }
    });
    ts.join();
  }
}

"
"In JavaFX (version 21), there exists CSS properties for changing the margins around labels. This is usually used to increase the space between the text in the label and its border.
However, it is possible to do the reverse by assigning negative numbers to these properties, in which case the distance between the text and the border shrinks. It's not very obvious why this would be useful if you're working with a well-behaved font, but there are some fonts that, by default, create really bizarre margins. The worst example that I have found is the font &quot;Harlow Solid Italic&quot;, demonstrated here with exaggerated colors on a black background:

.test-label {
    -fx-background-color: blue;
    -fx-border-color: red;
    -fx-font-family: &quot;Harlow Solid Italic&quot;;
    -fx-font-size: 100;
}


It's pretty plain to see the problem here.

As mentioned before, this can be adjusted by using the -fx-padding property like so:

.test-label {
    -fx-background-color: blue;
    -fx-border-color: red;
    -fx-font-family: &quot;Harlow Solid Italic&quot;;
    -fx-font-size: 100;
    -fx-padding: -40 20 0 20;
}


Much easier on the eyes.

At a cursory glance, it seems as though there is no issue here - the problem is solved. However, that is not the case. While, visually, everything seems to be in order, mouse event registration unfortunately is not. For example, consider making the label in the following way (forgive me for whatever slight mistakes may be in here, I'm cutting out a lot of the fat of manager classes for the sake of a minimally reproducible example):
public class MyApp extends Application {
    @Override
    public void start(Stage stage) {
        root = new Pane();
        
        root.setStyle(&quot;-fx-background-color: black;&quot;);
        root.getChildren().add(createTestLabel());
        scene = new Scene(root);

        scene.getStylesheets().add(&quot;/ExampleStylesheet.css&quot;);
        stage.setScene(scene);
        // stage.setMaximized(true); I do this for convenience
        stage.show();
    }

    public static void main(String[] args) {
        launch(args);
    }

    public Label createTestLabel() {
        Label testLabel = new Label(&quot;Placeholder\nText&quot;);

        //testLabel.setLayoutX(xPos); for your convenience should you
        //testLabel.setLayoutY(yPos); want to test this yourself
        testLabel.getStyleClass().add(&quot;test-label&quot;);
        testLabel.setTextAlignment(TextAlignment.CENTER);
        //testLabel.setPickOnBounds(false); I include this because it does not make a difference for the issue I'm getting at.
        testLabel.setOnMouseEntered(event -&gt; doSomething());
        testLabel.setOnMouseExited(event -&gt; doSomethingElse());

        return testLabel;
    }

    public void doSomething() {
        //System.out.println(&quot;Inside label.&quot;); for example
    }

    public void doSomethingElse() {
        //System.out.println(&quot;Outside label.&quot;); for example
    }
}

If you do the above, then the result will be the screenshots from before, but with one slight issue - your doSomething() and doSomethingElse() will happen when the mouse goes slightly above the label's border up to where the border would have been without the padding. Visually, that looks like this:


The green area is the area that was cut off of the label by the padding property. Despite being gone, it counts as being inside the label for mouse events.

Here, having the mouse inside the green area still counts as having it inside the label. That is what I would like to change, because if you were to do something slightly more complicated than a print statement (along the lines of drag-and-drop, for example) with the label, then the fact that you can do so to the label without it looking like the mouse is actually inside the border of the label is really jarring.
This only applies to area that has been removed from the label in css. It does not apply to area that was added to the label. In that sense, the area on the left and right sides of the label are clickable, which makes sense and is desirable.
Finally, I should mention that I have tried other combinations of -fx-padding with -fx-border-insets and -fx-background-insets, as well as changing the pickOnBounds property for the label (as seen in the comment in the example code above). Neither of these helps to resolve this issue.
To me, this really just seems like a bug. I can't imagine this being intended behavior. That is why I would like to be able to directly manipulate the &quot;clickable region&quot; of the label, as a means around the issue, hence the question - can I do that and if so, how?
(Note: this is a repost of an old question I posted back in December. It was too confusing and not detailed enough to convey the problem properly, so I have since deleted that question, with this being my second, revised attempt, clarifying the points of confusion that others had with the original question.)
","One way I can think of, is to check if the mouse position is in correct layout bounds of the node, to evaluate and proceed with mouse event handlers. Something like
testLabel.getLayoutBounds().contains(event.getX(), event.getY());

Below is a refactored demo of your demo. You can notice the right label is default label and the left one is the one with padding. Once I add for the valid position check, I can only trigger my desired actions in the mouse event handlers (i.e to highlight the label with yellow background).
Note: I think this can be even simplified to do the check directly in the custom event dispatcher rather than doing in each handler.


import javafx.application.Application;
import javafx.scene.Node;
import javafx.scene.Scene;
import javafx.scene.control.Label;
import javafx.scene.input.MouseEvent;
import javafx.scene.layout.HBox;
import javafx.scene.text.TextAlignment;
import javafx.stage.Stage;


public class ClickableLabelRegion_Demo extends Application {

    private final static String CSS = &quot;data:text/css,&quot; +
            &quot;&quot;&quot;
                                    .test-label1 {
                                        -fx-background-color: blue;
                                        -fx-border-color: red;
                                        -fx-font-family: &quot;Harlow Solid Italic&quot;;
                                        -fx-font-size: 100;
                                        -fx-padding: -40 20 0 20;
                                    }
                    
                                    .test-label2 {
                                        -fx-background-color: blue;
                                        -fx-border-color: red;
                                        -fx-font-family: &quot;Harlow Solid Italic&quot;;
                                        -fx-font-size: 100;
                                    }
                    
                                    .test-bg {
                                        -fx-background-color: yellow;
                                    }
                    &quot;&quot;&quot;;

    @Override
    public void start(Stage stage) {
        HBox root = new HBox();
        root.setStyle(&quot;-fx-background-color: black;-fx-padding:40px;-fx-spacing:50px;&quot;);
        root.getChildren().addAll(createTestLabel(1), createTestLabel(2));
        Scene scene = new Scene(root, 1150, 450);
        scene.getStylesheets().add(CSS);
        stage.setScene(scene);
        stage.setTitle(&quot;Clickable Label Region Demo&quot;);
        stage.show();
    }

    public static void main(String[] args) {
        launch(args);
    }

    private Label createTestLabel(int i) {
        Label testLabel = new Label(&quot;Placeholder\nText&quot;);
        testLabel.getStyleClass().add(&quot;test-label&quot; + i);
        testLabel.setTextAlignment(TextAlignment.CENTER);
        testLabel.setOnMouseEntered(event -&gt; mouseEntered(testLabel, event, i));
        testLabel.setOnMouseMoved(event -&gt; mouseMoved(testLabel, event, i));
        testLabel.setOnMouseExited(event -&gt; mouseExited(testLabel, event, i));
        testLabel.setOnMousePressed(event -&gt; mousePressed(testLabel, event, i));
        testLabel.setOnMouseReleased(event -&gt; mouseReleased(testLabel, event, i));
        testLabel.setOnMouseClicked(event -&gt; mouseClicked(testLabel, event, i));
        return testLabel;
    }

    private boolean isValidRegion(Node n, MouseEvent e) {
        return n.getLayoutBounds().contains(e.getX(), e.getY());
    }

    private void mouseClicked(Node n, MouseEvent e, int i) {
        if (isValidRegion(n, e)) {
            System.out.println(&quot;Mouse clicked &quot; + i);
        }
    }

    private void mousePressed(Node n, MouseEvent e, int i) {
        if (isValidRegion(n, e)) {
            System.out.println(&quot;Mouse pressed &quot; + i);
            n.getStyleClass().add(&quot;test-bg&quot;);
        }
    }

    private void mouseReleased(Node n, MouseEvent e, int i) {
        if (isValidRegion(n, e)) {
            System.out.println(&quot;Mouse released &quot; + i);
            n.getStyleClass().remove(&quot;test-bg&quot;);
        }
    }

    private void mouseEntered(Node n, MouseEvent e, int i) {
        if (isValidRegion(n, e)) {
            System.out.println(&quot;Mouse entered &quot; + i);
        }
    }

    private void mouseMoved(Node n, MouseEvent e, int i) {
        if (isValidRegion(n, e)) {
            System.out.println(&quot;Mouse moved &quot; + i);
        }
    }

    private void mouseExited(Node n, MouseEvent e, int i) {
        System.out.println(&quot;Mouse exited &quot; + i);
    }
}

UPDATE (using clip):
As mentioned by @jewelsea, applying a clip on the Label resolves the issue.
class CustomLabel extends Label {
    public CustomLabel(String text) {
        super(text);
        Rectangle clip = new Rectangle();
        setClip(clip);
        layoutBoundsProperty().addListener(p -&gt; {
            Bounds b = getLayoutBounds();
            clip.setWidth(b.getWidth());
            clip.setHeight(b.getHeight());
        });
    }
}

"
"I want to do something like this:

using TitledPane with GridPane inside. In every GridPane I have two columns - label column and control column. And I want the label column in the top TitlePane to have the same width as the bottom one. Because otherwise it looks terrible. And of course, I can't use fixed width in pixels because label texts depend on user language.
This is my code:
public class MyGridPanes extends Application {

    @Override
    public void start(Stage stage) {
        GridPane gridPane1 = new GridPane();
        gridPane1.add(new Label(&quot;One two three&quot;), 0, 0);
        gridPane1.add(new TextField(), 1, 0);
        gridPane1.setHgap(20);
        var titledPane1 = new TitledPane(&quot;Top&quot;, gridPane1);
        titledPane1.setCollapsible(false);

        GridPane gridPane2 = new GridPane();
        gridPane2.setHgap(20);
        gridPane2.add(new Label(&quot;Four five six seven&quot;), 0, 0);
        gridPane2.add(new TextField(), 1, 0);
        var titledPane2 = new TitledPane(&quot;Bottom&quot;, gridPane2);
        titledPane2.setCollapsible(false);

        Scene scene = new Scene(new VBox(titledPane1, titledPane2), 400, 200);
        stage.setScene(scene);
        stage.show();
    }

    public static void main(String[] args) {
        launch(args);
    }
}

And this is my result:

Could anyone say how to do it?
","+1 for @SedJ601 &amp; @jewelsea suggestions.
But if you have many labels/gridPanes to handle or if it is hard to maintain all labels in one list or if you don't want any custom control, you can try the below approach.
The general idea is: we go through all the nodes of first column in all gridPanes and keep track of the their widths to determine the max width. And this maxWidth is set as the ColumnConstraint minWidth to all gridPanes.
You can have a static utility method as below, where you just pass the gridPanes:
/**
 * Builds a common first column constraints for the provided gridPanes.
 * @param gridPanes
 */
private static void buildFirstColumnConstraint(GridPane... gridPanes) {
    /* Column constraint key. */
    final String COLUMN_INDEX_CONSTRAINT = &quot;gridpane-column&quot;;

    /* Checks if the node is a first column node or not. */
    Predicate&lt;Node&gt; isFirstColumn = node -&gt;{
        Integer constraint = (Integer) node.getProperties().get(COLUMN_INDEX_CONSTRAINT);
        return constraint != null &amp;&amp; constraint == 0;
    };

    /* Keep track of the max width. */
    DoubleProperty maxWidth = new SimpleDoubleProperty();
    ChangeListener&lt;Number&gt; widthListener = (obs, old, val) -&gt; {
        if (val.doubleValue() &gt; maxWidth.get()) {
            maxWidth.set(val.doubleValue());
        }
    };

    // Bind the minWidth to the calculated width
    final ColumnConstraints constraint = new ColumnConstraints();
    constraint.minWidthProperty().bind(maxWidth);

    // Go through each gridPane and set the first constraint
    Stream.of(gridPanes).forEach(gridPane -&gt; {
        gridPane.getColumnConstraints().add(0,constraint);

        // Filter for all first column children and add the widthListener to them
        gridPane.getChildren().stream().filter(isFirstColumn)
                .map(node -&gt; (Region) node)
                .forEach(region -&gt; region.widthProperty().addListener(widthListener));
    });
}

Below is the full demo code:

import javafx.application.Application;
import javafx.beans.property.DoubleProperty;
import javafx.beans.property.SimpleDoubleProperty;
import javafx.beans.value.ChangeListener;
import javafx.scene.Node;
import javafx.scene.Scene;
import javafx.scene.control.Label;
import javafx.scene.control.TextField;
import javafx.scene.control.TitledPane;
import javafx.scene.layout.ColumnConstraints;
import javafx.scene.layout.GridPane;
import javafx.scene.layout.Region;
import javafx.scene.layout.VBox;
import javafx.stage.Stage;

import java.util.function.Predicate;
import java.util.stream.Stream;

public class GridPane_Demo extends Application {

    @Override
    public void start(Stage stage) {


        GridPane gridPane1 = buildGrid(&quot;One&quot;, &quot;One Two&quot;);
        var titledPane1 = new TitledPane(&quot;Top&quot;, gridPane1);
        titledPane1.setCollapsible(false);

        GridPane gridPane2 = buildGrid(&quot;One Two Three&quot;, &quot;One Two Three Four&quot;);
        var titledPane2 = new TitledPane(&quot;Bottom&quot;, gridPane2);
        titledPane2.setCollapsible(false);

        buildFirstColumnConstraint(gridPane1, gridPane2);
        Scene scene = new Scene(new VBox(titledPane1, titledPane2), 400, 220);
        stage.setScene(scene);
        stage.show();
    }

    private GridPane buildGrid(String label1, String label2) {
        GridPane gridPane = new GridPane();
        gridPane.setHgap(20);
        gridPane.setVgap(5);
        gridPane.addRow(0, new Label(label1), new TextField());
        gridPane.addRow(1, new Label(label2), new TextField());
        return gridPane;
    }

    /**
     * Builds a common first column constraints for the provided gridPanes.
     *
     * @param gridPanes
     */
    private static void buildFirstColumnConstraint(GridPane... gridPanes) {
        /* Column constraint key. */
        final String COLUMN_INDEX_CONSTRAINT = &quot;gridpane-column&quot;;

        /* Checks if the node is a first column node or not. */
        Predicate&lt;Node&gt; isFirstColumn = node -&gt; {
            Integer constraint = (Integer) node.getProperties().get(COLUMN_INDEX_CONSTRAINT);
            return constraint != null &amp;&amp; constraint == 0;
        };

        /* Keep track of the max width. */
        DoubleProperty maxWidth = new SimpleDoubleProperty();
        ChangeListener&lt;Number&gt; widthListener = (obs, old, val) -&gt; {
            if (val.doubleValue() &gt; maxWidth.get()) {
                maxWidth.set(val.doubleValue());
            }
        };

        // Bind the minWidth to the calculated width
        final ColumnConstraints constraint = new ColumnConstraints();
        constraint.minWidthProperty().bind(maxWidth);

        // Go through each gridPane and set the first constraint
        Stream.of(gridPanes).forEach(gridPane -&gt; {
            gridPane.getColumnConstraints().add(0, constraint);

            // Filter for all first column children and add the widthListener to them
            gridPane.getChildren().stream().filter(isFirstColumn)
                    .map(node -&gt; (Region) node)
                    .forEach(region -&gt; region.widthProperty().addListener(widthListener));
        });
    }

    public static void main(String[] args) {
        launch(args);
    }
}

"
"We have Jenkins shared library project with some unit-tests that utilize Mockito.
After an upgrade of Jenkins-core from version 2.325 to 2.326 tests start failing on the following line:
class DSLMock {

  DSLMock() {

    this.mock = mock(DSL.class)

-&gt;  when(mock.invokeMethod(eq(&quot;error&quot;), any())).then(new Answer&lt;String&gt;() {
      @Override
      String answer(InvocationOnMock invocationOnMock) throws Throwable {
        throw new AbortException((String) invocationOnMock.getArguments()[1][0])
      }
    })
...

with error:

org.mockito.exceptions.misusing.InvalidUseOfMatchersException: 
Misplaced or misused argument matcher detected here:
-&gt; at com.devops.jenkins.testing.DSLMock.&lt;init&gt;(DSLMock.groovy:66)
-&gt; at com.devops.jenkins.testing.DSLMock.&lt;init&gt;(DSLMock.groovy:66)
You cannot use argument matchers outside of verification or stubbing.
Examples of correct usage of argument matchers:
    when(mock.get(anyInt())).thenReturn(null);
    doThrow(new RuntimeException()).when(mock).someVoidMethod(anyObject());
    verify(mock).someMethod(contains(&quot;foo&quot;))
This message may appear after an NullPointerException if the last matcher is returning an object 
like any() but the stubbed method signature expect a primitive argument, in this case,
use primitive alternatives.
    when(mock.get(any())); // bad use, will raise NPE
    when(mock.get(anyInt())); // correct usage use
Also, this error might show up because you use argument matchers with methods that cannot be mocked.
Following methods *cannot* be stubbed/verified: final/private/equals()/hashCode().
Mocking methods declared on non-public parent classes is not supported.

I've tried to replace any() with methods like anyString() and just value like &quot;&quot; but still got same error.
Also I've tried different stub syntax like
doAnswer(new Answer...).when(mock).invokeMethod(eq(&quot;error&quot;), any())

In changelog https://www.jenkins.io/changelog-old/#v2.326 I see Groovy patch version has been upgraded:

Upgrade Groovy from 2.4.12 to 2.4.21

I wonder if that would cause the issue. Other dependencies versions are not changed:
&lt;groovy.version&gt;2.4.12&lt;/groovy.version&gt;
&lt;junit-jupiter.version&gt;5.8.1&lt;/junit-jupiter.version&gt;
&lt;mockito.core.version&gt;3.3.3&lt;/mockito.core.version&gt;

","It seems like there have been similar issues with other versions of Groovy and Mockito: Is Groovy 3.0.5 incompatible with Mockito 3.6.28 ? Mocks are not usable
However, even the latest version of Mockito doesn't seem to fix the issue with the version of Groovy that you're using. In the meantime, this worked for me:
@Test
void shouldMockObject() {
    DSL mock = spy(new DSL(null) {
        @Override
        Object invokeMethod(String name, Object args) {
            return null
        }
    })

    when(mock.invokeMethod(eq(&quot;error&quot;), any())).then(new Answer&lt;String&gt;() {
        @Override
        String answer(InvocationOnMock invocationOnMock) throws Throwable {
            throw new AbortException((String) invocationOnMock.getArguments()[1][0])
        }
    })

    assertThrows(AbortException.class) {
        mock.invokeMethod(&quot;error&quot;, [ &quot;message&quot; ])
    }
}

"
"Any way to make the menu open to the up direction in JavaFX? Normally the menu bar is always at the top of a window and the menu is opening down-ways. I was wondering, if I could have my menu bar at the bottom of the window and open the menu up?
I tried the JavaFX CSS reference and the JavaFX Javadocs, and Google search. But I don't think there is anything about it.

","If you're okay with relying on implementation details, then you can utilize the fact that each menu in the menu bar is an instance of MenuButton (at least when using the default skin). That class defines the popupSide property which you can set to TOP.
Here's an example:
import javafx.application.Application;
import javafx.geometry.Side;
import javafx.scene.Scene;
import javafx.scene.control.Menu;
import javafx.scene.control.MenuBar;
import javafx.scene.control.MenuButton;
import javafx.scene.control.MenuItem;
import javafx.scene.layout.BorderPane;
import javafx.stage.Stage;

public class Main extends Application {

  @Override
  public void start(Stage primaryStage) {
    var fileMenu = new Menu(&quot;File&quot;);
    for (int i = 1; i &lt;= 5; i++) {
      fileMenu.getItems().add(new MenuItem(&quot;Item #&quot; + i));
    }
    var menuBar = new MenuBar(fileMenu);

    var root = new BorderPane();
    root.setBottom(menuBar);

    primaryStage.setScene(new Scene(root, 500, 300));
    primaryStage.show();

    for (var menu : menuBar.lookupAll(&quot;.menu&quot;)) {
      ((MenuButton) menu).setPopupSide(Side.TOP);
    }
  }

  public static void main(String[] args) {
    launch(Main.class);
  }
}


Which results in the following on Windows 10 using JavaFX 23.0.1:

This example uses Node#lookupAll(String) to get the underlying MenuButton instances. That should work for any number of menus in the menu bar. Unfortunately, the popupSide property does not appear to be styleable, so you can't set it from CSS.
Note you will have to:

Wait to execute the solution until after you added all your menus and the menu bar's skin has been initialized. One way to wait for the skin to be initialized is to wait until after the menu bar's window has been shown at least once (like in the example).

Based on the implementation of MenuBarSkin, you'll have to re-execute the solution if you:

Modify the menus list of the MenuBar in any way.

Change the visibility of a menu currently in the menu bar.


The reason is because the skin apparently rebuilds the entire UI of the menu bar in those cases, which results in new menu buttons being created. I'm not sure if modifying the menus themselves (beyond their visibility) requires you to re-execute the solution.


And keep in mind this solution relies on implementation details, which means it may not work in other versions or break in a future version. I only tested the example with JavaFX 23.0.1.

That all said, it may be best to not use a MenuBar in your case. Consider directly using multiple MenuButton in something like an HBox instead. That way you don't have to rely on implementation details. Though you may have to style each MenuButton to fit your needs.
"
"I know there have been many questions around computeIfAbsent.
Specifically what I am looking for is to understand the statement around atomicity for a concurrent hash map.
from the JavaDoc

The entire method invocation is performed atomically, so the function is applied at most once per key.

If two threads attempt to execute computeIfAbsent with different key's and find that in both cases the map does not contain them, might the resulting executions of the compute if absent function be concurrent? I understand they would not be concurrent in the event that both threads were trying to add the SAME key.
The word Atomic is used and it is mentioned that this means applied at most once per key. But there isn't a specific mention of synchronized behaviour on the method.
As a side note, this is relevant to me in that the method called by computeIfAbsent modifies then uses a field of the class in its body.*
I want to understand if there is a threading concern resulting from two different thread executions of the computeIfAbsent method for the two different keys.
Essentially do I have to look at something along the lines of synchronizing access to the field variable and its subsequent use within the computeIfAbsent method I call.
*( The computeIfAbsent method invoked is the only method which modifies the field.  There is no other invoker of the method outside of the call from the hash map computeIfAbsent method. There is only one instance of the concurrent hash map that calls the computeWithAbsent method that invokes the &quot;atomic&quot; method in question)
My field is volatile to avoid potential concerns with atomic visibility.
","There are situations where the mapping function could be executed concurrently for different key values so it is important that your mapping function is thread-safe.
The computeIfAbsent method only guarantees that the mapping function isn't called simultaneously for the same key value. Also note that a Map works by hashing muliple keys into buckets of entries and if computeIfAbsent(a, mapFunc) is called at same time as computeIfAbsent(b, mapFunc) with pair of keys a+b that map to same sub-table of ConcurrentHashMap, then mapFunc for each key will be run one after the other and not at same time.
However where the different keys do not resolve to same sub-table within ConcurrentHashMap you should expect your mapping function to be called simulaneously by different threads for different key values.
Here is an example which shows a thread-safe mapping function that detects concurrent callers:
public static void main(String[] args) throws InterruptedException {
    ConcurrentHashMap&lt;String, String&gt; map = new ConcurrentHashMap&lt;&gt;(2096, 1.0f);
    AtomicInteger concurrent = new AtomicInteger();
    Function&lt;String, String&gt; mappingFunction = s -&gt; {
        int c = concurrent.incrementAndGet();
        String value = &quot;Value:&quot;+s +&quot; concurrent=&quot;+c+&quot; thread=&quot;+Thread.currentThread().getName();
        if (c != 1)
            System.out.println(&quot;Multiple callers for &quot;+value);
        try { Thread.sleep(50); } catch (InterruptedException ignore) { }
        concurrent.decrementAndGet();
        return value;
    };
    Runnable task = () -&gt; {
        Random r = new Random();
        for (int i = 0; i &lt; 10_000; i++)
            map.computeIfAbsent(String.valueOf(r.nextInt(10240)), mappingFunction);
    };

    Thread a = new Thread(task, &quot;one&quot;);

    a.start();
    task.run();
    a.join();
    map.values().stream().limit(32).forEach(System.out::println);
}

If run enough, there will be occasions where the counter inside mappingFunction shows that 2 instances are running at same time on the pair of threads.
EDIT
To answer your comment about synchronized (r):
Note that there is infinite while loop inside the computeIfAbsent which only exits on break or return, and mappingFunction.apply(key) is called in two places:

when the key is the first entry into the sub-table it runs to the synchronized (r) block. As the line before declares Node&lt;K,V&gt; r = new ReservationNode&lt;K,V&gt;() there is NEVER contention on r from different threads, but only one thread successfully enters the if (casTabAt(...)) { binCount = 1; ... }  block and returns, other losing threads resume the loop.

when the key is not the first entry into the sub-table it runs to the synchronized (f) block which would block all but one threads trying to computeIfAbsent for different keys that are hashed to the same sub-table. As each thread enters the block it verifies f is unchanged, and if so returns existing or computed new value - otherwise resumes the loop.


"
"I am using itext5 to create pdf files with painted non-removable watermarks as follows:
public class TestWatermark {

    public static String resourcesPath = &quot;C:\\Users\\java\\Desktop\\TestWaterMark\\&quot;;
    public static String FILE_NAME = resourcesPath + &quot;test.pdf&quot;;

    public static void main(String[] args) throws IOException {
        System.out.println(&quot;########## STARTED ADDING WATERMARK ###########&quot;);
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        try {
            byte[] byteArray = Files.readAllBytes(Paths.get(FILE_NAME));
            String watermarkText = &quot;confidential&quot;;
            String fontPath = resourcesPath + &quot;myCustomFont.ttf&quot;;
            Font arabicFont = FontFactory.getFont(fontPath, BaseFont.IDENTITY_H, 16);

            BaseFont baseFont = arabicFont.getBaseFont();
            PdfReader reader = new PdfReader(byteArray);
            PdfStamper stamper = new PdfStamper(reader, baos);

            int numberOfPages = reader.getNumberOfPages();

            float height = baseFont.getAscentPoint(watermarkText, 24) + baseFont.getDescentPoint(watermarkText, 24);

            for (int i = 1; i &lt;= numberOfPages; i++) {

                Rectangle pageSize = reader.getPageSizeWithRotation(i);
                PdfContentByte overContent = stamper.getOverContent(i);

                PdfPatternPainter bodyPainter = stamper.getOverContent(i).createPattern(pageSize.getWidth(),
                        pageSize.getHeight());
                BaseColor baseColor = new BaseColor(10, 10, 10);
                bodyPainter.setColorStroke(baseColor);
                bodyPainter.setColorFill(baseColor);
                bodyPainter.setLineWidth(0.85f);
                bodyPainter.setLineDash(0.2f, 0.2f, 0.2f);

                PdfGState state = new PdfGState();
                state.setFillOpacity(0.3f);
                overContent.saveState();
                overContent.setGState(state);

                for (float x = 70f; x &lt; pageSize.getWidth(); x += height + 100) {
                    for (float y = 90; y &lt; pageSize.getHeight(); y += height + 100) {

                        bodyPainter.beginText();
                        bodyPainter.setTextRenderingMode(PdfPatternPainter.TEXT_RENDER_MODE_FILL);
                        bodyPainter.setFontAndSize(baseFont, 13);
                        bodyPainter.showTextAlignedKerned(Element.ALIGN_MIDDLE, watermarkText, x, y, 45f);
                        bodyPainter.endText();

                        overContent.setColorFill(new PatternColor(bodyPainter));
                        overContent.rectangle(pageSize.getLeft(), pageSize.getBottom(), pageSize.getWidth(),
                                pageSize.getHeight());
                        overContent.fill();

                    }
                }

                overContent.restoreState();

            }

            stamper.close();
            reader.close();
            byteArray = baos.toByteArray();
            File outputFile = new File(resourcesPath + &quot;output.pdf&quot;);
            if (outputFile.exists()) {
                outputFile.delete();
            }
            Files.write(outputFile.toPath(), byteArray);

            System.out.println(&quot;########## FINISHED ADDING WATERMARK ###########&quot;);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

the above code makes the watermark non-selectable and non-removable in the Adobe Pro editing function
but the issue is when opening this pdf file from the VMware Workspace ONE Boxer email, the watermark is not displayed!
Any advice on how to fix this issue?
UPDATE: the following code works fine in Boxer PDF Viewer and the watermark is showing fine, but the issue is that this watermark is selectable and removable by adobe pro:
public class TestWatermark2 {

    public static String resourcesPath = &quot;C:\\Users\\java\\Desktop\\TestWaterMark\\&quot;;
    public static String FILE_NAME = resourcesPath + &quot;test.pdf&quot;;

    public static void main(String[] args) throws IOException {
        System.out.println(&quot;########## STARTED ADDING WATERMARK ###########&quot;);
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        try {
            byte[] byteArray = Files.readAllBytes(Paths.get(FILE_NAME));
            String watermarkText = &quot;confidential&quot;;
            String fontPath = resourcesPath + &quot;myCustomFont.ttf&quot;;
            Font arabicFont = FontFactory.getFont(fontPath, BaseFont.IDENTITY_H, 16);

            BaseFont baseFont = arabicFont.getBaseFont();
            PdfReader reader = new PdfReader(byteArray);
            PdfStamper stamper = new PdfStamper(reader, baos);
            Phrase watermarkPhrase = new Phrase(watermarkText, arabicFont);

            int numberOfPages = reader.getNumberOfPages();

            float height = baseFont.getAscentPoint(watermarkText, 24) + baseFont.getDescentPoint(watermarkText, 24);

            for (int i = 1; i &lt;= numberOfPages; i++) {

                Rectangle pageSize = reader.getPageSizeWithRotation(i);
                PdfContentByte overContent = stamper.getOverContent(i);

                PdfGState state = new PdfGState();
                state.setFillOpacity(0.3f);
                overContent.saveState();
                overContent.setGState(state);

                for (float x = 70f; x &lt; pageSize.getWidth(); x += height + 100) {
                    for (float y = 90; y &lt; pageSize.getHeight(); y += height + 100) {
                        ColumnText.showTextAligned(overContent, Element.ALIGN_CENTER, watermarkPhrase, x, y, 45f,
                                PdfWriter.RUN_DIRECTION_RTL, ColumnText.DIGITS_AN2EN);
                    }
                }

                overContent.restoreState();

            }

            stamper.close();
            reader.close();
            byteArray = baos.toByteArray();
            File outputFile = new File(resourcesPath + &quot;output.pdf&quot;);
            if (outputFile.exists()) {
                outputFile.delete();
            }
            Files.write(outputFile.toPath(), byteArray);

            System.out.println(&quot;########## FINISHED ADDING WATERMARK ###########&quot;);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

UPDATE2: I tried mkl solution and it is working very fine but it has one small issue if the watermark text is arabic it is getting displayed incorrect LTR as in the attached image:

","After finding out in the comments that the original watermark actually could be selected and deleted in the current Adobe Acrobat, I there mentioned the following option:

What you currently can do is also putting the actual content into the pattern together with the watermark: Currently Adobe only allows to remove the whole filled rectangle, so if the actual content was also in the pattern, someone removing the rectangle would also remove the content. The downside: text in patterns cannot be copied&amp;pasted. Furthermore, text in patterns is not accessible, screen readers etc. won't read it.

You responded by asking for sample code for this proposed solution. This answer is focusing on quick &amp; dirty example code for it.
The following is a quick proof-of-concept for the proposed solution. In particular it may have to be adjusted for pages whose lower left corner (crop box) is not the coordinate origin (0, 0) or which have page rotation applied.
Furthermore, in the course of testing the code it turned out that Acrobat suddenly allows editing the pattern content if there is no other content in the page than the pattern.
Thus, the code also adds a short pseudo content, a string of spaces. Interestingly this suffices to make the pattern again selectable only as a whole, and at the same time, because it's merely a string of spaces, Acrobat does not allow to select this pseudo content, either...
Following are screenshots in the Acrobat Edit tool and example files without and with pseudo content respectively:



Result file without pseudo content
Result file with pseudo content








example PDF
example PDF



Beware, chances are that Acrobat will eventually also allow to edit this kind of watermarking. Maybe it even now is possible, merely not as obvious as before.
ByteArrayOutputStream baos = new ByteArrayOutputStream();

PdfReader reader = new PdfReader(byteArray);
PdfStamper stamper = new PdfStamper(reader, baos);

BaseFont baseFont = BaseFont.createFont(BaseFont.HELVETICA_OBLIQUE, BaseFont.WINANSI, false);
String watermarkText = &quot;confidential&quot;;

int numberOfPages = reader.getNumberOfPages();
for (int i = 1; i &lt;= numberOfPages; i++) {
    Rectangle pageSize = reader.getPageSizeWithRotation(i);

    // get handle for existing page content
    PdfImportedPage pageContent = stamper.getImportedPage(reader, i);
    // store that content as form XObject
    stamper.getWriter().addToBody(pageContent.getFormXObject(stamper.getWriter().getCompressionLevel()), pageContent.getIndirectReference());
    pageContent.setCopied();
    // reset page content
    reader.getPageN(i).put(PdfName.CONTENTS, null);

    // create pattern with former page content
    PdfPatternPainter bodyPainter = stamper.getOverContent(i).createPattern(pageSize.getWidth(),
            pageSize.getHeight());
    bodyPainter.addTemplate(pageContent, 0, 0);

    // add watermark to pattern
    PdfGState state = new PdfGState();
    state.setFillOpacity(0.3f);
    bodyPainter.saveState();
    bodyPainter.setGState(state);
    for (float x = 70f; x &lt; pageSize.getWidth(); x += 100) {
        for (float y = 90; y &lt; pageSize.getHeight(); y += 100) {
            bodyPainter.beginText();
            bodyPainter.setTextRenderingMode(PdfPatternPainter.TEXT_RENDER_MODE_FILL);
            bodyPainter.setFontAndSize(baseFont, 13);
            bodyPainter.showTextAlignedKerned(Element.ALIGN_MIDDLE, watermarkText, x, y, 45f);
            bodyPainter.endText();
        }
    }
    bodyPainter.restoreState();

    // create new page content
    PdfContentByte canvas = stamper.getUnderContent(i);
    // add pseudo-content
    canvas.beginText();
    canvas.setFontAndSize(baseFont, 13);
    canvas.showTextAlignedKerned(Element.ALIGN_MIDDLE, &quot;        &quot;, 0, 0, 45f);
    canvas.endText();
    // fill with pattern holding former page content
    canvas.setColorFill(new PatternColor(bodyPainter));
    canvas.rectangle(pageSize.getLeft(), pageSize.getBottom(), pageSize.getWidth(),
            pageSize.getHeight());
    canvas.fill();
}

stamper.close();
reader.close();
byteArray = baos.toByteArray();

(AddWatermark test testWatermarkAllInPattern)
"
"As I was reading up about virtual threads and their pitfalls I found this mention :

Don't Cache Expensive Reusable Objects in Thread-Local Variables


Virtual threads support thread-local variables just as platform
threads do. See Thread-Local Variables for more information. Usually,
thread-local variables are used to associate some context-specific
information with the currently running code, such as the current
transaction and user ID. This use of thread-local variables is
perfectly reasonable with virtual threads. However, consider using the
safer and more efficient scoped values. See Scoped Values for more
information.

Here : https://docs.oracle.com/en/java/javase/21/core/virtual-threads.html#GUID-68216B85-7B43-423E-91BA-11489B1ACA61
But i also remembered that Spring Security uses ThreadLocal to save the SecurityContext of a given request:

By default, SecurityContextHolder uses a ThreadLocal to store these
details, which means that the SecurityContext is always available to
methods in the same thread, even if the SecurityContext is not
explicitly passed around as an argument to those methods. Using a
ThreadLocal in this way is quite safe if you take care to clear the
thread after the present principalâ€™s request is processed. Spring
Securityâ€™s FilterChainProxy ensures that the SecurityContext is always
cleared.

Docs : https://docs.spring.io/spring-security/reference/servlet/authentication/architecture.html
So the question is : is it safe to use virtual threads in a Spring Boot REST Application with endpoints that do require authentication and authorization and therefor have a SecurityContext ? Is this considered a pitfall ?
Thanks !
","While it is possible 1) to implement a custom SecurityContextHolderStrategy which retrieves SecurityContext from a ScopedValue and saves it there:
public class ScopedSecurityContextHolderStrategy implements SecurityContextHolderStrategy {
    private static final ScopedValue&lt;SecurityContextScopedValueHolder&gt; SECURITY_CONTEXT = ScopedValue.newInstance();

    private static class SecurityContextScopedValueHolder {
        
        private SecurityContext securityContext;

        public SecurityContext getSecurityContext() {
            return securityContext;
        }

        public void setSecurityContext(SecurityContext securityContext) {
            this.securityContext = securityContext;
        }

    }
    
    @Override
    public void clearContext() {
        retrieveSecurityContextScopedValueHolder().setSecurityContext(null);
    }

    @Override
    public SecurityContext getContext() {
        return retrieveSecurityContextScopedValueHolder().getSecurityContext();
    }

    @Override
    public void setContext(SecurityContext context) {
        retrieveSecurityContextScopedValueHolder().setSecurityContext(context);
    }

    @Override
    public SecurityContext createEmptyContext() {
        return new SecurityContextImpl();
    }
    
    private SecurityContextScopedValueHolder retrieveSecurityContextScopedValueHolder() {
        if (SECURITY_CONTEXT.isBound()) {
            return SECURITY_CONTEXT.get();
        } else {
            throw new IllegalStateException(&quot;Security Context Scoped Value not bound&quot;);
        }
    }
    
    public static ScopedValue.Carrier getSecuriyContextCarrier() {
        return ScopedValue.where(SECURITY_CONTEXT, new SecurityContextScopedValueHolder());
    }

}  

and 2) configure Tomcat to start a virtual thread with the ScopedValue, bound to it:
@Component
public class TomcatVirtualThreadExecutorCustomizer 
        implements WebServerFactoryCustomizer&lt;TomcatServletWebServerFactory&gt; {

    private static class ScopedVirtualThreadExecutor extends VirtualThreadExecutor {

        public ScopedVirtualThreadExecutor(String namePrefix) {
            super(namePrefix);
        }

        @Override
        public void execute(Runnable command) {
            super.execute(() -&gt; ScopedSecurityContextHolderStrategy.getSecuriyContextCarrier().run(command));
        }

    }

    @Override
    public void customize(TomcatServletWebServerFactory factory) {
        factory.addProtocolHandlerCustomizers((protocolHandler) -&gt; protocolHandler
                .setExecutor(new ScopedVirtualThreadExecutor(&quot;tomcat-handler-&quot;)));
    }

}

it is easy to see, however, a substantial awkwardness in such approach.
First, the approach is tightly bound to type of web server/servlet container, Tomcat in our case. The solution for other servers, like Undertow or Jetty, might be different if at all possible.
Second, Spring Security is a ubiquitous thing that SecurityContext is meant to be used everywhere, not only on server's worker threads. For example, there might be a need to setup a SecurityContext on a cron/scheduler thread or just on a thread, managed by a standalone Executor. ScopeValue-based approach will require similar binding of it to such thread, while with a standard ThreadLocal-bound SecurityContextHolderStrategy the context can be set without any thread tweaking.
All in all, this technique introduces some not-very-welcome coupling between the code which creates a thread and the code which sets/retrieves SecurityContext.
From conceptual standpoint, I'd daresay that the concepts of Structured Programming and Spring Security don't get along with each other very well - at least in current versions of both.
The small POC Spring Boot project is available here.
Note that the example works for Spring Boot 3.2.2, its applicability to earlier and later versions is not guaranteed as things with Loom are rather volatile at the moment.

Yet another approach is a replacement of Spring Security's stock SecurityContextHolderFilter with a custom one which uses ScopedSecurityContextHolderStrategy, discussed above:
public class ScopedSecurityContextHolderFilter extends SecurityContextHolderFilter {
    
    ...

    @Override
    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain)
            throws IOException, ServletException {
        doFilter((HttpServletRequest) request, (HttpServletResponse) response, chain);
    }

    private void doFilter(HttpServletRequest request, HttpServletResponse response, FilterChain chain)
            throws ServletException, IOException {
        if (request.getAttribute(FILTER_APPLIED) != null) {
            chain.doFilter(request, response);
            return;
        }
        request.setAttribute(FILTER_APPLIED, Boolean.TRUE);
        DeferredSecurityContext deferredContext = securityContextRepository.loadDeferredContext(request);
        try {
            ScopedSecurityContextHolderStrategy.runWhere(deferredContext, () -&gt; {
                securityContextHolderStrategy.setDeferredContext(deferredContext);
                try {
                    chain.doFilter(request, response);
                } catch (IOException | ServletException e) {
                    throw new RuntimeException(e);
                }
            });
        } catch (RuntimeException e) {
            final Throwable cause = e.getCause();
            if (cause instanceof ServletException)
                throw (ServletException)cause;
            if (cause instanceof IOException)
                throw (IOException)cause;
            throw e;
        } finally {
            request.removeAttribute(FILTER_APPLIED);
        }
    }   
    
    ... 
    
}

In this scenario, a ScopedValue is bound to a thread not at the point of its initiation in Web Server/Servlet Container (Tomcat), but at arbitrary point up-stack of such initiation.
This allows to avoid the dependency of Web Server/Servlet Container (Tomcat), but brings another issues.
First, String Security Filter Chain implementation, FilterChainProxy, invokes Security Context clearing, SecurityContextHolderStrategy.clearContext() method, at the end of Security Filter Chain executing, thus employing a free, unrestricted  ThreadLocal design. Evidently, more restrictive ScopedValue design comes into conflict with the Spring Security design and Security Filters, that execute after  ScopedValue gets unbound from the thread,   appear top be SecurityContext-less, and whether this might be an issue is difficult to say in general.
Second issue is associated with the replacement of SecurityContextHolderFilter itself. Spring Security Filters configuration uses the correspondent SecurityContextConfigurer instance directly, as a source of SecurityContextRepository. Therefore, certain tricks are necessary to implement this replacement. One of such solutions is brought and discussed in the POC example, mentioned above, it is probably as &quot;hacky&quot; as its equivalents.
Finally, the implementation of ScopedSecurityContextHolderFilter is bound to be a shameless copy-paste job from SecurityContextHolderFilter, which also compromises upgradability and maintainability of the solution.
All in all, the approach of custom Spring Security Filter turns out to be even more questionable than the one that involves Tomcat Executor service customization.
"
"I have a class that builds a grid with an array of TextFields using GridPane. I need to insert this grid into a ScrollPane that only accepts Node in the setContent() method. So I extend this class from  GridPane. The Grid class is instantiated and set in the ScrollPane by the onMnuItemNewAction method of the MainViewController.java class, but the grid is not shown. Thanks for your help.
MainView.fxml
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;?import javafx.scene.control.Menu?&gt;
&lt;?import javafx.scene.control.MenuBar?&gt;
&lt;?import javafx.scene.control.MenuItem?&gt;
&lt;?import javafx.scene.control.ScrollPane?&gt;
&lt;?import javafx.scene.layout.BorderPane?&gt;
&lt;?import javafx.scene.layout.VBox?&gt;

&lt;BorderPane prefHeight=&quot;277.0&quot; prefWidth=&quot;495.0&quot; xmlns=&quot;http://javafx.com/javafx/17&quot; xmlns:fx=&quot;http://javafx.com/fxml/1&quot; 
fx:controller=&quot;br.com.ablogic.crossword.MainViewController&quot;&gt;
    &lt;top&gt;
       &lt;VBox prefWidth=&quot;100.0&quot; BorderPane.alignment=&quot;CENTER&quot;&gt;
         &lt;children&gt;
            &lt;MenuBar fx:id=&quot;mnuBar&quot; prefHeight=&quot;25.0&quot; prefWidth=&quot;360.0&quot;&gt;
              &lt;menus&gt;
                &lt;Menu mnemonicParsing=&quot;false&quot; text=&quot;File&quot;&gt;
                  &lt;items&gt;
                    &lt;MenuItem fx:id=&quot;mnuItemNew&quot; mnemonicParsing=&quot;false&quot; onAction=&quot;#onMnuItemNewAction&quot; text=&quot;New grid&quot; /&gt;
                  &lt;/items&gt;
                &lt;/Menu&gt;
              &lt;/menus&gt;
            &lt;/MenuBar&gt;
         &lt;/children&gt;
      &lt;/VBox&gt;
   &lt;/top&gt;
   &lt;center&gt;
      &lt;ScrollPane fx:id=&quot;scpGrid&quot; fitToHeight=&quot;true&quot; fitToWidth=&quot;true&quot; pannable=&quot;true&quot; style=&quot;-fx-background-color: #dbbb92; -fx-background: #dbbb92;&quot; BorderPane.alignment=&quot;CENTER&quot; /&gt;
   &lt;/center&gt;
&lt;/BorderPane&gt;

Main.java
import javafx.application.Application;
import javafx.fxml.FXMLLoader;
import javafx.scene.Scene;
import javafx.stage.Stage;
import java.io.IOException;

public class Main extends Application {
    @Override
    public void start(Stage stage) throws IOException {

        FXMLLoader fxmlLoader = new FXMLLoader(Main.class.getResource(&quot;MainView.fxml&quot;));
        Scene scene = new Scene(fxmlLoader.load(), 800, 600);
        stage.setTitle(&quot;Grid Demo&quot;);
        stage.setScene(scene);
        stage.show();
    }

    public static void main(String[] args) {
        launch();
    }

}

MainViewController.java (the calling method)
import javafx.geometry.Pos;
import javafx.scene.control.MenuItem;
import javafx.scene.control.ScrollPane;
import javafx.fxml.FXML;
import javafx.fxml.Initializable;
import java.net.URL;
import java.util.ResourceBundle;

public class MainViewController implements Initializable {

    @FXML
    private MenuItem mnuItemNew;

    @FXML
    private ScrollPane scpGrid;

    @FXML
    public void onMnuItemNewAction() {
        int cols = 10;
        int rows = 10;
        int horizontalGap = 1;
        int verticalGap = 1;
        int fieldHorizontalSize = 40;
        int fieldVerticalSize = 40;
        var newGrid = new Grid(cols, rows, horizontalGap, verticalGap, fieldHorizontalSize, fieldVerticalSize);
        scpGrid.setContent(newGrid);
        newGrid.setAlignment(Pos.CENTER);
    }

    @Override
    public void initialize(URL url, ResourceBundle rb) {

    }

}

Grid.java
import javafx.fxml.Initializable;
import javafx.scene.control.TextField;
import javafx.scene.layout.GridPane;
import java.net.URL;
import java.util.ResourceBundle;

public class Grid extends GridPane implements Initializable {
    private final int totalColumnFields;
    private final int totalRowFields;
    private final int horizontalGap;
    private final int verticalGap;
    private final int fieldHorizontalSize;
    private final int fieldVerticalSize;
        
    public Grid(int totalColumnFields, int totalRowFields, int horizontalGap, int verticalGap, int fieldHorizontalSize, int fieldVerticalSize) {
        this.totalColumnFields = totalColumnFields;
        this.totalRowFields = totalRowFields;
        this.horizontalGap = horizontalGap;
        this.verticalGap = verticalGap;
        this.fieldHorizontalSize = fieldHorizontalSize;
        this.fieldVerticalSize = fieldVerticalSize;
    }

    @Override
    public void initialize(URL url, ResourceBundle rb) {

        this.setHgap(horizontalGap);
        this.setVgap(verticalGap);
        TextField[][] arrayLetterField = new TextField[totalColumnFields][totalRowFields];

        for (int row = 0; row &lt; totalRowFields; row++) {
            for (int col = 0; col &lt; totalColumnFields; col++) {
                arrayLetterField[col][row] = new TextField();
                arrayLetterField[col][row].setMinSize(fieldHorizontalSize, fieldVerticalSize);
                arrayLetterField[col][row].setMaxSize(fieldHorizontalSize, fieldVerticalSize );
                this.add(arrayLetterField[col][row], col, row);
            }
        }            
    }    
}

","[TLDR]: The initialize(...) method in Grid is never called, so the text fields are never created and added to the grid pane. Consequently, even though the grid pane is displayed, there is nothing in it and so nothing is visible.

The Initializable interface and its corresponding void initialize(URL, ResourceBundle) method are intended for use by controller classes acting as controllers for FXML documents. When the FXMLLoader loads an FXML file which specifies a class in its fx:controller attribute, the FXMLLoader instantiates that class and then invokes the initialize(...) method on it.1
Your Grid class is not a controller class for any FXML document. It is not instantiated by an FXMLLoader (you instantiate it directly by calling var newGrid = new Grid(...) in the MainViewController class) and so the initialize(...) method is not automatically invoked for you at any point.
Consequently, the initialize() method in Grid is never called, so the text fields are never created and never added to the grid pane. So the grid you add to the scroll pane is empty, and nothing is visible.
There is no need for the Grid class to implement Initializable, since it is not associated with an FXML file. The code in the initialize() method is code you want to be executed when a Grid is created, so move it to the constructor:
import javafx.scene.control.TextField;
import javafx.scene.layout.GridPane;

public class Grid extends GridPane {
    private final int totalColumnFields;
    private final int totalRowFields;
    private final int horizontalGap;
    private final int verticalGap;
    private final int fieldHorizontalSize;
    private final int fieldVerticalSize;

    public Grid(int totalColumnFields, int totalRowFields, int horizontalGap, int verticalGap, int fieldHorizontalSize, int fieldVerticalSize) {
        this.totalColumnFields = totalColumnFields;
        this.totalRowFields = totalRowFields;
        this.horizontalGap = horizontalGap;
        this.verticalGap = verticalGap;
        this.fieldHorizontalSize = fieldHorizontalSize;
        this.fieldVerticalSize = fieldVerticalSize;
        this.setHgap(horizontalGap);
        this.setVgap(verticalGap);
        TextField[][] arrayLetterField = new TextField[totalColumnFields][totalRowFields];

        for (int row = 0; row &lt; totalRowFields; row++) {
            for (int col = 0; col &lt; totalColumnFields; col++) {
                arrayLetterField[col][row] = new TextField();
                arrayLetterField[col][row].setMinSize(fieldHorizontalSize, fieldVerticalSize);
                arrayLetterField[col][row].setMaxSize(fieldHorizontalSize, fieldVerticalSize );
                this.add(arrayLetterField[col][row], col, row);
            }
        }
    }

}

This gives the desired result:


Other comments on your code:
Note there is no need to replicate the values horizontalGap and verticalGap in your class, since these are already stored as the hgap and vgap properties inherited from GridPane. So you can reduce the size of your class a little with:
public class Grid extends GridPane {
    private final int totalColumnFields;
    private final int totalRowFields;
    private final int fieldHorizontalSize;
    private final int fieldVerticalSize;

    public Grid(int totalColumnFields, int totalRowFields, int horizontalGap, int verticalGap, int fieldHorizontalSize, int fieldVerticalSize) {
        this.totalColumnFields = totalColumnFields;
        this.totalRowFields = totalRowFields;
        this.fieldHorizontalSize = fieldHorizontalSize;
        this.fieldVerticalSize = fieldVerticalSize;
        this.setHgap(horizontalGap);
        this.setVgap(verticalGap);
        TextField[][] arrayLetterField = new TextField[totalColumnFields][totalRowFields];

        for (int row = 0; row &lt; totalRowFields; row++) {
            for (int col = 0; col &lt; totalColumnFields; col++) {
                arrayLetterField[col][row] = new TextField();
                arrayLetterField[col][row].setMinSize(fieldHorizontalSize, fieldVerticalSize);
                arrayLetterField[col][row].setMaxSize(fieldHorizontalSize, fieldVerticalSize );
                this.add(arrayLetterField[col][row], col, row);
            }
        }
    }
}

If you need to reference those values at any time, you can do so with getHgap() and getVgap().
I also recommend not subclassing GridPane here. You should reserve subclassing existing classes when you are adding functionality to them. Here you are really only configuring an instance of the existing class. Subclassing GridPane also exposes the internal details of the layout strategy to the rest of your application, potentially making it much more difficult to change the layout later (e.g. to a TilePane or some other strategy) if you wanted to do so. I recommend &quot;favoring aggregation over inheritance&quot; here and just giving access to an aggregated GridPane without exposing details of which layout you are using:
import javafx.scene.Node;
import javafx.scene.control.TextField;
import javafx.scene.layout.GridPane;

public class Grid {
    private final int totalColumnFields;
    private final int totalRowFields;
    private final int fieldHorizontalSize;
    private final int fieldVerticalSize;

    private final GridPane grid;

    public Grid(int totalColumnFields, int totalRowFields, int horizontalGap, int verticalGap, int fieldHorizontalSize, int fieldVerticalSize) {
        this.totalColumnFields = totalColumnFields;
        this.totalRowFields = totalRowFields;
        this.fieldHorizontalSize = fieldHorizontalSize;
        this.fieldVerticalSize = fieldVerticalSize;

        grid = new GridPane();
        grid.setHgap(horizontalGap);
        grid.setVgap(verticalGap);
        TextField[][] arrayLetterField = new TextField[totalColumnFields][totalRowFields];

        for (int row = 0; row &lt; totalRowFields; row++) {
            for (int col = 0; col &lt; totalColumnFields; col++) {
                arrayLetterField[col][row] = new TextField();
                arrayLetterField[col][row].setMinSize(fieldHorizontalSize, fieldVerticalSize);
                arrayLetterField[col][row].setMaxSize(fieldHorizontalSize, fieldVerticalSize );
                grid.add(arrayLetterField[col][row], col, row);
            }
        }
    }

    public Node getView() {
        return grid;
    }

}

And then the slight corresponding change to the client code:
    @FXML
    public void onMnuItemNewAction() {
        int cols = 10;
        int rows = 10;
        int horizontalGap = 1;
        int verticalGap = 1;
        int fieldHorizontalSize = 40;
        int fieldVerticalSize = 40;
        var newGrid = new Grid(cols, rows, horizontalGap, verticalGap, fieldHorizontalSize, fieldVerticalSize);
        var gridView = newGrid.getView();
        scpGrid.setContent(gridView);
        gridView.setStyle(&quot;-fx-alignment: center;&quot;);
    }


(1) Note that as of JavaFX 2.1 the Initializable interface is essentially redundant. From the documentation:

NOTE This interface has been superseded by automatic injection of location and resources properties into the controller. FXMLLoader will now automatically call any suitably annotated no-arg initialize() method defined by the controller. It is recommended that the injection approach be used whenever possible.

This means that even a controller class for an FXML document does not need to implement Initializable. If you need to perform initialization after the @FXML-annotated fields have been injected, just define a no-arg initialize() method to do so. You can even make this method private if you annotate it @FXML, better enforcing encapsulation. If you need access to the location or resources properties, those can be injected in the same way as the elements of the FXML file. For example:
public class MainViewController {

    @FXML
    private MenuItem mnuItemNew;

    @FXML
    private ScrollPane scpGrid;
    
    @FXML
    // Can omit this field if it is not needed
    // (It is very rare to need this.)
    private URL location;

    @FXML
    private void onMnuItemNewAction() {
        int cols = 10;
        int rows = 10;
        int horizontalGap = 1;
        int verticalGap = 1;
        int fieldHorizontalSize = 40;
        int fieldVerticalSize = 40;
        var newGrid = new Grid(cols, rows, horizontalGap, verticalGap, fieldHorizontalSize, fieldVerticalSize);
        var gridView = newGrid.getView();
        scpGrid.setContent(gridView);
        gridView.setStyle(&quot;-fx-alignment: center;&quot;);
    }

    @FXML
    private void initialize() {
        // Any required initialization code here
        // If no intialization needed, this method can be omitted
    }

}

"
"I'm developing an app for Arabic users so i have set:
root.setNodeOrientation(NodeOrientation.RIGHT_TO_LEFT);

This works fine when text fields contain right-to-left text, e.g., Arabic. However, it breaks the caret logic (moving the caret using keyboard left/right arrows) whenever the field contains numbers or Latin text.
Here is a demo:
import javafx.application.Application;
import javafx.geometry.NodeOrientation;
import javafx.geometry.Pos;
import javafx.scene.Scene;
import javafx.scene.control.TextField;
import javafx.scene.layout.VBox;
import javafx.stage.Stage;


public class NodeOrientationDemo extends Application {
    @Override
    public void start(Stage primaryStage) {
        TextField numbersTextField = new TextField(&quot;0123456789&quot;);
        numbersTextField.setMaxSize(80, 30);

        TextField latinTextField = new TextField(&quot;Hello&quot;);
        latinTextField.setMaxSize(80, 30);

        TextField arabicTextField = new TextField(&quot;Ù…Ø±Ø­Ø¨Ø§&quot;);
        arabicTextField.setMaxSize(80, 30);

        VBox root = new VBox(10, numbersTextField, latinTextField, arabicTextField);
        root.setAlignment(Pos.CENTER);
        root.setNodeOrientation(NodeOrientation.RIGHT_TO_LEFT);

        primaryStage.setScene(new Scene(root, 400, 400));
        primaryStage.setTitle(&quot;NodeOrientation&quot;);
        primaryStage.show();
    }
}

Is this a known bug? And how can I solve it?
Any help is appreciated, thanks in advance!
Update 1:
I have added event filters to all the text fields and they seem to work fine. However, only the Arabic text navigation is reversed (left should be right, right should be left) my plan is to detect if it is Arabic text and based on that I will add/subtract the caret position.
import javafx.application.Application;
import javafx.geometry.NodeOrientation;
import javafx.geometry.Pos;
import javafx.scene.Scene;
import javafx.scene.control.TextField;
import javafx.scene.input.KeyCode;
import javafx.scene.input.KeyEvent;
import javafx.scene.layout.VBox;
import javafx.stage.Stage;


public class NodeOrientationDemo extends Application {
    @Override
    public void start(Stage primaryStage) {
        TextField numbersTextField = new TextField(&quot;0123456789&quot;);
        numbersTextField.setMaxSize(80, 30);
        keyboardNavigation(numbersTextField);

        TextField latinTextField = new TextField(&quot;Hello&quot;);
        latinTextField.setMaxSize(80, 30);
        keyboardNavigation(latinTextField);

        TextField arabicTextField = new TextField(&quot;Ù…Ø±Ø­Ø¨Ø§&quot;);
        arabicTextField.setMaxSize(80, 30);
        keyboardNavigation(arabicTextField);

        VBox root = new VBox(10, numbersTextField, latinTextField, arabicTextField);
        root.setAlignment(Pos.CENTER);
        root.setNodeOrientation(NodeOrientation.RIGHT_TO_LEFT);

        primaryStage.setScene(new Scene(root, 400, 400));
        primaryStage.setTitle(&quot;NodeOrientation&quot;);
        primaryStage.show();
    }

    private void keyboardNavigation(TextField textField) {
        textField.addEventFilter(KeyEvent.KEY_PRESSED, event -&gt; {
            if (event.getCode() == KeyCode.LEFT) {
                textField.positionCaret(textField.getCaretPosition() - 1);
                event.consume();
            } else if (event.getCode() == KeyCode.RIGHT) {
                textField.positionCaret(textField.getCaretPosition() + 1);
                event.consume();
            }
        });
    }
}

Update 2:
I have implemented the plan in update 1 and it looks good. However, one remaining issue when the text field contains both Arabic text and Latin/numbers then the behavior is a bit unclear.

import javafx.application.Application;
import javafx.geometry.NodeOrientation;
import javafx.geometry.Pos;
import javafx.scene.Scene;
import javafx.scene.control.TextField;
import javafx.scene.input.KeyCode;
import javafx.scene.input.KeyEvent;
import javafx.scene.layout.VBox;
import javafx.stage.Stage;


public class NodeOrientationDemo extends Application {
    @Override
    public void start(Stage primaryStage) {
        TextField numbersTextField = new TextField(&quot;0123456789&quot;);
        numbersTextField.setMaxSize(80, 30);
        keyboardNavigation(numbersTextField);

        TextField latinTextField = new TextField(&quot;Hello&quot;);
        latinTextField.setMaxSize(80, 30);
        keyboardNavigation(latinTextField);

        TextField arabicTextField = new TextField(&quot;Ù…Ø±Ø­Ø¨Ø§&quot;);
        arabicTextField.setMaxSize(80, 30);
        keyboardNavigation(arabicTextField);

        VBox root = new VBox(10, numbersTextField, latinTextField, arabicTextField);
        root.setAlignment(Pos.CENTER);
        root.setNodeOrientation(NodeOrientation.RIGHT_TO_LEFT);

        primaryStage.setScene(new Scene(root, 400, 400));
        primaryStage.setTitle(&quot;NodeOrientation&quot;);
        primaryStage.show();
    }

    private void keyboardNavigation(TextField textField) {
        textField.addEventFilter(KeyEvent.KEY_PRESSED, event -&gt; {
            final int pos = clamp(1, textField.getCaretPosition(), textField.getLength());
            final boolean isArabic = isArabicCharacter(textField.getText().charAt(pos - 1));
            if (event.getCode() == KeyCode.LEFT) {
                textField.positionCaret(textField.getCaretPosition() + (isArabic ? 1 : -1));
                event.consume();
            } else if (event.getCode() == KeyCode.RIGHT) {
                textField.positionCaret(textField.getCaretPosition() + (isArabic ? -1 : 1));
                event.consume();
            }
        });
    }

    public static boolean isArabicCharacter(char c) {
        return Character.UnicodeBlock.of(c) == Character.UnicodeBlock.ARABIC;
    }

    public static int clamp(int min, int value, int max) {
        return value &lt; min ? min : Math.min(value, max);
    }
}

","I foolishly thought update 2 (in the question) would do the trick, however, that wasn't the case. According to what I have tried, you simply cannot get rid of the &quot;revers logic&quot;. Instead, you either have it in the Arabic text or vice versa.
The following code did the job for me:
import javafx.application.Application;
import javafx.geometry.NodeOrientation;
import javafx.geometry.Pos;
import javafx.scene.Scene;
import javafx.scene.control.TextField;
import javafx.scene.input.KeyCode;
import javafx.scene.input.KeyEvent;
import javafx.scene.layout.VBox;
import javafx.stage.Stage;
import org.jetbrains.annotations.NotNull;


public class NodeOrientationDemo extends Application {
    @Override
    public void start(Stage primaryStage) {
        TextField numbersTextField = new TextField(&quot;0123456789&quot;);
        numbersTextField.setMaxSize(80, 30);
        keyboardNavigation(numbersTextField);

        TextField latinTextField = new TextField(&quot;Hello&quot;);
        latinTextField.setMaxSize(80, 30);
        keyboardNavigation(latinTextField);

        TextField arabicTextField = new TextField(&quot;مرحبا&quot;);
        arabicTextField.setMaxSize(80, 30);
        keyboardNavigation(arabicTextField);

        VBox root = new VBox(10, numbersTextField, latinTextField, arabicTextField);
        root.setAlignment(Pos.CENTER);
        root.setNodeOrientation(NodeOrientation.RIGHT_TO_LEFT);

        primaryStage.setScene(new Scene(root, 400, 400));
        primaryStage.setTitle(&quot;NodeOrientation&quot;);
        primaryStage.show();
    }

    private void keyboardNavigation(@NotNull TextField textField) {
        textField.addEventFilter(KeyEvent.KEY_PRESSED, event -&gt; {
            final KeyCode code = event.getCode();
            if (code == KeyCode.LEFT || code == KeyCode.RIGHT) {
                boolean containsArabicText = false;
                for (char c : textField.getText().toCharArray()) {
                    if (Character.UnicodeBlock.of(c) == Character.UnicodeBlock.ARABIC) {
                        containsArabicText = true;
                        break;
                    }
                }
                textField.positionCaret(textField.getCaretPosition() + (containsArabicText ? 1 : -1) * (code == KeyCode.RIGHT ? -1 : 1));
                event.consume();
            }
        });
    }
}

"
"I have a Spring boot project with version 2.6.4.
And after I updated the jasperreports dependency to 6.19.0 all my RestControllers returns now XML instead of JSON
Where can I change this, without changing to
@GetMapping(produces = {&quot;application/json&quot;})

on each method?
","I just have same issue today, I checked with Chrome and saw it doesn't add application/json in Accept header.
My solution is create a wrapper filter:
@Component
public class JsonRequestHeaderFilter implements Filter {

    @Override
    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain)
            throws IOException, ServletException {
        HttpServletRequestWrapper requestWrapper = new HttpServletRequestWrapper((HttpServletRequest) request) {
            @Override
            public Enumeration&lt;String&gt; getHeaders(String name) {
                if (name.equals(&quot;Accept&quot;)) {
                    Set&lt;String&gt; customHeaders = new HashSet&lt;String&gt;();
                    Enumeration&lt;String&gt; curHeaders = super.getHeaders(name);
                    while (curHeaders.hasMoreElements()) {
                        String header = curHeaders.nextElement();
                        customHeaders.add(MediaType.APPLICATION_JSON_VALUE.concat(&quot;;&quot;).concat(header));
                    }

                    return Collections.enumeration(customHeaders);
                }
                return super.getHeaders(name);
            }
        };

        chain.doFilter(requestWrapper, response);
    }
}

"
"I'm trying to integrate the updated Spring Security in my project, instead of using the deprecated extending WebSecurityConfigurerAdapter. I've set up a good system in which the user gets authenticated (User implements UserDetails - I am using Hibernate) and a token gets generated. I get a 200 on this login and receive a token. This authetication part works fine.
Now the problem is that my users have roles (like ADMIN, USER, ...) These roles are added to the generated token. My controllers get the @PreAuthorize annotation. The request cannot pass these annotation and get a forbidden. When I don't use the @PreAuthorize, the requests get validated with the token.
@Configuration
@EnableWebSecurity
@EnableMethodSecurity
public class SecurityConfig {
    private RSAKey rsaKey;
    private final DefaultUserDetailsService defaultUserDetailsService;
    
    public SecurityConfig(DefaultUserDetailsService defaultUserDetailsService) {
        this.defaultUserDetailsService = defaultUserDetailsService;
    }
    
    @Bean
    public AuthenticationManager authenticationManager(AuthenticationConfiguration authenticationConfiguration) throws Exception {
        return authenticationConfiguration.getAuthenticationManager();
    }

    @Bean
    public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception {
        
    return http 
               .cors(Customizer.withDefaults())
               .csrf(AbstractHttpConfigurer::disable)
               .authorizeHttpRequests(auth -&gt; auth
                   .requestMatchers(&quot;/auth/**&quot;).permitAll()
                   .anyRequest().authenticated()
               )            
               .userDetailsService(defaultUserDetailsService)
               .sessionManagement(session -&gt;  session.sessionCreationPolicy(SessionCreationPolicy.STATELESS))
               .oauth2ResourceServer(OAuth2ResourceServerConfigurer::jwt)
               .headers(headers -&gt; headers
                   .frameOptions().sameOrigin()
               )
               .httpBasic(withDefaults())
               .build();
    }
    
    @Bean
    public JWKSource&lt;SecurityContext&gt; jwkSource() {
        rsaKey = Jwks.generateRsa();
        JWKSet jwkSet = new JWKSet(rsaKey);
        return (jwkSelector, securityContext) -&gt; jwkSelector.select(jwkSet);
    }
    
    @Bean
    JwtDecoder jwtDecoder() throws JOSEException {
        return NimbusJwtDecoder.withPublicKey(rsaKey.toRSAPublicKey()).build();
   }
    
    @Bean
    JwtEncoder jwtEncoder(JWKSource&lt;SecurityContext&gt; jwks) {
        return new NimbusJwtEncoder(jwks);
    }
    
    @Bean
    public PasswordEncoder getPasswordEncoder() {
        return new BCryptPasswordEncoder();
    }
        
    @Bean
    CorsConfigurationSource corsConfigurationSource() {
        CorsConfiguration configuration = new CorsConfiguration();
        configuration.setAllowedOrigins(List.of(&quot;http://localhost:4200&quot;));
        configuration.setAllowedMethods(List.of(&quot;GET&quot;,&quot;POST&quot;,&quot;DELETE&quot;));
        configuration.setAllowedHeaders(List.of(&quot;Authorization&quot;,&quot;Content-Type&quot;));
        UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource();
        source.registerCorsConfiguration(&quot;/**&quot;,configuration);
        return source;
    }

}

@Component
public class KeyGeneratorUtils {

    private KeyGeneratorUtils() {}

    static KeyPair generateRsaKey() {
        KeyPair keyPair;
        try {
            KeyPairGenerator keyPairGenerator = KeyPairGenerator.getInstance(&quot;RSA&quot;);
            keyPairGenerator.initialize(2048);
            keyPair = keyPairGenerator.generateKeyPair();
        } catch (Exception ex) {
            throw new IllegalStateException(ex);
        }
        return keyPair;
    }
}


public class Jwks {
       private Jwks() {}

        public static RSAKey generateRsa() {
            KeyPair keyPair = KeyGeneratorUtils.generateRsaKey();
            RSAPublicKey publicKey = (RSAPublicKey) keyPair.getPublic();
            RSAPrivateKey privateKey = (RSAPrivateKey) keyPair.getPrivate();
            return new RSAKey.Builder(publicKey)
                    .privateKey(privateKey)
                    .keyID(UUID.randomUUID().toString())
                    .build();
        }
}


@Service
public class DefaultTokenService implements TokenService {
    private final JwtEncoder encoder;

    public DefaultTokenService(JwtEncoder encoder) {
        this.encoder = encoder;
    }
    
    @Override
    public String generateToken(Authentication authentication) {
        Instant now = Instant.now();
        String scope = authentication.getAuthorities().stream()
                .map(GrantedAuthority::getAuthority)
                .collect(Collectors.joining(&quot; &quot;));
        
        System.out.println(&quot;scope: &quot; + scope);
        
        JwtClaimsSet claims = JwtClaimsSet.builder()
                .issuer(&quot;self&quot;)
                .issuedAt(now)
                .expiresAt(now.plus(1, ChronoUnit.HOURS))
                .subject(authentication.getName())
                .claim(&quot;scope&quot;, scope)
                .build();
        return this.encoder.encode(JwtEncoderParameters.from(claims)).getTokenValue();
    }
}


public class UserDetailsImpl implements UserDetails{
      private static final long serialVersionUID = 1L;
      private final Long id;
      private final String username;
      private final String riziv;
      private final boolean verified;
      @JsonIgnore
      private final String password;
      private final Collection&lt;? extends GrantedAuthority&gt; authorities;
    
        public UserDetailsImpl(Long id, String username, String riziv, String password,
                Collection&lt;? extends GrantedAuthority&gt; authorities, boolean verified) {
                this.id = id;
                this.username = username;
                this.riziv = riziv;
                this.password = password;
                this.authorities = authorities;
                this.verified = verified;
            }

        public static UserDetailsImpl build(AuthUser authUser) {
            List&lt;GrantedAuthority&gt; authorities = new ArrayList&lt;&gt;();
            authorities.add(new SimpleGrantedAuthority(authUser.getRol().toString()));
          
            
            return new UserDetailsImpl(
                    authUser.getId(),
                    authUser.getUsername(),
                    authUser.getRiziv(),
                    authUser.getPassword(),
                    authorities, authUser.isVerified());
        }
        @Override
        public Collection&lt;? extends GrantedAuthority&gt; getAuthorities() {
            return authorities;
        }
        public Long getId() {
            return id;
        }
        public boolean isVerified() {
            return verified;
        }
        public String getRiziv() {
            return riziv;
        }
        @Override
        public String getUsername() {
            return username;
        }
        @Override
        public String getPassword() {
            return password;
        }
        @Override
        public boolean isAccountNonExpired() {
            return true;
        }
        @Override
        public boolean isAccountNonLocked() {
            return true;
        }
        @Override
        public boolean isCredentialsNonExpired() {
            return true;
        }
        @Override
        public boolean isEnabled() {
            return true;
        }
        @Override
        public boolean equals(Object o) {
            if (this == o)
                return true;
            if (o == null || getClass() != o.getClass())
                return false;
            UserDetailsImpl klant = (UserDetailsImpl) o;
            return Objects.equals(id, klant.id);
        }

}

@Service
public class DefaultUserDetailsService implements UserDetailsService {
    private final AuthUserService authUserService;
    
    public DefaultUserDetailsService(AuthUserService authUserService) {
        this.authUserService = authUserService;
    }


    @Override
    public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException {
        AuthUser authUser = authUserService.findByUsername(username)
                .orElseThrow(() -&gt; new UsernameNotFoundException(&quot;User Not Found with username: &quot; + username));

        return UserDetailsImpl.build(authUser);
    }

}

    @PreAuthorize(&quot;hasAnyRole('USER', 'ADMIN')&quot;)

I am making a configuration mistake somewhere, but I cannot seem to find it. Spring docs are very very hard to figure out, but I have been reading them relentlessly. There is also not a lot of clear information on these topics yet. I can find youtube videos tutorials and some related topics, but they only explain small parts, never a full setup.
I have added below my securityConfig, KeyGenerator, Jwks and tokengenerate service. I also just added the Userdetailsimpl and service. I build my userdetailsImpl out of a user with a static build method. It might seem a strange construction but it works, it is because I did the security last and didn't think of it before. Also I added an example of my @Preauthorize.
I am very close and this could be a good example for other users trying to implement this, because I seem not te able to find an example somewhere.Does anyone have experience with setting the Spring Boot 3 security up and can they tell me how I am misconfiguring? Why is my role not getting 'read' by the @PreAuthorize?
","Okay so here's the thing, since you're implementing resource server, the class:
org.springframework.security.oauth2.server.resource.authentication.JwtGrantedAuthoritiesConverter 

is the one responsible for converting your scopes inside JWT token to granted authorities.
Now, this class prepends all the authorities with SCOPE_ prefix.
Since you're using
hasAnyRole('ADMIN', 'USER',...)

this method internally invokes,
hasAnyAuthorityName(defaultRolePrefix, roleName) 

method with the defaultRolePrefix as ROLE_ and the roleName as your passed in value(s).
Internal implementation:
@Override
public final boolean hasAnyRole(String... roles) {
    return hasAnyAuthorityName(this.defaultRolePrefix, roles);
}

@Override
public final boolean hasAnyAuthority(String... authorities) {
    return hasAnyAuthorityName(null, authorities);
}

On the other hand, the hasAnyAuthority method makes a call to the same method but with null passed in to the defaultRolePrefix.
Now since you're using:
.oauth2ResourceServer(OAuth2ResourceServerConfigurer::jwt)

in security config, it is using the default AuthenticationConverter for your JWT token which is
org.springframework.security.oauth2.server.resource.authentication.JwtAuthenticationConverter

which further invokes
org.springframework.security.oauth2.server.resource.authentication.JwtGrantedAuthoritiesConverter

As per the implementation in JwtGrantedAuthoritiesConverter, all the scopes in your JWT token are prefixed by SCOPE_ as I mentioned earlier.
Now assuming your granted authorities return ADMIN as one of the roles. Once you add it to your scope in JWT, the default converter will return SCOPE_ADMIN as an authority and similarly if you return ROLE_ADMIN in the scope, it will be converted to SCOPE_ROLE_ADMIN by default.
The JwtAuthenticationConverter class returns an instance of
org.springframework.security.oauth2.server.resource.authentication.JwtAuthenticationToken

So, it can be fixed in following ways:
Either, use hasAnyAuthority to check the authorities by appending SCOPE_ to the role names you have set in scope.
If your role name is ADMIN or ROLE_ADMIN you should use
@PreAuthorize(&quot;hasAnyAuthority('SCOPE_ADMIN')&quot;)
@PreAuthorize(&quot;hasAnyAuthority('SCOPE_ROLE_ADMIN')&quot;) 

and so on.
If you want to use hasAnyRole check then you must use
@PreAuthorize(&quot;hasAnyAuthority('ROLE_SCOPE_ADMIN')&quot;)
@PreAuthorize(&quot;hasAnyAuthority('ROLE_SCOPE_ROLE_ADMIN')&quot;) 

for ADMIN and ROLE_ADMIN values respectively.
Or, implement a custom authority converter and pass it to the oauth2ResourceServer in security config as follows,
Example Custom Converter
import org.springframework.core.convert.converter.Converter;
import org.springframework.security.oauth2.jwt.Jwt;
import org.springframework.stereotype.Component;

import java.util.Collections;

@Component
public class JwtCustomAuthoritiesConverter implements Converter&lt;Jwt, Collection&lt;GrantedAuthority&gt;&gt; {
    @Override
    public Collection&lt;GrantedAuthority&gt; convert(Jwt jwt) {
        Collection&lt;GrantedAuthority&gt; grantedAuthorities = new ArrayList&lt;&gt;();
        Collection&lt;String&gt; splitScopes = Arrays.asList(jwt.getClaim(&quot;scope&quot;).split(&quot; &quot;));
        for (String authority : splitScopes) {
            grantedAuthorities.add(new SimpleGrantedAuthority(authority));
        }
        return grantedAuthorities;
    }
}

Then update your Spring Security config as:
@Bean 
JwtCustomAuthoritiesConverter jwtCustomAuthoritiesConverter;

@Bean
JwtAuthenticationConverter jwtAuthenticationConverter(){
    JwtAuthenticationConverter jwtAuthenticationConverter = new JwtAuthenticationConverter();
    jwtAuthenticationConverter.setJwtGrantedAuthoritiesConverter(jwtCustomAuthoritiesConverter);
    return jwtAuthenticationConverter;
}

...

http.oauth2ResourceServer((oauth2) -&gt;
                        oauth2.jwt((jwt) -&gt; jwt.jwtAuthenticationConverter(jwtAuthenticationConverter()))
...

With the second option you can use your role names the way you want to in JWT token and the
hasAnyRole('ADMIN') 

check should get ROLE_ADMIN for ADMIN scope instead of ROLE_SCOPE_ADMIN which is the case now.
"
"I don't want to use powermock anymore. Because junit5 started mocking static classes. So i am trying to get rid of powermock methods.
As you know, you can create an instance of a class with whenNew keyword.
Is there any alternative in Junit5 for whenNew?
Here is a part of my code:
                whenNew(PDFDocument.class).withNoArguments().thenReturn(pdfDocument);
                whenNew(PSConverter.class).withNoArguments().thenReturn(converter);
                doNothing().when(pdfDocument).load(ArgumentMatchers.any(ByteArrayInputStream.class));
                doAnswer(invocationOnMock -&gt; {
                    ByteArrayOutputStream outputStream = invocationOnMock.getArgument(1);
                    outputStream.write(content);
                    return outputStream;
                }).when(converter).convert(ArgumentMatchers.any(), ArgumentMatchers.any(ByteArrayOutputStream.class));

","Mocking object construction is available since Mockito 3.5.0 according to documentation.
First of all you need add the mockito-inline instead of the mockito-core to your test dependencies.
mockito-inline provides ability to mock static or final methods, constructors. Difference between mockito-core vs mockito-inline
        &lt;dependency&gt;
            &lt;groupId&gt;org.mockito&lt;/groupId&gt;
            &lt;artifactId&gt;mockito-inline&lt;/artifactId&gt;
            &lt;version&gt;${mockito.version}&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;

Let's create a simple service for testing which instantiate objects.
public class A {
    private final String test;

    public A(String test) {
        this.test = test;
    }

    public String check() {
        return &quot;checked &quot; + this.test;
    }
}

public class B {
    private String check = &quot; B check &quot;;

    public String check() {
        return check;
    }

}

public class TestService {
    public String purchaseProduct(String param) {
        A a = new A(param);
        B b = new B();
        return a.check() + b.check();
    }
}

Example of constructor mock with comments:
import org.junit.jupiter.api.Assertions;
import org.junit.jupiter.api.Test;
import org.mockito.MockedConstruction;
import org.mockito.Mockito;

import static org.mockito.Mockito.when;

public class ConstructorMockTest {
    @Test
    public void test_mocked_construction()  {
        try (
             //create mock for object A
             MockedConstruction&lt;A&gt; mockedA = Mockito.mockConstruction(A.class,
                (mock, context) -&gt; {
                    //set return value for object A mock methods
                    when(mock.check()).thenReturn(&quot; Constructor Mock A &quot;);
             });
             //create mock for object B
             MockedConstruction&lt;B&gt; mockedB = Mockito.mockConstruction(B.class,
                     (mock, context) -&gt; {
                         //set return value for object B mock methods
                         when(mock.check()).thenReturn(&quot; Constructor Mock B &quot;);
             }))
        {
            // every A object creation is current try() scope returning a mock
            A aObject = new A(&quot;test&quot;);
            Assertions.assertEquals( aObject.check(), &quot; Constructor Mock A &quot;);

            // every B object creation is current try() scope returning a mock
            B bObject = new B();
            Assertions.assertEquals( bObject.check(), &quot; Constructor Mock B &quot;);

            //Example of testing service which creates A and B objects
            TestService service = new TestService();
            String serviceResult = service.purchaseProduct(&quot;test&quot;);

            Assertions.assertEquals(serviceResult, &quot; Constructor Mock A  Constructor Mock B &quot;);
        }
    }
}

For your classes example:
    @Test
    public void test() {
        byte[] content = new byte[] {1,1};

        try (
                MockedConstruction&lt;PDFDocument&gt; mockedPDFDocument = Mockito.mockConstruction(PDFDocument.class,
                        (mock, context) -&gt; {
                            doNothing().when(mock).load(ArgumentMatchers.any(ByteArrayInputStream.class));
                        });

                MockedConstruction&lt;PSConverter&gt; mockedPSConverter = Mockito.mockConstruction(PSConverter.class,
                        (mock, context) -&gt; {
                            doAnswer(invocationOnMock -&gt; {
                                ByteArrayOutputStream outputStream = invocationOnMock.getArgument(1);
                                outputStream.write(content);
                                return outputStream;
                            }).when(mock).convert(ArgumentMatchers.any(), ArgumentMatchers.any(ByteArrayOutputStream.class));
                        }))
        {
            //call services which instantiates PDFDocument and PSConverter
            PDFDocument pdfDocument = new PDFDocument();
            PSConverter psConverter = new PSConverter();

            Assertions.assertTrue(org.mockito.internal.util.MockUtil.isMock(pdfDocument));
            Assertions.assertTrue(org.mockito.internal.util.MockUtil.isMock(psConverter));
        }
    }

"
"I've read in Spring Security Reference that AuthorizationFilter supersedes FilterSecurityInterceptor. So I'm trying to migrate my application to this newer method.
I have something like
                http.authorizeRequests()
                        .mvcMatchers(&quot;/&quot;)
                        .hasIpAddress(&quot;127.0.0.1&quot;)

According to the linked page I should be able to write something like
                http.authorizeHttpRequests()
                        .mvcMatchers(&quot;/&quot;)
                        .access(&quot;hasIpAddress('127.0.0.1')&quot;)

but there's no access(String) method. I even tried to paste verbatim code from the documentation:
@Bean
SecurityFilterChain web(HttpSecurity http) throws Exception {
    http
        // ...
        .authorizeHttpRequests(authorize -&gt; authorize                                  
            .mvcMatchers(&quot;/resources/**&quot;, &quot;/signup&quot;, &quot;/about&quot;).permitAll()         
            .mvcMatchers(&quot;/admin/**&quot;).hasRole(&quot;ADMIN&quot;)                             
            .mvcMatchers(&quot;/db/**&quot;).access(&quot;hasRole('ADMIN') and hasRole('DBA')&quot;)   
            .anyRequest().denyAll()                                                
        );

    return http.build();
}

which does not compile for the same reason.
Here's compilation error:
Application.java:103:55
java: incompatible types: java.lang.String cannot be converted to org.springframework.security.authorization.AuthorizationManager&lt;org.springframework.security.web.access.intercept.RequestAuthorizationContext&gt;

How do I use authorizeHttpRequests with IP addresses or string expression? Is it issue with documentation?
I'm using Spring Boot 2.7.0 and Spring Security 5.7.1
","This does appear to be an issue with the docs. There is not currently a built-in implementation providing the hasIpAddress(String) access check, but you can use the IpAddressMatcher class to implement an AuthorizationManager capable of performing it.
Here's an example configuration:
@EnableWebSecurity
public class SecurityConfiguration {

    @Bean
    public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception {
        http
            .authorizeHttpRequests((authorizeRequests) -&gt; authorizeRequests
                .requestMatchers(&quot;/&quot;).access(hasIpAddress(&quot;127.0.0.1&quot;))
                .anyRequest().authenticated()
            )
            .formLogin(Customizer.withDefaults())
            .httpBasic(Customizer.withDefaults());
        return http.build();
    }

    private static AuthorizationManager&lt;RequestAuthorizationContext&gt; hasIpAddress(String ipAddress) {
        IpAddressMatcher ipAddressMatcher = new IpAddressMatcher(ipAddress);
        return (authentication, context) -&gt; {
            HttpServletRequest request = context.getRequest();
            return new AuthorizationDecision(ipAddressMatcher.matches(request));
        };
    }

}

"
"I am able to create the filter chain with http.oauth2ResourceServer().jwt() and I've also set spring.security.oauth2.resourceserver.jwt.issuer-uri. It is able to authenticate requests. However, I also need to do custom logging in the case of an authentication failure. The approach I'm taking is to use a custom authentication entry point to handle when no bearer token is present, combined with a custom BearerTokenAuthenticationFilter.authenticationFailureHandler to handle an invalid token. I'm open to other approaches to satisfy this goal.
I am able to configure a custom authentication entry point to handle the case where no token is present:
// in WebSecurityConfigurerAdapter::configure
http
    .exceptionHandling()
    .authenticationEntryPoint((request, response, exception) -&gt; { /* ... */ });

However I haven't found a way to access the BearerTokenAuthenticationFilter. The best I've been able to come up with is to new up a second configured the way I want it, but this is not appealing to me because the server ends up doing extra work with every successfully authenticated request:
// in WebSecurityConfigurerAdapter::configure
var filter = new BearerTokenAuthenticationFilter(authenticationManagerBean());
filter.setAuthenticationFailureHandler(new JwtAuthenticationFailureHandler());
http.addFilterBefore(tokenAuth, BearerTokenAuthenticationFilter.class);
// my filter runs first

Surely there is some way to set this property in the filter that spring security creates? Ideally it would be exposed by OAuth2ResourceServerConfigurer, but that only offers accessDeniedHandler.
I've tried accessing either the filter itself or the DefaultSecurityFilterChain as a bean, but they don't exist as beans in the application context. I found this answer which suggests configuring a bean in spring-servlet.xml and running it through a post processor. The idea of a BeanPostProcessor seems promising to me however I wasn't able to get it to work, because after modifying spring-servlet.xml as suggested the bean still doesn't exist. I can't use getBean to find it and it's not seen by the BeanPostProcessor:
&lt;http name=&quot;filterChain&quot;&gt;

","The two cases you're looking for can be handled with a combination of AuthenticationEntryPoints applied at different levels, one for the BearerTokenAuthenticationFilter and a second for the filter chain. For example:
@Bean
public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception {
    http
        .authorizeHttpRequests((authorize) -&gt; authorize
            .anyRequest().authenticated()
        )
        .oauth2ResourceServer((oauth2) -&gt; oauth2
            .jwt(Customizer.withDefaults())
            .authenticationEntryPoint((request, response, exception) -&gt; {
                System.out.println(&quot;Authentication failed&quot;);
                BearerTokenAuthenticationEntryPoint delegate = new BearerTokenAuthenticationEntryPoint();
                delegate.commence(request, response, exception);
            })
        )
        .exceptionHandling((exceptions) -&gt; exceptions
            .authenticationEntryPoint((request, response, exception) -&gt; {
                System.out.println(&quot;Authentication is required&quot;);
                BearerTokenAuthenticationEntryPoint delegate = new BearerTokenAuthenticationEntryPoint();
                delegate.commence(request, response, exception);
            })
        );

    return http.build();
}

The reason it works this way is that the BearerTokenAuthenticationFilter is not invoked when no bearer token is present. In that case, the entire filter chain is tried and no valid authentication is found. This would be the normal &quot;Authentication is required&quot; scenario for Spring Security.
However, in the case of an present but invalid token, the BearerTokenAuthenticationFilter needs to validate the token to determine that it failed, and uses its own AuthenticationFailureHandler for this case. But the failure handler simply delegates to a specific AuthenticationEntryPoint by default, which is the one configured above for the &quot;Authentication failed&quot; case.
If you want to override the failure handler to do something else (though you may not have to after trying the above), you can do so with an ObjectPostProcessor. For example:
@Bean
public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception {
    http
        .oauth2ResourceServer((oauth2) -&gt; oauth2
            .jwt(Customizer.withDefaults())
            .withObjectPostProcessor(new ObjectPostProcessor&lt;BearerTokenAuthenticationFilter&gt;() {
                @Override
                public &lt;O extends BearerTokenAuthenticationFilter&gt; O postProcess(O filter) {
                    filter.setAuthenticationFailureHandler((request, response, exception) -&gt; {
                        System.out.println(&quot;Authentication failed (and is being handled in a custom way)&quot;);
                        BearerTokenAuthenticationEntryPoint delegate = new BearerTokenAuthenticationEntryPoint();
                        delegate.commence(request, response, exception);
                    });
                    return filter;
                }
            })
        );

    return http.build();
}

"
"Is there anything in java that does the opposite of regular expressions?
My task is: given a defined total length for a string and each position can only consist of predefined specific characters, generate all possible strings.
To give an example: I want to create all stings of length 3 where the positions are defined as
[ABC][123][XYZ]

This means that the first position can only be A, B or C, the second position one of the numbers 1 to 3 and so on. Valid strings would therefore be
A1X 
A1Y 
A1Z 
A2X 
A2Y 
A2Z 
...
... 
C3Z 

For the length three I can of course use a nested loop. My problem is I don't know in advance how long the string has to be or how many valid characters each position has. Any ideas?
Code for length 3 and each position 3 possible chars:
public static void main(String[] args) {
    String[] first  = {&quot;A&quot;, &quot;B&quot;, &quot;C&quot;};
    String[] second = {&quot;1&quot;, &quot;2&quot;, &quot;3&quot;};
    String[] third  = {&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;};

    List&lt;String&gt; result = createStrings(first, second, third);

    result.forEach(System.out::println);
}

static List&lt;String&gt; createStrings(String[] ... strs) {
    String[] first  = strs[0];
    String[] second = strs[1];
    String[] third  = strs[2];

    List&lt;String&gt; result = new ArrayList&lt;&gt;();
    for (int i = 0; i &lt; first.length; i++) {
        for (int j = 0; j &lt; second.length; j++) {
            for (int k = 0; k &lt; third.length; k++) {
                result.add(first[i] + second[j] + third[k]);
            }
        }
    }
    return result;
}

I need something flexible, which works for all inputs. Or a way to dynamically create a nested loop depending on strs.length which defines how many loops I need.
","You can use recursion:
import java.util.ArrayList;
import java.util.List;

public class Main {
    public static void main(String[] args) {
        String[] first = { &quot;A&quot;, &quot;B&quot;, &quot;C&quot; };
        String[] second = { &quot;1&quot;, &quot;2&quot;, &quot;3&quot; };
        String[] third = { &quot;X&quot;, &quot;Y&quot;, &quot;Z&quot; };
        String[] fourth = { &quot;K&quot;, &quot;L&quot;, &quot;M&quot; };
        String[] fifth = { &quot;7&quot;, &quot;8&quot;, &quot;9&quot; };

        List&lt;String&gt; result = createStrings(first, second, third, fourth, fifth);

        result.forEach(System.out::println);
    }

    static List&lt;String&gt; createStrings(String[]... strs) {
        List&lt;String&gt; res = new ArrayList&lt;&gt;();
        getStrings(0, &quot;&quot;, res, strs);
        return res;
    }

    static void getStrings(int level, String curr, List&lt;String&gt; res, String[]... strs) {
        if (level == strs.length) {
            res.add(curr);
            return;
        }

        for (String ch : strs[level]) {
            getStrings(level + 1, curr + ch, res, strs);
        }
    }
}


Prints
A1XK7
A1XK8
A1XK9
A1XL7
A1XL8
A1XL9
A1XM7
...

C3ZK9
C3ZL7
C3ZL8
C3ZL9
C3ZM7
C3ZM8
C3ZM9


Tree level construction of a string:

                              &quot;&quot;
                            /  |  \
                          A    B    C
                         /|\  /|\  /|\
                       1 2 3 1 2 3 1 2 3
                      /|\ /|\ /|\ /|\ /|\
                     X Y Z X Y Z X Y Z X Y Z
                    /|\/|\/|\/|\/|\/|\/|\/|\
                   K L M K L M K L M K L M K L M
                  /|\/|\/|\/|\/|\/|\/|\/|\/|\/|\
                 ... ... ... ... ... ... ... ... 


In this example, we have five levels. We want to generate all possible combinations of characters by recursively concatenating each character (from each level) using the current array (strs[level]) and then move to the next level.

Initially, we call createStrings() with all five arrays, which calls getStrings(0, &quot;&quot;, res, strs).


Here are the recursion stacks:
First Level (level = 0):

Calls with curr = &quot;A&quot;, curr = &quot;B&quot;, curr = &quot;C&quot;

Second Level (level = 1):

For curr = &quot;A&quot;: Calls with curr = &quot;A1&quot;, curr = &quot;A2&quot;, curr = &quot;A3&quot;
For curr = &quot;B&quot;: Calls with curr = &quot;B1&quot;, curr = &quot;B2&quot;, curr = &quot;B3&quot;
For curr = &quot;C&quot;: Calls with curr = &quot;C1&quot;, curr = &quot;C2&quot;, curr = &quot;C3&quot;

Third Level (level = 2):

For curr = &quot;A1&quot;: Calls with curr = &quot;A1X&quot;, curr = &quot;A1Y&quot;, curr = &quot;A1Z&quot;
For curr = &quot;A2&quot;: Calls with curr = &quot;A2X&quot;, curr = &quot;A2Y&quot;, curr = &quot;A2Z&quot;
For curr = &quot;A3&quot;: Calls with curr = &quot;A3X&quot;, curr = &quot;A3Y&quot;, curr = &quot;A3Z&quot;
...

Fourth Level (level = 3):

For curr = &quot;A1X&quot;: Calls with curr = &quot;A1XK&quot;, curr = &quot;A1XL&quot;, curr = &quot;A1XM&quot;
For curr = &quot;A1Y&quot;: Calls with curr = &quot;A1YK&quot;, curr = &quot;A1YL&quot;, curr = &quot;A1YM&quot;
For curr = &quot;A1Z&quot;: Calls with curr = &quot;A1ZK&quot;, curr = &quot;A1ZL&quot;, curr = &quot;A1ZM&quot;
...

Fifth Level (level = 4):

For curr = &quot;A1XK&quot;: Calls with curr = &quot;A1XK7&quot;, curr = &quot;A1XK8&quot;, curr = &quot;A1XK9&quot;
For curr = &quot;A1XL&quot;: Calls with curr = &quot;A1XL7&quot;, curr = &quot;A1XL8&quot;, curr = &quot;A1XL9&quot;
For curr = &quot;A1XM&quot;: Calls with curr = &quot;A1XM7&quot;, curr = &quot;A1XM8&quot;, curr = &quot;A1XM9&quot;

...

Let's trace one path through the recursion stack:

First call: getStrings(0, &quot;&quot;, res, strs), calls getStrings(1, &quot;A&quot;, res, strs);
Second call: getStrings(1, &quot;A&quot;, res, strs), calls getStrings(2, &quot;A1&quot;, res, strs);
Third call: getStrings(2, &quot;A1&quot;, res, strs), calls getStrings(3, &quot;A1X&quot;, res, strs);
Fourth call: getStrings(3, &quot;A1X&quot;, res, strs), calls getStrings(4, &quot;A1XK&quot;, res, strs);
Fifth call: getStrings(4, &quot;A1XK&quot;, res, strs), calls getStrings(5, &quot;A1XK7&quot;, res, strs); and
Base case: getStrings(5, &quot;A1XK7&quot;, res, strs), adds &quot;A1XK7&quot; to the res.

"
"I'm trying to implement a dynamic search for a huge product collection. The object has several properties including productName, subCategoryName, categoryName, brandName, etc. The user could search for products using any of these properties. The order is fixed and the first priority for a search string is to find it in productName and then subCategoryName and so on.
I used aggregate to achieve this and then unionWith to concat records that matched with other properties. It seems to work when fired as a raw query but we also need support for pagination and I'm not being able to achieve that with Spring Data MongoDB
db.product.aggregate(
[
Â  { $match: { &quot;productName&quot; : { &quot;$regex&quot; : &quot;HYPER&quot;, &quot;$options&quot; : &quot;i&quot;}, 
Â  &quot;companyNo&quot; : { &quot;$in&quot; : [10000009]}, &quot;status&quot; : { &quot;$in&quot; : [&quot;ACTIVE&quot;, &quot;IN_ACTIVE&quot;, &quot;OUT_OF_STOCK&quot;]} }},
Â  { $unionWith: { coll: &quot;product&quot;, pipeline: [{ $match: { &quot;subCategoryName&quot; : { &quot;$regex&quot; : &quot;HYPER&quot;, &quot;$options&quot; : &quot;i&quot;},
Â  &quot;companyNo&quot; : { &quot;$in&quot; : [10000009]}, &quot;status&quot; : { &quot;$in&quot; : [&quot;ACTIVE&quot;, &quot;IN_ACTIVE&quot;, &quot;OUT_OF_STOCK&quot;]}} }] } },
Â  { $unionWith: { coll: &quot;product&quot;, pipeline: [{ $match: { &quot;categoryName&quot; : { &quot;$regex&quot; : &quot;HYPER&quot;, &quot;$options&quot; : &quot;i&quot;}, 
Â  &quot;companyNo&quot; : { &quot;$in&quot; : [10000009]}, &quot;status&quot; : { &quot;$in&quot; : [&quot;ACTIVE&quot;, &quot;IN_ACTIVE&quot;, &quot;OUT_OF_STOCK&quot;]}} }] } },
Â  { $unionWith: { coll: &quot;product&quot;, pipeline: [{ $match: { &quot;brandName&quot; : { &quot;$regex&quot; : &quot;HYPER&quot;, &quot;$options&quot; : &quot;i&quot;},
Â  &quot;companyNo&quot; : { &quot;$in&quot; : [10000009]}, &quot;status&quot; : { &quot;$in&quot; : [&quot;ACTIVE&quot;, &quot;IN_ACTIVE&quot;, &quot;OUT_OF_STOCK&quot;]}} }] } },
]
)

Also, this query only works if we pass the substring of the exact name. For example, the NIVEA BODY LOTION EXPRESS HYDRATION 200 ML HYPERmart product will be returned if I search with NIVEA BODY LOTION but it won't return anything if I search with HYDRATION LOTION
A Sample Product:
{
    &quot;_id&quot; : ObjectId(&quot;6278c1c2f2570d6f199435b2&quot;),
    &quot;companyNo&quot; : 10000009,
    &quot;categoryName&quot; : &quot;BEAUTY and PERSONAL CARE&quot;,
    &quot;brandName&quot; : &quot;HYPERMART&quot;,
    &quot;productName&quot; : &quot;NIVEA BODY LOTION EXPRESS HYDRATION 200 ML HYPERmart&quot;,
    &quot;productImageUrl&quot; : &quot;https://shop-now-bucket.s3.ap-south-1.amazonaws.com/shop-now-bucket/qa/10000009/product/BEAUTY%20%26%20PERSONAL%20CARE/HYPERMART/NIVEA%20BODY%20LOTION%20EXPRESS%20HYDRATION%20200%20ML/temp1652081080302.jpeg&quot;,
    &quot;compressProductImageUrl&quot; : &quot;https://shop-now-bucket.s3.ap-south-1.amazonaws.com/shop-now-bucket/qa/10000009/product/BEAUTY%20%26%20PERSONAL%20CARE/HYPERMART/NIVEA%20BODY%20LOTION%20EXPRESS%20HYDRATION%20200%20ML/temp1652081080302.jpeg&quot;,
    &quot;productPrice&quot; : 249.0,
    &quot;status&quot; : &quot;ACTIVE&quot;,
    &quot;subCategoryName&quot; : &quot;BODY LOTION &amp; BODY CREAM&quot;,
    &quot;defaultDiscount&quot; : 0.0,
    &quot;discount&quot; : 7.0,
    &quot;description&quot; : &quot;Give your skin fast-absorbing moisturisation and make it noticeably smoother for 48-hours with Nivea Express Hydration Body Lotion. The formula with Sea Minerals and Hydra IQ supplies your skin with moisture all day. The new improved formula contains Deep Moisture Serum to lock in deep moisture leaving you with soft and supple skin.&quot;,
    &quot;afterDiscountPrice&quot; : 231.57,
    &quot;taxPercentage&quot; : 1.0,
    &quot;availableQuantity&quot; : NumberLong(100),
    &quot;packingCharges&quot; : 0.0,
    &quot;available&quot; : true,
    &quot;featureProduct&quot; : false,
    &quot;wholesaleProduct&quot; : false,
    &quot;rewards&quot; : NumberLong(0),
    &quot;createAt&quot; : ISODate(&quot;2022-05-09T07:24:40.286Z&quot;),
    &quot;createdBy&quot; : &quot;companyAdmin_@+919146670758shivani.patni@apptware.com&quot;,
    &quot;isBulkUpload&quot; : true,
    &quot;buyPrice&quot; : 0.0,
    &quot;privateProduct&quot; : false,
    &quot;comboProduct&quot; : false,
    &quot;subscribable&quot; : false,
    &quot;discountAdded&quot; : false,
    &quot;_class&quot; : &quot;com.apptmart.product.entity.Product&quot;
}

I'm new to MongoDB. any references will be appretiated.
","Here is my working example in Spring Boot.
https://github.com/ConsciousObserver/MongoAggregationTest
You can invoke the /product REST service using following command
http://localhost:8080/products?productName=product&amp;brandName=BRAND1&amp;categoryName=CATEGORY2&amp;subCategoryName=SUB_CATEGORY3&amp;pageNumber=0&amp;pageSize=10

Implementation supports following

Text search on productName (Searches by words, needs text search index)
Exact match on brandName, categoryName and subCategoryName
Pagination using pageNumber and pageSize

All of it is implemented using Spring Data APIs. I generally avoid writing native queries in code, as they are not validated at compile time.
All classes are added to one Java file, it's just a sample so it's better to keep everything in one place.
Adding code below in case GitHub repository goes down.
pom.xml
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
    xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
    xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;2.7.4&lt;/version&gt;
        &lt;relativePath /&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;
    &lt;groupId&gt;com.example&lt;/groupId&gt;
    &lt;artifactId&gt;MongoAggregationTest&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;MongoAggregationTest&lt;/name&gt;
    &lt;description&gt;MongoAggregationTest&lt;/description&gt;
    &lt;properties&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
    &lt;/properties&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-data-mongodb&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-validation&lt;/artifactId&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;
            &lt;artifactId&gt;lombok&lt;/artifactId&gt;
            &lt;optional&gt;true&lt;/optional&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;excludes&gt;
                        &lt;exclude&gt;
                            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;
                            &lt;artifactId&gt;lombok&lt;/artifactId&gt;
                        &lt;/exclude&gt;
                    &lt;/excludes&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;

MongoAggregationTestApplication.java
package com.example;

import java.util.ArrayList;
import java.util.List;

import javax.annotation.PostConstruct;
import javax.validation.constraints.Max;
import javax.validation.constraints.Min;

import org.bson.BsonDocument;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.data.annotation.Id;
import org.springframework.data.mongodb.core.MongoTemplate;
import org.springframework.data.mongodb.core.aggregation.Aggregation;
import org.springframework.data.mongodb.core.aggregation.AggregationResults;
import org.springframework.data.mongodb.core.aggregation.LimitOperation;
import org.springframework.data.mongodb.core.aggregation.MatchOperation;
import org.springframework.data.mongodb.core.aggregation.SkipOperation;
import org.springframework.data.mongodb.core.index.TextIndexDefinition;
import org.springframework.data.mongodb.core.index.TextIndexDefinition.TextIndexDefinitionBuilder;
import org.springframework.data.mongodb.core.index.TextIndexed;
import org.springframework.data.mongodb.core.mapping.Document;
import org.springframework.data.mongodb.core.mapping.Field;
import org.springframework.data.mongodb.core.query.Criteria;
import org.springframework.data.mongodb.core.query.CriteriaDefinition;
import org.springframework.data.mongodb.core.query.Query;
import org.springframework.data.mongodb.core.query.TextCriteria;
import org.springframework.validation.annotation.Validated;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.RestController;

import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;

@RequiredArgsConstructor
@SpringBootApplication
@Slf4j
public class MongoAggregationTestApplication {

    public static void main(String[] args) {
        SpringApplication.run(MongoAggregationTestApplication.class, args);
    }

    private final MongoTemplate mongoTemplate;

    @PostConstruct
    void prepareData() {
        boolean collectionExists = mongoTemplate.collectionExists(Product.COLLECTION_NAME);

        log.info(&quot;####### product collection exists: {}&quot;, collectionExists);

        if (!collectionExists) {
            throw new RuntimeException(
                    String.format(&quot;Required collection {%s} does not exist&quot;, Product.COLLECTION_NAME));
        }

        //Adding index manually ------------- This is required for text search on productName
        TextIndexDefinition textIndex = new TextIndexDefinitionBuilder().onField(&quot;productName&quot;, 1F).build();
        mongoTemplate.indexOps(Product.class).ensureIndex(textIndex);

        boolean samplesAlreadyAdded = mongoTemplate
                .exists(new Query().addCriteria(Criteria.where(&quot;brandName&quot;).exists(true)), Product.class);

        //Uncomment to delete all rows from product collection
        //mongoTemplate.getCollection(Product.COLLECTION_NAME).deleteMany(new BsonDocument());

        if (!samplesAlreadyAdded) {
            for (int i = 1; i &lt;= 5; i++) {
                //adds 3 words in productName
                //product name term1
                String productName = &quot;product name term&quot; + i;

                Product product = new Product(null, &quot;ACTIVE&quot;, productName, &quot;BRAND&quot; + i, &quot;CATEGORY&quot; + i,
                        &quot;SUB_CATEGORY&quot; + 1);

                mongoTemplate.save(product);

                log.info(&quot;Saving sample product to database: {}&quot;, product);
            }
        } else {
            log.info(&quot;Skipping sample insertion as they're already in DB&quot;);
        }
    }
}

@Slf4j
@RestController
@RequiredArgsConstructor
@Validated
class ProductController {
    private final MongoTemplate mongoTemplate;

    //JSR 303 validations are returning 500 when validation fails, instead of 400. Will look into it later
    /**
     * Invoke using follwing command
     * &lt;p&gt;
     * &lt;code&gt;http://localhost:8080/products?productName=product&amp;brandName=BRAND1&amp;categoryName=CATEGORY2&amp;subCategoryName=SUB_CATEGORY3&amp;pageNumber=0&amp;pageSize=10&lt;/code&gt;
     * 
     * @param productName
     * @param brandName
     * @param categoryName
     * @param subCategoryName
     * @param pageNumber
     * @param pageSize
     * @return
     */
    @GetMapping(&quot;/products&quot;)
    public List&lt;Product&gt; getProducts(@RequestParam String productName, @RequestParam String brandName,
            @RequestParam String categoryName, @RequestParam String subCategoryName,
            @RequestParam @Min(0) int pageNumber, @RequestParam @Min(1) @Max(100) int pageSize) {

        log.info(
                &quot;Request parameters: productName: {}, brandName: {}, categoryName: {}, subCategoryName: {}, pageNumber: {}, pageSize: {}&quot;,
                productName, brandName, categoryName, subCategoryName, pageNumber, pageSize);
        //Query Start

        TextCriteria productNameTextCriteria = new TextCriteria().matchingAny(productName).caseSensitive(false);
        TextCriteriaHack textCriteriaHack = new TextCriteriaHack();
        textCriteriaHack.addCriteria(productNameTextCriteria);

        //Needs this hack to combine TextCriteria with Criteria in a single query
        //See TextCriteriaHack for details
        MatchOperation productNameTextMatch = new MatchOperation(textCriteriaHack);

        //Exact match
        Criteria brandNameMatch = Criteria.where(&quot;brandName&quot;).is(brandName);
        Criteria categoryNameMatch = Criteria.where(&quot;categoryName&quot;).is(categoryName);
        Criteria subCategoryNameMatch = Criteria.where(&quot;subCategoryName&quot;).is(subCategoryName);

        MatchOperation orMatch = Aggregation
                .match(new Criteria().orOperator(brandNameMatch, categoryNameMatch, subCategoryNameMatch));

        //Pagination setup
        SkipOperation skip = Aggregation.skip((long) pageNumber * pageSize);
        LimitOperation limit = Aggregation.limit(pageSize);

        Aggregation aggregation = Aggregation.newAggregation(productNameTextMatch, orMatch, skip, limit);

        //Query end

        //Query execution
        AggregationResults&lt;Product&gt; aggregateResults = mongoTemplate.aggregate(aggregation, Product.COLLECTION_NAME,
                Product.class);

        List&lt;Product&gt; products = new ArrayList&lt;&gt;();

        aggregateResults.iterator().forEachRemaining(products::add);

        log.info(&quot;Found products: {}&quot;, products);

        return products;
    }
}

@Data
@Document(Product.COLLECTION_NAME)
@NoArgsConstructor
@AllArgsConstructor
class Product {
    static final String COLLECTION_NAME = &quot;product&quot;;

    @Id
    @Field(&quot;_id&quot;)
    private String id;

    @Field(&quot;status&quot;)
    private String status;

    @TextIndexed
    @Field(&quot;productName&quot;)
    private String productName;

    @Field(&quot;brandName&quot;)
    private String brandName;

    @Field(&quot;categoryName&quot;)
    private String categoryName;

    @Field(&quot;subCategoryName&quot;)
    private String subCategoryName;
}

/**
 * https://stackoverflow.com/a/29925876 There is no way to combine
 * CriteriaDefinition and Criteria in one query This hack converts
 * CriteriaDefinition to Query which can be converted to Criteria
 */
class TextCriteriaHack extends Query implements CriteriaDefinition {
    @Override
    public org.bson.Document getCriteriaObject() {
        return this.getQueryObject();
    }

    @Override
    public String getKey() {
        return null;
    }
}

Here's the query that's being executed by /products, I got it from MongoTemplate logs
[
    {
        &quot;$match&quot;: {
            &quot;$text&quot;: {
                &quot;$search&quot;: &quot;name&quot;,
                &quot;$caseSensitive&quot;: false
            }
        }
    },
    {
        &quot;$match&quot;: {
            &quot;$or&quot;: [
                {
                    &quot;brandName&quot;: &quot;BRAND1&quot;
                },
                {
                    &quot;categoryName&quot;: &quot;CATEGORY2&quot;
                },
                {
                    &quot;subCategoryName&quot;: &quot;SUB_CATEGORY3&quot;
                }
            ]
        }
    },
    {
        &quot;$skip&quot;: 0
    },
    {
        &quot;$limit&quot;: 1
    }
]

Here's log contents, after a few requests have been fired
2022-10-06 04:50:01.209  INFO 26472 --- [           main] c.e.MongoAggregationTestApplication      : No active profile set, falling back to 1 default profile: &quot;default&quot;
2022-10-06 04:50:01.770  INFO 26472 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data MongoDB repositories in DEFAULT mode.
2022-10-06 04:50:01.780  INFO 26472 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 6 ms. Found 0 MongoDB repository interfaces.
2022-10-06 04:50:02.447  INFO 26472 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
2022-10-06 04:50:02.456  INFO 26472 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2022-10-06 04:50:02.456  INFO 26472 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.65]
2022-10-06 04:50:02.531  INFO 26472 --- [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2022-10-06 04:50:02.531  INFO 26472 --- [           main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 1277 ms
2022-10-06 04:50:02.679  INFO 26472 --- [           main] org.mongodb.driver.client                : MongoClient with metadata {&quot;driver&quot;: {&quot;name&quot;: &quot;mongo-java-driver|sync|spring-boot&quot;, &quot;version&quot;: &quot;4.6.1&quot;}, &quot;os&quot;: {&quot;type&quot;: &quot;Windows&quot;, &quot;name&quot;: &quot;Windows 10&quot;, &quot;architecture&quot;: &quot;amd64&quot;, &quot;version&quot;: &quot;10.0&quot;}, &quot;platform&quot;: &quot;Java/OpenLogic-OpenJDK/1.8.0-262-b10&quot;} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@22bd2039]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2022-10-06 04:50:02.725  INFO 26472 --- [localhost:27017] org.mongodb.driver.connection            : Opened connection [connectionId{localValue:1, serverValue:121}] to localhost:27017
2022-10-06 04:50:02.725  INFO 26472 --- [localhost:27017] org.mongodb.driver.connection            : Opened connection [connectionId{localValue:2, serverValue:122}] to localhost:27017
2022-10-06 04:50:02.726  INFO 26472 --- [localhost:27017] org.mongodb.driver.cluster               : Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=13, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=48972600}
2022-10-06 04:50:02.922  INFO 26472 --- [           main] org.mongodb.driver.connection            : Opened connection [connectionId{localValue:3, serverValue:123}] to localhost:27017
2022-10-06 04:50:02.933  INFO 26472 --- [           main] c.e.MongoAggregationTestApplication      : ####### product collection exists: true
2022-10-06 04:50:02.957 DEBUG 26472 --- [           main] o.s.data.mongodb.core.MongoTemplate      : Executing count: { &quot;brandName&quot; : { &quot;$exists&quot; : true}} in collection: product
2022-10-06 04:50:02.977 DEBUG 26472 --- [           main] o.s.data.mongodb.core.MongoTemplate      : Saving Document containing fields: [status, productName, brandName, categoryName, subCategoryName, _class]
2022-10-06 04:50:02.993  INFO 26472 --- [           main] c.e.MongoAggregationTestApplication      : Saving sample product to database: Product(id=633e1122297cce382aea07d4, status=ACTIVE, productName=product name term1, brandName=BRAND1, categoryName=CATEGORY1, subCategoryName=SUB_CATEGORY1)
2022-10-06 04:50:02.993 DEBUG 26472 --- [           main] o.s.data.mongodb.core.MongoTemplate      : Saving Document containing fields: [status, productName, brandName, categoryName, subCategoryName, _class]
2022-10-06 04:50:02.995  INFO 26472 --- [           main] c.e.MongoAggregationTestApplication      : Saving sample product to database: Product(id=633e1122297cce382aea07d5, status=ACTIVE, productName=product name term2, brandName=BRAND2, categoryName=CATEGORY2, subCategoryName=SUB_CATEGORY1)
2022-10-06 04:50:02.995 DEBUG 26472 --- [           main] o.s.data.mongodb.core.MongoTemplate      : Saving Document containing fields: [status, productName, brandName, categoryName, subCategoryName, _class]
2022-10-06 04:50:02.996  INFO 26472 --- [           main] c.e.MongoAggregationTestApplication      : Saving sample product to database: Product(id=633e1122297cce382aea07d6, status=ACTIVE, productName=product name term3, brandName=BRAND3, categoryName=CATEGORY3, subCategoryName=SUB_CATEGORY1)
2022-10-06 04:50:02.996 DEBUG 26472 --- [           main] o.s.data.mongodb.core.MongoTemplate      : Saving Document containing fields: [status, productName, brandName, categoryName, subCategoryName, _class]
2022-10-06 04:50:02.997  INFO 26472 --- [           main] c.e.MongoAggregationTestApplication      : Saving sample product to database: Product(id=633e1122297cce382aea07d7, status=ACTIVE, productName=product name term4, brandName=BRAND4, categoryName=CATEGORY4, subCategoryName=SUB_CATEGORY1)
2022-10-06 04:50:02.997 DEBUG 26472 --- [           main] o.s.data.mongodb.core.MongoTemplate      : Saving Document containing fields: [status, productName, brandName, categoryName, subCategoryName, _class]
2022-10-06 04:50:02.998  INFO 26472 --- [           main] c.e.MongoAggregationTestApplication      : Saving sample product to database: Product(id=633e1122297cce382aea07d8, status=ACTIVE, productName=product name term5, brandName=BRAND5, categoryName=CATEGORY5, subCategoryName=SUB_CATEGORY1)
2022-10-06 04:50:03.310  INFO 26472 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8080 (http) with context path ''
2022-10-06 04:50:03.318  INFO 26472 --- [           main] c.e.MongoAggregationTestApplication      : Started MongoAggregationTestApplication in 2.446 seconds (JVM running for 2.802)
2022-10-06 04:50:17.447  INFO 26472 --- [nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2022-10-06 04:50:17.447  INFO 26472 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2022-10-06 04:50:17.448  INFO 26472 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Completed initialization in 1 ms
2022-10-06 04:50:17.511  INFO 26472 --- [nio-8080-exec-1] com.example.ProductController            : Request parameters: productName: product, brandName: BRAND1, categoryName: CATEGORY2, subCategoryName: SUB_CATEGORY3, pageNumber: 0, pageSize: 10
2022-10-06 04:50:17.517 DEBUG 26472 --- [nio-8080-exec-1] o.s.data.mongodb.core.MongoTemplate      : Executing aggregation: [{ &quot;$match&quot; : { &quot;$text&quot; : { &quot;$search&quot; : &quot;product&quot;, &quot;$caseSensitive&quot; : false}}}, { &quot;$match&quot; : { &quot;$or&quot; : [{ &quot;brandName&quot; : &quot;BRAND1&quot;}, { &quot;categoryName&quot; : &quot;CATEGORY2&quot;}, { &quot;subCategoryName&quot; : &quot;SUB_CATEGORY3&quot;}]}}, { &quot;$skip&quot; : 0}, { &quot;$limit&quot; : 10}] in collection product
2022-10-06 04:50:17.527  INFO 26472 --- [nio-8080-exec-1] com.example.ProductController            : Found products: [Product(id=633e1122297cce382aea07d5, status=ACTIVE, productName=product name term2, brandName=BRAND2, categoryName=CATEGORY2, subCategoryName=SUB_CATEGORY1), Product(id=633e1122297cce382aea07d4, status=ACTIVE, productName=product name term1, brandName=BRAND1, categoryName=CATEGORY1, subCategoryName=SUB_CATEGORY1)]
2022-10-06 04:50:23.235  INFO 26472 --- [nio-8080-exec-2] com.example.ProductController            : Request parameters: productName: product, brandName: BRAND1, categoryName: CATEGORY2, subCategoryName: SUB_CATEGORY3, pageNumber: 0, pageSize: 1
2022-10-06 04:50:23.236 DEBUG 26472 --- [nio-8080-exec-2] o.s.data.mongodb.core.MongoTemplate      : Executing aggregation: [{ &quot;$match&quot; : { &quot;$text&quot; : { &quot;$search&quot; : &quot;product&quot;, &quot;$caseSensitive&quot; : false}}}, { &quot;$match&quot; : { &quot;$or&quot; : [{ &quot;brandName&quot; : &quot;BRAND1&quot;}, { &quot;categoryName&quot; : &quot;CATEGORY2&quot;}, { &quot;subCategoryName&quot; : &quot;SUB_CATEGORY3&quot;}]}}, { &quot;$skip&quot; : 0}, { &quot;$limit&quot; : 1}] in collection product
2022-10-06 04:50:23.238  INFO 26472 --- [nio-8080-exec-2] com.example.ProductController            : Found products: [Product(id=633e1122297cce382aea07d5, status=ACTIVE, productName=product name term2, brandName=BRAND2, categoryName=CATEGORY2, subCategoryName=SUB_CATEGORY1)]
2022-10-06 04:50:28.891  INFO 26472 --- [nio-8080-exec-3] com.example.ProductController            : Request parameters: productName: product, brandName: BRAND1, categoryName: CATEGORY2, subCategoryName: SUB_CATEGORY3, pageNumber: 0, pageSize: 10
2022-10-06 04:50:28.892 DEBUG 26472 --- [nio-8080-exec-3] o.s.data.mongodb.core.MongoTemplate      : Executing aggregation: [{ &quot;$match&quot; : { &quot;$text&quot; : { &quot;$search&quot; : &quot;product&quot;, &quot;$caseSensitive&quot; : false}}}, { &quot;$match&quot; : { &quot;$or&quot; : [{ &quot;brandName&quot; : &quot;BRAND1&quot;}, { &quot;categoryName&quot; : &quot;CATEGORY2&quot;}, { &quot;subCategoryName&quot; : &quot;SUB_CATEGORY3&quot;}]}}, { &quot;$skip&quot; : 0}, { &quot;$limit&quot; : 10}] in collection product
2022-10-06 04:50:28.894  INFO 26472 --- [nio-8080-exec-3] com.example.ProductController            : Found products: [Product(id=633e1122297cce382aea07d5, status=ACTIVE, productName=product name term2, brandName=BRAND2, categoryName=CATEGORY2, subCategoryName=SUB_CATEGORY1), Product(id=633e1122297cce382aea07d4, status=ACTIVE, productName=product name term1, brandName=BRAND1, categoryName=CATEGORY1, subCategoryName=SUB_CATEGORY1)]
2022-10-06 04:50:33.354  INFO 26472 --- [nio-8080-exec-4] com.example.ProductController            : Request parameters: productName: term3, brandName: BRAND1, categoryName: CATEGORY2, subCategoryName: SUB_CATEGORY3, pageNumber: 0, pageSize: 10
2022-10-06 04:50:33.355 DEBUG 26472 --- [nio-8080-exec-4] o.s.data.mongodb.core.MongoTemplate      : Executing aggregation: [{ &quot;$match&quot; : { &quot;$text&quot; : { &quot;$search&quot; : &quot;term3&quot;, &quot;$caseSensitive&quot; : false}}}, { &quot;$match&quot; : { &quot;$or&quot; : [{ &quot;brandName&quot; : &quot;BRAND1&quot;}, { &quot;categoryName&quot; : &quot;CATEGORY2&quot;}, { &quot;subCategoryName&quot; : &quot;SUB_CATEGORY3&quot;}]}}, { &quot;$skip&quot; : 0}, { &quot;$limit&quot; : 10}] in collection product
2022-10-06 04:50:33.356  INFO 26472 --- [nio-8080-exec-4] com.example.ProductController            : Found products: []
2022-10-06 04:50:36.667  INFO 26472 --- [nio-8080-exec-5] com.example.ProductController            : Request parameters: productName: term2, brandName: BRAND1, categoryName: CATEGORY2, subCategoryName: SUB_CATEGORY3, pageNumber: 0, pageSize: 10
2022-10-06 04:50:36.667 DEBUG 26472 --- [nio-8080-exec-5] o.s.data.mongodb.core.MongoTemplate      : Executing aggregation: [{ &quot;$match&quot; : { &quot;$text&quot; : { &quot;$search&quot; : &quot;term2&quot;, &quot;$caseSensitive&quot; : false}}}, { &quot;$match&quot; : { &quot;$or&quot; : [{ &quot;brandName&quot; : &quot;BRAND1&quot;}, { &quot;categoryName&quot; : &quot;CATEGORY2&quot;}, { &quot;subCategoryName&quot; : &quot;SUB_CATEGORY3&quot;}]}}, { &quot;$skip&quot; : 0}, { &quot;$limit&quot; : 10}] in collection product
2022-10-06 04:50:36.669  INFO 26472 --- [nio-8080-exec-5] com.example.ProductController            : Found products: [Product(id=633e1122297cce382aea07d5, status=ACTIVE, productName=product name term2, brandName=BRAND2, categoryName=CATEGORY2, subCategoryName=SUB_CATEGORY1)]

"
"The JLS states, that for arrays, &quot;The enhanced for statement is equivalent to a basic for statement of the form&quot;. However if I check the generated bytecode for JDK8, for both variants different bytecode is generated, and if I try to measure the performance, surprisingly, the enhanced one seems to be giving better results(on jdk8)... Can someone advise why it's that? I'd guess it's because of incorrect jmh testing, so if it's that, please suggest how to fix that. (I know that JMH states not to test using loops, but I don't think this applies here, as I'm actually trying to measure the loops here)
My JMH testing was rather simple (probably too simple), but I cannot explain the results. Testing JMH code is below, typical results are:
JdkBenchmarks.enhanced  avgt    5  2556.281 Â±  31.789  ns/op
JdkBenchmarks.indexed   avgt    5  4032.164 Â± 100.121  ns/op

meaning typically enhanced for loop is faster, and measurement for it is more accurate than for indexed loop, so we cannot address the difference to measurement uncertainty. Principally the same results are for array initialized with random integers, or bigger arrays.
public class JdkBenchmarks {

    @Benchmark
    @BenchmarkMode(AverageTime)
    @OutputTimeUnit(NANOSECONDS)
    public void indexed(Blackhole blackhole, TestState testState) {
        int length = testState.values.length;
        for(int i = 0; i &lt; length; i++) {
            blackhole.consume(testState.values[i]);
        }
    }

    @Benchmark
    @BenchmarkMode(AverageTime)
    @OutputTimeUnit(NANOSECONDS)
    public void enhanced(Blackhole blackhole, TestState testState) {
        for (int value : testState.values) {
            blackhole.consume(value);
        }
    }


    @State(Scope.Benchmark)
    public static class TestState {
        public int[] values;

        @Setup
        public void setupArray() {
            int count = 1000;
            values = new int[count];
            for(int i = 0; i &lt; count; i++) {
                values[i] = i;
            }
        }
    }

    public static void main(String[] args) throws RunnerException {
        Options opt = new OptionsBuilder()
                .include(JdkBenchmarks.class.getSimpleName())
                .forks(1)
                .build();

        new Runner(opt).run();
    }

}

","TL;DR: You are observing what happens when JIT compiler cannot trust that values are not changing inside the loop. Additionally, in the tiny benchmark like this, Blackhole.consume costs dominate, obscuring the results.
Simplifying the test:
@Warmup(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS)
@Measurement(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS)
@Fork(3)
@BenchmarkMode(Mode.AverageTime)
@OutputTimeUnit(TimeUnit.NANOSECONDS)
@State(Scope.Benchmark)
public class JdkBenchmarks {

    public int[] values;

    @Setup
    public void setupArray() {
        int count = 1000;
        values = new int[count];
        for(int i = 0; i &lt; count; i++) {
            values[i] = i;
        }
    }

    @Benchmark
    @CompilerControl(CompilerControl.Mode.DONT_INLINE)
    public void indexed(Blackhole bh) {
        for(int i = 0; i &lt; values.length; i++) {
            bh.consume(values[i]);
        }
    }

    @Benchmark
    @CompilerControl(CompilerControl.Mode.DONT_INLINE)
    public void indexed_cached(Blackhole bh) {
        int[] vs = values;
        int length = vs.length;
        for(int i = 0; i &lt; length; i++) {
            bh.consume(vs[i]);
        }
    }

    @Benchmark
    @CompilerControl(CompilerControl.Mode.DONT_INLINE)
    public void enhanced(Blackhole bh) {
        for (int value : values) {
            bh.consume(value);
        }
    }
}

Running both enhanced and indexed_cached under -prof perfasm reveals this hot loop (I specifically did @CompilerControl(DONT_INLINE) to let @Benchmark method be compiled alone, which makes an easier to digest perfasm output):
         ↗  0x...4240: mov  0x10(%r8,%rsi,4),%r10d  ; load values[i], blackhole it
 22.68%  │  0x...4245: mov  0x14(%r8,%rsi,4),%r11d  ; ... repeat 7 more times...
         │  0x...424a: mov  0x18(%r8,%rsi,4),%r10d  ;
 20.95%  │  0x...424f: mov  0x1c(%r8,%rsi,4),%r10d  ;
  0.02%  │  0x...4254: mov  0x20(%r8,%rsi,4),%r11d  ;
 24.73%  │  0x...4259: mov  0x24(%r8,%rsi,4),%r10d  ;
  0.24%  │  0x...425e: mov  0x28(%r8,%rsi,4),%r11d  ;
 20.04%  │  0x...4263: mov  0x2c(%r8,%rsi,4),%r10d  ; 
  0.22%  │  0x...4268: add  $0x8,%esi               ; i += 8
         │  0x...426b: cmp  %ebp,%esi               ; compare i with length (in %ebp)
  0.26%  ╰  0x...426d: jl   0x...4240               ; circle back if 8 more elements available

Very efficient!
Running indexed with -prof perfasm reveals:
         ↗  0x...4170: mov  0xc(%r12,%r8,8),%r9d    ; array bounds check, load values.length
  3.42%  │  0x...4175: cmp  %r9d,%r10d              ; array bounds check, compare i
 16.02%  │  0x...4178: jae  0x...41b1               ;  failed? jump to exception handling
         │  0x...417a: lea  (%r12,%r8,8),%r11       ; load values[i], part 1
  0.04%  │  0x...417e: mov  0x10(%r11,%r10,4),%r11d ; load values[i], part 2
         │                                          ; %r11d is blackholed
 35.69%  │  0x...4183: mov  0xc(%rsi),%r8d          ; get &quot;values&quot;
  0.71%  │  0x...4187: mov  0x348(%r15),%r11        ; safepoint poll, part 1 (JVM infra)
  4.03%  │  0x...418e: inc  %r10d                   ; i++
  0.12%  │  0x...4191: test %eax,(%r11)             ; safepoint poll, part 2 (JVM infra)
 27.74%  │  0x...4194: mov  0xc(%r12,%r8,8),%r9d    ; load values.length
  8.53%  │  0x...4199: cmp  %r9d,%r10d              ; check i &lt; values.length
  0.24%  ╰  0x...419c: jl   0x...4170               ; circle back if more 

This is because Blackhole.consume call is opaque to the compiler (like many other non-inlined calls), so it has to conservatively presume that values can change in the middle of the loop!
Which means, compiler cannot stash values in a register, it cannot trust the array bounds check to always succeed, it cannot even guarantee the loop terminates (hence safepoint polls), and on top of that, loop unrolling does not want to multiply that per-element mess even more.
So you get the penalty like this (TR 3970X, JDK 17.0.2 EA, Linux x86_64):
Benchmark                     Mode  Cnt     Score   Error  Units
JdkBenchmarks.enhanced        avgt    5   144.962 ± 0.918  ns/op
JdkBenchmarks.indexed         avgt    5  1030.981 ± 3.775  ns/op ; + 880 ns/op!
JdkBenchmarks.indexed_cached  avgt    5   144.799 ± 0.643  ns/op ; same as enhanced

Additional fun part:
On most JDKs the dominating costs are the costs of calling the Blackhole.consume in this test. Compared to the cost of array access, the cost of Java-style Blackhole is quite bad. With JDK 17+ and JMH 1.34, the compiler Blackholes would be used, and thus provide much more fidelity for the test.
Without compiler blackholes, the effect hides in the Blackhole overhead nearly completely (&gt;25x overhead means we can execute a lot of bad code preceding the Blackhole call!):
Benchmark                     Mode  Cnt     Score   Error  Units
JdkBenchmarks.enhanced        avgt    5  4062.866 ± 4.736  ns/op
JdkBenchmarks.indexed         avgt    5  4071.620 ± 1.057  ns/op ; + 10 ns/op [whoops]
JdkBenchmarks.indexed_cached  avgt    5  4061.390 ± 0.692  ns/op ; same as enhanced

It would re-manifest if we drop @CompilerControl(DONT_INLINE), because the resulting generated code would be much messier:
Benchmark                     Mode  Cnt     Score    Error  Units
JdkBenchmarks.enhanced        avgt    5  4067.118 ± 40.699  ns/op
JdkBenchmarks.indexed         avgt    5  4601.370 ±  0.632  ns/op ; + 530 ns/op
JdkBenchmarks.indexed_cached  avgt    5  4064.455 ±  1.554  ns/op ; same as enhanced

"
"So I recently ran a benchmark where I compared the performance of nested streams in 3 cases:

Parallel outer stream and sequential inner stream
Parallel outer and inner streams (using parallelStream) - this effectively tests `ForkJoinPool.commonPool()
Parallel outer and inner streams but inner streams create new ForkJoinPool for each task

Here's the benchmark code (I've used JMH):
public class NestedPerf {
  @State(Scope.Benchmark)
  public static class StateData{
    public static final List&lt;Integer&gt; outerLoop = IntStream.range(0, 32).boxed().toList();
    public static final List&lt;Integer&gt; innerLoop = IntStream.range(0, 32).boxed().toList();
  }
  private static void runInNewPool(Runnable task) {
    ForkJoinPool pool = new ForkJoinPool();
    try {
      pool.submit(task).join();
    } finally {
      pool.shutdown();
    }
  }
  private static void innerParallelLoop() {
    StateData.innerLoop.parallelStream().unordered().forEach(i -&gt; {
      try {
        Thread.sleep(5);
      } catch (InterruptedException e) {
        Thread.currentThread().interrupt();
      }
    });
  }
  private static void innerSequentialLoop() {
    StateData.innerLoop.stream().unordered().forEach(i -&gt; {
      try {
        Thread.sleep(5);
      } catch (InterruptedException e) {
        Thread.currentThread().interrupt();
      }
    });
  }
  @Benchmark
  public void testingNewPool(Blackhole bh){
    StateData.outerLoop.parallelStream().unordered().forEach(i -&gt; {
      runInNewPool(ParallelPerf::innerParallelLoop);
      bh.consume(i);
    });
  }

  @Benchmark
  public void testingCommonPoolWithSequentialInner(Blackhole bh){
    StateData.outerLoop.parallelStream().unordered().forEach(i -&gt; {
      innerSequentialLoop();
      bh.consume(i);
    });
  }
  @Benchmark
  public void testingCommonPool(Blackhole bh){
    StateData.outerLoop.parallelStream().unordered().forEach(i -&gt; {
      innerParallelLoop();
      bh.consume(i);
    });
  }
}

And here is the output:
Benchmark                                         Mode  Cnt   Score   Error  Units
NestedPerf.testingCommonPool                     thrpt   25   1.935 Â± 0.005  ops/s
NestedPerf.testingCommonPoolWithSequentialInner  thrpt   25   1.744 Â± 0.007  ops/s
NestedPerf.testingNewPool                        thrpt   25  22.648 Â± 0.559  ops/s

The difference between the method with new Pools vs the method with commonPool is surprising. Does anyone have an idea as to why creating new pools makes things around 20x faster for this benchmark ?
If it helps, I'm running this on a Core i7 10850H system with 12 available CPUs (hexcore + hyperthreading).
","Why Throughput Increases
Your tasks are simply a call to Thread::sleep. That blocks the calling thread, which means the OS will not schedule the thread for execution until the specified duration elapses. This leaves the CPU free to execute any other threads. In other words, your tasks are not CPU-bound and thus do not burden the CPU. Which means throwing more threads at your set of tasks is going to increase throughput without overwhelming the CPU.
By using multiple fork-join pools, you are effectively increasing the number of threads available to execute your tasks. It's not much different from simply increasing the number of threads in a single pool. Whether you have 1 pool with 15 threads or 3 pools with 5 threads each, you still end up with a total of 15 threads.
Let's say you have 10 tasks that each sleep for 5 milliseconds. If you have 5 threads to execute those tasks, then you'll roughly see:
Start 5 tasks =&gt; Wait 5 ms =&gt; Start 5 tasks =&gt; Wait 5 ms =&gt; Done!

But if you have 10 threads you'll roughly see:
Start 10 tasks =&gt; Wait 5 ms =&gt; Done!

The first takes a total of 10 milliseconds to execute every task, the second only takes 5 milliseconds. And that's basically where the increased throughput is coming from in your tests.

Maintaining Parallelism
All that said, a ForkJoinPool has a set level of parallelism. One way it tries to maintain this parallelism is by spawning a new thread (if the maximum number of threads hasn't already been reached) when one of its threads is blocked. From the documentation:

[A ForkJoinPool] attempts to maintain enough active (or available) threads by dynamically adding, suspending, or resuming internal worker threads, even if some tasks are stalled waiting to join others. However, no such adjustments are guaranteed in the face of blocked I/O or other unmanaged synchronization. The nested ForkJoinPool.ManagedBlocker interface enables extension of the kinds of synchronization accommodated.

You're calling Thread::sleep in an unmanaged way. In other words, you're blocking the threads of the pool in such a way that the pool cannot compensate. To prevent that, consider using a ManagedBlocker. Here's an example implementation:
import java.time.Duration;
import java.util.concurrent.ForkJoinPool;

public class SleepManagedBlocker implements ForkJoinPool.ManagedBlocker {

  private final Duration sleepDuration;
  private boolean slept; // Does this need to be volatile?

  public SleepManagedBlocker(Duration slepDuration) {
    this.sleepDuration = slepDuration;
  }

  @Override
  public boolean block() throws InterruptedException {
    if (!slept) {
      slept = true;
      Thread.sleep(sleepDuration);
    }
    return slept;
  }

  @Override
  public boolean isReleasable() {
    return slept;
  }
}

Then you would replace the Thread.sleep(5) calls with:
ForkJoinPool.managedBlock(new SleepManagedBlocker(Duration.ofMillis(5)))

You should see similar throughput increases in your tests without needing to using multiple fork-join pools.

JMH Benchmarks
Here is a benchmark showing the effect of using ManagedBlocker in this case. It was compiled and executed on Java 23.
import java.time.Duration;
import java.util.concurrent.ForkJoinPool;
import java.util.concurrent.TimeUnit;
import java.util.stream.IntStream;
import org.openjdk.jmh.annotations.Benchmark;
import org.openjdk.jmh.annotations.BenchmarkMode;
import org.openjdk.jmh.annotations.Fork;
import org.openjdk.jmh.annotations.Measurement;
import org.openjdk.jmh.annotations.Mode;
import org.openjdk.jmh.annotations.OutputTimeUnit;
import org.openjdk.jmh.annotations.Param;
import org.openjdk.jmh.annotations.Scope;
import org.openjdk.jmh.annotations.State;
import org.openjdk.jmh.annotations.Warmup;
import org.openjdk.jmh.infra.Blackhole;

@Fork(value = 1, jvmArgsAppend = {&quot;-Djava.util.concurrent.ForkJoinPool.common.maximumSpares=1024&quot;})
@Warmup(iterations = 5)
@Measurement(iterations = 5)
@BenchmarkMode(Mode.Throughput)
@OutputTimeUnit(TimeUnit.SECONDS)
public class FJPBenchmarks {

  @Benchmark
  public void runTest(TestState state, Blackhole bh) {
    state.executeOuterLoop(bh);
  }

  @State(Scope.Benchmark)
  public static class TestState {

    private static final Duration SLEEP_DURATION = Duration.ofMillis(5);
    private static final int OUTER_LOOP_COUNT = 32;
    private static final int INNER_LOOP_COUNT = 32;

    @Param({&quot;sequential&quot;, &quot;parallel&quot;})
    private String sequentialMode;

    @Param({&quot;common&quot;, &quot;separate&quot;})
    private String poolMode;

    @Param({&quot;raw&quot;, &quot;managed&quot;})
    private String sleepMode;

    void executeOuterLoop(Blackhole bh) {
      IntStream.range(0, OUTER_LOOP_COUNT)
          .unordered()
          .parallel()
          .forEach(i -&gt; {
            executeInnerLoop(createInnerLoop());
            bh.consume(i);
          });
    }

    IntStream createInnerLoop() {
      var stream = IntStream.range(0, INNER_LOOP_COUNT).unordered();
      return switch (sequentialMode) {
        case &quot;sequential&quot; -&gt; stream.sequential();
        case &quot;parallel&quot; -&gt; stream.parallel();
        default -&gt; throw new IllegalStateException(&quot;bad sequentialMode: &quot; + sequentialMode);
      };
    }

    void executeInnerLoop(IntStream loop) {
      var sleeper = getSleeper();
      switch (poolMode) {
        case &quot;common&quot; -&gt; loop.forEach(_ -&gt; sleeper.sleepUnchecked());
        case &quot;separate&quot; -&gt; {
          try (var pool = new ForkJoinPool()) {
            loop.forEach(_ -&gt; pool.submit(sleeper::sleepUnchecked).join());
          }
        }
        default -&gt; throw new IllegalStateException(&quot;bad poolMode: &quot; + poolMode);
      }
    }

    Sleeper getSleeper() {
      return switch (sleepMode) {
        case &quot;raw&quot; -&gt; () -&gt; Thread.sleep(SLEEP_DURATION);
        case &quot;managed&quot; -&gt; () -&gt; ForkJoinPool.managedBlock(new SleepManagedBlocker());
        default -&gt; throw new IllegalStateException(&quot;bad sleepMode: &quot; + sleepMode);
      };
    }

    @FunctionalInterface
    interface Sleeper {
  
      void sleep() throws InterruptedException;

      default Void sleepUnchecked() {
        try {
          sleep();
        } catch (InterruptedException ex) {
          throw new RuntimeException(ex);
        }
        return null;
      }
    }

    static class SleepManagedBlocker implements ForkJoinPool.ManagedBlocker {

      private boolean slept;

      @Override
      public boolean block() throws InterruptedException {
        if (!slept) {
          slept = true;
          Thread.sleep(SLEEP_DURATION);
        }
        return true;
      }

      @Override
      public boolean isReleasable() {
        return slept;
      }
    }
  }
}

Results (from executing the benchmark on a computer with 8 processors):
Benchmark              (poolMode)  (sequentialMode)  (sleepMode)   Mode  Cnt   Score   Error  Units
FJPBenchmarks.runTest      common        sequential          raw  thrpt    5   1.463 � 0.022  ops/s
FJPBenchmarks.runTest      common        sequential      managed  thrpt    5   5.858 � 0.026  ops/s
FJPBenchmarks.runTest      common          parallel          raw  thrpt    5   1.454 � 0.044  ops/s
FJPBenchmarks.runTest      common          parallel      managed  thrpt    5  35.997 � 0.234  ops/s
FJPBenchmarks.runTest    separate        sequential          raw  thrpt    5   1.426 � 0.325  ops/s
FJPBenchmarks.runTest    separate        sequential      managed  thrpt    5   1.348 � 0.157  ops/s
FJPBenchmarks.runTest    separate          parallel          raw  thrpt    5  13.505 � 1.175  ops/s
FJPBenchmarks.runTest    separate          parallel      managed  thrpt    5  16.864 � 0.186  ops/s

"
"I am trying to have a JavaFX 3D Sphere, textured with a texture of the earth. The texture is this one (from Wikipedia, an equirectangular projection):

The sphere is rendered as follows:

You can clearly see that, at the poles, the texture is not preserving the proportions anymore. I found a bug files on the openJDK system, which I think is related to this behaviour: https://bugs.java.com/bugdatabase/view_bug.do?bug_id=8092112
Sadly, in 7 years nobody made the change that the person filing the bug requested. Do you know if there is an alternative way to properly render an equirectangular sphere projection on a JavaFX 3D Sphere?
Just for reference, the code that I using is:
    Sphere earthSphere = new Sphere(EARTH_RADIUS, 256);
    PhongMaterial material = new PhongMaterial();
    material.setDiffuseMap(new Image(Main.class.getResourceAsStream(&quot;/images/earth2.jpg&quot;)));
    earthSphere.setMaterial(material);

","At the end I implement it myself using a mesh, for the purposes I needed for. Here is the code, in case you are interested (it will end up in a GitHub project anyway):
public static Group createEarthSphere() {
    // Use triangular mesh
    int latLevels = 90;
    int lonLevels = 180;

    TriangleMesh mesh = new TriangleMesh(VertexFormat.POINT_NORMAL_TEXCOORD);
    double radius = EARTH_RADIUS;

    double latIncAngle = (Math.PI/latLevels);
    double lonIncAngle = (Math.PI * 2)/lonLevels;
    double textLatIncr = 1.0/latLevels;
    double textLonIncr = 1.0/lonLevels;

    int currentPointOffset = 0;
    int currentNormalOffset = 0;
    int currentTextOffset = 0;
    for(int i = 0; i &lt; latLevels; ++i) {
        for(int j = 0; j &lt; lonLevels; ++j) {
            // The point list is: top left - bottom left - bottom right - top right
            // The faces-normal points are: (0,0) (1,1) (2,2) (0,3) (2,4) (3,5)
            Point3D tp1 = new Point3D(0,radius * Math.cos(Math.PI - (i * latIncAngle)), radius * Math.sin(Math.PI - (i * latIncAngle)));
            Point3D tp2 = new Point3D(0,radius * Math.cos(Math.PI - (i * latIncAngle + latIncAngle)), radius * Math.sin(Math.PI - (i * latIncAngle + latIncAngle)));
            Point3D topLeft = new Rotate(Math.toDegrees(j * lonIncAngle), new Point3D(0, 1, 0)).transform(tp1);
            Point3D bottomLeft =  new Rotate(Math.toDegrees(j * lonIncAngle), new Point3D(0, 1, 0)).transform(tp2);
            Point3D bottomRight = new Rotate(Math.toDegrees(j * lonIncAngle + lonIncAngle), new Point3D(0, 1, 0)).transform(tp2);
            Point3D topRight = new Rotate(Math.toDegrees(j * lonIncAngle + lonIncAngle), new Point3D(0, 1, 0)).transform(tp1);

            // Compute normals
            Point3D topLeftNormal_1 = computeNormal(topLeft, bottomLeft, bottomRight); // 0
            Point3D bottomLeftNormal_1 = computeNormal(bottomLeft, bottomRight, topLeft); // 1
            Point3D bottomRightNormal_1 = computeNormal(bottomRight, topLeft, bottomLeft); // 2
            Point3D topLeftNormal_2 = computeNormal(topLeft, bottomRight, topRight); // 3
            Point3D bottomRightNormal_2 = computeNormal(bottomRight, topRight, topLeft); // 4
            Point3D topRightNormal_2 = computeNormal(topRight, topLeft, bottomRight); // 5

            // Add points
            mesh.getPoints().addAll((float) topLeft.getX(), (float) topLeft.getY(), (float) topLeft.getZ()); // 0
            mesh.getPoints().addAll((float) bottomLeft.getX(), (float) bottomLeft.getY(), (float) bottomLeft.getZ()); // 1
            mesh.getPoints().addAll((float) bottomRight.getX(), (float) bottomRight.getY(), (float) bottomRight.getZ()); // 2
            mesh.getPoints().addAll((float) topRight.getX(), (float) topRight.getY(), (float) topRight.getZ()); // 3

            // Add normals
            mesh.getNormals().addAll((float) topLeftNormal_1.getX(), (float) topLeftNormal_1.getY(), (float) topLeftNormal_1.getZ()); // 0
            mesh.getNormals().addAll((float) bottomLeftNormal_1.getX(), (float) bottomLeftNormal_1.getY(), (float) bottomLeftNormal_1.getZ()); // 1
            mesh.getNormals().addAll((float) bottomRightNormal_1.getX(), (float) bottomRightNormal_1.getY(), (float) bottomRightNormal_1.getZ()); // 2
            mesh.getNormals().addAll((float) topLeftNormal_2.getX(), (float) topLeftNormal_2.getY(), (float) topLeftNormal_2.getZ()); // 3
            mesh.getNormals().addAll((float) bottomRightNormal_2.getX(), (float) bottomRightNormal_2.getY(), (float) bottomRightNormal_2.getZ()); // 4
            mesh.getNormals().addAll((float) topRightNormal_2.getX(), (float) topRightNormal_2.getY(), (float) topRightNormal_2.getZ()); // 5

            // Add texture
            float[] p0t = { (float) (i * textLatIncr), 1.0f - (float) (j * textLonIncr) };
            float[] p1t = { (float) (i * textLatIncr + textLatIncr), 1.0f - (float) (j * textLonIncr) };
            float[] p2t = { (float) (i * textLatIncr + textLatIncr), 1.0f - (float) (j * textLonIncr + textLonIncr) };
            float[] p3t = { (float) (i * textLatIncr), 1.0f - (float) (j * textLonIncr + textLonIncr) };

            mesh.getTexCoords().addAll(
                    p0t[1], p0t[0],
                    p1t[1], p1t[0],
                    p2t[1], p2t[0],
                    p3t[1], p3t[0]
            );

            // Add faces
            mesh.getFaces().addAll(
                    currentPointOffset + 0, currentNormalOffset + 0, currentTextOffset + 0,
                    currentPointOffset + 2, currentNormalOffset + 2, currentTextOffset + 2,
                    currentPointOffset + 1, currentNormalOffset + 1, currentTextOffset + 1,
                    currentPointOffset + 0, currentNormalOffset + 3, currentTextOffset + 0,
                    currentPointOffset + 3, currentNormalOffset + 5, currentTextOffset + 3,
                    currentPointOffset + 2, currentNormalOffset + 4, currentTextOffset + 2

            );

            currentPointOffset += 4;
            currentNormalOffset += 6;
            currentTextOffset += 4;
        }
    }

    MeshView meshView = new MeshView(mesh);
    meshView.setCullFace(CullFace.BACK);
    PhongMaterial material = new PhongMaterial();
    material.setDiffuseMap(new Image(Main.class.getResourceAsStream(&quot;/images/earth.jpg&quot;)));
    meshView.setMaterial(material);
    return new Group(meshView);
}

private static Point3D computeNormal(Point3D p1, Point3D p2, Point3D p3) {
    return (p3.subtract(p1).normalize()).crossProduct(p2.subtract(p1).normalize()).normalize();
}

The result is:

Now everything is exactly where it should be, and lat/lon are correctly matching the texture.
"
"I am learning Java.  When I call go(x), I get â€˜intâ€™.  Why not â€˜Shortâ€™?
public class test {
    public static void go(Short n) {System.out.println(&quot;Short&quot;);}
    public static void go(int n) {System.out.println(&quot;int&quot;);}
    
    public static void main(String[] args) {
        short x=11;
        go(x);
    }
}

","When you make a call to an overloaded method, the compiler needs to decide statically which of the overloads will be called.  Intuitively, this is done by looking at each of the overloads' signatures, and working out which one is the best match ... based on the static types of the argument expressions.  If there is a tie (i.e. no single &quot;best&quot; match), the compiler gives an error saying that the method call is ambiguous.
This is the code of your example.
public class test {
    public static void go(Short n) {System.out.println(&quot;Short&quot;);}
    public static void go(int n) {System.out.println(&quot;int&quot;);}
    
    public static void main(String[] args) {
        short x=11;
        go(x);
    }
}

The reason that this prints &quot;int&quot; rather than &quot;Short&quot; is because go(int) is deemed to be a better match than go(Short) to a call where the argument expression's static type is short.
(Technically, short is a subtype of int, but not of Short.)
If we change your code to this:
public class test {
    public static void go(short n) {System.out.println(&quot;short&quot;);}
    public static void go(int n) {System.out.println(&quot;int&quot;);}
    
    public static void main(String[] args) {
        short x=11;
        go(x);
    }
}

we will now see that short is printed.   The type short is a subtype1 of both short and int, but go(short) is a closer match than go(int).
The relevant section of the Java Language Specification is JLS 15.2.   But beware that is long and complicated ... and NOT recommended for beginners to try to read.  (Indeed, I would posit that most Java programmers don't fully understand all of what it says.  Myself included!)

1 - This is according to the definition of subtype used in the JLS.
"
"I have a Streaming Processor that processes messages from a Kafka InputTopic to an OutputTopic. Furthermore I have multiple tenants for whom this processing shall take place. Lets call them tenant A and tenant B, but there can be more than a dozen tenants that the application should process. The input and output topics follow the naming convention: A-input, B-input, ... and A-output, B-output...
The function definition is like:
@Configuration
public class StreamProcessorConfig {

    @Bean
    public Function&lt;KStream&lt;String, InputType&gt;, KStream&lt;String, OutputType&gt;&gt; myfunctiondefinition() {
        return inputTypeStream -&gt; inputTypeStream.map((String k, InputType v) -&gt; {
            return KeyValue.pair(k, OutputType.createFrom(v));
        });
    }

}

My application.yaml now configures the streaming application for tenant A:
tenant: A

spring.cloud.function.definition: myfunctiondefinition
spring.cloud.stream.kafka.streams.binder.functions.myfunctiondefinition:
    applicationId: ${spring.application.name}-myfunctiondefinition

spring.cloud.stream.bindings.myfunctiondefinition-in-0:
  destination: ${tenant}-input
spring.cloud.stream.bindings.myfunctiondefinition-out-0:
  destination: ${tenant}-output


How can I modify the configuration to add an instance for tenant B? Of course I could duplicate myfunctiondefinition() as well as all configuration keys, but I'm looking for a way to dynamically add tenants fast and clean solely through configuration. Is this possible?
Note: Running another instance of the application for tenant B and further tenants is sadly not an option.
","We found a solution to this problem by manually registering the function beans. Sadly this was not quite as easy as we thought it would be. FunctionDetectorCondition (https://github.com/spring-cloud/spring-cloud-stream-binder-kafka/blob/main/spring-cloud-stream-binder-kafka-streams/src/main/java/org/springframework/cloud/stream/binder/kafka/streams/function/FunctionDetectorCondition.java) requires an AnnotatedBeanDefinition that used as a template for the actual Stream Processing bean. This could be taken as a  proposal to spring cloud streams for registering a function defintion template that can be used multiple times.
To reach this goal we initialise a factory bean instead of the stream processor function itself:
@Configuration
public class StreamProcessorConfig {

    @Bean
    public MyFunctionDefinitionFactory myFunctionDefinitionFactory() {
        return new MyFunctionDefinitionFactory();
    }
}

The factory creates the stream processor function:
public class MyFunctionDefinitionFactory {

    public Function&lt;KStream&lt;String, InputType&gt;, 
               KStream&lt;String, OutputType&gt;&gt; myfunctiondefinition() {
        return inputTypeStream -&gt; inputTypeStream.map((String k, InputType v) -&gt; {
            return KeyValue.pair(k, OutputType.createFrom(v));
        });
    }
}

Now we need a Dummy Bean Interface that is Required for Spring Cloud Streams to apply its logic to create the stream processor:
// Behaves as dummy bean for spring cloud stream
// Has to be the same name as the original streaming function in the factory.
// In this case we named the method &quot;myfunctiondefinition&quot;,
// so the dummy-bean has to get the name &quot;Myfunctiondefinition&quot;.
public class Myfunctiondefinition implements Function&lt;KStream&lt;String, InputType&gt;, 
                KStream&lt;String, OutputType&gt;&gt; {

    // !!! It could be that changes are needed if spring cloud streams changes the logic
    // Method myfunctiondefinition() is needed, because spring cloud streams searches for 
    // a method with the same name as the class in 
    // FunctionDetectorCondition:pruneFunctionBeansForKafkaStreams
    public Function&lt;KStream&lt;String, InputType&gt;, 
               KStream&lt;String, OutputType&gt;&gt; myfunctiondefinition() {
        return null;
    }

    // Needed for the interface implementation. Spring cloud streams needs
    // the class Function to identify a stream processor candidate.
    @Override
    public KStream&lt;String, OutputType&gt; apply(KStream&lt;String, InputType&gt; input) {
        return null;
    }
}

Now that we have all things in place we can register a bean per tenant. We do this within an ApplicationContextInitializer that creates a bean definition with a factory method and iterate over the functions that we will define in the configuration file application.yaml.
public class StreamProcessorInitializer 
    implements ApplicationContextInitializer&lt;GenericWebApplicationContext&gt; {

    @Override
    public void initialize(GenericWebApplicationContext context) {
        String functionDefinitions = context.getEnvironment()
            .getProperty(&quot;spring.cloud.function.definition&quot;);
        String splitter = context.getEnvironment()
            .getProperty(&quot;spring.cloud.function.definition.splitter&quot;);
        String factoryName = CaseFormat.UPPER_CAMEL.
            .to(CaseFormat.LOWER_CAMEL, MyFunctionDefinitionFactory.class.getSimpleName());
        String factoryMethodName =
             MyFunctionDefinitionFactory.class.getMethods()[0].getName();

        AnnotatedGenericBeanDefinition def = 
            new AnnotatedGenericBeanDefinition(Myfunctiondefinition.class);
        def.setFactoryBeanName(factoryName);
        def.setFactoryMethodName(factoryMethodName);

        Arrays.stream(functionDefinitions.split(splitter))
           .forEach(function -&gt; context.registerBeanDefinition(function, def));
    }
}

Finally we can dynamically define functions within the application.yaml. This can be done by helm oder kustomize to configure the specific tenant environment:
#--------------------------------------------------------------------------------------------------------------------------------------
# streaming processor functions (going to be filled by helm)
#--------------------------------------------------------------------------------------------------------------------------------------
spring.cloud.function.definition: &lt;name1&gt;,&lt;name2&gt;,...
#--Note-- required as spring cloud streams has changed the splitter in the past
spring.cloud.function.definition.splitter: ;

# Properties per function (&lt;name&gt;)
spring.cloud.stream.kafka.streams.binder.functions.&lt;name&gt;.applicationId: ${tenant}-${spring.application.name}-&lt;name&gt;
# configuring dlq (if you have one)
spring.cloud.stream.kafka.streams.bindings.&lt;name&gt;-in-0.consumer.deserializationExceptionHandler: sendToDlq
spring.cloud.stream.kafka.streams.bindings.&lt;name&gt;-in-0.consumer.dlqName: ${tenant}-&lt;name&gt;-dlq
# configuring in- and output topics
spring.cloud.stream.bindings.&lt;name&gt;-in-0.destination: ${tenant}-&lt;inputname&gt;
spring.cloud.stream.bindings.&lt;name&gt;-out-0.destination: ${tenant}-&lt;outputname&gt;

"
"I am updating a Kotlin (v1.9.25) Spring Boot (v3.3.1) project from Java 17 to Java 21 in order to enable Virtual Threads.
In our service, almost all requests acquire one database connection and hold to it for the whole request, while some very specific ones require more than one. To avoid database connection starvation, we set the maximum number of database connections to be just a little above the maximum concurrent requests.
spring.threads.virtual.enabled: true
spring.datasource.hikari.maximum-pool-size: 50
server.tomcat.threads.max: 4 # used to be 45 before virtual threads 

Up to now, we control maximum concurrent requests by means of server.tomcat.threads.max, but with virtual threads it all changes: the idea, as far as I understand, is to have a executor receiving an unlimited amount of tasks, so no limits here.
That leaves me to my question: how can I limit the maximum number of concurrent connections on my service while using virtual threads?
I thought of implementing a semaphore but something seems to be off with this approach, I though that it would be configurable.
Thank you very much!
","While recommended way to restrict number of threads accessing a limited resource  is to Use Semaphores to Limit Concurrency of Virtual Threads, Stack Overflow thread In java How to migrate from Executors.newFixedThreadPool(MAX_THREAD_COUNT()) to Virtual Thread discusses alternative way to do the restriction - a fixed thread pool and argues that it is equivalent to Semaphore in terms of functionality and efficiency.
In addition, the latter way allows more Spring-&quot;configurable&quot; solutions.
To restrict the amount of Tomcat worker thread, like the OP requested, we could set our own Tomcat Protocol Handler Executor:
@Configuration
public class TomcatFixedThreadsCustomizer
    implements WebServerFactoryCustomizer&lt;ConfigurableTomcatWebServerFactory&gt;, Ordered {

    @Value(&quot;${max.thread.count}&quot;)
    private int maxThreadCount;

    @Override
    public void customize(ConfigurableTomcatWebServerFactory factory) {
        factory.addProtocolHandlerCustomizers((protocolHandler) -&gt; 
            protocolHandler.setExecutor(Executors.newFixedThreadPool(maxThreadCount, Thread.ofVirtual().factory())));
    }

    @Override
    public int getOrder() {
        return 2;// need to be executed after TomcatWebServerFactoryCustomizer;
   }

}

Note that the line
Executors.newFixedThreadPool(maxThreadCount, Thread.ofVirtual().factory())

actually overrides spring.threads.virtual.enabled setting for this custom  executor - the Tomcat worker threads will be virtual anyway. By other hand, if you don't want to have Tomcat worker threads be virtual, you don't have to - use a default fixed pool for that:
Executors.newFixedThreadPool(maxThreadCount)

Remember, however, that in this case all Tomcat worker threads will be under the restriction of maxThreadCount, even those who don't use critical resources like DB Connections in the OP case.
Other solution is based upon Semaphore, recommended by Oracle documentation, and Spring AOP. An AOP Aspect restricts an access to critical resource:
@Aspect
public class RestrictedResourceAspect {

    private final Semaphore semaphore;

    public RestrictedResourceAspect(int permits) {
        semaphore = new Semaphore(permits);
    }

    @Around(&quot;within(com.github.webapp.controller..*)&quot;)
    public Object accessRestrictedResource(ProceedingJoinPoint jp) throws Throwable  {
        semaphore.acquire();
        try {
            return jp.proceed();
        } finally {
            semaphore.release();
        }
    }

}

The pointcut expression above
&quot;within(com.github.webapp.controller..*)&quot;

applies the Aspect to each public method in com.github.webapp.controller package as an example. It is possible to apply it to methods, annotated with certain annotation, RequestMapping, for example; the only requirement is that the method should belong to a Spring Bean and be public. For pointcut syntax please see details in Spring AOP documentation.
Finally, we need to configure the Aspect:
@Configuration
@EnableAsync
@EnableAspectJAutoProxy
public class AppConfig {

    @Value(&quot;${max.thread.count}&quot;)
    private int maxThreadCount;

    @Bean
    public RestrictedResourceAspect restrictedResourceAspect() {
        return new RestrictedResourceAspect(maxThreadCount);
    }
}

This second approach is more flexible than Tomcat Protocol Handler Executor-based one as it allows to make a distinction between the methods, which do access critical resources, and methods, which do not access them.
"
"I am trying to make my JavaFx-Application executable using Maven and Visual Studio Code.
After some time spent on this topic, I found some posts mentioning jlink.
I am a newcomer when it comes to packaging Java/JavaFX applications, so I gave it a try.
Currently, I can at least execute the launcher for the package.
But immediately after starting the application, a NullPointerException is thrown:
Cannot invoke &quot;Object.toString()&quot; because the return value of &quot;java.lang.Class.getResource(String)&quot; is null.
For styling the components of my view I created some .css-files and put them inside a /style directory. This directory I placed this, according to the sample JavaFx application, inside a /resources directory created by Maven. In a similar manner, I proceeded with my sound and image files.
Here you can see an excerpt of my directory structure.
|
|--src/main
|  |
|  |-- java
|  |   | ...
|  |
|  |-- resources
|      |
|      |-- img
|      |   | ...    
|      |
|      |-- style
|      |   | ...
|      |
|      |-- sound
|          | ...
|
|-- target
    |
    |-- classes
    |   | ...
    |   |
    |   |-- img
    |   |   | ...
    |   |
    |   |-- style
    |   |   | ...
    |   |
    |   |-- sound
    |   |   | ...
    |
    |-- ...
    |
    |-- app
        |
        |-- bin
        |-- ...

Now I am trying to access my resources from within my application.
This was my first approach. It works just fine when running from VSCode.
    public static final String PATH_TO_STYLESHEET = App.class.getResource(&quot;/style&quot;).toString();
    public static final String PATH_TO_IMG = App.class.getResource(&quot;/img&quot;).toString();
    public static final String PATH_TO_SOUNDS = App.class.getResource(&quot;/sounds&quot;).toString();

But after running jlink, my application crashes, showing the NullPointerException mentioned earlier.
Here is my pom.xml:
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt;
    
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
    &lt;artifactId&gt;App&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    
    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;maven.compiler.release&gt;19&lt;/maven.compiler.release&gt;
        &lt;javafx.version&gt;19&lt;/javafx.version&gt;
        &lt;javafx.maven.plugin.version&gt;0.0.8&lt;/javafx.maven.plugin.version&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
            &lt;artifactId&gt;javafx-controls&lt;/artifactId&gt;
            &lt;version&gt;${javafx.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
            &lt;artifactId&gt;javafx-media&lt;/artifactId&gt;
            &lt;version&gt;${javafx.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
            &lt;artifactId&gt;javafx-fxml&lt;/artifactId&gt;
            &lt;version&gt;${javafx.version}&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.8.1&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;release&gt;${maven.compiler.release}&lt;/release&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
                &lt;artifactId&gt;javafx-maven-plugin&lt;/artifactId&gt;
                &lt;version&gt;${javafx.maven.plugin.version}&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;release&gt;${maven.compiler.release}&lt;/release&gt;
                    &lt;jlinkImageName&gt;App&lt;/jlinkImageName&gt;
                    &lt;launcher&gt;launcher&lt;/launcher&gt;
                    &lt;mainClass&gt;com.test.App&lt;/mainClass&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
        &lt;resources&gt;
            &lt;resource&gt;
                &lt;directory&gt;src/main/resources&lt;/directory&gt;
            &lt;/resource&gt;
        &lt;/resources&gt;
    &lt;/build&gt;
    
&lt;/project&gt;

And this is the command I have been using for creating the package.
mvn javafx:jlink -f pom.xml
Does anyone have an idea how I can get the path to my stylesheets, images, and sounds, after running jlink? The path is absolutely sufficient. I do not need a file itself.
Is there an option to copy the resources to a specific location?
","Problem
You have code such as the following:

public static final String PATH_TO_IMG = App.class.getResource(&quot;/img&quot;).toString();


This is trying to get the resource &quot;/img&quot;. But according to your question, that is not a resource per se, but instead a directory (i.e., a package). And the problem appears to be the inconsistent behavior of Class#getResource(String) when the String argument denotes a directory. When your code is not in a JRT image then the call to #getResource(String) will return a URL; when your code is packaged in a JRT image then the same call will return null, despite the fact the directory exists.
I don't know if this behavior is a bug or simply undefined. One interesting thing is ModuleReader#find(String) clearly is capable of finding directories:

Finds a resource, returning a URI to the resource in the module.
If the module reader can determine that the name locates a directory then the resulting URI will end with a slash ('/').

That indicates, to me at least, that what you're trying to do should be possible. But even that method fails when the module is packaged in a JRT image (by returning an empty Optional). Note that if you query the ModuleReader#list() method it will include directories when the module is not in a JRT image, but those same directories are not included when the module is in a JRT image.
Example
I've put a minimal example demonstrating this problem at the end of this answer.

A Solution
I assume you're using these constants (e.g., PATH_TO_IMG) to do stuff like the following:
Image image = new Image(PATH_TO_IMG + &quot;foo.png&quot;);

Which avoids having calls to SomeClass.class.getResource(&quot;...&quot;).toString() everywhere. If this is your goal, then I can think of at least one solution. Change your constants to simply reference the resource root. For example:
public static final String IMG_ROOT = &quot;/img&quot;;

Then create a utility method to resolve the resource:
public static String getImagePath(String name) {
    var resource = IMG_ROOT + &quot;/&quot; + name;
    var url = App.class.getResource(resource);
    if (url == null) {
        throw new RuntimeException(&quot;could not find resource: &quot; + resource);
    }
    return url.toString();
}

And then you can use that utility method like so:
Image image = new Image(getImagePath(&quot;foo.png&quot;));

Possible Alternative
Another option might be to make use of the JRT FileSystem implementation. Something like the following:
FileSystem jrtFs = FileSystems.getFileSystem(URI.create(&quot;jrt:/&quot;));
Path path = jrtFs.getPath(&quot;modules&quot;, &quot;&lt;module-name&gt;&quot;, &quot;img&quot;);
// Note: Doesn't seem to include the trailing '/'
String pathToImg = path.toUri().toString();

Though you'll have to detect if your code is in a JRT image or not.

Minimal Example
Given this doesn't have to do with JavaFX specifically, I've created a minimal example to demonstrate this problem.

Maven 3.8.6
OpenJDK 19.0.1 2022-10-18
Tested on Windows 11

Source Code
Project structure:
|   pom.xml
|
\---src
    \---main
        +---java
        |   |   module-info.java
        |   |
        |   \---sample
        |           Main.java
        |
        \---resources
            \---data
                    file.txt

pom.xml:
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt;
    
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;groupId&gt;sample&lt;/groupId&gt;
    &lt;artifactId&gt;app&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    
    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;maven.compiler.release&gt;19&lt;/maven.compiler.release&gt;
    &lt;/properties&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.10.1&lt;/version&gt;
            &lt;/plugin&gt;
            
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.3.0&lt;/version&gt;
            &lt;/plugin&gt;

            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.3.0&lt;/version&gt;
            &lt;/plugin&gt;

            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-jlink-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.1.0&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;launcher&gt;app=app/sample.Main&lt;/launcher&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;

module-info.java:
module app {}

Main.java:
package sample;

public class Main {

    public static void main(String[] args) {
        var modRef = Main.class.getModule()
            .getLayer()
            .configuration()
            .findModule(Main.class.getModule().getName())
            .orElseThrow()
            .reference();
        System.out.printf(&quot;Module Location = %s%n%n&quot;, modRef.location().orElseThrow());

        var dataUrl = Main.class.getResource(&quot;/data&quot;);
        var fileUrl = Main.class.getResource(&quot;/data/file.txt&quot;);
        System.out.printf(&quot;Data URL = %s%nFile URL = %s%n%n&quot;, dataUrl, fileUrl);
    }
}

Building
I ran these two commands to build the project:

mvn compile jar:jar
mvn jlink:jlink

For whatever reason, doing mvn compile jar:jar jlink:jlink caused the jlink task to fail.
Output
And here is the different output for the different packaging:
Exploded module:
...&gt; java -p target\classes -m app/sample.Main
Module Location = file:///C:/Users/***/Desktop/jlink-tests/target/classes/

Data URL = file:/C:/Users/***/Desktop/jlink-tests/target/classes/data/
File URL = file:/C:/Users/***/Desktop/jlink-tests/target/classes/data/file.txt

Modular JAR:
...&gt; java -p target\app-1.0-SNAPSHOT.jar -m app/sample.Main
Module Location = file:///C:/Users/***/Desktop/jlink-tests/target/app-1.0-SNAPSHOT.jar

Data URL = jar:file:///C:/Users/***/Desktop/jlink-tests/target/app-1.0-SNAPSHOT.jar!/data/
File URL = jar:file:///C:/Users/***/Desktop/jlink-tests/target/app-1.0-SNAPSHOT.jar!/data/file.txt

JRT Image:
...&gt; .\target\maven-jlink\default\bin\app
Module Location = jrt:/app

Data URL = null
File URL = jrt:/app/data/file.txt

Results
As you can see, the call to getResource(&quot;/data/file.txt&quot;) worked every time, but the call to getResource(&quot;/data&quot;) did not work for the JRT-packaged version.
"
"I have this simple class:
Test.java:
import javafx.animation.FadeTransition;
import javafx.animation.Timeline;
import javafx.application.Application;
import javafx.scene.Scene;
import javafx.scene.control.Button;
import javafx.scene.layout.Pane;
import javafx.stage.Stage;
import javafx.util.Duration;

public class Test extends Application {
    @Override
    public void start(Stage stage) throws Exception {
        Pane pane = new Pane();
        
        Button testButton = new Button(&quot;Test&quot;);
        testButton.setStyle(&quot;-fx-background-color: green;&quot;);

        pane.getChildren().add(testButton);
        pane.setStyle(&quot;-fx-background-color: red;&quot;);

        FadeTransition transition = new FadeTransition(Duration.millis(5000), pane);
        transition.setFromValue(1.0);
        transition.setToValue(0.0);
        transition.setCycleCount(Timeline.INDEFINITE);
        transition.setAutoReverse(true);
        transition.play();

        Scene scene = new Scene(pane, 500, 500);

        stage.setMinWidth(500);
        stage.setMinHeight(500);

        stage.setTitle(&quot;Test&quot;);
        stage.setResizable(false);

        stage.setScene(scene);
        stage.show();
    }
}

It looks like this:

when it fades however it becomes this:

How do I make it so that the fade transition only affects the red background and doesn't affect the green button?
So that it looks like this:

","using stackpane

You can use StackPane  as root and both : Pane and Button  children of stackpane . Button is not affected  by transition since is no longer child of pane .
if you need different aligments for different nodes you can use static method setAligment from StackPane class , wich requires a child node and position as arguments
public class  App extends Application {
    @Override
    public void start(Stage stage) throws Exception {
        
        Pane pane = new Pane();
        
        Button testButton = new Button(&quot;Test&quot;);
        testButton.setStyle(&quot;-fx-background-color: green;&quot;);

        StackPane stackPane = new StackPane(pane,testButton);
        stackPane.setAlignment(Pos.TOP_LEFT);
        
        pane.setStyle(&quot;-fx-background-color: red;&quot;);

        FadeTransition transition = new FadeTransition(Duration.millis(5000), pane);
        transition.setFromValue(1.0);
        transition.setToValue(0.0);
        transition.setCycleCount(Timeline.INDEFINITE);
        transition.setAutoReverse(true);
        transition.play();

        Scene scene = new Scene(stackPane, 500, 500);

        stage.setMinWidth(500);
        stage.setMinHeight(500);

        stage.setTitle(&quot;Test&quot;);
        stage.setResizable(false);

        stage.setScene(scene);
        stage.show();
    }
}

"
"I want to generate QR code with some text using JAVA like this.
please check this image. This is how I want to generate my QR code.
(with user name and event name text)
This is my code and this generate only (QR) code, (not any additional text). If anyone know how to generate QR code with text please help me.
import java.io.File;
import java.util.HashMap;
import java.util.Map;
import com.google.zxing.BarcodeFormat;
import com.google.zxing.EncodeHintType;
import com.google.zxing.MultiFormatWriter;
import com.google.zxing.client.j2se.MatrixToImageWriter;
import com.google.zxing.common.BitMatrix;
import com.google.zxing.qrcode.decoder.ErrorCorrectionLevel;
public class Create_QR {
    public static void main(String[] args) {
        try {
            String qrCodeData = &quot;This is the text&quot;;
            String filePath = &quot;C:\\Users\\Nirmalw\\Desktop\\Projects\\QR\\test\\test_img\\my_QR.png&quot;;
            String charset = &quot;UTF-8&quot;; // or &quot;ISO-8859-1&quot;

            Map &lt; EncodeHintType, ErrorCorrectionLevel &gt; hintMap = new HashMap &lt; EncodeHintType, ErrorCorrectionLevel &gt; ();

            hintMap.put(EncodeHintType.ERROR_CORRECTION, ErrorCorrectionLevel.L);

            BitMatrix matrix = new MultiFormatWriter().encode(new String(qrCodeData.getBytes(charset), charset),
                    BarcodeFormat.QR_CODE, 500, 500, hintMap);

            MatrixToImageWriter.writeToFile (matrix, filePath.substring(filePath.lastIndexOf('.') + 1), new File(filePath));

            System.out.println(&quot;QR Code created successfully!&quot;);
        } catch (Exception e) {
            System.err.println(e);
        }
    }
}

","You can generate a QR code with text in Java using Free Spire.Barcode for Java API. First, download the API's jar from this link or install it from Maven Repository:
    &lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;com.e-iceblue&lt;/id&gt;
        &lt;name&gt;e-iceblue&lt;/name&gt;
        &lt;url&gt;https://repo.e-iceblue.com/nexus/content/groups/public/&lt;/url&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;
&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;e-iceblue&lt;/groupId&gt;
        &lt;artifactId&gt;spire.barcode.free&lt;/artifactId&gt;
        &lt;version&gt;5.1.1&lt;/version&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;

Next, refer to the following code sample:
import com.spire.barcode.BarCodeGenerator;
import com.spire.barcode.BarCodeType;
import com.spire.barcode.BarcodeSettings;
import com.spire.barcode.QRCodeECL;

import javax.imageio.ImageIO;
import java.awt.image.BufferedImage;
import java.io.File;
import java.io.IOException;

public class GenerateQRCode {
    public static void main(String []args) throws IOException {
        //Instantiate a BarcodeSettings object
        BarcodeSettings settings = new BarcodeSettings();
        //Set barcode type
        settings.setType(BarCodeType.QR_Code);
        //Set barcode data
        String data = &quot;https://stackoverflow.com/&quot;;
        settings.setData(data);
        //Set barcode module width
        settings.setX(2);
        //Set error correction level
        settings.setQRCodeECL(QRCodeECL.M);

        //Set top text
        settings.setTopText(&quot;User Name&quot;);
        //Set bottom text
        settings.setBottomText(&quot;Event Name&quot;);

        //Set text visibility
        settings.setShowText(false);
        settings.setShowTopText(true);
        settings.setShowBottomText(true);

        //Set border visibility
        settings.hasBorder(false);

        //Instantiate a BarCodeGenerator object based on the specific settings
        BarCodeGenerator barCodeGenerator = new BarCodeGenerator(settings);
        //Generate QR code image
        BufferedImage bufferedImage = barCodeGenerator.generateImage();
        //save the image to a .png file
        ImageIO.write(bufferedImage,&quot;png&quot;,new File(&quot;QR_Code.png&quot;));
    }
}

The following is the generated QR code image with text:

"
"I'm using Java's WatchService API within my Spring Boot application to monitor a directory, and perform some actions on created files. This process is executed asynchronously: it starts automatically right after the application is ready and monitors the directory in the background until the application is stopped.
This is the configuration class:
@Configuration
public class DirectoryWatcherConfig {

    @Value(&quot;${path}&quot;)
    private String path;

    @Bean
    public WatchService watchService() throws IOException {
        WatchService watchService = FileSystems.getDefault().newWatchService();
        Path directoryPath = Paths.get(path);
        directoryPath.register(watchService, StandardWatchEventKinds.ENTRY_CREATE);
        return watchService;
    }

}

And this is the monitoring service:
@Service
@RequiredArgsConstructor
public class DirectoryWatcherService {

    private final WatchService watchService;

    @Async
    @EventListener(ApplicationReadyEvent.class)
    public void startWatching() throws InterruptedException {
        WatchKey key;
        while ((key = watchService.take()) != null) {
            for (WatchEvent&lt;?&gt; event : key.pollEvents()) {
                // actions on created files
            }

            key.reset();
        }
    }

}

This code is working as expected, with the following exception, which I'd like to fix:

Any failure during the execution makes the monitoring to stop (obviously), and I don't know how to restart the monitoring after such events occur

","Actually the solution was quite simple. Wrapping the desired actions with try/catch (catching desired exceptions) in DirectoryWatcherService like this allows the thread to continue monitoring directories:
@Service
@RequiredArgsConstructor
public class DirectoryWatcherService {

    private final WatchService watchService;

    @Async
    @EventListener(ApplicationReadyEvent.class)
    public void startWatching() throws InterruptedException {
        WatchKey key;
        while ((key = watchService.take()) != null) {
            for (WatchEvent&lt;?&gt; event : key.pollEvents()) {
                try {
                    // actions on created files
                } catch (RuntimeException ex) {
                    // log exception or whatever you choose, as long as execution continues
                }
            }

            key.reset();
        }
    }

}

"
"I have added opentelemetry javaagent to a project and used it to instrument the project. Is there a way to test the instrumentation(for example created spans) in the unit tests?
Lets say this is my whole project code:
public class Main {
    public static void main(String[] args) {
        System.out.println(hello());
    }

    @WithSpan(&quot;hello&quot;)
    private static String hello() {
        return &quot;Hello world!&quot;;
    }
}

How can I test that calling the hello() function creates a hello span?
","To write unit tests you can access the exported spans with AgentTestingExporterAccess. You need to import these packages:
&lt;dependency&gt;
    &lt;groupId&gt;io.opentelemetry.javaagent&lt;/groupId&gt;
    &lt;artifactId&gt;opentelemetry-testing-common&lt;/artifactId&gt;
    &lt;version&gt;1.23.0-alpha&lt;/version&gt;
    &lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;io.opentelemetry.javaagent&lt;/groupId&gt;
    &lt;artifactId&gt;opentelemetry-agent-for-testing&lt;/artifactId&gt;
    &lt;version&gt;1.23.0-alpha&lt;/version&gt;
    &lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;

A simple unit test can look like this:
import io.opentelemetry.api.trace.SpanKind;
import io.opentelemetry.javaagent.testing.common.AgentTestingExporterAccess;


import static io.opentelemetry.api.common.AttributeKey.stringKey;


import io.opentelemetry.sdk.trace.data.StatusData;


public class MainTest {
    @Test
    public void testHello() {
        AgentTestingExporterAccess.reset();
        Main.hello(); // This a function that creates a span
        var spans = AgentTestingExporterAccess.getExportedSpans();
        assertEquals(spans.get(0).getName(), &quot;hello&quot;);
        assertEquals(spans.get(0).getKind(), SpanKind.INTERNAL);
        assertEquals(spans.get(0).getStatus(), StatusData.unset());
        assertEquals(spans.get(0).getAttributes().get(stringKey(&quot;service.name&quot;)), &quot;search&quot;);
    }
}

Please note that to be able to use AgentTestingExporterAccess, you need to run your tests with the javaagent too. If the java agent is not attached when running the tests, you will get an exception from AgentTestingExporterAccess like this:
java.lang.AssertionError: Error accessing fields with reflection.
...
Caused by: java.lang.NullPointerException
...

Another way of doing this is to write a mock server and capture the spans. Opentelemetry has an example here
"
"I am trying to implement simple JMS Producer and Consumer within Wildfly(Version 24) and remote ActiveMQ Artemis broker.
standalone.xml
&lt;subsystem xmlns=&quot;urn:jboss:domain:messaging-activemq:13.0&quot;&gt;
    &lt;remote-connector name=&quot;remote-artemis&quot; socket-binding=&quot;remote-artemis&quot;/&gt;
    &lt;pooled-connection-factory
        name=&quot;remote-artemis&quot;
        entries=&quot;java:/jms/remoteCF&quot; 
        connectors=&quot;remote-artemis&quot; 
        client-id=&quot;producer-pooled-connection-factory&quot;
        user=&quot;${artemismq.user}&quot;
        password=&quot;${artemismq.password}&quot;
        enable-amq1-prefix=&quot;true&quot;
    /&gt;
    &lt;external-jms-queue name=&quot;testQueue&quot; entries=&quot;java:/queue/testQueue&quot;/&gt;
&lt;/subsystem&gt;

&lt;socket-binding-group name=&quot;standard-sockets&quot; default-interface=&quot;public&quot; port-offset=&quot;${jboss.socket.binding.port-offset:0}&quot;&gt;
    &lt;socket-binding name=&quot;ajp&quot; port=&quot;${jboss.ajp.port:8009}&quot;/&gt;
    &lt;socket-binding name=&quot;http&quot; port=&quot;${jboss.http.port:8080}&quot;/&gt;
    &lt;socket-binding name=&quot;https&quot; port=&quot;${jboss.https.port:8443}&quot;/&gt;
    &lt;socket-binding name=&quot;management-http&quot; interface=&quot;management&quot; port=&quot;${jboss.management.http.port:9990}&quot;/&gt;
    &lt;socket-binding name=&quot;management-https&quot; interface=&quot;management&quot; port=&quot;${jboss.management.https.port:9993}&quot;/&gt;
    &lt;socket-binding name=&quot;txn-recovery-environment&quot; port=&quot;4712&quot;/&gt;
    &lt;socket-binding name=&quot;txn-status-manager&quot; port=&quot;4713&quot;/&gt;
    &lt;outbound-socket-binding name=&quot;mail-smtp&quot;&gt;
        &lt;remote-destination host=&quot;${jboss.mail.server.host:localhost}&quot; port=&quot;${jboss.mail.server.port:25}&quot;/&gt;
    &lt;/outbound-socket-binding&gt;
    &lt;outbound-socket-binding name=&quot;remote-artemis&quot;&gt;
        &lt;remote-destination host=&quot;${artemismq.host}&quot; port=&quot;${artemismq.port}&quot;/&gt;
    &lt;/outbound-socket-binding&gt;
&lt;/socket-binding-group&gt;

Producer and Consumer
    @Inject
    @JMSConnectionFactory(&quot;java:/jms/remoteCF&quot;)
    private JMSContext context;

    @Resource(lookup = &quot;java:/queue/testQueue&quot;)
    private Queue queue;

    @Override
    protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {
        resp.setContentType(&quot;text/html&quot;);
        PrintWriter out = resp.getWriter();
        try {
            out.write(&quot;&lt;p&gt;Sending messages to &lt;em&gt;&quot; + queue + &quot;&lt;/em&gt;&lt;/p&gt;&quot;);
            out.write(&quot;&lt;p&gt;Using context &lt;em&gt;&quot; + context + &quot;&lt;/em&gt;&lt;/p&gt;&quot;);
            out.write(&quot;&lt;h2&gt;The following messages will be sent to the destination:&lt;/h2&gt;&quot;);
            for (int i = 0; i &lt; MSG_COUNT; i++) {
                String text = &quot;This is message &quot; + (i + 1);
                context.createProducer().send(queue, text);
                out.write(&quot;Message (&quot; + i + &quot;): &quot; + text + &quot;&lt;/br&gt;&quot;);

                JMSConsumer consumer = context.createConsumer(queue);
                TextMessage message = (TextMessage) consumer.receive();
                out.write(&quot;Message received (&quot; + i + &quot;): &quot; + message.getText() + &quot;&lt;/br&gt;&quot;);
            }
        } catch (JMSException e) {
            e.printStackTrace();
        } finally {
            if (out != null) {
                out.close();
            }
        }
    }

Misc:
Producer alone works.
Result:
First message is received by broker but consumption is not executed and nothing happens(no logs).
","Regarding the pooled-connection-factory the documentation says:

It should only be used to send (i.e. produce) messages when looked up in JNDI or injected.

You should create a &quot;normal&quot; connection-factory for your consumer to use, e.g.:
&lt;connection-factory name=&quot;MyConnectionFactory&quot;
                    connectors=&quot;remote-artemis&quot;
                    entries=&quot;java:/jms/MyConnectionFactory&quot; /&gt;

Or via the JBoss CLI:
/subsystem=messaging-activemq/server=default/connection-factory=MyConnectionFactory:add(connectors=[&quot;remote-artemis&quot;], entries=[&quot;java:/jms/MyConnectionFactory&quot;])

Then in your code you could do something like the following:
@Inject
@JMSConnectionFactory(&quot;java:/jms/remoteCF&quot;)
private JMSContext context;

@Inject
@JMSConnectionFactory(&quot;java:/jms/MyConnectionFactory&quot;)
private JMSContext consumerContext;

@Resource(lookup = &quot;java:/queue/testQueue&quot;)
private Queue queue;

@Override
protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {
    resp.setContentType(&quot;text/html&quot;);
    PrintWriter out = resp.getWriter();
    try {
        out.write(&quot;&lt;p&gt;Sending messages to &lt;em&gt;&quot; + queue + &quot;&lt;/em&gt;&lt;/p&gt;&quot;);
        out.write(&quot;&lt;p&gt;Using context &lt;em&gt;&quot; + context + &quot;&lt;/em&gt;&lt;/p&gt;&quot;);
        out.write(&quot;&lt;h2&gt;The following messages will be sent to the destination:&lt;/h2&gt;&quot;);
        for (int i = 0; i &lt; MSG_COUNT; i++) {
            String text = &quot;This is message &quot; + (i + 1);
            context.createProducer().send(queue, text);
            out.write(&quot;Message (&quot; + i + &quot;): &quot; + text + &quot;&lt;/br&gt;&quot;);

            try (JMSConsumer consumer = consumerContext.createConsumer(queue)) {
               TextMessage message = (TextMessage) consumer.receive();
               out.write(&quot;Message received (&quot; + i + &quot;): &quot; + message.getText() + &quot;&lt;/br&gt;&quot;);
            }
        }
    } catch (JMSException e) {
        e.printStackTrace();
    } finally {
        if (out != null) {
            out.close();
        }
    }
}

If you need to set credentials on the injected JMSContext then you can annotate it with @JMSPasswordCredential.
"
"I have use case like this where I need to aggregate values in a list by multiple group by but then calculate percentage of distribution of each of those values and create a new list.
An example of list of items:
week1  source1  destination1   100
week1  source1  destination2   200
week1  source2  destination1   200
week1  source2  destination2   100
week2  source1  destination1   200
week2  source1  destination2   200

From this I want to group by week and source and calculate the total quantity and then distribute percentage based on quantity.
As an example the total quantity for week 1 from source 1 is 300, which is going to destination 1(100) and destination 2(200). Now the percentage of distribution is for week 1 from source 1 to destination1 is 33.33% and for week1 from source 1 to destination 2 to 66.66%
For example the output would be:
week1  source1  destination1   33.33%
week1  source1  destination2   66.66%
week1  source2  destination1   66.66%
week1  source2  destination2   33.33%
week2  source1  destination1   50%
week2  source1  destination2   50%

How can I achieve this result using Java 8 streams.
Say I have list of these objects as List into &quot;records&quot; object:
public class Record {
    private String sourceNode;
    private String destinationNode;
    private String weekIndex;
    private String quantity;
}

Map&lt;String, Map&lt;String, List&lt;Record&gt;&gt;&gt; RecordsGroupByWeekAndSource = records.stream()
                .collect(Collectors.groupingBy(Record::getWeekIndex, Collectors.groupingBy(Record::getSourceNode)));

This would give me the items group by week and source. But I will have to iterate this map again to calculate the total quantity in each list that resides inside map of map object. But Is there a way I can do this percenatage calcualtion within the groupingBy collection itslef?
","You can achieve it by using stream twice:

In first collection, you can do group by and do sum
Stream your records again and use sum result from the fist step to calculate percentage

Sample Code:
import java.util.*;

import java.util.stream.*;

class Record {
    public String week;
    public String source;
    public String destination;
    public Integer qty;

    Record(String week, String source, String destination, Integer qty) {
        this.week = week;
        this.source = source;
        this.destination = destination;
        this.qty = qty;
    }
}

public class Main {
    public static void main(String[] args) {
        List&lt;Record&gt; records = new ArrayList&lt;&gt;();
        records.add(new Record(&quot;w1&quot;, &quot;hyd&quot;, &quot;kur&quot;, 10));
        records.add(new Record(&quot;w1&quot;, &quot;hyd&quot;, &quot;gwd&quot;, 20));
        records.add(new Record(&quot;w2&quot;, &quot;hyd&quot;, &quot;kur&quot;, 40));
        records.add(new Record(&quot;w2&quot;, &quot;hyd&quot;, &quot;gwd&quot;, 10));
        
        
        Map&lt;String, Map&lt;String, Integer&gt;&gt; sums = records
        .stream()
        .collect(Collectors.groupingBy(rec -&gt; rec.week,
            Collectors.groupingBy(rec -&gt; rec.source, 
            Collectors.summingInt(rec-&gt;rec.qty))));

        records = records
        .stream()
        .map(rec -&gt; {
            rec.qty = rec.qty*100 / sums.get(rec.week).get(rec.source);
            return rec;
        }).collect(Collectors.toList());
        records.forEach((rec)-&gt;System.out.println(rec.week+&quot;\t&quot;+rec.source+&quot;\t&quot;+rec.destination+&quot;\t&quot;+rec.qty));
    }
}

"
"Introduction
I'm currently developing a program in which I use Java.util.Collection.parallelStream(), and wondering if it's possible to make it more Multi-threaded.
Several small map
I was wondering if using multiple map might allow the Java.util.Collection.parallelStream() to distribute the tasks better:
List&lt;InsertOneModel&lt;Document&gt;&gt; bulkWrites = puzzles.parallelStream()
        .map(gson::toJson)
        .map(Document::parse)
        .map(InsertOneModel::new)
        .toList();

Single big map
For example a better distribution than:
List&lt;InsertOneModel&lt;Document&gt;&gt; bulkWrites = puzzles.parallelStream()
        .map(puzzle -&gt; new InsertOneModel&lt;&gt;(Document.parse(gson.toJson(puzzle))))
        .toList();

Question
Is there one of the solutions that is more suitable for Java.util.Collection.parallelStream(), or the two have no big difference?
","I don't think it will do any better if you chain it with multiple maps. In case your code is not very complex I would prefer to use a single big map. 
To understand this we have to check the code inside the map function. link
public final &lt;R&gt; Stream&lt;R&gt; map(Function&lt;? super P_OUT, ? extends R&gt; mapper) {
    Objects.requireNonNull(mapper);
    return new StatelessOp&lt;P_OUT, R&gt;(this, StreamShape.REFERENCE,
                                 StreamOpFlag.NOT_SORTED | StreamOpFlag.NOT_DISTINCT) {
        @Override
        Sink&lt;P_OUT&gt; opWrapSink(int flags, Sink&lt;R&gt; sink) {
            return new Sink.ChainedReference&lt;P_OUT, R&gt;(sink) {
                @Override
                public void accept(P_OUT u) {
                    downstream.accept(mapper.apply(u));
                }
            };
        }
    };
}

As you can see a lot many things happen behind the scenes. Multiple objects are created and multiple methods are called. Hence, for each chained map function call all these are repeated.
Now coming back to ParallelStreams, they work on the concept of Parallelism .
Streams Documentation


A parallel stream is a stream that splits its elements into multiple chunks, processing each chunk with a different thread. Thus, you can automatically partition the workload of a given operation on all the cores of your multicore processor and keep all of them equally busy.
Parallel streams internally use the default ForkJoinPool, which by default has as many threads as you have processors, as returned by Runtime.getRuntime().availableProcessors(). But you can change the size of this pool using the system property java.util.concurrent.ForkJoinPool.common.parallelism.
ParallelStream calls spliterator() on the collection object which returns a Spliterator implementation that provides the logic of splitting a task. Every source or collection has their own spliterator implementations. Using these spliterators, parallel stream splits the task as long as possible and finally when the task becomes too small it executes it sequentially and merges partial results from all the sub tasks.
So I would prefer parallelStream when

I have huge amount of data to process at a time
I have multiple cores to process the data
Performance issues with the existing implementation
I already don't have multiple threaded process running, as it will add to the complexity.

Performance Implications

Overhead : Sometimes when dataset is small converting a sequential stream into a parallel one results in worse performance. The overhead of managing threads, sources and results is a more expensive operation than doing the actual work.
Splitting: Arrays can split cheaply and evenly, while LinkedList has none of these properties. TreeMap and HashSet split better than LinkedList but not as well as arrays.
Merging:The merge operation is really cheap for some operations, such as reduction and addition, but merge operations like grouping to sets or maps can be quite expensive.

Conclusion: A large amount of data and many computations done per element indicate that parallelism could be a good option.
"
"I got vulnerabilities flaws from the scan report for Java code, did some research, and found this recommendation to resolve such issues:

Improper Restriction of XML External Entity Reference (CWE ID 611)

This is the code including the fix for the XXE Attack issue:
    public static String convertNodeToString(Node node) {
        TransformerFactory tf = TransformerFactory.newInstance();
        Transformer transformer;
        try {
            tf.setAttribute(XMLConstants.ACCESS_EXTERNAL_DTD, &quot;&quot;);
            tf.setAttribute(XMLConstants.ACCESS_EXTERNAL_STYLESHEET, &quot;&quot;);
            transformer = tf.newTransformer();
            // below code to remove XML declaration
            // transformer.setOutputProperty(OutputKeys.OMIT_XML_DECLARATION, &quot;yes&quot;);
            StringWriter writer = new StringWriter();
            transformer.transform(new DOMSource(node), new StreamResult(writer));
            String output = writer.getBuffer().toString();
            return output;
        } catch (TransformerException e) {
            e.printStackTrace();
        }

        return null;
    }

The good thing is that JUnit testing was a success, but, when I deployed the code on a running instance, I got this error:

java.lang.IllegalArgumentException: Not supported: http://javax.xml.XMLConstants/property/accessExternalDTD

As per my experience, this is because the running instance uses some dependencies which caused such a conflict and resulted in this error.
Following is part of the stack trace form the console:

java.lang.IllegalArgumentException: Not supported: http://javax.xml.XMLConstants/property/accessExternalDTD
at org.apache.xalan.processor.TransformerFactoryImpl.setAttribute(TransformerFactoryImpl.java:571)

How I can find which dependency is causing the such error? Is there anything I can do to resolve such an error? I am also suspecting that I missed including a dependency. Please help me solve this issue.
Edit 1:
I did further research and I think this happens because of this reference in the java.exe command used to launch the actual instance:

java.exe -Xbootclasspath/p:../lib/xalan.jar;../lib/xercesImpl.jar;...

Now, I need to find out how I can overcome this issue. I came across some articles proposing to ensure the creation of the factory instance using the correct package. I think the above code ends up using the wrong package.
The question now is how to use java code to ensure using the correct package to create the TransformerFactory instance.
Edit 2:
The first answer helped me make some progress. I found that the classpath of the deployed instance has a reference to org.apache.xalan.processor.TransformerFactoryImpl in xalan.jar which seems it is used by TransformerFactory.newInstance() to create the transformer factory. I think the question is how I can make the needed changes to ensure using the proper class to create the transformer.
Edit 3:
I followed the recommendation here and added this code:
TransformerFactory factory = TransformerFactory.newInstance();
factory.setFeature(javax.xml.XMLConstants.FEATURE_SECURE_PROCESSING, true);

The error was resolved in the running instance, but, the scan tool is still reporting this vulnerability flaw XXE Attack. According to this article, this happens because an outdated XML processor is present on the classpath (e.g. Xerces, Xalan) which is exactly my case.
I think I came across an article recommending changing some system properties that will indicate the factory to create the transformer instance using the correct class. I am trying to find this article now.
I appreciate your help.
","Thanks to @Gabriel who helped me find the answer. This is another way to resolve the issue using system properties. The other method is to specify the name of the correct class using the TransformerFactory.newInstance(); method. The correct class is com.sun.org.apache.xalan.internal.xsltc.trax.TransformerFactoryImpl:
tf = TransformerFactory.newInstance(&quot;com.sun.org.apache.xalan.internal.xsltc.trax.TransformerFactoryImpl&quot;, NameOfContainerClass.class.getClassLoader());

Below is the final answer which worked for me and the scan report is clean.
private final static String JAVAX_TRANSFORMER_PROP = &quot;javax.xml.transform.TransformerFactory&quot;;
private final static String JAVAX_TRANSFORMER_PROP_VAL = &quot;com.sun.org.apache.xalan.internal.xsltc.trax.TransformerFactoryImpl&quot;;

public static String convertNodeToString(Node node) throws Exception  {
        TransformerFactory tf=null;
        Transformer transformer;
        String errMsg=null;
        String output=null;
        //Prevent XXE Attack: Ensure using the correct factory class to create TrasformerFactory instance
        //  This will instruct Java to use to version which supports using ACCESS_EXTERNAL_DTD argument.
        // Use:
        //   - com.sun.org.apache.xalan.internal.xsltc.trax.TransformerFactoryImpl
        //  instead of:
        //   - org.apache.xalan.processor.TransformerFactoryImpl
        if (System.getProperty(JAVAX_TRANSFORMER_PROP) == null || 
            !System.getProperty(JAVAX_TRANSFORMER_PROP).equals(JAVAX_TRANSFORMER_PROP_VAL))
            System.setProperty(JAVAX_TRANSFORMER_PROP, JAVAX_TRANSFORMER_PROP_VAL);
        try {
            tf = TransformerFactory.newInstance();
        } catch (TransformerFactoryConfigurationError  e) {
            e.printStackTrace();
            errMsg=&quot;Error 'TransformerFactoryConfigurationError' in convertNodeToString() while creating 'TransformerFactory' instance: &quot; + e.toString();
        } catch (Exception  e) {
            e.printStackTrace();
            errMsg=&quot;Error in convertNodeToString() while creating 'TransformerFactory' instance:: &quot; + e.toString();
        }
        if (errMsg != null)
            throw new Exception(errMsg);
        //Prevent XXE Attack: Set attributes to prevent XXE Attack vulnerabilities.
        try {
            tf.setAttribute(XMLConstants.ACCESS_EXTERNAL_DTD, &quot;&quot;);
            tf.setAttribute(XMLConstants.ACCESS_EXTERNAL_STYLESHEET, &quot;&quot;);
        } catch (IllegalArgumentException e) {
            e.printStackTrace();
            errMsg = &quot;Error 'IllegalArgumentException' in convertNodeToString() while attempting to prevent XXE Attack: &quot; + e.toString();
        } catch (Exception e) {
            e.printStackTrace();
            errMsg = &quot;Error in convertNodeToString() while attempting to prevent XXE Attack: &quot; + e.toString();
        }
        if (errMsg != null)
            throw new Exception(errMsg);
        //tf.setFeature(javax.xml.XMLConstants.FEATURE_SECURE_PROCESSING, true);
        try {
            transformer = tf.newTransformer();
            // below code to remove XML declaration
            // transformer.setOutputProperty(OutputKeys.OMIT_XML_DECLARATION, &quot;yes&quot;);
            StringWriter writer = new StringWriter();
            transformer.transform(new DOMSource(node), new StreamResult(writer));
            output = writer.getBuffer().toString();
        } catch (TransformerException e) {
            e.printStackTrace();
            errMsg = &quot;Error 'TransformerException' in convertNodeToString() while converting the node to string: &quot; + e.toString();
        } catch (Exception e) {
            e.printStackTrace();
            errMsg = &quot;Error 'TransformerException' in convertNodeToString() while converting the node to string: &quot; + e.toString();
        }
        if (errMsg != null)
            throw new Exception(errMsg);
        return output;
}

Reference: https://stackoverflow.com/a/50219550/4180447
"
"I want to convert map to json but with changing case using jackson. For example, I have this map:
 &quot;test_first&quot; -&gt; 1,
 &quot;test_second&quot; -&gt; 2,

I want to convert it to json but with changing from underscore case to lowerCamelCase. How do I do that? Using this didn't help:
// Map&lt;String, String&gt; fields;

var mapper = new ObjectMapper();
mapper.setPropertyNamingStrategy(PropertyNamingStrategy.LOWER_CAMEL_CASE); 
// setPropertyNamingStrategy(PropertyNamingStrategy.SNAKE_CASE) didn't help too
String json = mapper.writeValueAsString(fields);

","There is StringKeySerializer in Jackson which may implement the functionality to change presentation of the keys in some map (e.g. using Guava CaseFormat):
// custom key serializer
class SnakeToCamelMapKeySerialiser extends StdKeySerializers.StringKeySerializer {
    @Override
    public void serialize(Object value, JsonGenerator g, SerializerProvider provider)
            throws IOException {
        g.writeFieldName(CaseFormat.LOWER_UNDERSCORE.to(CaseFormat.LOWER_CAMEL, (String) value));
    }
}

// map with the custom serializer
@JsonSerialize(keyUsing = SnakeToCamelMapKeySerialiser.class)
class MyMap&lt;K extends String, V&gt; extends HashMap&lt;K, V&gt; {
}

Then the map is serialized with the required format:
Map&lt;String, Integer&gt; map = new MyMap&lt;&gt;();
map.put(&quot;first_key&quot;, 1);
map.put(&quot;second_key&quot;, 2);

ObjectMapper mapper = new ObjectMapper();


String json = mapper.writeValueAsString(map);

System.out.println(json);
// -&gt; {&quot;firstKey&quot;:1,&quot;secondKey&quot;:2}

"
"I am trying to update Spring Boot application from 2.4 to 2.6.4 but I am getting following error:
The dependencies of some of the beans in the application context form a cycle:
â”Œâ”€â”€â”€â”€â”€â”
|  webSecurityConfig
â†‘     â†“
|  org.springframework.boot.autoconfigure.web.servlet.WebMvcAutoConfiguration$EnableWebMvcConfiguration.
â””â”€â”€â”€â”€â”€â”˜


Following is WebSecurityConfig code:
import javax.sql.DataSource;
import com.jay.liqui.jwt.JWTAuthorizationFilter;
import com.jay.liqui.jwt.JwtTokenProvider;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Lazy;
import org.springframework.security.config.annotation.authentication.builders.AuthenticationManagerBuilder;
import org.springframework.security.config.annotation.web.builders.HttpSecurity;
import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;
import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;
import org.springframework.security.core.userdetails.UserDetailsService;
import org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder;
import org.springframework.security.crypto.password.PasswordEncoder;
import org.springframework.security.web.util.matcher.AntPathRequestMatcher;
import org.springframework.web.servlet.config.annotation.CorsRegistry;
import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;
import org.springframework.core.annotation.Order;

@Configuration
//@Order(1)
@EnableWebSecurity
public class WebSecurityConfig  extends WebSecurityConfigurerAdapter {

    @Autowired
    private DataSource dataSource;

    @Autowired
    private JwtTokenProvider jwtTokenProvider;


    @Bean
    public static PasswordEncoder passwordEncoder(){
        return new  BCryptPasswordEncoder();
    }


    @Override
    protected void configure(HttpSecurity http) throws Exception {
        //Cross-origin-resource-sharing: localhost:8080, localhost:4200(allow for it.)
        http.cors().and()
                .authorizeRequests()
                //These are public paths
                .antMatchers(&quot;/resources/**&quot;,  &quot;/error&quot;, &quot;/api/user/**&quot;).permitAll()
                //These can be reachable for just have admin role.
                .antMatchers(&quot;/api/admin/**&quot;).hasRole(&quot;ADMIN&quot;)
                //All remaining paths should need authentication.
                .anyRequest().fullyAuthenticated()
                .and()
                //logout will log the user out by invalidated session.
                .logout().permitAll()
                .logoutRequestMatcher(new AntPathRequestMatcher(&quot;/api/user/logout&quot;, &quot;POST&quot;))
                .and()
                //login form and path
                .formLogin().loginPage(&quot;/api/user/login&quot;).and()
                //enable basic authentication
                .httpBasic().and()
                //We will handle it later.
                //Cross side request forgery
                .csrf().disable();

        //jwt filter
        http.addFilter(new JWTAuthorizationFilter(authenticationManager(),jwtTokenProvider));
    }

    @Autowired
    public void configAuthentication(AuthenticationManagerBuilder authBuilder) throws Exception {
        authBuilder.jdbcAuthentication()
                .dataSource(dataSource)
                .passwordEncoder(new BCryptPasswordEncoder())
                .usersByUsernameQuery(&quot;select username, password, enabled from usr01 where username=?&quot;)
                .authoritiesByUsernameQuery(&quot;select username, role from usr01 where username=?&quot;)
        ;


    }

    //Cross origin resource sharing.
    @Bean
    public WebMvcConfigurer corsConfigurer(){
        return new WebMvcConfigurer() {
            @Override
            public void addCorsMappings(CorsRegistry registry) {
                registry.addMapping(&quot;/**&quot;).allowedOrigins(&quot;*&quot;).allowedMethods(&quot;*&quot;);
            }
        };
    }
}

","The cause of this error is
Spring Boot 2.4 disable spring.main.allow-bean-definition-overriding by default and Spring Boot 2.6.4 enable
There are 2 solutions to fix it
Solution 1: You set allow-bean-definition-overriding is true in application.properties
spring.main.allow-bean-definition-overriding=true

Solution 2: You should move Bean WebMvcConfigurer to another class
Example:
@EnableWebMvc
@Configuration
public class WebConfig implements WebMvcConfigurer {
    @Autowired
    LogInterceptor logInterceptor;

    @Override
    public void addResourceHandlers(ResourceHandlerRegistry registry) {
        registry.addResourceHandler(&quot;/swagger-ui/**&quot;).addResourceLocations(&quot;classpath:/META-INF/resources/swagger-ui.html&quot;);
        registry.addResourceHandler(&quot;/webjars/**&quot;).addResourceLocations(&quot;classpath:/META-INF/resources/webjars/&quot;);
    }

    @Override
    public void addInterceptors(InterceptorRegistry registry) {
        registry.addInterceptor(logInterceptor);
    }

    @Override
    public void addCorsMappings(CorsRegistry registry) {
        registry.addMapping(&quot;/**&quot;).allowedOrigins(&quot;*&quot;).allowedMethods(&quot;*&quot;);
    }

    @Bean
    public InternalResourceViewResolver defaultViewResolver() {
        return new InternalResourceViewResolver();
    }

}

@Component
public class LogInterceptor implements HandlerInterceptor {

    @Autowired
    LoggingService loggingService;

    @Override
    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) {
        if (DispatcherType.REQUEST.name().equals(request.getDispatcherType().name()) &amp;&amp; request.getMethod().equals(HttpMethod.GET.name())) {
            loggingService.logRequest(request, null);
        }
        return true;
    }

    @Override
    public void postHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, ModelAndView modelAndView) throws Exception {

    }

    @Override
    public void afterCompletion(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) throws Exception {

    }
}

"
"requirement is like this: user input is single character followed by an array of integers, such as 'A 1 2', 'B 3 4 5', 'C 1', etc. The single character means which class to construct and integers are input parameter to that constructor. Please note different classes might need different number of integers.
Then we need to write a program to parse user input and create objects accordingly.
My approach was to use regular expression for parsing and hard code which class to call.
But another senior developer said a better idea would be using dependency injection to automatically create objects based on user input. He gave another hint to create an interface and use spring framework dependency injection (not spring boot).
I am still confused how to create beans dynamically in this way. Can anybody help please?
","You can create a common interface for the classes that can be created, and a Factory bean that transforms the input.
// common interface
interface MyObject {
  void someMethod();
}
class A implements MyObject {
  public A(List&lt;Integer&gt; params) { ... }
}
class B implements MyObject {
  public B(List&lt;Integer&gt; params) { ... }
}

// parsed data
class Input {
 char character;
 List&lt;Integer&gt; ints;
 // getters, setters
}
interface MyObjectFactory {
  public MyObject apply(Input input);
}

@Bean
class MyObjectFactory implements MyObjectFactory {
  public MyObject apply(Input input) {
    // create object from input, eg switch over input.getCharacter()
  };
}
// injected
class MyClientService {
 @Autowired
 MyObjectFactory objectFactory;

 public void doStuff() {
   List&lt;Input&gt; parsedInputs = parse(something);
   for (Input input : parsedInputs) {
     MyObject object = objectFactory.apply(input);
     // ...
   }
 }
}

"
"I know this question might seem overly familiar to the community, but I swear I've never been able to reproduce the issue related to this question even once throughout my programming journey.
I understand what the strictfp modifier does and how it ensures full compliance with the IEEE754 standard. However, I've never encountered a situation in practice where the set of values with an extended exponent is used, as described in the official specification.
I've tried using options like -XX:+UseFPUForSpilling to stimulate the use of the FPU block for calculations on my relatively modern processor, but it had no effect.
I even went as far as installing Windows 98 SE on a virtual machine and emulating an Intel Pentium II processor through Bochs, which does not support the SSE instruction set, hoping that the use of the FPU block in this case would be virtually the only option. However, even such an experiment yielded no results.
The essence of the experiment was to take the maximum possible value of the double type and multiply it by 2 to take the intermediate result beyond the permissible range of the double type. Then, I divided the obtained value by 4, and the final result was saved back into a double variable. In theory, I should have gotten some more meaningful result, but in all situations, I ended up with Infinity. In general, I haven't found a single reproducible example on the entire internet (even as of 2024!) that would show different results with and without the use of strictfp. Is it really possible that in almost 30 years of the language's history, there isn't a single example on this topic that clearly demonstrates the difference?
P.S. I'm well aware of Java 17+. All experiments were conducted on earlier versions, where the difference should, in theory, be observable. I installed Java SE 1.3 on the virtual machine.
","Understanding strictfp in Java: A Deep Dive Into JVM Behavior
If you’ve ever worked with floating-point arithmetic in Java, you may have come across the strictfp keyword. It guarantees platform-independent results by strictly adhering to the IEEE 754 floating-point standard. But how does it actually work under the hood? In this post, I’ll walk you through my detailed exploration of strictfp, including examples, assembly code, and insights into the JVM’s behavior on different architectures.
This is not just theoretical – I spent a significant amount of time analyzing the output of a 32-bit JVM on x86 processors, including disassembled JIT-compiled code. This might be one of the few hands-on explanations you’ll find, showcasing real examples of how strictfp affects floating-point calculations.

What Is strictfp?
Floating-point types (float and double) in Java are governed by the IEEE 754 standard. The Java Language Specification (JLS §4.2.3) (link) defines two standard value sets for floating-point numbers:

float value set (binary32)
double value set (binary64)

In addition to these, the JVM may support extended-exponent value sets:

float-extended-exponent
double-extended-exponent

Key Differences Between strictfp and Default Behavior:

Without strictfp: The JVM can use extended precision for intermediate calculations. For example, on x86 processors, it may use 80-bit floating-point registers. This can lead to platform-specific results due to differences in rounding and precision.
With strictfp: All intermediate calculations are confined to the binary32 (float) or binary64 (double) value sets, ensuring consistency across platforms.


The Experiment: How Does strictfp Affect Results?
To explore the effects of strictfp, I tested two examples illustrating overflow and underflow behavior on an x86 processor using a 32-bit JVM. These examples demonstrate how intermediate results behave differently with and without strictfp.

Why Local Variables Were Used Instead of Compile-Time Constants
It’s important to highlight that local variables were deliberately used instead of compile-time constants. This decision was crucial for ensuring that calculations were performed at runtime rather than being optimized away by the compiler.
If compile-time constants (e.g., System.out.println(Double.MIN_VALUE / 2 * 4);) were used directly, the Java compiler would likely compute the result at compile time. During this process, the compiler adheres strictly to the IEEE 754 standard, enforcing binary32 or binary64 precision for intermediate results. This means the calculations would effectively mimic the behavior of strictfp, regardless of whether the modifier is present or not.
By introducing local variables, we force the JVM to defer the computation to runtime. This runtime calculation allows us to observe the effects of extended precision (80-bit x87 registers) or strict IEEE 754 conformance in real-time, as influenced by the presence or absence of the strictfp modifier. Without this approach, the experimental results would not reflect the differences we’re trying to illustrate.

Example 1: Underflow Behavior
public class StrictTest {
    public static void main(String[] args) {
        double secondOperand = 2;
        double thirdOperand = 4;

        System.out.println(Double.MIN_VALUE / secondOperand * thirdOperand);
    }
}

Results:

Without strictfp:
Extended precision (80-bit x87 registers) avoids underflow, preserving the intermediate result:
1.0E-323


With strictfp:
Intermediate calculations adhere to binary64 precision, causing underflow:
0.0




Example 2: Overflow Behavior
public class StrictTest {
    public static void main(String[] args) {
        double secondOperand = 2;
        double thirdOperand = 4;

        System.out.println(Double.MAX_VALUE * secondOperand / thirdOperand);
    }
}

Results:

Without strictfp:
Extended precision allows the intermediate result to fit within the 80-bit range, avoiding immediate overflow:
8.988465674311579E307


With strictfp:
Calculations confined to binary64 precision result in an overflow to positive infinity:
Infinity




Key Insight:
The use of local variables ensured that these calculations occurred at runtime, allowing us to capture the runtime differences between strictfp and non-strictfp behavior. If compile-time constants had been used, the compiler would have optimized the calculations based on strict IEEE 754 conformance, negating the ability to observe the effects of extended precision on intermediate results. This distinction is critical for reproducibility and understanding the nuances of strictfp.

What Happens Under the Hood?
Using a disassembler (hsdis), I examined the assembly code generated by the JVM to understand how calculations are performed. The goal was to observe how the strictfp modifier impacts floating-point operations at the machine code level.
JVM Options
To replicate the results, the following JVM options were used:
-server -Xcomp -XX:UseSSE=0 -XX:+UnlockDiagnosticVMOptions -XX:+PrintAssembly -XX:CompileCommand=compileonly,StrictTest.main

For the minimal setup required to observe differences, use:
-Xcomp -XX:UseSSE=0

Why These Options Are Necessary

-Xcomp: This option forces the JVM to compile all methods using the Just-In-Time (JIT) compiler immediately. It is mandatory in this experiment because:

Without -Xcomp, or when using -Xint (interpreted mode), the methods might not be compiled, and the JVM will execute them in interpreted mode. This results in no JIT-compiled assembly output, which is essential for the disassembler (hsdis) to provide meaningful results.
In interpreted mode, floating-point operations would rely entirely on the bytecode interpreter, making it impossible to observe the low-level differences caused by strictfp.


-XX:UseSSE=0: This disables the use of Streaming SIMD Extensions (SSE) instructions for floating-point operations. Instead, the JVM falls back to the x87 FPU instructions, which utilize 80-bit extended precision registers. This option was critical because:

By default, modern JVMs on x86 use SSE instructions for floating-point operations, which comply with IEEE 754 by default and do not use extended precision. As a result, there would be no observable difference in behavior with or without strictfp.
Disabling SSE ensures that the JVM uses x87 FPU instructions, where intermediate results can utilize 80-bit extended precision unless constrained by strictfp. This allows us to demonstrate the impact of strictfp effectively.


-XX:+PrintAssembly: This option outputs the generated assembly code for the compiled methods. Combined with hsdis, it allows for precise observation of how floating-point calculations are executed at the machine level.
-XX:+CompileCommand=compileonly,StrictTest.main: This restricts compilation to the specific method under investigation (StrictTest.main), reducing noise in the assembly output.

By combining these options, the experiment isolates the floating-point operations affected by strictfp and ensures that the results are observable at the assembly level. Without this configuration, the differences introduced by strictfp would remain hidden, or the disassembly would lack the necessary precision.

Assembly Analysis: Without strictfp
Here’s the disassembly output when running the underflow example without the strictfp modifier:
0x02f52326: fldl    0x2f522c0   ; Load Double.MIN_VALUE
0x02f5232c: fdivl   0x2f522c8   ; Divide by secondOperand (2.0)
0x02f52332: fmull   0x2f522d0   ; Multiply by thirdOperand (4.0)
0x02f52338: fstpl   (%esp)      ; Store the result for printing

Explanation:

The JVM uses 80-bit extended precision for intermediate calculations, preserving the value beyond the IEEE 754 binary64 precision. As a result, underflow is avoided, and the intermediate result is preserved:
Result: 1.0E-323




Assembly Analysis: With strictfp
When the strictfp modifier is applied, the disassembly for the underflow example includes additional type conversion steps to enforce strict adherence to binary64 precision:
0x02fe2306: fldl    0x2fe22a0   ; Load Double.MIN_VALUE
0x02fe230c: fldt    0x6f4c40a4  ; Extended load
0x02fe2312: fmulp   %st(1)      ; Multiply and store in st(1)
0x02fe2314: fdivl   0x2fe22a8   ; Divide by secondOperand (2.0)
0x02fe231a: fldt    0x6f4c40b0  ; Extended load
0x02fe2320: fmulp   %st(1)      ; Multiply and store in st(1)
0x02fe2322: fstpl   0x18(%esp)  ; Store intermediate result
0x02fe2326: fldl    0x18(%esp)  ; Reload and enforce binary64 rounding
0x02fe232a: fldt    0x6f4c40a4  ; Extended load
0x02fe2330: fmulp   %st(1)      ; Multiply again
0x02fe2332: fmull   0x2fe22b0   ; Multiply by thirdOperand (4.0)
0x02fe2338: fldt    0x6f4c40b0  ; Extended load
0x02fe233e: fmulp   %st(1)      ; Multiply and store in st(1)
0x02fe2340: fstpl   0x20(%esp)  ; Final result stored

Explanation:

The key difference lies in the intermediate rounding and type conversion steps (e.g., fstpl followed by fldl). This forces compliance with the binary64 value set, leading to underflow:
Result: 0.0




Behavior on Modern 64-Bit JVMs
On modern 64-bit JVMs, the behavior is fundamentally different from 32-bit JVMs due to architectural and implementation changes. Extended precision (80-bit x87 floating-point registers) is not utilized, even when SIMD (SSE or AVX) is explicitly disabled via JVM options. Instead:

Relying on Native Implementations: Calculations appear to rely on native libraries or other internal JVM mechanisms for processing floating-point arithmetic. This can be inferred from the runtime call observed in the disassembled assembly code:
0x00000230aeae7e13: callq        0x230aea25820  ; OopMap{off=24}
                                          ;*getstatic out
                                          ; - StrictTest::main@8 (line 6)
                                          ;   {runtime_call}

This instruction indicates that instead of performing the floating-point calculation directly via hardware registers, the JVM delegates it to a runtime component. This component likely ensures that intermediate results conform to the binary64 (double) precision standard.

Disabling SSE and AVX Has No Effect: When using the -XX:UseSSE=0 and -XX:UseAVX=0 flags, one might expect the JVM to fall back to utilizing x87 80-bit FPU registers for floating-point operations. However, the runtime behavior remains unchanged, and x87 registers are not employed. Even the additional flag -XX:+UseFPUForSpilling, which should theoretically allow spilling intermediate results to x87 FPU registers, has no noticeable effect on the 64-bit JVM.

Intermediate Results Conform to Binary64 Rules: Regardless of the absence of strictfp, intermediate floating-point calculations adhere to IEEE 754 binary64 standards. This behavior ensures consistent results, simplifying cross-platform development. However, it also means that the potential benefits of extended precision for intermediate calculations (e.g., reducing rounding errors) are not available.

Internal Handling of Floating-Point Arithmetic: The reliance on a runtime component, as indicated by the disassembled code, suggests that floating-point calculations in a 64-bit JVM are heavily abstracted. This aligns with the broader trend of modern JVMs to use platform-independent mechanisms for floating-point arithmetic, reducing reliance on specific hardware features.


Observed Assembly Code
The following disassembled output demonstrates the runtime call used for floating-point calculations on a 64-bit JVM:
0x00000230aeae7e13: callq        0x230aea25820  ; OopMap{off=24}
                                              ;*getstatic out
                                              ; - StrictTest::main@8 (line 6)
                                              ;   {runtime_call}

This instruction explicitly calls into a runtime function for handling floating-point operations, bypassing hardware-level x87 or SIMD (SSE/AVX) capabilities.
Implications
While the strictfp modifier remains important for ensuring cross-platform consistency, its significance is diminished on 64-bit JVMs due to the inherent adherence of intermediate calculations to binary64 standards. This behavior is consistent even when hardware optimizations (like SSE or AVX) are disabled, and no fallback to x87 FPU registers occurs.
This architectural design underscores the JVM's emphasis on platform independence, even at the cost of foregoing hardware-specific optimizations for extended precision.

Diving Into the Java Language Specification
The JLS §4.2.3 (link) provides detailed insights into floating-point value sets. Here are the key points:

Value Sets:

float and double value sets (binary32, binary64).
Extended-exponent value sets (broader range of exponents, same precision).


Compliance:

All JVM implementations must support float and double value sets.
Extended-exponent value sets are optional but may be used for intermediate results unless restricted by strictfp.



Quote From the JLS:

&quot;The float, float-extended-exponent, double, and
double-extended-exponent value sets are not types. It is always
correct for an implementation of the Java programming language to use
an element of the float value set to represent a value of type float;
however, it may be permissible in certain regions of code for an
implementation to use an element of the float-extended-exponent value
set instead.&quot;


System Configuration
Here’s my setup for these experiments:

Processor: Intel Core i7-2960XM Extreme Edition
OS: Windows 10 Enterprise 22H2
JVM: Oracle OpenJDK 1.8.0_431 (32-bit) with hsdis installed.

Notes on Potential Variability
These experiments were conducted exclusively on an x86-64 processor architecture. Results may differ on other architectures (e.g., ARM64), operating systems, or JVM versions/vendors. This variability arises from the differences in how specific architectures and JVM implementations handle floating-point arithmetic and their internal optimizations.
Several factors that could influence results include:

Bytecode Compiler Optimizations: The Java compiler may optimize code differently depending on the runtime context or specific constructs used.

JVM Implementation Details: The behavior may vary based on the JVM vendor or version due to differences in policies around extended-exponent value set support and floating-point arithmetic handling.

OS and Hardware Optimizations: Operating systems and processor microarchitectures may influence how low-level instructions are executed, potentially affecting intermediate results.

JVM Flags: The specific flags used to launch the JVM can have a substantial impact on how calculations are handled. For instance, options like -XX:UseSSE or -XX:+UseFPUForSpilling directly alter the floating-point arithmetic behavior.


Understanding these dependencies is crucial for accurately interpreting experimental results and for reproducing the behavior across different environments.

Compatibility with Older JVM Versions
This analysis extends beyond the JVM versions explicitly mentioned in the earlier sections. I successfully reproduced the observed behavior on 32-bit JVMs starting from J2SE 1.4. Notably, these results were achieved on the Java HotSpot™ Client VM (version 1.4.2_18), which predates the widespread adoption of the SSE instruction set for floating-point calculations.
Key Findings on J2SE 1.4:

Critical Role of the -Xcomp Flag:

The -Xcomp flag is essential for achieving the desired results on J2SE 1.4. Without this flag, the JVM operates in interpreted mode or mixed mode, which prevents the Just-In-Time (JIT) compiler from generating the assembly-level output necessary for observing the behavior of floating-point operations.
Enabling -Xcomp ensures that all methods, including those under test, are compiled immediately, exposing the differences in intermediate precision with and without strictfp.


No Need for -XX:UseSSE=0:

Unlike modern JVMs, the -XX:UseSSE=0 flag is not recognized in J2SE 1.4. This is likely because, during that era, the SSE instruction set was either not fully utilized or had minimal integration into JVM implementations.
Despite the absence of this flag, the behavior is consistent with what was observed on more recent 32-bit JVMs using x87 FPU instructions, further confirming the reliance on 80-bit extended precision for intermediate floating-point calculations.


Reproducibility on HotSpot-Based JVMs:

The experiments were conducted on a system running the following configuration:
Processor: Intel Core i7-2960XM Extreme Edition
JVM: Java(TM) 2 Runtime Environment, Standard Edition (build 1.4.2_18-b06)


Results were reproducible, confirming that HotSpot-based JVMs consistently exhibit this behavior when strictfp is absent, provided that the computation is deferred to runtime (e.g., using local variables instead of compile-time constants).



Broader Implications:
These findings reinforce the idea that the behavior described in this post is not exclusive to modern JVM versions. Instead, it aligns with a long-standing design choice in the HotSpot VM to leverage x87 FPU instructions for floating-point arithmetic on 32-bit architectures. This historical consistency ensures that users can reproduce these experiments across various JVM versions, provided that they use the correct configuration and flags (notably, -Xcomp).
This compatibility further emphasizes the importance of understanding both the historical evolution of JVM implementations and the subtle ways in which flags and internal mechanisms influence runtime behavior.

Final Thoughts
This exploration demonstrates the nuanced behavior of strictfp and its impact on floating-point calculations in Java. The examples provided offer a rare glimpse into how intermediate precision is handled by the JVM, supported by real assembly output. By understanding these details, you can make informed decisions about when to use strictfp in your code.

P.S.
Starting from Java SE 17, the strictfp modifier is redundant as strict IEEE 754 adherence became the default and only mode of operation in the JVM.

Update (November 23, 2024): Revisiting How Extended-Exponent Value Sets Are Activated
After a series of additional experiments and thorough analysis, I have reached an important new conclusion about the conditions under which extended-exponent value sets can be utilized. Previously, I claimed that using the -Xcomp flag was mandatory for achieving this behavior on 32-bit JVMs. However, further testing revealed that my earlier understanding was incomplete. Below, I present the refined insights, supported by new experimental evidence and practical examples.

JVM Execution Modes: A Crucial Context
The JVM can operate in three primary execution modes, and understanding these is key to replicating the behavior:

Interpretation Mode (-Xint):
All code is executed by the bytecode interpreter. No JIT compilation occurs. In this mode, extended-exponent value sets cannot be used, as the interpreter enforces strict rounding of all intermediate results to either binary32 or binary64, depending on the expected result type.
Compilation Mode (-Xcomp):
All code is eagerly compiled by the JIT compiler, bypassing the interpreter entirely. This mode reliably activates extended-exponent value sets for floating-point calculations, as JIT-compiled machine code utilizes the x87 FPU instructions (for 32-bit JVMs).
Mixed Mode (default):
Combines interpretation and JIT compilation. Code is initially interpreted, but frequently executed or &quot;hot&quot; code is compiled by the JIT compiler as needed. In this mode, results vary depending on whether a specific block of code is interpreted or compiled.


Key Discovery: JIT Compilation Is the Real Enabler
The earlier assumption that -Xcomp was mandatory stemmed from the fact that it guarantees JIT compilation of all methods. However, my latest findings suggest that it is not the flag itself, but the use of JIT compilation that enables extended-exponent value sets. In mixed mode, it is possible to achieve the same results by ensuring that the relevant code is compiled. Here’s how:

By introducing a high number of iterations for the code block in question, the JVM's built-in heuristics classify it as &quot;hot,&quot; triggering JIT compilation.
Once compiled, the JIT-generated machine code leverages the x87 FPU instructions, enabling the use of extended-exponent value sets.

Example: Forcing JIT Compilation Without -Xcomp
The following code demonstrates this principle:
public class StrictTest {
    public static void main(String[] args) {
        double result = 0.0;

        for (int i = 0; i &lt; 1000000; i++) { 
            double secondOperand = 2.0;
            double thirdOperand = 4.0;

            result = Double.MIN_VALUE / secondOperand * thirdOperand;
        }

        System.out.println(result);
    }
}

Here, the repeated execution (1,000,000 iterations) ensures that the loop is compiled by the JIT compiler in mixed mode. As a result, the intermediate calculation avoids underflow, yielding the following output:
1.0E-323

This behavior is identical to what was observed with -Xcomp. It confirms that JIT compilation, not the mode flag, is the crucial factor for enabling extended-exponent calculations.

Historical Compatibility: Testing on Earlier JVM Versions
The extended-exponent value set has been supported since J2SE 1.2, aligning with the introduction of IEEE 754 compliance. Testing across various 32-bit JVM versions revealed the following:

Classic VM (J2SE 1.2–1.3):

Classic VM (e.g., java version &quot;1.2.2&quot;) already supports extended-exponent calculations when JIT compilation is enabled via the symcjit compiler.
Results are consistent with later HotSpot versions when the same conditions are met.


HotSpot VM (J2SE 1.4 and beyond):

The introduction of HotSpot VM in J2SE 1.3 as an add-on (and as the default VM in J2SE 1.4) solidified this behavior.
On J2SE 1.4 and later versions, results were identical across all 32-bit JVMs, confirming that the reliance on x87 FPU instructions remained unchanged.


32-bit JVMs (up to Java SE 9):

This behavior persisted until Java SE 9, the last version to offer 32-bit JVMs. Beyond this, 32-bit JVM support was deprecated.


64-bit JVMs:

Extended-exponent value sets are not available on 64-bit JVMs. Testing on J2SE 5.0 and later confirmed that these JVMs adhere strictly to binary64 precision for all intermediate calculations, regardless of flags.




Important Observations on JVM Flags and Versions
Early JVMs (J2SE 1.2–1.5):

The -XX:UseSSE=0 flag is unnecessary and unrecognized in 32-bit JVMs during this period, as SSE instructions were either not utilized or minimally integrated.
Notably, in J2SE 5.0, the -XX:UseSSE=N flag is available exclusively in 64-bit JVMs. In the corresponding 32-bit version, this flag is not supported, as 32-bit JVMs in this era relied solely on x87 FPU instructions for floating-point calculations.
Results for 32-bit JVMs align with x87 FPU usage by default.

JVMs Starting From Java SE 6:

The -XX:UseSSE=0 flag becomes mandatory in 32-bit JVMs to explicitly disable SSE instructions and enable x87 FPU behavior. Without this flag, calculations default to SSE-based precision, resulting in strict binary64 adherence.

64-bit JVMs:

Disabling SSE via -XX:UseSSE=0 has no effect in 64-bit JVMs across all versions. Intermediate results remain confined to binary64, as x87 FPU registers are not utilized.


Broader Implications
This refined understanding clarifies several points about JVM behavior:

Extended-exponent value sets rely on the x87 FPU, which is only available in 32-bit JVMs.
JIT compilation is the critical enabler for accessing this behavior. Without it, the bytecode interpreter enforces strict rounding to binary32 or binary64.
The -Xcomp flag is helpful but not mandatory, provided the relevant code is compiled by the JIT in mixed mode.


Updated Testing Results
I successfully reproduced the behavior across all tested 32-bit JVM versions, from J2SE 1.2 to Java SE 9, provided that JIT compilation was enabled. The table below summarizes the results:
JVM Version          Architecture   Behavior   Notes
-------------------- -------------- ---------- -----------------------------------------
J2SE 1.2.2 (Classic) 32-bit         Success    Enabled by symcjit; no SSE support.
J2SE 1.4 (HotSpot)   32-bit         Success    Default behavior with JIT compilation.
Java SE 6 (HotSpot)  32-bit         Success    Requires -XX:UseSSE=0 to disable SSE.
Java SE 9 (HotSpot)  32-bit         Success    Last version supporting 32-bit architecture.
J2SE 5.0–Java SE 16  64-bit         Failure    x87 FPU not utilized; no extended precision.


Final Thoughts
This update reinforces the nuanced relationship between JVM internals and extended-exponent value sets. By ensuring JIT compilation, it is possible to activate this behavior on 32-bit JVMs across a wide range of versions. This finding highlights the importance of understanding how different execution modes and JVM implementations interact with floating-point arithmetic.
For anyone exploring this area, I recommend replicating the tests with and without -Xcomp and experimenting with &quot;hot code&quot; to better understand the role of JIT compilation in this process.
"
"I came across this problem.
Given a weighted tree T, find the minimum number of edges to form a simple path (no duplicate vertices or edges) of weight (sum of weights of edges) exactly L.

More details:
L is given as input and it can be different for each case.
There are N vertices in the tree numbered from 0 to N - 1.
My first thought was the best I can do is go over all the N^2 paths in T. Here is a runnable code with example input.
import java.util.*;
class Edge {
    int toVertex, weight;
    Edge(int v, int w) {
        toVertex = v; weight = w;
    }
}
class Solver {
    // method called with the tree T given as adjacency list and the path length L to achieve
    // method to return minimum edges to create path of length L or -1 if impossible
    public static int solve(List&lt;List&lt;Edge&gt;&gt; T, long L) {
        int min = (int) 1e9;
        for (int i = 0; i &lt; T.size(); i++) {
            min = Math.min(min, test(T, L, i, -1, 0, 0));
        }
        if (min == (int) 1e9) {
            return -1;
        } else {
            return min;
        }
    }
    static int test(List&lt;List&lt;Edge&gt;&gt; T, long L, int vertex, int parent, long length, int edges) {
        if (length == L) {
            return edges;
        } else if (length &lt; L) {
            int min = (int) 1e9;
            for (Edge edge : T.get(vertex)) {
                if (edge.toVertex != parent) {
                    min = Math.min(min, test(T, L, edge.toVertex, vertex, length + edge.weight, edges + 1));
                }
            }
            return min;
        } else {
            return (int) 1e9; // overshoot
        }
    }
}
// provided code
public class Main {
    static void putEdge(List&lt;List&lt;Edge&gt;&gt; T, int vertex1, int vertex2, int weight) {
        T.get(vertex1).add(new Edge(vertex2, weight));
        T.get(vertex2).add(new Edge(vertex1, weight));
    }
    public static void main(String[] args) {
        // example input
        List&lt;List&lt;Edge&gt;&gt; T = new ArrayList&lt;List&lt;Edge&gt;&gt;();
        int N = 8;
        for (int i = 0; i &lt; N; i++) T.add(new ArrayList&lt;Edge&gt;());
        putEdge(T, 0, 1, 2);
        putEdge(T, 1, 2, 1);
        putEdge(T, 1, 3, 2);
        putEdge(T, 2, 6, 1);
        putEdge(T, 6, 7, 1);
        putEdge(T, 3, 4, 1);
        putEdge(T, 3, 5, 4);
        System.out.println(Solver.solve(T, 5L)); // path from 4 to 5 have 2 edges and length 5
    }
}

But this exceeds time limit when N reaches around 10,000. I also considered binary search on the answer, but checking a particular answer is possible looks just as hard as solving the original problem.
Is there a more efficient way to solve this to somehow avoid testing all paths?
","There are two main ways to solve this problem; I will describe the simpler method.
To start, root the tree arbitrarily (vertex 0 tends to be a good choice as it always exists). Let dist[x] denote the sum of weights of edges on the path from the root to x and let depth[x] denote the number of edges on this path.
For any two distinct nodes u and v, there is one unique simple path between them which goes from u to the lowest common ancestor (LCA) of the two nodes, then to v. We can express the total weight on this path as dist[u] + dist[v] - 2 * dist[LCA(u, v)] since the edges from the root to the LCA are counted in both dist[u] and dist[v]. Similarly, the number of edges on the path is depth[u] + depth[v] - 2 * depth[LCA(u, v)].
Next, let's consider every node n as a possible LCA on a path between two nodes of weight L. In this case, these two nodes must both be in the subtree of n (including itself). To compute the optimal answer at each node, we will store a map for each node that associates each possible dist[x] with the minimum depth[x] that can achieve that distance for any x in the subtree of n.
To process a node n, we iterate over each child c and compute this map for it first and then combine that result into the map of the current node in two stages. For each (d, e) key-value pair in the child's subtree, we check the map of the current node n for a key k that satisfies k + d - 2 * dist[n] = L. If it exists, we have found a path of weight L. Now we can update our answer with the minimum of the current answer and the sum of the number of edges for the two parts of this path. After performing all necessary updates of the answer with the subtree of c, we update the map for node n with the map for c, maintaining the minimum number of edges for all distances seen so far (in order to make sure we can find optimal paths from the map when considering later subtrees).
To update these maps efficiently, we will choose to always update the map with more keys using the map with less keys. In the worst case, all nodes have distinct distances from the root. Whenever a particular's node distance is added from one map to another, the resulting map must be at least twice as large as the size of the old map. The size of any map cannot exceed N keys, so each element can only be added to at most log N maps. Each node contributes to log N updates, so the time and space complexity are both O(N log N).
public static int solve(List&lt;List&lt;Edge&gt;&gt; T, long L) {
    var minEdgesForDist = Stream.&lt;Map&lt;Long, Integer&gt;&gt;generate(HashMap::new).limit(T.size()).collect(Collectors.toCollection(ArrayList::new));
    return new Object() { 
        // creating new object to define methods inside the context of the solve method
        int dfs(int node, int par, int depth, long dist) {
            minEdgesForDist.get(node).put(dist, depth); // for node itself
            int ret = Integer.MAX_VALUE;
            for (var edge : T.get(node))
                if (edge.toVertex != par) {
                    ret = Math.min(ret, dfs(edge.toVertex, node, depth + 1, dist + edge.weight));
                    if (minEdgesForDist.get(edge.toVertex).size() &gt; minEdgesForDist.get(node).size())
                        Collections.swap(minEdgesForDist, edge.toVertex, node); // important!
                    for (var entry : minEdgesForDist.get(edge.toVertex).entrySet()) {
                        var other = minEdgesForDist.get(node).get(L + 2 * dist - entry.getKey());
                        if (other != null)
                            ret = Math.min(ret, entry.getValue() + other - 2 * depth);
                    }
                    for (var entry : minEdgesForDist.get(edge.toVertex).entrySet())
                        minEdgesForDist.get(node).merge(entry.getKey(), entry.getValue(), Math::min);
                }
            return ret;
        }
        
        int getAnswer() {
            int ret = dfs(0, 0, 0, 0);
            return ret != Integer.MAX_VALUE ? ret : -1;
        }
    }.getAnswer();
}

"
"I try to deserialize a generic Object W&lt;T&gt; from JSON with Micronaut Serialization , it works, but the compiler produces an &quot;unchecked assignment&quot; warning.
I would like to achieve the same result without the warning or using @SuppressWarnings(&quot;unchecked&quot;).
The following is a reduced version of the code I use. It works, but there is a @SuppressWarnings(&quot;unchecked&quot;) annotation.
1st note: The ObjectMapper is not the Jackson ObjectMapper, but the io.micronaut.serde.ObjectMapper2nd note: I removed common java and slf4j imports for brevity
import io.micronaut.context.annotation.Prototype;
import io.micronaut.core.type.Argument;
import io.micronaut.serde.ObjectMapper;
import jakarta.inject.Inject;

@Prototype
public class Scratch {
    private static final Logger LOG = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());
    private final ObjectMapper objectMapper;

    @Inject
    public Scratch(ObjectMapper objectMapper) {
        this.objectMapper = objectMapper;
    }

    @SuppressWarnings(&quot;unchecked&quot;)
    private Optional&lt;CommonResponse&lt;JobResponse&gt;&gt; getCommonResponse(final String entry) {
        try {
            return Optional.of(objectMapper.readValue(entry, Argument.of(CommonResponse.class, JobResponse.class)));
        } catch (IOException e) {
            LOG.warn(&quot;Could not deserialize, skipping entry: '{}'&quot;, entry, e);
        }
        return Optional.empty();
    }
}

","There is no way to achieve this with Argument.of, but there is GenericArgument which is exactly what is needed.
Full example:
import io.micronaut.context.annotation.Prototype;
import io.micronaut.core.type.GenericArgument;
import io.micronaut.serde.ObjectMapper;
import jakarta.inject.Inject;

@Prototype
public class Scratch {
    private static final Logger LOG = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());
    private final ObjectMapper objectMapper;

    @Inject
    public Scratch(ObjectMapper objectMapper) {
        this.objectMapper = objectMapper;
    }

    private Optional&lt;CommonResponse&lt;JobResponse&gt;&gt; getCommonResponse(final String entry) {
        try {
            return Optional.of(objectMapper.readValue(entry, new GenericArgument&lt;&gt;() {}));
        } catch (IOException e) {
           LOG.warn(&quot;Could not deserialize, skipping entry: '{}'&quot;, entry, e);
        }
        return Optional.empty();
    }
}

See https://github.com/micronaut-projects/micronaut-core/issues/2204
"
"Arrays in Java are limited to Integer.MAX_VALUE for initial capacity &amp; indexable elements (around 2 billion). I would like to write a data structure class that uses a long for this instead.
I know there are two methods in wide circulation:

Use an array of arrays
Use APIs in sun.misc.Unsafe to manually allocate and access large slabs of memory

I don't want to use an array of arrays and using sun.misc.Unsafe is heavily discouraged, producing compilation warnings that cannot be silenced using ordinary methods.
Starting in Java 9 there began efforts to standardize &amp; replace sun.misc.Unsafe with the addition of java.lang.invoke.VarHandle in JEP 193. Then in Java 22 there was the addition of java.lang.foreign.MemorySegment in JEP 454. JEP 471 coming in Java 23 is going to deprecate the memory access methods in sun.misc.Unsafe for removal.
So it seems like there should be a way to use the existing VarHandle and MemorySegment APIs to write a long array in Java. How do I do this?
","You can use SegmentAllocator::allocate(MemoryLayout,long) to create a MemorySegment that can be used as an array of &quot;objects&quot; represented by the given MemoryLayout. Then you can wrap the segment in a Java class to encapsulate the &quot;array access&quot;.
Note this means the data has to be able to be put into off-heap memory. In other words, the data has to be a Java primitive or, for more complex types, a MemorySegment. You won't be able to fill the array with arbitrary Java reference types. If you want to treat complex elements as Java objects, you'll have to write a class that wraps the MemorySegment. Or at least, that's the only approach I'm aware of.

Primitive Types
For primitive types, this is relatively easy:
import java.lang.foreign.MemorySegment;
import java.lang.foreign.SegmentAllocator;
import java.lang.foreign.ValueLayout;
import java.util.Objects;

public final class LargeIntArray {

  public static final ValueLayout.OfInt LAYOUT = ValueLayout.JAVA_INT_UNALIGNED;
  
  private final MemorySegment segment;
  private final long length;

  public LargeIntArray(SegmentAllocator allocator, long length) {
    this.segment = allocator.allocate(LAYOUT, length);
    this.length = length;
  }

  public MemorySegment address() {
    return MemorySegment.ofAddress(segment.address());
  }

  public int get(long index) {
    return segment.getAtIndex(LAYOUT, index);
  }

  public void set(long index, int element) {
    segment.setAtIndex(LAYOUT, index, element);
  }

  public long length() {
    return length;
  }
}

There are ValueLayout.OfXXX interfaces for each of the primitive Java types.

Complex Types
For more complex data types, you will be working with MemorySegment instead of primitive types:
import java.lang.foreign.AddressLayout;
import java.lang.foreign.MemoryLayout;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.SegmentAllocator;
import java.lang.foreign.ValueLayout;

public final class LargeArray {

  private final MemorySegment segment;
  private final long length;
  private final AddressLayout layout;

  public LargeArray(SegmentAllocator allocator, MemoryLayout elementLayout, long length) {
    this.segment = allocator.allocate(elementLayout, length);
    this.layout = ValueLayout.ADDRESS.withTargetLayout(elementLayout);
    this.length = length;
  }

  public AddressLayout layout() {
    return layout;
  }

  public MemorySegment address() {
    return MemorySegment.ofAddress(segment.address());
  }

  public MemorySegment get(long index) {
    return segment.getAtIndex(layout, index);
  }

  public void set(long index, MemorySegment element) {
    segment.setAtIndex(layout, index, element);
  }

  public long length() {
    return length;
  }
}

Better Encapsulation
One potential improvement is to create a Java class that wraps the MemorySegment representing the &quot;objects&quot;. This will make working with the array more natural on the Java side. First, you need a generic way to map between MemorySegment and a Java class:
import java.lang.foreign.MemoryLayout;
import java.lang.foreign.MemorySegment;
import java.util.Objects;
import java.util.function.Function;

public interface ElementDescriptor&lt;T&gt; {

  public static &lt;T&gt; ElementDescriptor&lt;T&gt; of(
      MemoryLayout layout,
      Function&lt;MemorySegment, T&gt; toElement,
      Function&lt;T, MemorySegment&gt; toAddress) {
    Objects.requireNonNull(layout);
    Objects.requireNonNull(toElement);
    Objects.requireNonNull(toAddress);
    return new ElementDescriptor&lt;&gt;() {
      @Override
      public MemoryLayout layout() {
        return layout;
      }

      @Override
      public T elementFrom(MemorySegment segment) {
        if (segment.equals(MemorySegment.NULL)) {
          return null;
        }
        return toElement.apply(segment);
      }

      @Override
      public MemorySegment addressOf(T element) {
        if (element == null) {
          return MemorySegment.NULL;
        }
        return toAddress.apply(element);
      }
    };
  }

  MemoryLayout layout();

  T elementFrom(MemorySegment segment);

  MemorySegment addressOf(T element);
}

Then you need to update LargeArray to work with the above:
import java.lang.foreign.AddressLayout;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.SegmentAllocator;
import java.lang.foreign.ValueLayout;

public final class LargeArray&lt;T&gt; {

  private final MemorySegment segment;
  private final long length;
  private final AddressLayout layout;
  private final ElementDescriptor&lt;T&gt; descriptor;

  public LargeArray(SegmentAllocator allocator, long length, ElementDescriptor&lt;T&gt; descriptor) {
    this.segment = allocator.allocate(descriptor.layout(), length);
    this.layout = ValueLayout.ADDRESS.withTargetLayout(descriptor.layout());
    this.length = length;
    this.descriptor = descriptor;
  }

  public AddressLayout layout() {
    return layout;
  }

  public MemorySegment address() {
    return MemorySegment.ofAddress(segment.address());
  }

  public T get(long index) {
    return descriptor.elementFrom(segment.getAtIndex(layout, index));
  }

  public void set(long index, T element) {
    segment.setAtIndex(layout, index, descriptor.addressOf(element));
  }

  public long length() {
    return length;
  }
}

And finally, you need a data structure. For example, here is a Point struct with x and y coordinates:
import static java.lang.foreign.ValueLayout.JAVA_INT;

import java.lang.foreign.MemoryLayout;
import java.lang.foreign.MemoryLayout.PathElement;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.SegmentAllocator;
import java.lang.foreign.StructLayout;
import java.lang.invoke.MethodHandles;
import java.lang.invoke.VarHandle;
import java.util.Objects;

public final class Point {

  public static final StructLayout LAYOUT;
  public static final ElementDescriptor&lt;Point&gt; DESCRIPTOR;

  private static final VarHandle X;
  private static final VarHandle Y;

  static {
    LAYOUT = MemoryLayout.structLayout(JAVA_INT.withName(&quot;x&quot;), JAVA_INT.withName(&quot;y&quot;));

    var x = LAYOUT.varHandle(PathElement.groupElement(&quot;x&quot;));
    X = MethodHandles.insertCoordinates(x, 1, 0L);

    var y = LAYOUT.varHandle(PathElement.groupElement(&quot;y&quot;));
    Y = MethodHandles.insertCoordinates(y, 1, 0L);

    DESCRIPTOR = ElementDescriptor.of(LAYOUT, Point::new, Point::address);
  }

  private final MemorySegment segment;

  public Point(SegmentAllocator allocator) {
    segment = allocator.allocate(LAYOUT);
  }

  public Point(MemorySegment segment) {
    this.segment = Objects.requireNonNull(segment);
  }

  public MemorySegment address() {
    return MemorySegment.ofAddress(segment.address());
  }

  public int getX() {
    return (int) X.get(segment);
  }

  public void setX(int x) {
    X.set(segment, x);
  }

  public int getY() {
    return (int) Y.get(segment);
  }

  public void setY(int y) {
    Y.set(segment, y);
  }

  @Override
  public String toString() {
    return &quot;Point(x=&quot; + getX() + &quot;, y=&quot; + getY() + &quot;)&quot;;
  }
}

Example Use
Here is an example of using a LargeArray&lt;Point&gt;:
import java.lang.foreign.Arena;

public class Main {

  public static void main(String[] args) throws Throwable {
    try (var arena = Arena.ofConfined()) {
      var array = new LargeArray&lt;Point&gt;(arena, 10L, Point.DESCRIPTOR);

      // populate array
      for (long i = 0; i &lt; array.length(); i++) {
        var point = new Point(arena);
        point.setX((int) i);
        point.setY((int) i * 2);
        array.set(i, point);
      }

      // show modification of element in array
      var midPoint = array.get(5L);
      midPoint.setX(42);
      midPoint.setY(117);

      // print array contents
      for (long i = 0; i &lt; array.length(); i++) {
        System.out.printf(&quot;array[%d] = %s%n&quot;, i, array.get(i));
      }
    }
  }
}

Output:
array[0] = Point(x=0, y=0)
array[1] = Point(x=1, y=2)
array[2] = Point(x=2, y=4)
array[3] = Point(x=3, y=6)
array[4] = Point(x=4, y=8)
array[5] = Point(x=42, y=117)
array[6] = Point(x=6, y=12)
array[7] = Point(x=7, y=14)
array[8] = Point(x=8, y=16)
array[9] = Point(x=9, y=18)


Notes
Few notes:

The above examples are not necessarily the best ways to implement &quot;large arrays&quot; with FFM.

You may want to have the array classes implement Iterable.

You may want to add one or more constructors that accept MemorySegment to the array classes. This would facilitate using arrays allocated in native code. Make sure the MemorySegment is of the correct size in those cases; see the reinterpret methods. That last bit also applies to classes like Point.

You may want to add the ability to choose between aligned and unaligned.

It may be better to use segment.reinterpret(0L) to return the address of the array/data structure so that the returned MemorySegment has the same scope instead of always being in the global scope.


"
"As my CS project, I am creating a full Touhou-esque bullet dodging game that involves rendering thousands of bullet images on their proper coordinates on JPanel. Fortunately, the JVM could hold tens of thousands of bufferedImage without any noticeable frame drop, so I wasn't expecting this giant roadblock I hit: rotating images.
What I initially wanted to achieve is rotating the enemy bullet's BufferedImage; I used the rotation methods on other Stack Overflow question with a small sample, and they worked just fine. The problem arose when I tried to rotate thousands of bullet sprites in the ArrayList of bullet objects. Tens of thousands of new BufferedImage and Graphics2D creation completely halted JVM upon running.
I looked into all the questions relating to Java's image rotation to find a lightweight method that wouldn't cause severe frame drops or downright heap space issues. However, the methods all included at least some form of Object creation or manipulation, and the program simply couldn't take it.
I did attempt to make a lightweight rotation method myself by sacrificing two weeks and at least seven IQ points. Still, without any knowledge of more inherent understanding of computer science, the &quot;best&quot; performance I could get was this method, modifying the field images:
public Bullet(... , double deg, ... , BufferedImage shape /*actual bullet sprite*/, String tag, BufferedImage emp /*empty bufferedimage to act as a template to modify image then redraw*/ ) throws IOException
    {
        rotor = emp;
        img = shape;
        rotate(deg);
        setDeg(deg);
        this.deg = deg;
        ...
    }
public void rotate(double angle) { //tried AffineTransform and image Op and everything but all the same...
        Graphics2D g = rotor.createGraphics();
        g.setBackground(new Color(255, 255, 255, 0));
        g.clearRect(0,0, rotor.getWidth(), rotor.getHeight());
        g.rotate(Math.toRadians(angle), img.getWidth() / 2, img.getHeight() / 2);
        g.drawImage(img, null, img.getWidth() - rotor.getWidth(), img.getHeight() - rotor.getHeight());
        g.dispose();
        img = rotor;
}

Still, with so many bullets to render(at least 10,000), the method makes no innovative difference. Is there any way to make the image rotation as light as possible so as not to add a relevant weight to rendering (and hopefully salvage the project from destined doom)?
Without the rotation the knives look so wrong. Pls help :c
","Consider pre-computing and storing rotated images during the game's initialization to efficiently rotate. This approach minimizes the real-time computational load to enhance performance. Here’s how you can implement this:
Pre-computation Phase:
Pre-compute rotated versions of each bullet sprite at fixed intervals (e.g., every 1 degree).
Store these pre-rotated images in an array or a map, where each index/key represents a rotation angle.
Implementation Example:
// Precompute rotated images
BufferedImage[] preRotatedImages = new BufferedImage[360];
for (int i = 0; i &lt; 360; i++) {
    preRotatedImages[i] = rotateImage(originalImage, i);
}

// Rotate function
private BufferedImage rotateImage(BufferedImage img, int angle) {
    int w = img.getWidth();
    int h = img.getHeight();
    int newW = (int) Math.ceil(Math.sqrt(w * w + h * h));
    BufferedImage rotated = new BufferedImage(newW, newW, BufferedImage.TYPE_INT_ARGB);
    Graphics2D g2d = rotated.createGraphics();
    g2d.setRenderingHint(RenderingHints.KEY_ANTIALIASING, RenderingHints.VALUE_ANTIALIAS_ON);
    g2d.setRenderingHint(RenderingHints.KEY_INTERPOLATION, RenderingHints.VALUE_INTERPOLATION_BILINEAR);
    g2d.rotate(Math.toRadians(angle), newW / 2, newW / 2);
    g2d.drawImage(img, (newW - w) / 2, (newW - h) / 2, null);
    g2d.dispose();
    return rotated;
}

// During gameplay, fetch pre-rotated images
public void render(Graphics g, int angle, int x, int y) {
    g.drawImage(preRotatedImages[angle % 360], x, y, null);
}

Optimizations:
As @Abra suggested, rotate and store images only for visible bullets, reducing memory usage. Use hardware-accelerated image drawing with Java’s built-in capabilities if you haven't already. Test memory usage, loading times, and performance to see if this approach works for you.
"
"I want to add a custom CSS integer property (in this example I use -fx-foo) to my custom Label. This is my code:
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import javafx.application.Application;
import javafx.beans.property.IntegerProperty;
import javafx.css.CssMetaData;
import javafx.css.Styleable;
import javafx.css.StyleableIntegerProperty;
import javafx.css.converter.SizeConverter;
import javafx.scene.Scene;
import javafx.scene.control.Label;
import javafx.scene.layout.VBox;
import javafx.stage.Stage;

public class NewMain extends Application {

    public static class FooLabel extends Label {

        private static final CssMetaData&lt;FooLabel, Number&gt; FOO_PROPERTY = new CssMetaData&lt;FooLabel, Number&gt;(&quot;-fx-foo&quot;,
                SizeConverter.getInstance(), 10) {

            @Override
            public boolean isSettable(FooLabel label) {
                return true;
            }

            @Override
            public StyleableIntegerProperty getStyleableProperty(FooLabel label) {
                return (StyleableIntegerProperty) label.fooProperty();
            }
        };

        private static final List&lt;CssMetaData&lt;? extends Styleable, ?&gt;&gt; CSS_META_DATA;

        static {
            List&lt;CssMetaData&lt;? extends Styleable, ?&gt;&gt; list = new ArrayList&lt;&gt;(Label.getClassCssMetaData());
            list.add(FOO_PROPERTY);
            CSS_META_DATA = Collections.unmodifiableList(list);
        }

        public static List&lt;CssMetaData&lt;? extends Styleable, ?&gt;&gt; getClassCssMetaData() {
            return CSS_META_DATA;
        }

        private final StyleableIntegerProperty foo = new StyleableIntegerProperty(10) {

            @Override
            public CssMetaData getCssMetaData() {
                return FOO_PROPERTY;
            }

            @Override
            public Object getBean() {
                return FooLabel.this;
            }

            @Override
            public String getName() {
                return &quot;foo&quot;;
            }
        };

        public FooLabel() {
            super();
            foo.addListener((observable, oldValue, newValue) -&gt; {
                System.out.println(&quot;NEW VALUE:&quot; + newValue);
            });
        }

        public IntegerProperty fooProperty() {
            return foo;
        }

        public void setFoo(int foo) {
            this.foo.set(foo);
        }

        public int getFoo() {
            return foo.get();
        }
    }

    /**************** MAIN APP  *****************/

    @Override
    public void start(Stage primaryStage) {
        var fooLabel = new FooLabel();
        fooLabel.getStyleClass().add(&quot;test&quot;);
        fooLabel.setText(&quot;abc&quot;);
        VBox root = new VBox(fooLabel);
        root.getStylesheets().add(NewMain.class.getResource(&quot;test.css&quot;).toExternalForm());
        Scene scene = new Scene(root, 100, 100);
        primaryStage.setScene(scene);
        primaryStage.show();
    }

    public static void main(String[] args) {
        launch(args);
    }
}

and CSS:
.test {
    -fx-foo: 100;
    -fx-background-color: yellow;
}

The code is compiled and when it works it doesn't throw any exceptions. The label is yellow. However, the foo property never changes, it seems that -fx-foo is just ignored. Could anyone say how to fix it?
","Types that are styleable from CSS implement Styleable. In order for the CSS engine to know which properties are styleable, the Styleable must report its CSS metadata. It does this via its getCssMetaData() method. Note that's an instance method. If you have your own types with their own CSS metadata, then you have to override that method. The returned list should contain the CSS metadata for both the supertypes and your own type if all styleable properties are to continue to work properly.
That's where the static getClassCssMetaData() methods defined by most styleable classes comes into play. It's not strictly required by the system, and in fact cannot be enforced by the compiler due to the method being static (there's no way in Java to say a type must have a static method). But it's basically part of the expected API and provides at least two functions:

It makes it easy for subtypes to get the metadata of their supertype in a static way without reflection (note since CSS metadata is shared between all instances of a class, it makes sense to define it statically).

It allows tools to inspect the CSS metadata without having to instantiate the class.


The getCssMetaData() implementation typically delegates to the static method.
However, you actually want to override Control#getControlCssMetaData() for types that inherit from Control. It serves the same function as Styleable#getCssMetaData(), but Control already provides a final implementation of the latter. That implementation combines the CSS metadata from Control#getControllCssMetaData() and SkinBase#getCssMetaData(), which allows skins to add their own separate styleable properties.
So, you should just have to add the following to your FooLabel class:
@Override
public List&lt;CssMetaData&lt;?, ?&gt;&gt; getControlCssMetaData() {
  return getClassCssMetaData();
}

Note: The method is protected in Control, but the Labeled class makes it public.

Full Example
Source code
FooLabel.java
Implemented similarly to how the standard controls implement styleable properties. Though note it makes use of StyleablePropertyFactory for creating the CssMetaData.
package com.example;

import java.util.List;
import javafx.beans.property.IntegerProperty;
import javafx.css.CssMetaData;
import javafx.css.SimpleStyleableIntegerProperty;
import javafx.css.StyleableIntegerProperty;
import javafx.css.StyleablePropertyFactory;
import javafx.scene.Node;
import javafx.scene.control.Label;

public class FooLabel extends Label {

  private final StyleableIntegerProperty foo = new SimpleStyleableIntegerProperty(Css.FOO, this, &quot;foo&quot;);
  public final void setFoo(int foo) { this.foo.set(foo); }
  public final int getFoo() { return foo.get(); }
  public final IntegerProperty fooProperty() { return foo; }

  public FooLabel() {
    init();
  }

  public FooLabel(String text) {
    super(text);
    init();
  }

  public FooLabel(String text, Node graphic) {
    super(text, graphic);
    init();
  }

  private void init() {
    getStyleClass().add(&quot;foo-label&quot;);
  }

  public static List&lt;CssMetaData&lt;?, ?&gt;&gt; getClassCssMetaData() {
    return Css.META_DATA;
  }

  @Override
  public List&lt;CssMetaData&lt;?, ?&gt;&gt; getControlCssMetaData() {
    return getClassCssMetaData();
  }

  private static class Css {

    private static final CssMetaData&lt;FooLabel, Number&gt; FOO;
    private static final List&lt;CssMetaData&lt;?, ?&gt;&gt; META_DATA;

    static {
      var factory = new StyleablePropertyFactory&lt;FooLabel&gt;(Label.getClassCssMetaData());
      FOO = factory.createSizeCssMetaData(&quot;-fx-foo&quot;, s -&gt; s.foo);
      META_DATA = List.copyOf(factory.getCssMetaData());
    }
  }
}

Main.java
package com.example;

import javafx.application.Application;
import javafx.scene.Scene;
import javafx.scene.layout.StackPane;
import javafx.stage.Stage;

public class Main extends Application {

  private static final String STYLESHEET =
      &quot;&quot;&quot;
      .foo-label {
        -fx-foo: 100;
      }
      &quot;&quot;&quot;;

  @Override
  public void start(Stage primaryStage) throws Exception {
    var label = new FooLabel(&quot;Hello, World!&quot;);
    label.fooProperty().subscribe(value -&gt; System.out.printf(&quot;foo = %d%n&quot;, value));

    var scene = new Scene(new StackPane(label), 500, 300);
    scene.getStylesheets().add(&quot;data:text/css,&quot; + STYLESHEET);

    primaryStage.setScene(scene);
    primaryStage.show();
  }

  public static void main(String[] args) {
    launch(Main.class);
  }
}

Output
foo = 0
foo = 100

"
"I have version 21 installed and as far as I understand, I can't use Nashorn engine anymore, but maybe GraalVM is available for me?
I have installed all the necessary dependencies:
&lt;dependency&gt;
    &lt;groupId&gt;org.graalvm.js&lt;/groupId&gt;
    &lt;artifactId&gt;js&lt;/artifactId&gt;
    &lt;version&gt;21.1.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.graalvm.js&lt;/groupId&gt;
    &lt;artifactId&gt;js-scriptengine&lt;/artifactId&gt;
    &lt;version&gt;21.1.0&lt;/version&gt;
&lt;/dependency&gt;

Here is a simple example that I'm trying to execute:
public static void main(String[] args) throws ScriptException {
    try (Context context = Context.newBuilder(&quot;js&quot;)
            .allowAllAccess(true)
            .build()) {
        // Evaluate JavaScript code
        String jsCode = &quot;console.log('Hello, GraalVM!');&quot;;
        context.eval(&quot;js&quot;, jsCode);
    } catch (Exception e) {
        throw new ScriptException(&quot;Script execution failed: &quot; + e.getMessage());
    }
}

However, I get an error:

Exception in thread &quot;main&quot; javax.script.ScriptException: Script
execution failed: A language with id 'js' is not installed. Installed
languages are: [].    at org.example.Main.main(Main.java:23)

I also tried something like this:
public static void main(String[] args) throws ScriptException {
    ScriptEngineManager manager = new ScriptEngineManager();
    ScriptEngine engine  = manager.getEngineByName(&quot;JavaScript&quot;);
    engine.eval(&quot;print('HI');&quot;);
}

But I got another error:

Exception in thread &quot;main&quot; java.lang.NullPointerException: Cannot
invoke &quot;javax.script.ScriptEngine.eval(String)&quot; because &quot;engine&quot; is
null  at org.example.Main.main(Main.java:20)

The problem is that manual installation of any components is impossible for some reason. I just need to some dependencies and make everything works. Something &quot;out of the box&quot;. Is there any workaround for this problem? Maybe there are any other available engines?
","Graalvm js - Project
jsEngineScriptJDK21_graalvm_js
├── pom.xml
└── src
    └── main
        └── java
            └── com
                └── example
                    └── JsTest.java


pom.xml
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;groupId&gt;com.example&lt;/groupId&gt;
    &lt;artifactId&gt;graalvm-js-engine-script&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;GraalvmJsEngineScript&lt;/name&gt;
    &lt;description&gt;jsEngineScript App Project&lt;/description&gt;
    &lt;properties&gt;
        &lt;maven.compiler.source&gt;21&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;21&lt;/maven.compiler.target&gt;
        &lt;maven.compiler.encoding&gt;UTF-8&lt;/maven.compiler.encoding&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;org.graalvm.js&lt;/groupId&gt;
        &lt;artifactId&gt;js&lt;/artifactId&gt;
        &lt;version&gt;23.0.0&lt;/version&gt;
    &lt;/dependency&gt;
        
    &lt;/dependencies&gt;    

    &lt;build&gt;
        &lt;finalName&gt;app&lt;/finalName&gt;
    &lt;/build&gt;

&lt;/project&gt;

JsTest.java
package com.example;

import org.graalvm.polyglot.Context;
import org.graalvm.polyglot.Value;

public class JsTest {

  public static void main(String[] args) throws Exception {
  
    try (Context context = Context.newBuilder(&quot;js&quot;)
            .allowAllAccess(true)
            .build()) {
        // Evaluate JavaScript code
        String jsCode = &quot;console.log('Hello, GraalVM!');&quot;;
        context.eval(&quot;js&quot;, jsCode);
    } catch (Exception e) {
        throw new Exception(&quot;Script execution failed: &quot; + e.getMessage());
    }
    
  }
  
}

JsTest3.java
package com.example;

import org.graalvm.polyglot.Context;
import org.graalvm.polyglot.Value;

public class JsTest3 {

    public static void main(String[] args) throws Exception {

        try (Context context = Context.newBuilder(&quot;js&quot;)
                .option(&quot;engine.WarnInterpreterOnly&quot;, &quot;false&quot;)
                .allowAllAccess(true)
                .build()) {

            Value result = context.eval(&quot;js&quot;, &quot;2 + 2&quot;);
            System.out.println(&quot;Result: &quot; + result.asInt());

            // Call JavaScript Function
            context.eval(&quot;js&quot;, &quot;function greet(name) { return 'Hello, ' + name; }&quot;);
            Value greetFunction = context.getBindings(&quot;js&quot;).getMember(&quot;greet&quot;);
            System.out.println(&quot;Greeting: &quot; + greetFunction.execute(&quot;World&quot;).asString());


            // Evaluate JavaScript code
            String jsCode = &quot;print('HI');&quot;;
            context.eval(&quot;js&quot;, jsCode);

        } catch (Exception e) {
            throw new Exception(&quot;Script execution failed: &quot; + e.getMessage());
        }

    }

}


Build And Run
Build
mvn clean package

mvn dependency:copy-dependencies -DoutputDirectory=target/libs

Run
JsTest
java -cp &quot;target/libs/*:target/app.jar&quot; \
  com.example.JsTest

Result:
[To redirect Truffle log output to a file use one of the following options:
* '--log.file=&lt;path&gt;' if the option is passed using a guest language launcher.
* '-Dpolyglot.log.file=&lt;path&gt;' if the option is passed using the host Java launcher.
* Configure logging using the polyglot embedding API.]
[engine] WARNING: The polyglot context is using an implementation that does not support runtime compilation.
The guest application code will therefore be executed in interpreted mode only.
Execution only in interpreted mode will strongly impact the guest application performance.
For more information on using GraalVM see https://www.graalvm.org/java/quickstart/.
To disable this warning the '--engine.WarnInterpreterOnly=false' option or use the '-Dpolyglot.engine.WarnInterpreterOnly=false' system property.
Hello, GraalVM!

JsTest3
java -cp &quot;target/libs/*:target/app.jar&quot; \
  com.example.JsTest3

Result: There is no warning message.
Result: 4
Greeting: Hello, World
HI

JDK Version
$ javac -version
javac 21.0.4

$ java -version
openjdk version &quot;21.0.4&quot; 2024-07-16 LTS
OpenJDK Runtime Environment Temurin-21.0.4+7 (build 21.0.4+7-LTS)
OpenJDK 64-Bit Server VM Temurin-21.0.4+7 (build 21.0.4+7-LTS, mixed mode, sharing)

Notice:

(1) This example project uses a general JDK; you can try to use it with GraalVM CE or GraalVM Oracle JDK for your testing.
(2) Do not use Zulu JDK; if you use Zulu JDK to execute this sample project, there will be other error messages.
(3) When using IDE (Eclipse, IDEA) to develop Maven projects, if there are errors such as class not found, you can do this:

(a) Try to execute Command Line in Terminal, mvn clean package. If no error occurs, it is a problem with the IDE.
(b) Try Refresh Maven in the IDE. Let the IDE capture the content after updating the pom.xml settings.



nashorn script engine - Project
jsEngineScriptJDK21
├── pom.xml
└── src
    └── main
        └── java
            └── com
                └── example
                    └── JsTest.java

pom.xml
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;groupId&gt;com.example&lt;/groupId&gt;
    &lt;artifactId&gt;js-engine-script&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;jsEngineScript&lt;/name&gt;
    &lt;description&gt;jsEngineScript App Project&lt;/description&gt;
    &lt;properties&gt;
        &lt;maven.compiler.source&gt;21&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;21&lt;/maven.compiler.target&gt;
        &lt;maven.compiler.encoding&gt;UTF-8&lt;/maven.compiler.encoding&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.openjdk.nashorn&lt;/groupId&gt;
            &lt;artifactId&gt;nashorn-core&lt;/artifactId&gt;
            &lt;version&gt;15.4&lt;/version&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;finalName&gt;app&lt;/finalName&gt;
    &lt;/build&gt;

&lt;/project&gt;

JsTest.java
package com.example;

import javax.script.ScriptEngine;
import javax.script.ScriptEngineManager;
import javax.script.ScriptException;

public class JsTest {
    public static void main(String[] args) throws ScriptException {
        try {
            //ScriptEngine engine = new ScriptEngineManager().getEngineByName(&quot;nashorn&quot;);
            //ScriptEngine engine  =new ScriptEngineManager().getEngineByName(&quot;JavaScript&quot;);
            ScriptEngineManager manager = new ScriptEngineManager();
            ScriptEngine engine  = manager.getEngineByName(&quot;JavaScript&quot;);

            if (engine == null) {
                System.out.println(&quot;Nashorn script engine not available.&quot;);
                return;
            }

            engine.eval(&quot;print('HI');&quot;);
            engine.eval(&quot;print('Hello, Nashorn!');&quot;);
            engine.eval(&quot;var x = 10 + 20; print('x = ' + x);&quot;);

        } catch (ScriptException e) {
            e.printStackTrace();
        }
    }
}

Build And Run
Build
mvn clean package

mvn dependency:copy-dependencies -DoutputDirectory=target/libs

Run
java -cp &quot;target/libs/*:target/app.jar&quot; \
  com.example.JsTest

Result:
HI
Hello, Nashorn!
x = 30

"
"Description
Guru gave a task to his students.Â Â  He gave a sentence, Â and the students have to swap the first and the last words and reverse all the characters between those words. Â Â Help the students to solve this task using a java program.
Requirements:

The words present in the sentence must be more than 2, else print &quot;Invalid Length&quot;

The word should contain only alphabets and space, else print &quot; is an invalid sentence&quot;


Note:

In the Sample Input / Output provided, Â the highlighted text in bold corresponds to the input given by the user, Â and the rest of the text represents the output.

Ensure to follow the object-oriented specifications provided in the question description.

Ensure to provide the names for classes, Â attributes, Â and methods as specified in the question description.

Adhere to the code template, Â if provided


Please do not use System.exit(0) to terminate the program.
Example input/output examples.  Â All input is preceded by the prompt Enter the sentence
Example 1:
Input:  Do you wear your mask
Output: mask ruoy raew uoy Do
Example 2:
Input:  Card reader
Output: Invalid Length
Example 3:
Input:  Refer @ friend
Output: Refer @ friend is an invalid sentence
import java.util.Scanner;

class SentenceProcessor {
    
    // Method to check if the sentence is valid
    public boolean isValidSentence(String sentence) {
        return sentence.matches(&quot;[a-zA-Z ]+&quot;); // Only alphabets and spaces allowed
    }

    // Method to process the sentence
    public String processSentence(String sentence) {
        if (!isValidSentence(sentence)) {
            return sentence + &quot; is an invalid sentence&quot;;
        }

        String[] words = sentence.trim().split(&quot;\\s+&quot;); // Split by whitespace

        if (words.length &lt;= 2) {
            return &quot;Invalid Length&quot;;
        }

        // Swap first and last words
        String firstWord = words[0];
        String lastWord = words[words.length - 1];
        words[0] = lastWord;
        words[words.length - 1] = firstWord;

        // Reverse middle words
        for (int i = 1; i &lt; words.length - 1; i++) {
            words[i] = new StringBuilder(words[i]).reverse().toString();
        }

        return String.join(&quot; &quot;, words); // Join words with a space
    }
}

public class UserInterface {
    
    public static void main(String[] args) {
        Scanner sc = new Scanner(System.in);
        
        System.out.println(&quot;Enter the sentence&quot;);
        String input = sc.nextLine();
        
        SentenceProcessor processor = new SentenceProcessor();
        String result = processor.processSentence(input);
        
        System.out.println(result);
        
        sc.close(); // Close the scanner to avoid resource leaks
    }
}

Output:-
Enter the sentence&lt;br&gt;
Do you wear your mask&lt;br&gt;
mask uoy raew ruoy Do&lt;br&gt;

Expected output:-
Enter the sentence&lt;br&gt;
Do you wear your mask&lt;br&gt;
mask ruoy raew uoy Do&lt;br&gt;

Tried resolving this but I am failing to get desired output. I also tried using various open sources which were not able to give me correct code. They are repetitively giving me same output(like chatgpt, copilot).
","I originally thought your requirements didn't match one of your expected outputs. I was corrected in the comments.
This method reverses the order of the words in an array and then reverses each word, excluding the first and last words.
// Method to process the sentence
static public String processSentence(String sentence) {
    if (!isValidSentence(sentence)) {
        return sentence + &quot; is an invalid sentence&quot;;
    }

    String[] words = sentence.trim().split(&quot;\\s+&quot;); // Split by whitespace

    if (words.length &lt;= 2) {
        return &quot;Invalid Length&quot;;
    }

    //Swap all words
    String[] reverseWords = new String[words.length];
    for(int i = words.length - 1; i &gt;= 0; i--)
    {
        reverseWords[(reverseWords.length - 1) - i] = words[i];
    }

    // Reverse middle words
    for (int i = 1; i &lt; words.length - 1; i++) {
        reverseWords[i] = new StringBuilder(reverseWords[i]).reverse().toString();
    }

    return String.join(&quot; &quot;, reverseWords); // Join words with a space
}

Output:
Do you wear your mask
mask ruoy raew uoy Do

"
"I want to have a special style class for a ComboBox that I could reuse. For example, I want to create a class yellowed that will provide yellow background. This is my code:
Java:
public class NewMain extends Application {

    @Override
    public void start(Stage primaryStage) {
        ComboBox&lt;String&gt; comboBox = new ComboBox&lt;&gt;();
        comboBox.getItems().addAll(&quot;Option 1&quot;, &quot;Option 2&quot;, &quot;Option 3&quot;);
        comboBox.getStyleClass().add(&quot;yellowed&quot;);

        VBox vbox = new VBox(comboBox);
        Scene scene = new Scene(vbox, 400, 300);
        scene.getStylesheets().add(NewMain.class.getResource(&quot;test.css&quot;).toExternalForm());
        primaryStage.setScene(scene);
        primaryStage.show();
    }

    public static void main(String[] args) {
        launch(args);
    }
}

CSS:
.combo-box.yellowed {
    -fx-background-color: yellow;
}

.combo-box-popup.yellowed &gt; .list-view &gt; .virtual-flow &gt; .clipped-container &gt; .sheet &gt; .list-cell {
    -fx-background-color: yellow;
}

The problem is that the popup (of my ComboBox with yellowed style class) that will be shown won't have a yellowed class.
Could anyone say, if there is a way to add a style class to the popup of a specific ComboBox?
","The popup is considered a descendant of the ComboBox1. This is documented by the JavaFX CSS Reference Guide:

ComboBox
Style class: combo-box
The ComboBox control has all the properties and pseudo‑classes of ComboBoxBase
Substructure

list-cell — a ListCell instance used to show the selection in the button area of a non-editable ComboBox
text-input — a TextField instance used to show the selection and allow input in the button area of an editable ComboBox
combo-box-popup — a PopupControl that is displayed when the button is pressed [emphasis added]

list-view — a ListView

list-cell — a ListCell






So, all you need to do is:
.combo-box.yellowed .combo-box-popup .list-cell {
  -fx-background-color: yellow;
}

Technically, combo-box.yellowed .list-cell is sufficient, but note that will also target the node used to display the actual combo box (i.e., the &quot;button cell&quot;, which is also a ListCell). And of course you can make the selector more specific if you want/need to.
Here's a runnable example:
import javafx.application.Application;
import javafx.geometry.Pos;
import javafx.scene.Scene;
import javafx.scene.control.ComboBox;
import javafx.scene.layout.HBox;
import javafx.stage.Stage;

public class Main extends Application {

  // Text blocks require Java 15+
  private static final String STYLESHEET =
      &quot;&quot;&quot;
      .combo-box.yellow,
      .combo-box.yellow .combo-box-popup .list-cell {
        -fx-background-color: yellow;
      }

      .combo-box.red,
      .combo-box.red .combo-box-popup .list-cell {
        -fx-background-color: red;
      }
      &quot;&quot;&quot;;

  @Override
  public void start(Stage primaryStage) {
    var box1 = createComboBox();
    box1.getStyleClass().add(&quot;yellow&quot;);

    var box2 = createComboBox();
    box2.getStyleClass().add(&quot;red&quot;);

    var root = new HBox(10, box1, box2);
    root.setAlignment(Pos.CENTER);

    var scene = new Scene(root, 500, 300);
    // Adding stylesheet via data URI requires JavaFX 17+
    scene.getStylesheets().add(&quot;data:text/css,&quot; + STYLESHEET);

    primaryStage.setScene(scene);
    primaryStage.show();
  }

  private ComboBox&lt;String&gt; createComboBox() {
    var box = new ComboBox&lt;String&gt;();
    for (int i = 1; i &lt;= 5; i++) {
      box.getItems().add(&quot;Option #&quot; + i);
    }
    // The 'getFirst' method requires Java 21+
    box.setValue(box.getItems().getFirst());
    return box;
  }

  public static void main(String[] args) {
    launch(Main.class);
  }
}

Note the example, as written, requires Java 21+ and JavaFX 17+.

1. For the curious, this is implemented by overriding PopupControl#getStyleableParent() to return the control.
"
"I have written the mandelbrotset in java,but if i want to zoom into it it gets blurry after around 14 clicks, no matter the Maxiterration number, if its 100 it gets blurry and if its 100000 it gets blurry after 14 zoom ins.Something i noticed is that after i zoom in twice, all of the next zoom ins are instant in contrast to the first two which usually take a few seconds, this may help finding the solution. The code:
import java.util.*;
import java.awt.*;
import java.awt.image.*;
import java.awt.event.*;
import javax.swing.*;
import java.math.BigDecimal;

public class test extends JFrame {
  
  static final int WIDTH  = 400;
  static final int HEIGHT = WIDTH;
  
  Canvas canvas;
  BufferedImage fractalImage;
  
  static final int MAX_ITER = 10000;
  static final BigDecimal DEFAULT_TOP_LEFT_X = new BigDecimal(-2.0);
  static final BigDecimal DEFAULT_TOP_LEFT_Y = new BigDecimal(1.4); 
  static final double DEFAULT_ZOOM       = Math.round((double) (WIDTH/3));
  final int numThreads = 10;
  
  double zoomFactor = DEFAULT_ZOOM;
  BigDecimal topLeftX   = DEFAULT_TOP_LEFT_X;
  BigDecimal topLeftY   = DEFAULT_TOP_LEFT_Y;
  
  BigDecimal z_r = new BigDecimal(0.0);
  BigDecimal z_i = new BigDecimal(0.0);

// -------------------------------------------------------------------
  public test() {
    setInitialGUIProperties();
    addCanvas();
    canvas.addKeyStrokeEvents();
    updateFractal();
    this.setVisible(true);
  }
  
// -------------------------------------------------------------------

  public static void main(String[] args) {
    new test();
  }
  
// -------------------------------------------------------------------

  private void addCanvas() {

    canvas = new Canvas();
    fractalImage = new BufferedImage(WIDTH, HEIGHT, BufferedImage.TYPE_INT_RGB);
    canvas.setVisible(true);
    this.add(canvas, BorderLayout.CENTER);

  } // addCanvas

// -------------------------------------------------------------------
    
    private void setInitialGUIProperties() {
      
      this.setTitle(&quot;Fractal Explorer&quot;);
      this.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);
      this.setSize(WIDTH, HEIGHT);
      this.setResizable(false);
      this.setLocationRelativeTo(null);
    } // setInitialGUIProperties

// -------------------------------------------------------------------
  private BigDecimal getXPos(double x) {
    return topLeftX.add(new BigDecimal(x/zoomFactor));
  } // getXPos
// -------------------------------------------------------------------
  private BigDecimal getYPos(double y) {
    return topLeftY.subtract(new BigDecimal(y/zoomFactor));
  } // getYPos
// -------------------------------------------------------------------
  
  /**
   * Aktualisiert das Fraktal, indem die Anzahl der Iterationen fÃ¼r jeden Punkt im Fraktal berechnet wird und die Farbe basierend darauf geÃ¤ndert wird.
   **/
  
  public void updateFractal() {
    Thread[] threads = new Thread[numThreads];
    int rowsPerThread = HEIGHT / numThreads;
    
    // Construct each thread
    for (int i=0; i&lt;numThreads; i++) {
      threads[i] = new Thread(new FractalThread(i * rowsPerThread, (i+1) * rowsPerThread));
    }
    
    // Starte jeden thread
    for (int i=0; i&lt;numThreads; i++) {
      threads[i].start();
    }
    
    // Warten bis alle threads fertig sind
    for (int i=0; i&lt;numThreads; i++) {
      try {
        threads[i].join();
      } catch (InterruptedException e) {
        e.printStackTrace();
      }
    }
    
    canvas.repaint();
    
  } // updateFractal
// -------------------------------------------------------------------  
  //Gibt basierend auf der Iterationsanzahl eine trennungsfarbe zurÃ¼ck eines gegebenen Punktes im Fraktal
  private class FractalThread implements Runnable {
    
    int startY;
    int endY;
    
    public FractalThread(int startY, int endY) {
      this.startY = startY;
      this.endY = endY;
    }
    
    public void run() {
      BigDecimal c_r;
      BigDecimal c_i;
      for (int x = 0; x &lt; WIDTH; x++ ) {
        for (int y = startY; y &lt; endY; y++ ) {
          
          c_r = getXPos(x);
          c_i = getYPos(y);
          int iterCount = computeIterations(c_r, c_i);
                         
          int pixelColor = makeColor(iterCount);   
          fractalImage.setRGB(x, y, pixelColor);
        }
        System.out.println(x);
      }
      
    } // run
                           
  } // FractalThread
  private int makeColor( int iterCount ) {
    
    int color = 0b011011100001100101101000; 
    int mask  = 0b000000000000010101110111; 
    int shiftMag = iterCount / 13;
    
    if (iterCount == MAX_ITER) 
      return Color.BLACK.getRGB();
    
    return color | (mask &lt;&lt; shiftMag);
    
  } // makeColor

// -------------------------------------------------------------------

  private int computeIterations(BigDecimal c_r, BigDecimal c_i) {
    BigDecimal z_r = new BigDecimal(0.0);
    BigDecimal z_i = new BigDecimal(0.0);
    BigDecimal z_r_tmp = z_r;
    BigDecimal dummy2 = new BigDecimal(2.0);
    int iterCount = 0;
    while ( z_r.doubleValue()*z_r.doubleValue() + z_i.doubleValue()*z_i.doubleValue() &lt;= 4.0 ) { 
      z_r_tmp = z_r;
      z_r = z_r.multiply(z_r).subtract(z_i.multiply(z_r)).add(c_r);
      z_i = z_i.multiply(dummy2).multiply(z_i).multiply(z_r_tmp).add(c_i);
      
      if (iterCount &gt;= MAX_ITER) return MAX_ITER;
      iterCount++;
    }
    
    return iterCount;
    
  } // computeIterations
// -------------------------------------------------------------------
  private void moveUp() {
    double curHeight = HEIGHT / zoomFactor;
    topLeftY = topLeftY.add(new BigDecimal(curHeight / 6));
    updateFractal();
  } // moveUp
// -------------------------------------------------------------------
  private void moveDown() {
    double curHeight = HEIGHT / zoomFactor;
    topLeftY = topLeftY.subtract(new BigDecimal(curHeight / 6));
    updateFractal();
  } // moveDown
// -------------------------------------------------------------------
  private void moveLeft() {
    double curWidth = WIDTH / zoomFactor;
    topLeftX = topLeftX.subtract(new BigDecimal(curWidth / 6));
    updateFractal();
  } // moveLeft
// -------------------------------------------------------------------
  private void moveRight() {
    double curWidth = WIDTH / zoomFactor;
    topLeftX = topLeftX.add(new BigDecimal(curWidth / 6));;
    updateFractal();
  } // moveRight
// -------------------------------------------------------------------    

  private void adjustZoom( double newX, double newY, double newZoomFactor ) {
    
    topLeftX = topLeftX.add(new BigDecimal(newX/zoomFactor));
    topLeftY = topLeftY.subtract(new BigDecimal(newX/zoomFactor));
    zoomFactor = newZoomFactor;
    
    topLeftX = topLeftX.subtract(new BigDecimal(( WIDTH/2) / zoomFactor));
    topLeftY = topLeftY.add(new BigDecimal( (HEIGHT/2) / zoomFactor));
    updateFractal();
    
  } // adjustZoom

// -------------------------------------------------------------------  
  
  private class Canvas extends JPanel implements MouseListener {
    
    public Canvas() {
      addMouseListener(this);
    } 
    
    @Override public Dimension getPreferredSize() {
      return new Dimension(WIDTH, HEIGHT);
    } // getPreferredSize
    
    @Override public void paintComponent(Graphics drawingObj) {
      drawingObj.drawImage( fractalImage, 0, 0, null );
    } // paintComponent
    
    @Override public void mousePressed(MouseEvent mouse) {
      
      double x = (double) mouse.getX();
      double y = (double) mouse.getY();
      
      switch( mouse.getButton() ) {
        
        //Links
        case MouseEvent.BUTTON1:
          adjustZoom( x, y, zoomFactor*10 );
          break;

       // Rechts
        case MouseEvent.BUTTON3:
          adjustZoom( x, y, zoomFactor/2 );
          break; 
      }
    } // mousePressed
    
    public void addKeyStrokeEvents() {
      
      KeyStroke wKey = KeyStroke.getKeyStroke(KeyEvent.VK_W, 0 );
      KeyStroke aKey = KeyStroke.getKeyStroke(KeyEvent.VK_A, 0 );
      KeyStroke sKey = KeyStroke.getKeyStroke(KeyEvent.VK_S, 0 );
      KeyStroke dKey = KeyStroke.getKeyStroke(KeyEvent.VK_D, 0 );
      
      Action wPressed = new AbstractAction() {
        @Override public void actionPerformed(ActionEvent e) {
          moveUp();
        }
      };
      
      Action aPressed = new AbstractAction() {
        @Override public void actionPerformed(ActionEvent e) {
          moveLeft();
        }
      };
      
      Action sPressed = new AbstractAction() {
        @Override public void actionPerformed(ActionEvent e) {
          moveDown();
        }
      };
      
      Action dPressed = new AbstractAction() {
        @Override public void actionPerformed(ActionEvent e) {
          moveRight();
        }
      };  
      
      this.getInputMap().put( wKey, &quot;w_key&quot; );
      this.getInputMap().put( aKey, &quot;a_key&quot; );
      this.getInputMap().put( sKey, &quot;s_key&quot; );
      this.getInputMap().put( dKey, &quot;d_key&quot; );    
      
      this.getActionMap().put( &quot;w_key&quot;, wPressed );
      this.getActionMap().put( &quot;a_key&quot;, aPressed );
      this.getActionMap().put( &quot;s_key&quot;, sPressed );
      this.getActionMap().put( &quot;d_key&quot;, dPressed );
      
    } // addKeyStrokeEvents
    
    @Override public void mouseReleased(MouseEvent mouse){ }
    @Override public void mouseClicked(MouseEvent mouse) { }
    @Override public void mouseEntered(MouseEvent mouse) { }
    @Override public void mouseExited (MouseEvent mouse) { }
    
  } // Canvas
  
} // FractalExplorer



I updated the code to use BigDecimals, and tried using less heapspace, because i got a few errors because of it, but know the for loop with x which picks a color just stops when the value of x equals 256-258, and if i change the width/height, then the program stops at around half of the width+an eight of the width.
I did more testing, and it stops at computIterations(...);, i don't know why, but i hope this helps. It seems like it doesn't stop but rather slow down after a certain amount of times.
","I finnaly solved it. The code:
import java.util.*;
import java.awt.*;
import java.awt.image.*;
import java.awt.event.*;
import javax.swing.*;
import java.math.BigDecimal;

public class FractalExplorer2 extends JFrame {
  
  static final int WIDTH  = 400;
  static final int HEIGHT = WIDTH;
  
  Canvas canvas;
  BufferedImage fractalImage;
  
  static final int MAX_ITER = 1000;
  static final BigDecimal DEFAULT_TOP_LEFT_X = new BigDecimal(-2.0);
  static final BigDecimal DEFAULT_TOP_LEFT_Y = new BigDecimal(1.4); 
  static final double DEFAULT_ZOOM       = Math.round((double) (WIDTH/3));
  static final int SCALE = 20;
  static final int ROUND = BigDecimal.ROUND_CEILING;
  final int numThreads = 10;
  
  double zoomFactor = DEFAULT_ZOOM;
  BigDecimal topLeftX   = DEFAULT_TOP_LEFT_X;
  BigDecimal topLeftY   = DEFAULT_TOP_LEFT_Y;
   
  
  

// -------------------------------------------------------------------
  public FractalExplorer2() {
    long a = System.nanoTime();
    setup();
    addCanvas();
    canvas.addKeyStrokeEvents();
    updateFractal();
    this.setVisible(true);
    long b = System.nanoTime();
    System.out.println((b-a));
  }
  
// -------------------------------------------------------------------

  public static void main(String[] args) {
    new FractalExplorer2();
  }
  
// -------------------------------------------------------------------

  private void addCanvas() {

    canvas = new Canvas();
    fractalImage = new BufferedImage(WIDTH, HEIGHT, BufferedImage.TYPE_INT_RGB);
    canvas.setVisible(true);
    this.add(canvas, BorderLayout.CENTER);

  } // addCanvas

// -------------------------------------------------------------------
    
    private void setup() {
      
      this.setTitle(&quot;Fractal Explorer&quot;);
      this.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);
      this.setSize(WIDTH, HEIGHT);
      this.setResizable(false);
      this.setLocationRelativeTo(null);
    } // setInitialGUIProperties

// -------------------------------------------------------------------
  private BigDecimal getXPos(double x) {
    return topLeftX.add(new BigDecimal(x/zoomFactor));
  } // getXPos
// -------------------------------------------------------------------
  private BigDecimal getYPos(double y) {
    return topLeftY.subtract(new BigDecimal(y/zoomFactor));
  } // getYPos
// -------------------------------------------------------------------
  
  /**
   * Aktualisiert das Fraktal, indem die Anzahl der Iterationen für jeden Punkt im Fraktal berechnet wird und die Farbe basierend darauf geändert wird.
   **/
  
  public void updateFractal() {
    Thread[] threads = new Thread[numThreads];
    int rowsPerThread = HEIGHT / numThreads;
    
    // Construct each thread
    for (int i=0; i&lt;numThreads; i++) {
      threads[i] = new Thread(new FractalThread(i * rowsPerThread, (i+1) * rowsPerThread));
    }
    
    // Starte jeden thread
    for (int i=0; i&lt;numThreads; i++) {
      threads[i].start();
    }
    
    // Warten bis alle threads fertig sind
    for (int i=0; i&lt;numThreads; i++) {
      try {
        threads[i].join();
      } catch (InterruptedException e) {
        e.printStackTrace();
      }
    }
    
    canvas.repaint();
    
  } // updateFractal
// -------------------------------------------------------------------  
  //Gibt basierend auf der Iterationsanzahl eine trennungsfarbe zurück eines gegebenen Punktes im Fraktal
  private class FractalThread implements Runnable {
    
    int startY;
    int endY;
    
    public FractalThread(int startY, int endY) {
      this.startY = startY;
      this.endY = endY;
    }
    
    public void run() {
      BigDecimal c_r;
      BigDecimal c_i;
      for (int x = 0; x &lt; WIDTH; x++ ) {
        for (int y = startY; y &lt; endY; y++ ) {
          c_r = getXPos(x);
          c_i = getYPos(y);
          int iterCount = computeIterations(c_r, c_i);             
          int pixelColor = makeColor(iterCount);  
          fractalImage.setRGB(x, y, pixelColor);
        } 
      }            
      
    } // run                       
  
  } // FractalThread
  private int makeColor( int iterCount ) {
    
    int color = 0b011011100001100101101000; 
    int mask  = 0b000000000000010101110111; 
    int shiftMag = iterCount / 13;
    
    if (iterCount == MAX_ITER) 
      return Color.BLACK.getRGB();
    
    return color | (mask &lt;&lt; shiftMag);
    
  } // makeColor

// -------------------------------------------------------------------

  private int computeIterations(BigDecimal c_r, BigDecimal c_i) {
    BigDecimal z_r = new BigDecimal(0.0).setScale(SCALE,ROUND);
    BigDecimal z_i = new BigDecimal(0.0).setScale(SCALE,ROUND);
    BigDecimal z_r_tmp;
    BigDecimal dummy2 = new BigDecimal(2.0).setScale(SCALE,ROUND);
    BigDecimal dummy4 = new BigDecimal(4.0).setScale(SCALE,ROUND);
    int iterCount = 0;
    while (z_r.multiply(z_r).add((z_i.multiply(z_i))).compareTo(dummy4) != 1) { 
      z_r_tmp = z_r.setScale(SCALE,ROUND);
      z_r = z_r.multiply(z_r).subtract(z_i.multiply(z_i)).add(c_r).setScale(SCALE,ROUND);
      z_i = dummy2.multiply(z_i).multiply(z_r_tmp).add(c_i).setScale(SCALE,ROUND);
      if (iterCount &gt;= MAX_ITER) return MAX_ITER;
      iterCount++;
    }
    return iterCount;
  } // computeIterations
// -------------------------------------------------------------------
  private void moveUp() {
    double curHeight = HEIGHT / zoomFactor;
    topLeftY = topLeftY.add(new BigDecimal(curHeight / 6));
    updateFractal();
  } // moveUp
// -------------------------------------------------------------------
  private void moveDown() {
    double curHeight = HEIGHT / zoomFactor;
    topLeftY = topLeftY.subtract(new BigDecimal(curHeight / 6));
    updateFractal();
  } // moveDown
// -------------------------------------------------------------------
  private void moveLeft() {
    double curWidth = WIDTH / zoomFactor;
    topLeftX = topLeftX.subtract(new BigDecimal(curWidth / 6));
    updateFractal();
  } // moveLeft
// -------------------------------------------------------------------
  private void moveRight() {
    double curWidth = WIDTH / zoomFactor;
    topLeftX = topLeftX.add(new BigDecimal(curWidth / 6));
    updateFractal();
  } // moveRight
// -------------------------------------------------------------------    

  private void adjustZoom( double newX, double newY, double newZoomFactor ) {
    topLeftX = topLeftX.add(new BigDecimal(newX/zoomFactor)).setScale(SCALE,ROUND);
    topLeftY = topLeftY.subtract(new BigDecimal(newY/zoomFactor)).setScale(SCALE,ROUND);
    zoomFactor = newZoomFactor;
    
    topLeftX = topLeftX.subtract(new BigDecimal(( WIDTH/2) / zoomFactor)).setScale(SCALE,ROUND);
    topLeftY = topLeftY.add(new BigDecimal( (HEIGHT/2) / zoomFactor)).setScale(SCALE,ROUND);
    updateFractal();
  } // adjustZoom

// -------------------------------------------------------------------  
  
  private class Canvas extends JPanel implements MouseListener {
    
    public Canvas() {
      addMouseListener(this);
    } 
    
    @Override public Dimension getPreferredSize() {
      return new Dimension(WIDTH, HEIGHT);
    } // getPreferredSize
    
    @Override public void paintComponent(Graphics drawingObj) {
      drawingObj.drawImage( fractalImage, 0, 0, null );
    } // paintComponent
    
    @Override public void mousePressed(MouseEvent mouse) {
      
      double x = (double) mouse.getX();
      double y = (double) mouse.getY();
      
      switch( mouse.getButton() ) {
        
        //Links
        case MouseEvent.BUTTON1:
          adjustZoom( x, y, zoomFactor*5 );
          break;

       // Rechts
        case MouseEvent.BUTTON3:
          adjustZoom( x, y, zoomFactor/2 );
          break; 
      }
    } // mousePressed
    
    public void addKeyStrokeEvents() {
      
      KeyStroke wKey = KeyStroke.getKeyStroke(KeyEvent.VK_W, 0 );
      KeyStroke aKey = KeyStroke.getKeyStroke(KeyEvent.VK_A, 0 );
      KeyStroke sKey = KeyStroke.getKeyStroke(KeyEvent.VK_S, 0 );
      KeyStroke dKey = KeyStroke.getKeyStroke(KeyEvent.VK_D, 0 );
      
      Action wPressed = new AbstractAction() {
        @Override public void actionPerformed(ActionEvent e) {
          moveUp();
        }
      };
      
      Action aPressed = new AbstractAction() {
        @Override public void actionPerformed(ActionEvent e) {
          moveLeft();
        }
      };
      
      Action sPressed = new AbstractAction() {
        @Override public void actionPerformed(ActionEvent e) {
          moveDown();
        }
      };
      
      Action dPressed = new AbstractAction() {
        @Override public void actionPerformed(ActionEvent e) {
          moveRight();
        }
      };  
      
      this.getInputMap().put( wKey, &quot;w_key&quot; );
      this.getInputMap().put( aKey, &quot;a_key&quot; );
      this.getInputMap().put( sKey, &quot;s_key&quot; );
      this.getInputMap().put( dKey, &quot;d_key&quot; );    
      
      this.getActionMap().put( &quot;w_key&quot;, wPressed );
      this.getActionMap().put( &quot;a_key&quot;, aPressed );
      this.getActionMap().put( &quot;s_key&quot;, sPressed );
      this.getActionMap().put( &quot;d_key&quot;, dPressed );
      
    } // addKeyStrokeEvents
    
    @Override public void mouseReleased(MouseEvent mouse){ }
    @Override public void mouseClicked(MouseEvent mouse) { }
    @Override public void mouseEntered(MouseEvent mouse) { }
    @Override public void mouseExited (MouseEvent mouse) { }
    
  } // Canvas
  
} // FractalExplorer



I just replaced the double variables with BigDecimal and set a Scale, so that the calculation doesn't take too long. I think the code can still be improved, but this is my code right now.
"
"When i query ` the sql bellow the error keep showing
search = session.createQuery(&quot;FROM QLKH_DTO a WHERE a.Fullname LIKE :temp&quot;, QLKH_DTO.class)
                .setParameter(&quot;temp&quot;,temp)
                .list();

However if it just like this then it does work 
  search = session.createQuery(&quot;FROM QLKH_DTO&quot;, QLKH_DTO.class)
                .list();

Here my entity class QLKH_DTO

package DTO;

import jakarta.persistence.Column;
import jakarta.persistence.Entity;
import jakarta.persistence.GeneratedValue;
import jakarta.persistence.GenerationType;
import jakarta.persistence.Id;
import jakarta.persistence.Table;


@Entity
@Table(name = &quot;customers&quot;, catalog = &quot;market&quot;)

public class QLKH_DTO implements java.io.Serializable {
    private Integer CustomerID;
    private String Password;
    private String Fullname; 
    private String Address;
    private String City;
    
    public QLKH_DTO(){}
     public QLKH_DTO(String Password,String Fullname, String Address, String City) {
        this.Password = Password;
        this.Fullname = Fullname;
        this.Address = Address;
        this.City = City;
    }
     
    @Id
    @GeneratedValue(strategy =GenerationType.IDENTITY)
    @Column(name = &quot;CustomerID&quot;)
    public Integer getCustomerID() {
        return CustomerID;
    }

    public void setCustomerID(Integer CustomerID) {
        this.CustomerID = CustomerID;
    }
    @Column(name = &quot;Password&quot;)
    public String getPassword() {
        return Password;
    }

    public void setPassword(String Password) {
        this.Password = Password;
    }
    @Column(name = &quot;Fullname&quot;)
    public String getFullname() {
        return Fullname;
    }

    public void setFullname(String Fullname) {
        this.Fullname = Fullname;
    }
    @Column(name = &quot;Address&quot;, length = 20)
    public String getAddress() {
        return Address;
    }

    public void setAddress(String Address) {
        this.Address = Address;
    }
    @Column(name = &quot;City&quot;, length = 20)
    public String getCity() {
        return City;
    }

    public void setCity(String City) {
        this.City = City;
    }
   
}


The HibernateUtil class
package utils;
 
import org.hibernate.SessionFactory;
import org.hibernate.boot.Metadata;
import org.hibernate.boot.MetadataSources;
import org.hibernate.boot.registry.StandardServiceRegistryBuilder;
import org.hibernate.service.ServiceRegistry;
 
public class HibernateUtil {
    private static final SessionFactory sessionFactory = buildSessionFactory();
    public static SessionFactory buildSessionFactory() {
        try {
            ServiceRegistry serviceRegistry;
            serviceRegistry = new StandardServiceRegistryBuilder()
                    .configure()
                    .build();
            Metadata metadata = new MetadataSources(serviceRegistry)                                     
                        .getMetadataBuilder().build();
            return metadata.getSessionFactoryBuilder().build();
        } catch (Throwable ex) {
        }
        return sessionFactory;
    }
    public static SessionFactory getSessionFactory() {
        return sessionFactory;
    }
}

;

The test class

package GUI.QLKH;


import org.hibernate.Session;
import org.hibernate.SessionFactory;
 
import DTO.QLKH_DTO;
import java.util.List;
import utils.HibernateUtil;
 
public class QLKH {
    private static SessionFactory factory;
    Session session=null;
   // Transaction txn = null;
    public static void main(String[] args) {
        factory = HibernateUtil.getSessionFactory();
        QLKH Customer = new QLKH();
       
        System.out.println(&quot;search customers:&quot;);
        Customer.search();
}
     public void search(){
         String temp=&quot;John&quot;;
         session = factory.openSession();
         session.beginTransaction(); 
        List&lt;QLKH_DTO&gt; search;
        search = session.createQuery(&quot;FROM QLKH_DTO a WHERE a.Fullname LIKE :temp&quot;, QLKH_DTO.class)
                .setParameter(&quot;temp&quot;,temp)
                .list();
        
        session.getTransaction().commit();      
        
        for (QLKH_DTO customer : search) {
            System.out.print(&quot;Password: &quot; + customer.getPassword());
            System.out.print(&quot;Fullname: &quot; + customer.getFullname());
            System.out.println(&quot;Address: &quot; + customer.getAddress());
            System.out.println(&quot;City: &quot; + customer.getCity());
        }
        
     }
}


The sql
CREATE TABLE `Customers` (
  `CustomerID` int(10) NOT NULL auto_increment,
  `Password` varchar(20) NOT NULL,
  `Fullname` varchar(40) NOT NULL,
  `Address` varchar(50) DEFAULT NULL,
  `City` varchar(20) DEFAULT NULL,
    PRIMARY KEY (CustomerID)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

--
-- Dumping data for table `Customers`
--

INSERT INTO `Customers` (`CustomerID`, `Password`, `Fullname`, `Address`, `City`) VALUES
(1, 'Abcd1234', 'John Smith', '30 Broadway', 'London'),
(2, 'Abcd1234', 'Jonny English', '99 River View', 'Reading'),
(3, 'Abcd1234', 'Elizabeth', '23 Buckinghamshire', 'York'),
(4, 'Abcd1234', 'Beatrix', '66 Royal Crescent', 'Bath');

Hibernate.cfg.xml
&lt;?xml version = &quot;1.0&quot; encoding = &quot;utf-8&quot;?&gt;
&lt;!DOCTYPE hibernate-configuration SYSTEM 
&quot;http://www.hibernate.org/dtd/hibernate-configuration-3.0.dtd&quot;&gt;

&lt;hibernate-configuration&gt;
   &lt;session-factory&gt;
   
      &lt;property name = &quot;hibernate.dialect&quot;&gt;
         org.hibernate.dialect.MySQLDialect
      &lt;/property&gt;
   
      &lt;property name = &quot;hibernate.connection.driver_class&quot;&gt;
         com.mysql.jdbc.Driver
      &lt;/property&gt;

      &lt;!-- Assume students is the database name --&gt;
   
      &lt;property name = &quot;hibernate.connection.url&quot;&gt;
          jdbc:mysql://localhost:3306/market
      &lt;/property&gt;
   
      &lt;property name = &quot;hibernate.connection.username&quot;&gt;
         root
      &lt;/property&gt;
   
      &lt;property name = &quot;hibernate.connection.password&quot;&gt;
         
      &lt;/property&gt;
    &lt;mapping class=&quot;DTO.QLKH_DTO&quot; /&gt;
   &lt;/session-factory&gt;
&lt;/hibernate-configuration&gt;

And the error log
cd C:\Users\MyPC\Documents\NetBeansProjects\QLKH; &quot;JAVA_HOME=C:\\Program Files\\Java\\jdk-14.0.1&quot; cmd /c &quot;\&quot;C:\\Program Files\\NetBeans-15\\netbeans\\java\\maven\\bin\\mvn.cmd\&quot; -Dexec.vmArgs= \&quot;-Dexec.args=${exec.vmArgs} -classpath %classpath ${exec.mainClass} ${exec.appArgs}\&quot; \&quot;-Dexec.executable=C:\\Program Files\\Java\\jdk-14.0.1\\bin\\java.exe\&quot; -Dexec.mainClass=GUI.QLKH.QLKH -Dexec.classpathScope=runtime -Dexec.appArgs= \&quot;-Dmaven.ext.class.path=C:\\Program Files\\NetBeans-15\\netbeans\\java\\maven-nblib\\netbeans-eventspy.jar\&quot; -Dfile.encoding=UTF-8 org.codehaus.mojo:exec-maven-plugin:3.0.0:exec&quot;
Running NetBeans Compile On Save execution. Phase execution is skipped and output directories of dependency projects (with Compile on Save turned on) will be used instead of their jar artifacts.
Scanning for projects...

------------------------------&lt; GUI:QLKH &gt;------------------------------
Building QLKH 1.0
--------------------------------[ jar ]---------------------------------

--- exec-maven-plugin:3.0.0:exec (default-cli) @ QLKH ---
Nov 22, 2022 9:55:29 AM org.hibernate.Version logVersion
INFO: HHH000412: Hibernate ORM core version 6.1.5.Final
Nov 22, 2022 9:55:31 AM org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl configure
WARN: HHH10001002: Using built-in connection pool (not intended for production use)
Nov 22, 2022 9:55:31 AM org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl buildCreator
INFO: HHH10001005: Loaded JDBC driver class: com.mysql.jdbc.Driver
Nov 22, 2022 9:55:31 AM org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl buildCreator
INFO: HHH10001012: Connecting with JDBC URL [jdbc:mysql://localhost:3306/market]
Nov 22, 2022 9:55:31 AM org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl buildCreator
INFO: HHH10001001: Connection properties: {password=****, user=root}
Nov 22, 2022 9:55:31 AM org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl buildCreator
INFO: HHH10001003: Autocommit mode: false
Nov 22, 2022 9:55:31 AM org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl$PooledConnections &lt;init&gt;
INFO: HHH10001115: Connection pool size: 20 (min=1)
Nov 22, 2022 9:55:32 AM org.hibernate.engine.jdbc.dialect.internal.DialectFactoryImpl logSelectedDialect
INFO: HHH000400: Using dialect: org.hibernate.dialect.MySQLDialect
Nov 22, 2022 9:55:35 AM org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator initiateService
INFO: HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
search customers:
Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: org.hibernate.query.sqm.InterpretationException: Error interpreting query [FROM QLKH_DTO a WHERE a.Fullname LIKE :temp]; this may indicate a semantic (user query) problem or a bug in the parser [FROM QLKH_DTO a WHERE a.Fullname LIKE :temp]
    at org.hibernate.internal.ExceptionConverterImpl.convert(ExceptionConverterImpl.java:141)
    at org.hibernate.internal.ExceptionConverterImpl.convert(ExceptionConverterImpl.java:175)
    at org.hibernate.internal.ExceptionConverterImpl.convert(ExceptionConverterImpl.java:182)
    at org.hibernate.internal.AbstractSharedSessionContract.createQuery(AbstractSharedSessionContract.java:761)
    at org.hibernate.internal.AbstractSharedSessionContract.createQuery(AbstractSharedSessionContract.java:127)
    at GUI.QLKH.QLKH.search(QLKH.java:28)
    at GUI.QLKH.QLKH.main(QLKH.java:21)
Caused by: org.hibernate.query.sqm.InterpretationException: Error interpreting query [FROM QLKH_DTO a WHERE a.Fullname LIKE :temp]; this may indicate a semantic (user query) problem or a bug in the parser [FROM QLKH_DTO a WHERE a.Fullname LIKE :temp]
    at org.hibernate.query.hql.internal.StandardHqlTranslator.translate(StandardHqlTranslator.java:97)
    at org.hibernate.internal.AbstractSharedSessionContract.lambda$createQuery$2(AbstractSharedSessionContract.java:748)
    at org.hibernate.query.internal.QueryInterpretationCacheStandardImpl.createHqlInterpretation(QueryInterpretationCacheStandardImpl.java:141)
    at org.hibernate.query.internal.QueryInterpretationCacheStandardImpl.resolveHqlInterpretation(QueryInterpretationCacheStandardImpl.java:128)
    at org.hibernate.internal.AbstractSharedSessionContract.createQuery(AbstractSharedSessionContract.java:745)
    ... 3 more
Caused by: java.lang.IllegalArgumentException: org.hibernate.query.SemanticException: Could not resolve attribute 'Fullname' of 'DTO.QLKH_DTO'
    at org.hibernate.query.sqm.SqmPathSource.getSubPathSource(SqmPathSource.java:61)
    at org.hibernate.query.sqm.tree.domain.AbstractSqmPath.get(AbstractSqmPath.java:160)
    at org.hibernate.query.sqm.tree.domain.AbstractSqmFrom.resolvePathPart(AbstractSqmFrom.java:192)
    at org.hibernate.query.hql.internal.DomainPathPart.resolvePathPart(DomainPathPart.java:42)
    at org.hibernate.query.hql.internal.BasicDotIdentifierConsumer.consumeIdentifier(BasicDotIdentifierConsumer.java:91)
    at org.hibernate.query.hql.internal.SemanticQueryBuilder.visitSimplePath(SemanticQueryBuilder.java:4808)
    at org.hibernate.query.hql.internal.SemanticQueryBuilder.visitIndexedPathAccessFragment(SemanticQueryBuilder.java:4755)
    at org.hibernate.query.hql.internal.SemanticQueryBuilder.visitGeneralPathFragment(SemanticQueryBuilder.java:4724)
    at org.hibernate.query.hql.internal.SemanticQueryBuilder.visitGeneralPathExpression(SemanticQueryBuilder.java:1423)
    at org.hibernate.grammars.hql.HqlParser$GeneralPathExpressionContext.accept(HqlParser.java:6963)
    at org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visitChildren(AbstractParseTreeVisitor.java:46)
    at org.hibernate.grammars.hql.HqlParserBaseVisitor.visitBarePrimaryExpression(HqlParserBaseVisitor.java:671)
    at org.hibernate.grammars.hql.HqlParser$BarePrimaryExpressionContext.accept(HqlParser.java:6437)
    at org.hibernate.query.hql.internal.SemanticQueryBuilder.visitLikePredicate(SemanticQueryBuilder.java:2217)
    at org.hibernate.query.hql.internal.SemanticQueryBuilder.visitLikePredicate(SemanticQueryBuilder.java:243)
    at org.hibernate.grammars.hql.HqlParser$LikePredicateContext.accept(HqlParser.java:5442)
    at org.hibernate.query.hql.internal.SemanticQueryBuilder.visitWhereClause(SemanticQueryBuilder.java:1949)
    at org.hibernate.query.hql.internal.SemanticQueryBuilder.visitWhereClause(SemanticQueryBuilder.java:243)
    at org.hibernate.grammars.hql.HqlParser$WhereClauseContext.accept(HqlParser.java:5290)
    at org.hibernate.query.hql.internal.SemanticQueryBuilder.visitQuery(SemanticQueryBuilder.java:857)
    at org.hibernate.query.hql.internal.SemanticQueryBuilder.visitQuerySpecExpression(SemanticQueryBuilder.java:629)
    at org.hibernate.query.hql.internal.SemanticQueryBuilder.visitQuerySpecExpression(SemanticQueryBuilder.java:243)
    at org.hibernate.grammars.hql.HqlParser$QuerySpecExpressionContext.accept(HqlParser.java:1218)
    at org.hibernate.query.hql.internal.SemanticQueryBuilder.visitSimpleQueryGroup(SemanticQueryBuilder.java:623)
    at org.hibernate.query.hql.internal.SemanticQueryBuilder.visitSimpleQueryGroup(SemanticQueryBuilder.java:243)
    at org.hibernate.grammars.hql.HqlParser$SimpleQueryGroupContext.accept(HqlParser.java:1131)
    at org.hibernate.query.hql.internal.SemanticQueryBuilder.visitSelectStatement(SemanticQueryBuilder.java:399)
    at org.hibernate.query.hql.internal.SemanticQueryBuilder.visitStatement(SemanticQueryBuilder.java:358)
    at org.hibernate.query.hql.internal.SemanticQueryBuilder.buildSemanticModel(SemanticQueryBuilder.java:285)
    at org.hibernate.query.hql.internal.StandardHqlTranslator.translate(StandardHqlTranslator.java:81)
    ... 7 more
Caused by: org.hibernate.query.SemanticException: Could not resolve attribute 'Fullname' of 'DTO.QLKH_DTO'
    ... 37 more
Command execution failed.
org.apache.commons.exec.ExecuteException: Process exited with an error: 1 (Exit value: 1)
    at org.apache.commons.exec.DefaultExecutor.executeInternal (DefaultExecutor.java:404)
    at org.apache.commons.exec.DefaultExecutor.execute (DefaultExecutor.java:166)
    at org.codehaus.mojo.exec.ExecMojo.executeCommandLine (ExecMojo.java:982)
    at org.codehaus.mojo.exec.ExecMojo.executeCommandLine (ExecMojo.java:929)
    at org.codehaus.mojo.exec.ExecMojo.execute (ExecMojo.java:457)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)
    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:370)
    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:351)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:215)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:171)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:163)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:294)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)
    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)
    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:960)
    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:293)
    at org.apache.maven.cli.MavenCli.main (MavenCli.java:196)
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke (Method.java:564)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)
------------------------------------------------------------------------
BUILD FAILURE
------------------------------------------------------------------------
Total time:  12.026 s
Finished at: 2022-11-22T09:55:37+07:00
------------------------------------------------------------------------
Failed to execute goal org.codehaus.mojo:exec-maven-plugin:3.0.0:exec (default-cli) on project QLKH: Command execution failed.: Process exited with an error: 1 (Exit value: 1) -&gt; [Help 1]

To see the full stack trace of the errors, re-run Maven with the -e switch.
Re-run Maven using the -X switch to enable full debug logging.

For more information about the errors and possible solutions, please read the following articles:
[Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException


`
I don't know what wrong,i have try another propety like City,Password and it could not reslove the propety too.Eventhough make the propety name and name in database the same to avoid error
","I solved the problem,it because I tagged the @column in the wrong place
@Table(name = &quot;customers&quot;, catalog = &quot;market&quot;)

public class QLKH_DTO implements java.io.Serializable {
    
    @Id
    @GeneratedValue(strategy =GenerationType.IDENTITY) 
    @Column(name = &quot;CustomerID&quot;)
    private Integer CustomerID;
   
    @Column(name = &quot;Password&quot;)
    private String Password;
    
    @Column(name = &quot;Fullname&quot;)
    private String Fullname; 
    
    @Column(name = &quot;Address&quot;)
    private String Address;
    
    @Column(name = &quot;City&quot;)
    private String City;
    
    public QLKH_DTO(){}
     
    public QLKH_DTO(String Password,String Fullname, String Address, String City) {
        this.Password = Password;
        this.Fullname = Fullname;
        this.Address = Address;
        this.City = City;
    }
     
 
    public Integer getCustomerID() {
        return this.CustomerID;
    }

    public void setCustomerID(Integer CustomerID) {
        this.CustomerID = CustomerID;
    }
    
    public String getPassword() {
        return this.Password;
    }

    public void setPassword(String Password) {
        this.Password = Password;
    }
   
    public String getFullname() {
        return this.Fullname;
    }

    public void setFullname(String Fullname) {
        this.Fullname = Fullname;
    }
    
    public String getAddress() {
        return this.Address;
    }

    public void setAddress(String Address) {
        this.Address = Address;
    }
   
    public String getCity() {
        return this.City;
    }

    public void setCity(String City) {
        this.City = City;
    } 
}

"
"I'm creating a small personal project using Java 20, JavaFX 20 and Maven. I'm having trouble creating reusable components and manipulating them through the main scene's controller.
First, I followed the steps listed in the official documentation. After that, I went to SceneBuilder and imported my custom component's FXML file in SceneBuilder (Click on the small engine icon where it says &quot;Library&quot; -&gt; JAR/FXML Manager -&gt; Add Library/FXML from file system) and added it to the scene like you would with any default component. I then gave my custom component a fx:id and added it to my scene's controller class so I can to stuff with it, but I get the following error.
Exception in Application start method
java.lang.reflect.InvocationTargetException
    at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:119)
    at java.base/java.lang.reflect.Method.invoke(Method.java:578)
    at javafx.graphics@20/com.sun.javafx.application.LauncherImpl.launchApplicationWithArgs(LauncherImpl.java:464)
    at javafx.graphics@20/com.sun.javafx.application.LauncherImpl.launchApplication(LauncherImpl.java:363)
    at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)
    at java.base/java.lang.reflect.Method.invoke(Method.java:578)
    at java.base/sun.launcher.LauncherHelper$FXHelper.main(LauncherHelper.java:1081)
Caused by: java.lang.RuntimeException: Exception in Application start method
    at javafx.graphics@20/com.sun.javafx.application.LauncherImpl.launchApplication1(LauncherImpl.java:893)
    at javafx.graphics@20/com.sun.javafx.application.LauncherImpl.lambda$launchApplication$2(LauncherImpl.java:195)
    at java.base/java.lang.Thread.run(Thread.java:1623)
Caused by: javafx.fxml.LoadException: 
/C:/Users/user/Desktop/eclipse-workspace/Project 3/target/classes/app/views/fxml/Menu.fxml:43

    at javafx.fxml@20/javafx.fxml.FXMLLoader.constructLoadException(FXMLLoader.java:2722)
    at javafx.fxml@20/javafx.fxml.FXMLLoader.loadImpl(FXMLLoader.java:2700)
    at javafx.fxml@20/javafx.fxml.FXMLLoader.loadImpl(FXMLLoader.java:2563)
    at javafx.fxml@20/javafx.fxml.FXMLLoader.load(FXMLLoader.java:2531)
    at app/app.Main.loadFXML(Main.java:29)
    at app/app.Main.start(Main.java:17)
    at javafx.graphics@20/com.sun.javafx.application.LauncherImpl.lambda$launchApplication1$9(LauncherImpl.java:839)
    at javafx.graphics@20/com.sun.javafx.application.PlatformImpl.lambda$runAndWait$12(PlatformImpl.java:483)
    at javafx.graphics@20/com.sun.javafx.application.PlatformImpl.lambda$runLater$10(PlatformImpl.java:456)
    at java.base/java.security.AccessController.doPrivileged(AccessController.java:400)
    at javafx.graphics@20/com.sun.javafx.application.PlatformImpl.lambda$runLater$11(PlatformImpl.java:455)
    at javafx.graphics@20/com.sun.glass.ui.InvokeLaterDispatcher$Future.run(InvokeLaterDispatcher.java:95)
    at javafx.graphics@20/com.sun.glass.ui.win.WinApplication._runLoop(Native Method)
    at javafx.graphics@20/com.sun.glass.ui.win.WinApplication.lambda$runLoop$3(WinApplication.java:185)
    ... 1 more
Caused by: java.lang.IllegalArgumentException: Can not set app.components.Custom field app.controllers.Menu.cc to javafx.scene.layout.VBox
    at java.base/jdk.internal.reflect.FieldAccessorImpl.throwSetIllegalArgumentException(FieldAccessorImpl.java:228)
    at java.base/jdk.internal.reflect.FieldAccessorImpl.throwSetIllegalArgumentException(FieldAccessorImpl.java:232)
    at java.base/jdk.internal.reflect.MethodHandleObjectFieldAccessorImpl.set(MethodHandleObjectFieldAccessorImpl.java:115)
    at java.base/java.lang.reflect.Field.set(Field.java:834)
    at javafx.fxml@20/javafx.fxml.FXMLLoader.injectFields(FXMLLoader.java:1175)
    at javafx.fxml@20/javafx.fxml.FXMLLoader$ValueElement.processValue(FXMLLoader.java:870)
    at javafx.fxml@20/javafx.fxml.FXMLLoader$ValueElement.processStartElement(FXMLLoader.java:764)
    at javafx.fxml@20/javafx.fxml.FXMLLoader.processStartElement(FXMLLoader.java:2853)
    at javafx.fxml@20/javafx.fxml.FXMLLoader.loadImpl(FXMLLoader.java:2649)
    ... 13 more
Exception running application app.Main

A weird thing I noticed is that when I add the component to the main scene, it shows up as a VBox and not a Custom even though when I drag it in the &quot;Hierarchy&quot; tab it says the component's name is Custom, not VBox.
Here are the files related
Custom.java
package app.components;

import java.io.IOException;

import app.Main;
import javafx.fxml.FXML;
import javafx.fxml.FXMLLoader;
import javafx.scene.control.Button;
import javafx.scene.control.Label;
import javafx.scene.layout.VBox;

public class Custom extends VBox {
    
    @FXML private Button plusBtn;
    @FXML private Button minusBtn;
    @FXML private Label label;
    
    public Custom() {
        FXMLLoader loader = new FXMLLoader(Main.class.getResource(&quot;components/Custom.fxml&quot;));
        loader.setRoot(this);
        loader.setController(this);
        try {
            loader.load();
        } catch (IOException e) {
            throw new RuntimeException(e);
        }
    }
    
    public void newText(String text) {
        label.setText(text);
    }
}


Custom.fxml
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;?import javafx.scene.control.Button?&gt;
&lt;?import javafx.scene.control.Label?&gt;
&lt;?import javafx.scene.layout.VBox?&gt;

&lt;VBox alignment=&quot;CENTER&quot; maxHeight=&quot;-Infinity&quot; maxWidth=&quot;-Infinity&quot; minHeight=&quot;-Infinity&quot; minWidth=&quot;-Infinity&quot; xmlns=&quot;http://javafx.com/javafx/19&quot; xmlns:fx=&quot;http://javafx.com/fxml/1&quot;&gt;
   &lt;children&gt;
      &lt;Button fx:id=&quot;plusBtn&quot; mnemonicParsing=&quot;false&quot; text=&quot;+&quot; /&gt;
      &lt;Label fx:id=&quot;label&quot; text=&quot;Label&quot; /&gt;
      &lt;Button fx:id=&quot;minusBtn&quot; mnemonicParsing=&quot;false&quot; text=&quot;-&quot; /&gt;
   &lt;/children&gt;
&lt;/VBox&gt;


At the moment, my main scene is just an empty StackPane with my custom component in the center to which I gave &quot;cc&quot; as the fx:id.
Menu.java
package app.controllers;

import app.components.Custom;

public class Menu {

    @FXML
    private Custom cc;

        public void initialize() {
        cc.newText(&quot;Test&quot;);
    }
}

module-info.java
module app {
    requires javafx.controls;
    requires javafx.fxml;
    requires javafx.media;
    requires javafx.graphics;
    requires javafx.base;
    
    opens app to javafx.fxml;
    opens app.controllers to javafx.fxml;
    
    exports app;
}

The problem is that when I add (drag and drop from Custom section to the StackPane) my component, it shows up as VBox and not Custom. Here's a screenshot, it might make what I mean clearer:

I want the component to show up just as Custom, not VBox, because SceneBuilder tells me that it doesn't find an injectable field for 'cc' even though I have the field in my controller class.
","This answer is long, but there is a lot going on so it is what it is.
These steps worked for me.  If followed exactly, it will likely work for you.  If you deviate from the steps in any way, there are no guarantees it will work, nor that I will support you much with it.
I'm just going to note what to do without a lot of explanation.
The basic approach is to:

Create one module with the custom control(s) needed.
Import the module with the custom controls into SceneBuilder.
Use SceneBuilder to design your app using your custom controls.
Create a new project for the app which uses the FXML generated by SceneBuilder.  The new project depends on the custom control module for functionality.
Build and run the application project.

Idea can create and work with multi-module maven projects, so the multiple modules can be in a single project (which might be a reasonable approach for this), but that setup is more complex and not core to solving your problem, so I did not document it here.
Step 1: Create a JavaFX Project for your Custom Component
In Idea -&gt; New Project -&gt; JavaFX Project -&gt; (use Maven) -&gt; group name &quot;com.example&quot; artifact name &quot;custom-component&quot;.
Replace the generated java source files, fxml and pom.xml with those below.
src/main/java/com/example/customcomponent/CustomComponent.java
package com.example.customcomponent;

import javafx.fxml.FXML;
import javafx.fxml.FXMLLoader;
import javafx.scene.control.Button;
import javafx.scene.control.Label;
import javafx.scene.layout.VBox;

import java.io.IOException;

public class CustomComponent extends VBox {
    
    @FXML private Button plusBtn;
    @FXML private Button minusBtn;
    @FXML private Label label;
    
    public CustomComponent() {
        FXMLLoader loader = new FXMLLoader(
                CustomComponent.class.getResource(
                        &quot;custom-component.fxml&quot;
                )
        );
        loader.setRoot(this);
        loader.setController(this);
        try {
            loader.load();
        } catch (IOException e) {
            throw new RuntimeException(e);
        }
    }
    
    public void newText(String text) {
        label.setText(text);
    }
}

src/main/java/module-info.java
module com.example.customcomponent {
    requires javafx.controls;
    requires javafx.fxml;

    opens com.example.customcomponent to javafx.fxml;
    exports com.example.customcomponent;
}

src/main/resources/com/example/customcomponent/custom-component.fxml
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;?import javafx.scene.control.Button?&gt;
&lt;?import javafx.scene.control.Label?&gt;
&lt;fx:root type=&quot;javafx.scene.layout.VBox&quot; alignment=&quot;CENTER&quot; maxHeight=&quot;-Infinity&quot; maxWidth=&quot;-Infinity&quot; minHeight=&quot;-Infinity&quot; minWidth=&quot;-Infinity&quot; xmlns=&quot;http://javafx.com/javafx/19&quot; xmlns:fx=&quot;http://javafx.com/fxml/1&quot;&gt;
    &lt;children&gt;
        &lt;Button fx:id=&quot;plusBtn&quot; mnemonicParsing=&quot;false&quot; text=&quot;+&quot; /&gt;
        &lt;Label fx:id=&quot;label&quot; text=&quot;Label&quot; /&gt;
        &lt;Button fx:id=&quot;minusBtn&quot; mnemonicParsing=&quot;false&quot; text=&quot;-&quot; /&gt;
    &lt;/children&gt;
&lt;/fx:root&gt;

pom.xml
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;com.example&lt;/groupId&gt;
    &lt;artifactId&gt;custom-component&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;custom-component&lt;/name&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
            &lt;artifactId&gt;javafx-controls&lt;/artifactId&gt;
            &lt;version&gt;19&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
            &lt;artifactId&gt;javafx-fxml&lt;/artifactId&gt;
            &lt;version&gt;19&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.11.0&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;source&gt;17&lt;/source&gt;
                    &lt;target&gt;17&lt;/target&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;

Reimport the maven project after changing it.
Go to the maven window and do maven -&gt; install.
Step 2: Import the Custom Component into SceneBuilder
Download and install SceneBuilder 19.
Create a new (Empty) project in SceneBuilder.
Click on the cog icon next to the library search field.

Select &quot;JAR/FXML Manager&quot;.
Choose &quot;Manually add Library from repository&quot;.
Enter Group ID: &quot;com.example&quot;, Artifact ID: &quot;custom-component&quot;, and press TAB.
Select Version: &quot;1.0-SNAPSHOT (local)&quot; from the drop down.

Choose &quot;Add JAR&quot;.
The &quot;Import&quot; dialog will show &quot;CustomComponent&quot; with a tick next to it and a preview image of the component.

Keep the default sizing settings for the new component, and choose &quot;Import Component&quot;.
The newly installed library will be listed in the &quot;Library Manager&quot; dialog.

Close the &quot;Library Manager&quot; dialog.
Step 3: Design your Application UI using the Custom Component
Go to the library search field, type &quot;StackPane&quot;.
Drag the StackPane into the scene you are creating.
Go to the library search field, type &quot;CustomComponent&quot;.
Drag the CustomComponent into the center of StackPane.
Click on the CustomComponent, then click on the &quot;Code&quot; panel and enter an &quot;fx:id&quot; as: &quot;customComponent&quot;.
Click on the controller pane and enter the controller class name &quot;com.example.customcomponentdemo.HelloController&quot;.
Save your FXML file as &quot;hello-view.xml&quot;, it will look like this:

&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;?import com.example.customcomponent.CustomComponent?&gt;
&lt;?import javafx.scene.layout.StackPane?&gt;


&lt;StackPane maxHeight=&quot;-Infinity&quot; maxWidth=&quot;-Infinity&quot; minHeight=&quot;-Infinity&quot; minWidth=&quot;-Infinity&quot; prefHeight=&quot;400.0&quot; prefWidth=&quot;600.0&quot; xmlns=&quot;http://javafx.com/javafx/19&quot; xmlns:fx=&quot;http://javafx.com/fxml/1&quot; fx:controller=&quot;com.example.customcomponentdemo.HelloController&quot;&gt;
   &lt;children&gt;
      &lt;CustomComponent fx:id=&quot;customComponent&quot; /&gt;
   &lt;/children&gt;
&lt;/StackPane&gt;

Step 4: Create a JavaFX Project for your Application using the Custom Component
In Idea -&gt; new JavaFX project (maven) -&gt; group id: &quot;com.example&quot;, artifact id: &quot;custom-component-demo&quot; -&gt; replace the generated files as below:
pom.xml
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;com.example&lt;/groupId&gt;
    &lt;artifactId&gt;custom-component-demo&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;custom-component-demo&lt;/name&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
            &lt;artifactId&gt;javafx-controls&lt;/artifactId&gt;
            &lt;version&gt;20&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
            &lt;artifactId&gt;javafx-fxml&lt;/artifactId&gt;
            &lt;version&gt;20&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.example&lt;/groupId&gt;
            &lt;artifactId&gt;custom-component&lt;/artifactId&gt;
            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.11.0&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;source&gt;20&lt;/source&gt;
                    &lt;target&gt;20&lt;/target&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;

(reimport maven project after editing the pom.xml)
src/main/java/module-info.java
module com.example.customcomponentdemo {
    requires javafx.controls;
    requires javafx.fxml;
    requires com.example.customcomponent;

    opens com.example.customcomponentdemo to javafx.fxml;
    exports com.example.customcomponentdemo;
}

src/main/resources/com/example/customcomponentdemo/hello-view.fxml
Use the file which you saved from SceneBuilder.
src/main/java/com/example/customcomponentdemo/HelloApplication.java
Unchanged from generated code:
package com.example.customcomponentdemo;

import javafx.application.Application;
import javafx.fxml.FXMLLoader;
import javafx.scene.Scene;
import javafx.stage.Stage;

import java.io.IOException;

public class HelloApplication extends Application {
    @Override
    public void start(Stage stage) throws IOException {
        FXMLLoader fxmlLoader = new FXMLLoader(HelloApplication.class.getResource(&quot;hello-view.fxml&quot;));
        Scene scene = new Scene(fxmlLoader.load(), 320, 240);
        stage.setTitle(&quot;Hello!&quot;);
        stage.setScene(scene);
        stage.show();
    }

    public static void main(String[] args) {
        launch();
    }
}

src/main/java/com/example/customcomponentdemo/HelloController.java
package com.example.customcomponentdemo;

import com.example.customcomponent.CustomComponent;
import javafx.fxml.FXML;

public class HelloController {
    @FXML
    private CustomComponent customComponent;

    @FXML
    private void initialize() {
        customComponent.newText(&quot;xyzzy&quot;);
    }
}

Step 5: Run your Application
Double click on the HelloApplication.java file and right click to build the app and run it.
The app will show your custom component with the text of the component initialized by the app controller.

Important note on versions
Look carefully at the Java source and target versions in the custom component project file, it lists 17, not 20.  Also the JavaFX version is 19.  These are set to match the version of SceneBuilder used.  SceneBuilder 19 can only understand JavaFX 19 components (and lower) and runs on JDK 17, so can only understand JAR files compiled to Java 17 (or lower).  If you try using a higher target JDK for building your custom component than SceneBuilder understands, then it won't find your component in the JAR file to import it.
As you can see for the execution project you don't have the same restrictions, so it can freely run on JavaFX 20 and JDK 20 with no issue.
Related

How to create an FXML file for an already created new component in java than add it to scene builder?
Especially see the section titled &quot;Component Pre-requisites&quot; in the linked related answer, which some code requirements for the custom components you create (not all of which are intuitive).

Load custom components in scenebuilder 17
The above is for importing 3rd party libraries into SceneBuilder which is a bit simpler than creating libraries yourself as was demonstrated in this answer.


"
"I'm trying to launch context from android MainActivity class to flutter.
code :
val authResult = ComponentActivity().registerForActivityResult(PianoIdAuthResultContract()) { r -&gt;
                when (r) {
                    null -&gt; { /* user cancelled Authorization process */ }
                    is PianoIdAuthSuccessResult -&gt; {
                        val token = r.token
                        val isNewUserRegistered = r.isNewUser
                        if (token != null) {
                            if (token.emailConfirmationRequired) {
                                // process enabled Double opt-in
                            }
                        }
                        // process successful authorization
                    }
                    is PianoIdAuthFailureResult -&gt; {
                        val e = r.exception
                        // Authorization failed, check e.cause for details
                    }
                }
            }

and then calling the method launch
code :
            try{
                authResult.launch(PianoId.signIn());
            }catch (e : Exception){
                val text = e.message
                val duration = Toast.LENGTH_SHORT

                val toast = Toast.makeText(applicationContext, text, duration)
                toast.show()
            }

and then I call this method from flutter by creating a channel between flutter and android and invoke it :
signInChannel.invokeMethod('testSignIn');

when I press the sign in button it shows me this exception :

Attempt to invoke virtual method 'android.app.ActivityThread$ApplicationThread android.app.ActivityThread.getApplicationThread()' on a null object reference

","I too was searching for the solution. what i did was extend MainActivity with FlutterFragmentActivity and pass this to method channel handler.
    public class MainActivity extends FlutterFragmentActivity{
           ...
           @Override
           public void configureFlutterEngine(@NonNull FlutterEngine flutterEngine) 
           {
              handlePickerMethodChannel(flutterEngine);
           }
        

        private void handlePickerMethodChannel(FlutterEngine flutterEngine) {
            PickerMethodChannelHandler PickerMethodChannelHandler = new PickerMethodChannelHandler(new WeakReference&lt;&gt;(this));
            new MethodChannel(flutterEngine.getDartExecutor().getBinaryMessenger(), PHOTO_PICKER_METHOD_CHANNEL)
                    .setMethodCallHandler(pickerMethodChannelHandler);
        }
    }

class PickerMethodChannelHandler(
    private val activity: WeakReference&lt;Activity&gt;,
) : MethodChannel.MethodCallHandler {

    private val pickMedia = (activity.get() as ComponentActivity).registerForActivityResult(ActivityResultContracts.PickVisualMedia()) { uri -&gt;
        if (uri != null) {
            Log.d(&quot;PhotoPicker&quot;, &quot;Selected URI: $uri&quot;)
        } else {
            Log.d(&quot;PhotoPicker&quot;, &quot;No media selected&quot;)
        }
    }

    override fun onMethodCall(call: MethodCall, result: MethodChannel.Result) {
        when (call.method) {
            &quot;pickMedia&quot; -&gt; pickMedia(call,result)
            else -&gt; result.notImplemented()
        }
    }

    private fun pickMedia(call: MethodCall, result: MethodChannel.Result) {
        val context = activity.get() as ComponentActivity

        Log.i(&quot;PICK_MEDIA&quot;,&quot;PICK ${context != null}&quot;)
        pickMedia.launch(PickVisualMediaRequest(ActivityResultContracts.PickVisualMedia.ImageAndVideo))
    }

}

It worked
"
"Now I want to use Junit 5 + Mockito 4.x version + Mockito-inline 4.x Version instead of Junit 4 + PowerMock 2.0.9
Because the Junit 5 doesn't support PowerMock also Mockito-inline can mock static, look like it doesn't need PowerMock anymore.
But when I use Mockito mock static, I want to use the same effect like Powermock.whenNew(xxx.class).withArgument(1,2,3,4).thanReturn(someThing).
This is part of my code and it can work.
    @Test
    void get_report_page() {
        ReportPageRequest reportPageRequest = prepare_request();
        prepare_reportPage(context, 9999L, pageable);

        when(reportConverter.toReportSpecification(user, reportPageRequest)).thenReturn(reportSpecification);
        when(PageRequest.of(1, 100)).thenReturn(pageRequest);
        when(reportRepository.findAll(reportSpecification, pageRequest)).thenReturn(reportPage);
        when(reportConverter.toReportPageResponse(context)).thenReturn(reportPageResponses);
        pageMockedConstruction = Mockito.mockConstruction(PageImpl.class,
                withSettings().useConstructor(reportPageResponses, pageable, 9999L), (mock, context) -&gt; {
                    when(mock.getTotalElements()).thenReturn(123456L);
                    when(mock.getTotalPages()).thenReturn(1);
                    when(mock.getContent()).thenReturn(reportPageResponses);
                });

        Page&lt;ReportPageResponse&gt; actual = sut.getReportPage(user, reportPageRequest);

        assertThat(actual.getTotalElements()).isEqualTo(123456L);
        assertThat(actual.getTotalPages()).isEqualTo(1);
        assertThat(actual.getContent()).isEqualTo(reportPageResponses);
    }
}

And my question is I just can verify the mock static object behavior, but can't verify the result, this is my try
pageMockedConstruction = Mockito.mockConstruction(PageImpl.class,
                withSettings().useConstructor(reportPageResponses, pageable, 9999L), (mock, context) -&gt; {
                    when(mock.getTotalElements()).thenReturn(123456L);
                    when(mock.getTotalPages()).thenReturn(1);
                    when(mock.getContent()).thenReturn(reportPageResponses);
                });

        // I thought here will be the same mock object
        // when expected and actual will throught the Mockito.mockConstruction, but actually generate the different object
        PageImpl&lt;ReportPageResponse&gt; expected = new PageImpl&lt;&gt;(this.reportPageResponses, pageable, 9999L);
        Page&lt;ReportPageResponse&gt; actual = sut.getReportPage(user, reportPageRequest);

        // Here will be wrong, because actual and expected has different hashCode
        Assertions.assertThat(actual).isEqualTo(expected);

I research so many articles, but I can't find the answer.
Have somebody encountered the same question?
","The main difference between Powermock.whenNew and Mockito.mockConstruction is that Mokito creates a new mock each time when the new object is instantiating when constructor is calling. But Powermock.whenNew can be configured to return one mock always for the construction of several objects.
According to documentation:

Represents a mock of any object construction of the represented type.
Within the scope of the mocked construction, the invocation of any
interceptor will generate a mock which will be prepared as specified
when generating this scope. The mock can also be received via this
instance.

You can use MockedConstruction&lt;T&gt;.constructed() to get all generated mocks in context. They can be used for verification.
Example of test to check behavior:
public class A {
    private final String test;

    public A(String test) {
        this.test = test;
    }

    public String check() {
        return &quot;checked &quot; + this.test;
    }
}

public class TestService {
    public String purchaseProduct(String param) {
        A a = new A(param);
        return a.check();
    }
}

import org.junit.jupiter.api.AfterEach;
import org.junit.jupiter.api.Assertions;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.mockito.MockedConstruction;
import org.mockito.Mockito;

import static org.mockito.Mockito.*;

public class ConstructorMockTest {
    private MockedConstruction&lt;A&gt; mockAController;

    @BeforeEach
    public void beginTest() {
        //create mock controller for all constructors of the given class
        mockAController = Mockito.mockConstruction(A.class,
                (mock, context) -&gt; {
                    //implement initializer for mock. Set return value for object A mock methods
                    //this initializer will be called each time during mock creation 
                    when(mock.check()).thenReturn(&quot; Constructor Mock A &quot;);
                });
    }

    @Test
    public void test() {
        //each instantiation of class A will return new mock, which initialized by initializer from beginTest method
        //new mock will be stored to mockAController.constructed() collection of mocks
        A aObject = new A(&quot;test&quot;);
        //ensure that method check() returns mocked value
        Assertions.assertEquals(aObject.check(), &quot; Constructor Mock A &quot;);
        //get just created mock for class A from controller. It will be first element of mockAController.constructed() collection
        A aMock = mockAController.constructed().get(0);
        //ensure that we get correct mock from mock controller, that it is equal from new created object
        Assertions.assertEquals(aMock, aObject);
        //verify that check method was executed on Mock
        verify(aMock, times(1)).check();

        //create new A object, new mock created and stored to mockAController.constructed()
        A aObject2 = new A(&quot;test&quot;);
        //ensure that method check() returns mocked value
        Assertions.assertEquals(aObject2.check(), &quot; Constructor Mock A &quot;);
        //get just created mock for class A from controller, it will be second object from constructed collection
        A aMock2 = mockAController.constructed().get(1);
        //ensure that we get correct mock from mock controller, that it is equal from just created A object
        Assertions.assertEquals(aObject2, aMock2);
        //verify that check method was executed on Mock
        verify(aMock2, times(1)).check();

        //Example of testing service which creates A object
        TestService service = new TestService();
        String serviceResult = service.purchaseProduct(&quot;test&quot;);
        //ensure that service returned value  from A mock
        Assertions.assertEquals(serviceResult, &quot; Constructor Mock A &quot;);
        //get just created mock for class A from controller, it will be third object from constructed collection
        A aMock3 = mockAController.constructed().get(2);
        //verify that check method was executed on Mock
        verify(aMock3, times(1)).check();
    }

    @AfterEach
    public void endTest() {
        mockAController.close();
    }
}

"
"I've made a dummy project just to show what is trying to be made. I'm going for a transition on color for text without CSS (as I just can't wrap my head around CSS). In the dummy project, the text starts from red

then goes to blue

Found out about FillTransition though that only works with shapes, so this would be a similar function. My other attempt was trying to get the RGB values of both colors then stick them in a do while with a new Color just to test it out though the transition is almost instant as the application starts so it changes the color but without transitioning effect. I'm thinking of making an Timeline for this similar to the do while but I haven't tried that yet.
Before going into this what are some ways to make this effect?
Here's the dummy code:
package application;
    
import javafx.application.Application;
import javafx.scene.Scene;
import javafx.scene.layout.BorderPane;
import javafx.scene.layout.StackPane;
import javafx.scene.paint.Color;
import javafx.scene.text.Font;
import javafx.scene.text.Text;
import javafx.stage.Stage;


public class Main extends Application {
    @Override
    public void start(Stage primaryStage) {
        try {
            BorderPane root = new BorderPane();

            Scene scene = new Scene(root,400,400);

            StackPane stack = new StackPane();
            
            Text text = new Text(&quot;Hello there StackOverflow, how are you? (:&quot;);
            
            Color color1 = Color.RED;
            Color color2 = Color.BLUE;
            
            double r1 = color1.getRed();
            double g1 = color1.getGreen();
            double b1 = color1.getBlue();
            
            double r2 = color2.getRed();
            double g2 = color2.getGreen();
            double b2 = color2.getBlue();
            
            Color colorEffect = new Color(r1, g1, b1, 1.0);
            
            stack.setLayoutX(200);
            stack.setLayoutY(200);
            
            text.setFont(Font.font(16));
            text.setFill(colorEffect);
            
            stack.getChildren().add(text);
            root.getChildren().add(stack);
            
            scene.getStylesheets().add(getClass().getResource(&quot;application.css&quot;).toExternalForm());
            primaryStage.setScene(scene);
            primaryStage.show();
        } catch(Exception e) {
            e.printStackTrace();
        }
    }
    
    public static void main(String[] args) {
        launch(args);
    }
}

","Because Color implements Interpolatable, you can use it directly as a KeyValue in a Timeline, without the need for a custom interpolator.

import javafx.animation.Interpolator;
import javafx.animation.KeyFrame;
import javafx.animation.KeyValue;
import javafx.animation.Timeline;
import javafx.application.Application;
import javafx.geometry.Insets;
import javafx.scene.Scene;
import javafx.scene.layout.StackPane;
import javafx.scene.paint.Color;
import javafx.scene.text.Text;
import javafx.stage.Stage;
import javafx.util.Duration;

/**
 * @see https://stackoverflow.com/a/74249258/230513
 */
public class FillAnimation extends Application {

    @Override
    public void start(Stage stage) {
        stage.setTitle(&quot;FillAnimation&quot;);
        var color1 = Color.RED;
        var color2 = Color.BLUE;
        var text = new Text(&quot;Hello, StackOverflow!&quot;);
        text.setFill(color1);
        text.setStyle(&quot;-fx-font-family: serif; -fx-font-size: 42;&quot;
            + &quot;-fx-font-style: oblique; -fx-font-weight: bold&quot;);
        var timeline = new Timeline();
        var c = new KeyValue(text.fillProperty(), color2, Interpolator.EASE_BOTH);
        var k = new KeyFrame(new Duration(1000), c);
        timeline.getKeyFrames().add(k);
        timeline.setCycleCount(Timeline.INDEFINITE);
        timeline.setAutoReverse(true);
        var root = new StackPane(text);
        root.setPadding(new Insets(16));
        var scene = new Scene(root);
        stage.setScene(scene);
        stage.show();
        timeline.play();
    }

    public static void main(String[] args) {
        launch(args);
    }
}

"
"I have a spring boot project , I want to get those properties as map by prefix , in this exemple the prefix is root :
application.properties :
root.prop = xxxx
root.prop2 = yyyy
root.prop3 = zzzz

I dont want to change my filetype from properties to YAML.
","&quot;Easy-peasy&quot;:
package com.example.demo;

import java.util.HashMap;
import java.util.Map;
import org.springframework.boot.CommandLineRunner;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.boot.context.properties.EnableConfigurationProperties;
import org.springframework.context.annotation.Bean;

@SpringBootApplication 
@EnableConfigurationProperties(MyProps.class) // !
public class DemoApplication {

    public static void main(String[] args) {
        SpringApplication.run(DemoApplication.class, args);
    }

    @Bean // a simple test bean
    CommandLineRunner cmd(/*@Autowired*/ MyProps props) {
        return (args) -&gt; {
            System.err.println(props.getRoot());
        };
    }
}

@ConfigurationProperties // ! ..no prefix, because we are &quot;close to&quot;/root! 
class MyProps {

    // this gets our &quot;root.&quot; prefix
    private final Map&lt;String, ?&gt; root = new HashMap&lt;&gt;();

    // since initialized, getter sufficient 
    public Map&lt;String, ?&gt; getRoot() {
        return root;
    }

}


With, application.properties:
root.prop = xxxx
root.prop2 = yyyy
root.prop3 = zzzz

And pom.xml:
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
    xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;2.7.4&lt;/version&gt;
        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;
    &lt;groupId&gt;com.example&lt;/groupId&gt;
    &lt;artifactId&gt;demo&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;demo&lt;/name&gt;
    &lt;description&gt;Demo project for Spring Boot&lt;/description&gt;
    &lt;properties&gt;
        &lt;java.version&gt;17&lt;/java.version&gt;
    &lt;/properties&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;

... console prints:
...
2022-10-05 18:12:20.101  INFO 9684 --- [           main] com.example.demo.DemoApplication         : Started DemoApplication in 0.834 seconds (JVM running for 1.111)
{prop2=yyyy, prop=xxxx, prop3=zzzz}
------------------------------------------------------------------------
...


To reside the properties in a &quot;non-default location&quot; (foo.properties e.g.), just:
//@ ... ion
@EnableConfigurationProperties(MyProps.class)
@PropertySource(&quot;classpath:/foo.properties&quot;) // ! ...

..but attention: application.properties will have precedence!
Refs:

https://docs.spring.io/spring-boot/docs/current/reference/html/features.html#features.external-config
esp.: https://docs.spring.io/spring-boot/docs/current/reference/html/features.html#features.external-config.typesafe-configuration-properties

"
"I try to use a api with OAuth2. With Postman it works.
But now I try to write this in Java. I don't have spring boot, it is a simple Maven project
The only example I found was this
Example okhttp
But it seems it only works with base authentication.
My question is, is it possible to do a Oauth2 with okhttp? Or is it the wrong library?
","So my solution was to generate post request to get the token
 private static void postCall() throws IOException {

    // Create a new HTTP client        
    OkHttpClient client = new OkHttpClient()
            .newBuilder()           
            .build();

    // Create the request body
    MediaType mediaType = MediaType.parse(&quot;application/x-www-form-urlencoded&quot;);
    RequestBody body = RequestBody.Companion.create(&quot;password=yourPassword&amp;grant_type=password&amp;client_id=yoirClientId&amp;username=yourUserName&quot;,mediaType);
    
    // Build the request object, with method, headers
    Request request = new Request.Builder()
            .url(&quot;https://your-address-to-get-the-token/openid-connect/token&quot;)
            .method(&quot;POST&quot;, body)               
            .build();
            
    // Perform the request, this potentially throws an IOException
    Response response = client.newCall(request).execute();
    // Read the body of the response into a hashmap
    Map&lt;String, Object&gt; responseMap = new ObjectMapper().readValue(response.body().byteStream(), HashMap.class);
    // Read the value of the &quot;access_token&quot; key from the hashmap
    String accessToken = (String) responseMap.get(&quot;access_token&quot;);
    //System.out.println(responseMap.toString());
    // Return the access_token value
    System.out.println(&quot;accessToken &quot; + accessToken);

     request = new Request.Builder()
            .url(&quot;https://your-endpoint-rest-call&quot;)
            .method(&quot;GET&quot;, null)
            .addHeader(&quot;Authorization&quot;, &quot;Bearer &quot; + accessToken)
            .build();

     response = client.newCall(request).execute();
     System.out.println(&quot;Response&quot; + response.body().string());


}

"
"I'm trying to embed an HTML file with google maps in a JavaFX application using WebView. I tested my code with simpler HTML files (just paragraph text and divs) and it embedded correctly but for some reason google maps will not embed.  Instead it displays a white rectangle with a scroll bar.
Here is my HTML for google maps:
&lt;!DOCTYPE html&gt;
&lt;html lang=&quot;en&quot;&gt;
&lt;head&gt;
  &lt;meta charset=&quot;UTF-8&quot;&gt;
  &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
  &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&gt;
  &lt;title&gt; GUI Embedded Map Test &lt;/title&gt;
  &lt;style&gt;
    #map{
        height:600px;
        width:100%;
    }
  &lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
  &lt;div id=&quot;map&quot;&gt;&lt;/div&gt;
  &lt;script&gt;
    function initMap() {
      window.map = new google.maps.Map(document.getElementById(&quot;map&quot;), {
        zoom: 15,
        center:{lat:32.99069195330653, lng:-106.97436738069189},
        mapTypeId: &quot;terrain&quot;,
      });
    }

    window.initMap = initMap;
  &lt;/script&gt;
  &lt;script async defer
      src=&quot;https://maps.googleapis.com/maps/api/js?key=RANDOMKEY&amp;callback=initMap&quot; /c/
    &gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;

(for reference, here's what it looks like when opened in a browser)
Here is my Java code for embedding it:
WebView webView = new WebView();
WebEngine webEngine = webView.getEngine();
String map_html = &quot;&quot;;
    try {
            File myObj = new File(System.getProperty(&quot;user.dir&quot;)+&quot;/src/main/resources/embeddedMap.html&quot;);
            Scanner myReader = new Scanner(myObj);
            while (myReader.hasNextLine()) {
            map_html += myReader.nextLine();
        map_html += &quot;\n&quot;;
            }
            myReader.close();
    } catch (FileNotFoundException e) {
            System.out.println(&quot;An error occurred.&quot;);
        e.printStackTrace();
    }
webEngine.loadContent(map_html);
mapPane.getChildren().add(webView);

Instead of the above HTML, I've tried using iframes tags copied from the 'share' feature on google maps. I've also tried different API keys.
Any help would be greatly appreciated!
","What you are trying to do won’t work with JavaFX 18.0.1 -&gt; 20, see the FAQ for more detail.
Workaround
Try the latest version of JavaFX available and see if it will work.  If not, try a version of JavaFX &lt;=18 (some users have reported that such versions function OK with Google maps).
Ensure that whichever version you choose, the version of all JavaFX components is exactly the same, because mixing JavaFX component versions is not supported.
Official support for Google Maps display in JavaFX WebView
Google doesn’t officially support JavaFX WebView in Google Maps.
Also, there is no explicit guarantee of support the other way round from the JavaFX release notes for Google Maps in WebView.
That doesn't mean that some versions of WebView won't work with Google maps (older versions of JavaFX have been verified to work with Google maps), only that Google don't guarantee that JavaFX WebView will be supported.
FAQ (responses to comments by José Pereda)

And what about GMapsFX?

The last time I tried GMapsFX, it did not work either (that was probably with JavaFX 19 or 20).  Internally GMapsFX relies on WebView for rendering Google Maps.
The lead developer on that project comments in an issue:

No maps are rendered when using JavaFX 18.0.1 with its upgraded WebKit version.



Interesting, it works for me for 18 and lower, but indeed fails since 18.0.1. I wonder what change caused this, and if it is a WebKit issue, it is not that Google doesn't support WebView, but the other way around?

This is a list of clients that Google support for their JavaScript maps client API. It does not include JavaFX, if it did they would be obligated to make it work with supported versions of JavaFX, but they don’t.


Honestly I don't think Google considers the JavaFX WebView a &quot;true&quot; web browser that needs to be supported. As you know, it is an embedded browser, based on the Apple Safari Port and the WebKit GTK releases, with some modifications to make it work with Java/JavaFX. It is possible that one of those modifications broke the support for Google Maps, because it stopped working only after WebKit was updated from 612.1 to 613.1 (that is between JavaFX 18 and 18.0.1).

When I originally wrote this answer I was unaware of the issue tracker report which mentions breakage of Google Maps between the JavaFX 18 and 18.0.1 release.  I had instead assumed that Google had released a newer version of Google Maps that was incompatible with JavaFX.  Perhaps this is not the case.
I do see that Google currently explicitly support (at least for the moment), embedded browsers on other platforms like the deprecated iOS UIWebView that are not &quot;true&quot; browsers, so it would probably be possible for them to explicitly support the JavaFX WebView if they wanted to (my opinion).  It would probably be harder for the JavaFX team to explicitly support Google Maps because it can't be known if Google will make some change to their Maps API implementation that makes it incompatible with WebView.

Example of Google Map Display in JavaFX
This example worked as of April 2024, using JavaFX 18 (exact version, not 18.0.1 or another version), JDK 21, OS X (x64) Sonoma 14.4.1.

pom.xml
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;org.example&lt;/groupId&gt;
    &lt;artifactId&gt;maps&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;maps&lt;/name&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
            &lt;artifactId&gt;javafx-controls&lt;/artifactId&gt;
            &lt;version&gt;18&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
            &lt;artifactId&gt;javafx-web&lt;/artifactId&gt;
            &lt;version&gt;18&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.11.0&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;source&gt;21&lt;/source&gt;
                    &lt;target&gt;21&lt;/target&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;

src/main/java/module-info.java
module org.example.maps {
    requires javafx.controls;
    requires javafx.web;

    exports org.example.maps;
}

src/main/java/org/example/maps/MapApplication.java
package org.example.maps;

import javafx.application.Application;
import javafx.scene.Scene;
import javafx.scene.web.WebView;
import javafx.stage.Stage;

public class MapApplication extends Application {
    @Override
    public void start(Stage stage) {
        WebView webView = new WebView();
        webView.getEngine().load(&quot;https://maps.google.com&quot;);

        stage.setScene(new Scene(webView));
        stage.show();
    }

    public static void main(String[] args) {
        launch();
    }
}

"
"I want to print all field access list for each method of a class in Java with JavaParser Library (3.25.8).

not variables access into method, only access list for fields of class
all types of access (assigns, ++, --,...)
It is better to print separately (read access and write access)
only fields access list for fields of desired class (not other classes fields access)

I try this:
import com.github.javaparser.StaticJavaParser;
import com.github.javaparser.ast.CompilationUnit;
import com.github.javaparser.ast.body.ClassOrInterfaceDeclaration;
import com.github.javaparser.ast.body.MethodDeclaration;
import com.github.javaparser.ast.expr.FieldAccessExpr;
import java.io.File;
import java.io.IOException;

public class FieldAccessList {

    public static void main(String[] args) throws IOException {

        File sourceFile = new File(&quot;Example.java&quot;);
        CompilationUnit cu = StaticJavaParser.parse(sourceFile);

        cu.findAll(ClassOrInterfaceDeclaration.class).forEach(classDeclaration -&gt; {
            System.out.println(&quot;Class: &quot; + classDeclaration.getNameAsString());

            classDeclaration.findAll(MethodDeclaration.class).forEach(methodDeclaration -&gt; {
                System.out.println(&quot;  Method: &quot; + methodDeclaration.getNameAsString());

                methodDeclaration.findAll(FieldAccessExpr.class).forEach(fieldAccessExpr -&gt; {
                    System.out.println(&quot;    Field Access: &quot; + fieldAccessExpr.getNameAsString());
                });
            });
        });
    }
}

and my pom.xml is:
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
         
    
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;groupId&gt;Sahand&lt;/groupId&gt;
    &lt;artifactId&gt;Importance&lt;/artifactId&gt;
    &lt;version&gt;2.0&lt;/version&gt;
    &lt;name&gt;Sahand Project Extension&lt;/name&gt;
    
    &lt;properties&gt;
        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
    &lt;/properties&gt;
    
    
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.github.javaparser&lt;/groupId&gt;
            &lt;artifactId&gt;javaparser-core&lt;/artifactId&gt;
            &lt;version&gt;3.25.8&lt;/version&gt;
        &lt;/dependency&gt;

        
        &lt;dependency&gt;
            &lt;groupId&gt;com.github.javaparser&lt;/groupId&gt;
            &lt;artifactId&gt;javaparser-symbol-solver-core&lt;/artifactId&gt;
            &lt;version&gt;3.25.8&lt;/version&gt;
        &lt;/dependency&gt;
    
    &lt;/dependencies&gt;
    
    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.8.1&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;source&gt;1.8&lt;/source&gt;
                    &lt;target&gt;1.8&lt;/target&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
    
&lt;/project&gt;

for Example.java:
public class Example {

    private int field1;
    private String field2;

    public void method1() {
        field1 = 10;
        System.out.println(field2);
    }

    public void method2() {
        field2 = &quot;Hello&quot;;
    }
}

The output I expected should be:
Class: Example
  Method: method1
    Field Access: field1
    Field Access: field2
  Method: method2
    Field Access: field2

But the output is:
Class: Example
  Method: method1
    Field Access: out
  Method: method2

","The javadoc of FieldAccessExpr says it is meant for detecting accesses of the type person.name, which is probably why it detected System.out. Through some trial and error, I figured out that the expression field1 = 10; is of type AssignExpr and that System.out.println(field2); is of type MethodCallExpr. A ++ or -- expression is of type UnaryExpr. Also to filter out expressions that only use local variables, I collected all of the class level fields at the start and included only expressions that involved any of those fields. Combining all of that together, for the Example class, I'm able to get the expected output using below code:
import com.github.javaparser.StaticJavaParser;
import com.github.javaparser.ast.CompilationUnit;
import com.github.javaparser.ast.body.ClassOrInterfaceDeclaration;
import com.github.javaparser.ast.body.FieldDeclaration;
import com.github.javaparser.ast.body.MethodDeclaration;
import com.github.javaparser.ast.expr.AssignExpr;
import com.github.javaparser.ast.expr.Expression;
import com.github.javaparser.ast.expr.MethodCallExpr;
import com.github.javaparser.ast.expr.UnaryExpr;

import java.io.File;
import java.io.FileNotFoundException;
import java.util.ArrayList;
import java.util.List;
import java.util.stream.Collectors;

public class TestUtil {

    public static void listFieldAccess() throws FileNotFoundException {
        File sourceFile = new File(&quot;Example.java&quot;);
        CompilationUnit cu = StaticJavaParser.parse(sourceFile);

        cu.findAll(ClassOrInterfaceDeclaration.class).forEach(classDeclaration -&gt; {
            System.out.println(&quot;Class: &quot; + classDeclaration.getNameAsString());
            List&lt;String&gt; fields = new ArrayList&lt;&gt;();

            // Find all field names
            classDeclaration.findAll(FieldDeclaration.class).forEach(fieldDeclaration -&gt; {
                fieldDeclaration.getVariables().forEach(variable -&gt; {
                    fields.add(variable.getNameAsString());
                });
            });

            classDeclaration.findAll(MethodDeclaration.class).forEach(methodDeclaration -&gt; {
                System.out.println(&quot;  Method: &quot; + methodDeclaration.getNameAsString());

                methodDeclaration.findAll(Expression.class).forEach(expression -&gt; {
                    // Process only specific types of expressions
                    if (expression instanceof MethodCallExpr || expression instanceof AssignExpr ||
                            expression instanceof UnaryExpr) {
                        // Check if any of the expression fields match the class level fields
                        List&lt;String&gt; matchedFields = fields.stream().filter(field -&gt; {
                            return expression.getChildNodes().stream().anyMatch((node) -&gt; node.toString().contains(field));
                        }).collect(Collectors.toList());
                        System.out.println(&quot;Field access: &quot; + matchedFields);
                    }
                });
            });
        });
    }
}

Couldn't figure out exactly how to differentiate between a read and write access as something like a method call could read or write internally. Also you might still need to include more expression types and add some filtering, to include only fields within the desired class.
"
"I have a custom annotation with a single variable.
I use it to annotate attributes in a class and what i need is that the annotation default value for the variable, be the type of the attribute declared. Here the example:
Annotation:
@Target(ElementType.FIELD)
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface Annotation{
    Class&lt;?&gt; className() default ???????; // &lt;- here i need to set something that tells my annotation to take the class of the attribute annotated
}

Class using Annotation:
public class Main {

    @Annotation
    private AnotherClass annotatedAttribute;

    //other code
}

And so what i need is that when i get the annotatedAttribute field and i get its annotation and its value of the className() variable, the default value should be the equivalent to AnotherClass.class unless i state otherwise in the declaration of the @Annotation
E.g:
@Annotation(classname= YetAnotherClass.class)

Is there a way to do this?
I saw some posts talking about an annotation processor, but in my case i don't want to generate new classes files since my class already exist and i'm fetching the field and the annotation through reflection (so i'm at runtime level)
","There is no way to specify a custom logic in an annotation, so you have to leave it to the code processing the annotation at runtime, however, you can’t use null as a marker value either.
The only way to tell your annotation processing tool that custom processing is required, is by choosing a dedicated marker type as default value. This might be a type that would otherwise never occur as a regular annotation value, e.g. void.class, or you create a class solely for serving as the marker.
To show a similar real life example, JUnit’s @Test annotation has an expected element denoting an expected type to be thrown. The default, supposed to express that no exception is expected, can’t be null nor a type outside the Throwable hierarchy as the value must conform to the declared type Class&lt;? extends Throwable&gt;. Therefore, the default value is a dedicated type Test.None that is never thrown and treated specially by the framework when processing the annotation.
For your case, you have to decide for a suitable marker type or create a dedicated type and adapt the processing code to check for the type. E.g.
public final class UseFieldType {
    private UseFieldType() {}
}

@Target(ElementType.FIELD)
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface YourAnnotation {
    Class&lt;?&gt; className() default UseFieldType.class;
}
class YourCodeUsedAtRuntime {
    public static Optional&lt;Class&lt;?&gt;&gt; getAnnotationValue(Field f) {
       YourAnnotation a = f.getAnnotation(YourAnnotation.class);
       if(a == null) return Optional.empty();

       Class&lt;?&gt; type = a.className();
       if(type == UseFieldType.class) type = f.getType();
       return Optional.of(type);
    }
}
class Example {
    @YourAnnotation String string;
    @YourAnnotation(className = Pattern.class) String regEx;
    String none;

    public static void main(String[] args) {
        for(Field f: Example.class.getDeclaredFields()) {
            System.out.println(f.getName() + &quot;: &quot;
                + YourCodeUsedAtRuntime.getAnnotationValue(f)
                      .map(Class::getName).orElse(&quot;No annotation&quot;));
        }
    }
}

string: java.lang.String
regEx: java.util.regex.Pattern
none: No annotation

"
"I want to display the current version of my app on screen (using a Label).

How I can do it?
Where do I need to set version variable if I use Gradle?
Or how can I get the version from the build.gradle file?
Or how can I get the version from an external file like manifest to use it in build.gradle?

Or, how do I display the current version of my app on screen, where I need to set this current version and how to use this version in Gradle?
","One way I've done this in the past is to leverage the build tool to fill in placeholders in a properties file resource, then load that properties file at run-time. This is the same as described by jewelsea in the question comments.

In Maven there are various ways to write the project version to a properties file, there should be a similar mechanism with Gradle.
Once the version is in a properties file, you can load the properties from a resource stream (or inject the property values if you are using Spring) and set that as text in a JavaFX Label to display in your UI.

The primary benefit of this approach is that it keeps your build tool as the source of truth. Change the value and it will automatically propagate to the built application.

Example
Here is an example tested with Java 22.0.2, JavaFX 22.0.2, and Gradle 8.9. This example uses the project's version as the application's version, but you can of course use a different value if the two versions are not the same.
Resources
app.properties
app.version = ${version}

Java Sources
Main.java
package sample;

import javafx.application.Application;
import javafx.scene.Scene;
import javafx.scene.control.Label;
import javafx.scene.layout.StackPane;
import javafx.stage.Stage;
import java.util.Properties;

public class Main extends Application {

  private String appVersion;

  @Override
  public void init() throws Exception {
    var props = new Properties();
    try (var in = Main.class.getResourceAsStream(&quot;/app.properties&quot;)) {
      props.load(in);
    }
    appVersion = props.getProperty(&quot;app.version&quot;, &quot;&lt;NO-VERSION&gt;&quot;);
  }

  @Override
  public void start(Stage primaryStage) throws Exception {
    var label = new Label(&quot;Hello, from Example v&quot; + appVersion + &quot;!&quot;);
    primaryStage.setScene(new Scene(new StackPane(label), 500, 300));
    primaryStage.setTitle(&quot;Example - &quot; + appVersion);
    primaryStage.show();
  }
}

Gradle Files (Kotlin DSL)
settings.gradle.kts
rootProject.name = &quot;demo&quot;

build.gradle.kts
plugins {
  id(&quot;org.openjfx.javafxplugin&quot;) version &quot;0.1.0&quot;
  application
}

group = &quot;sample&quot;
version = &quot;0.1.0&quot;

repositories { 
  mavenCentral()
}

javafx {
  modules(&quot;javafx.controls&quot;)
  version = &quot;22.0.2&quot;
}

application {
  mainClass = &quot;sample.Main&quot;
}

tasks {
  // This is the part responsible for filling in the placeholders in the
  // properties file.
  processResources {
    inputs.property(&quot;version&quot;, project.version) // Configure task to rerun if value changes
    filesMatching(&quot;app.properties&quot;) {
      expand(&quot;version&quot; to project.version)
    }
  }
}

Project Directory
&lt;project-dir&gt;
|   build.gradle.kts
|   gradlew
|   gradlew.bat
|   settings.gradle.kts
|   
+---gradle
|   |   
|   \---wrapper
|           gradle-wrapper.jar
|           gradle-wrapper.properties
|           
\---src
    \---main
        +---java
        |   \---sample
        |           Main.java
        |           
        \---resources
                app.properties

Output
Here is the result from running ./gradlew run.

"
"I am currently working on the localization of my (second) Spring Boot project. However, I have come to a halt after several hours of struggling: I am unable to get a user-editable Session localization.
The problem appears to occur as soon as the user sends a GET request with the lang parameter. (travel down below to see the results I am getting)
Details
Spring Boot version:(3.0.0-M3)
Expected localized content
i18n/messages.properties is empty
i18n/messages_en_US.properties:
morning=good morning
afternoon=bye

i18n/messages_fr_FR.properties:
morning=salut
afternoon=a+

i18n/messages_ja_JP.properties:
morning=ohayou
afternoon=jane

Configuration
application.properties (section related to this issue):
spring.messages.always-use-message-format=true
spring.messages.basename=i18n.messages
spring.messages.fallback-to-system-locale=false
spring.messages.use-code-as-default-message=false

LocalizationConfiguration file:
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.web.servlet.LocaleResolver;
import org.springframework.web.servlet.config.annotation.InterceptorRegistry;
import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;
import org.springframework.web.servlet.i18n.LocaleChangeInterceptor;
import org.springframework.web.servlet.i18n.SessionLocaleResolver;

@Configuration
public class LocalizationConfiguration implements WebMvcConfigurer {

    @Bean
    public LocaleResolver localeResolver() {
        SessionLocaleResolver localeResolver = new SessionLocaleResolver();
        // localeResolver.setDefaultLocale(Locale.US);
        return localeResolver;
    }

    @Bean
    public LocaleChangeInterceptor localeChangeInterceptor() {
        LocaleChangeInterceptor localeChangeInterceptor = new LocaleChangeInterceptor();
        localeChangeInterceptor.setParamName(&quot;lang&quot;);
        return localeChangeInterceptor;
    }

    @Override
    public void addInterceptors(InterceptorRegistry interceptorRegistry) {
        interceptorRegistry.addInterceptor(localeChangeInterceptor());
    }

}

Display
Page Controller:
@GetMapping
@RequestMapping(value = &quot;/international&quot;)
public String getInternationalView(Model model) {
    return &quot;international&quot;;
}

Template loaded (international.html):
&lt;!DOCTYPE html&gt;
&lt;html xmlns:th=&quot;https://www.thymeleaf.org&quot; th:with=&quot;lang=${#locale.language}&quot; th:lang=&quot;${lang}&quot;&gt;
&lt;head&gt;
&lt;script src=&quot;https://kit.fontawesome.com/2f4c03ee9b.js&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt;

&lt;script th:src=&quot;@{/webjars/jquery/3.0.0/jquery.min.js}&quot;&gt;&lt;/script&gt;
&lt;script th:src=&quot;@{/webjars/popper.js/2.9.3/umd/popper.min.js}&quot;&gt;&lt;/script&gt;
&lt;script th:src=&quot;@{/webjars/bootstrap/5.1.3/js/bootstrap.min.js}&quot;&gt;&lt;/script&gt;
&lt;link th:rel=&quot;stylesheet&quot; th:href=&quot;@{/webjars/bootstrap/5.1.3/css/bootstrap.min.css} &quot;/&gt;

&lt;meta charset=&quot;UTF-8&quot;/&gt;
&lt;title&gt;Localization tests&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;p th:text=&quot;${#locale}&quot;&gt;&lt;/p&gt;
    &lt;p th:text=&quot;#{morning}&quot;&gt;&lt;/p&gt;
    &lt;p th:text=&quot;#{afternoon}&quot;&gt;&lt;/p&gt;
    
    &lt;div class=&quot;dropdown&quot;&gt;
        &lt;button class=&quot;btn btn-primary dropdown-toggle&quot; type=&quot;button&quot; id=&quot;dropdownMenuButton1&quot; data-bs-toggle=&quot;dropdown&quot; aria-expanded=&quot;false&quot;&gt;
            &lt;i class=&quot;fa-solid fa-language fa-4x&quot;&gt;&lt;/i&gt;
        &lt;/button&gt;
        &lt;ul class=&quot;dropdown-menu&quot; aria-labelledby=&quot;dropdownMenuButton1&quot;&gt;
            &lt;li&gt;&lt;a class=&quot;dropdown-item&quot; th:href=&quot;@{''(lang=en)}&quot;&gt;English&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a class=&quot;dropdown-item&quot; th:href=&quot;@{''(lang=fr)}&quot;&gt;FranÃ§ais&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a class=&quot;dropdown-item&quot; th:href=&quot;@{''(lang=jp)}&quot;&gt;æ—¥æœ¬èªž&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
    &lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;

What is being displayed
Found result
As you can see in the above gif, the first display of the page shows the messages in the browser's language. However, as soon as an other language is selected the page breaks apart, with the exception of the #locale parameter.
","Try it.
import org.springframework.context.MessageSource;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.support.ReloadableResourceBundleMessageSource;
import org.springframework.web.servlet.LocaleResolver;
import org.springframework.web.servlet.config.annotation.InterceptorRegistry;
import org.springframework.web.servlet.config.annotation.ViewControllerRegistry;
import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;
import org.springframework.web.servlet.i18n.CookieLocaleResolver;
import org.springframework.web.servlet.i18n.LocaleChangeInterceptor;
import org.springframework.web.servlet.i18n.SessionLocaleResolver;

import java.util.Locale;

@Configuration
public class ApplicationConfig implements WebMvcConfigurer {

    @Bean
    public MessageSource messageSource() {
        ReloadableResourceBundleMessageSource messageSource = new ReloadableResourceBundleMessageSource();
        messageSource.setBasenames(&quot;classpath:/i18n/messages&quot;);
        messageSource.setDefaultEncoding(&quot;UTF-8&quot;);
        return messageSource;
    }

    @Bean
    public LocaleChangeInterceptor localeChangeInterceptor() {
        LocaleChangeInterceptor localeChangeInterceptor = new LocaleChangeInterceptor();
        localeChangeInterceptor.setParamName(&quot;lang&quot;);
        return localeChangeInterceptor;
    }

    @Bean(name = &quot;localeResolver&quot;)
    public SessionLocaleResolver sessionLocaleResolver() {
        SessionLocaleResolver localeResolver = new SessionLocaleResolver();
        localeResolver.setDefaultLocale(new Locale(&quot;en&quot;));
        return localeResolver;
    }

    @Override
    public void addInterceptors(InterceptorRegistry registry) {
        registry.addInterceptor(localeChangeInterceptor());
    } 
}

"
"Let's consider the following code:
switch ( &lt;em&gt;switchTreeExpression&lt;/em&gt; ) {
    &lt;em&gt;cases&lt;/em&gt;
}

I want to find out, what type for switchTreeExpression is .
I have the following code draft:
...
MethodTree methodTree = trees.getTree(method);
BlockTree blockTree = methodTree.getBody();

for (StatementTree statementTree : blockTree.getStatements()) {
    if (statementTree.getKind() == Tree.Kind.SWITCH) {
        SwitchTree switchTree = (SwitchTree) statementTree;
        ExpressionTree switchTreeExpression = switchTree.getExpression();
        // I need to get the type of *switchTreeExpression* here
    }
}

It is interesting, that I can get the type of switchTreeExpression from .class file. However it seems that there is no way to get byte code of the current class in this phase of annotation processing (if I am wrong, I would be happy just get byte code and analyze it with ObjectWeb ASM library).
","Possible solutions
Annotation processor
Let's consider an annotation processor for the type annotations (@Target(ElementType.TYPE)).
Limitation: Processor.process() method: No method bodies
Processing Code:

Annotation processing occurs at a specific point in the timeline of a compilation, after all source files and classes specified on the command line have been read, and analyzed for the types and members they contain, but before the contents of any method bodies have been analyzed.

Overcoming limitation: Using com.sun.source.util.TaskListener
The idea is to handle the type element analysis completion events.

Processor.init() method: Register a task listener and handle the type element analysis completion events using the captured annotated type elements.
Processor.process() method: Capture the annotated type elements.

Some related references:

Overview. Processing Code.

Inspiration. Checker Framework.

The Checker Framework.
checker-framework/AbstractTypeProcessor.java at checker-framework-3.22.2 · typetools/checker-framework.


Related question. java - How to access TypeUse annotation via AnnotationProcessor - Stack Overflow.

Related question. java - JAXB bind subclasses dynamically - Stack Overflow.

Related question. Make the java compiler warn when an annotated method is used (like @deprecated) - Stack Overflow.


Note on implementation approaches
Some third-party dependencies (libraries and frameworks) may be used to implement an annotation processor.
For example, the already mentioned Checker Framework.
Some related references:

The homepage: The Checker Framework.
The Checker Framework Manual: Custom pluggable types for Java: Chapter 35 How to create a new checker.

Please, note that the Checker Framework processors use @SupportedAnnotationTypes(&quot;*&quot;).
Draft implementation
Let's consider a draft implementation, which does not use third-party dependencies mentioned in the «Note on implementation approaches» section.
Annotation processor project
Maven project
&lt;properties&gt;
    &lt;auto-service.version&gt;1.0.1&lt;/auto-service.version&gt;
&lt;/properties&gt;

&lt;plugin&gt;
    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
    &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
    &lt;configuration&gt;
        &lt;annotationProcessorPaths&gt;
            &lt;path&gt;
                &lt;groupId&gt;com.google.auto.service&lt;/groupId&gt;
                &lt;artifactId&gt;auto-service&lt;/artifactId&gt;
                &lt;version&gt;${auto-service.version}&lt;/version&gt;
            &lt;/path&gt;
        &lt;/annotationProcessorPaths&gt;
    &lt;/configuration&gt;
&lt;/plugin&gt;

&lt;dependency&gt;
    &lt;groupId&gt;com.google.auto.service&lt;/groupId&gt;
    &lt;artifactId&gt;auto-service-annotations&lt;/artifactId&gt;
    &lt;version&gt;${auto-service.version}&lt;/version&gt;
&lt;/dependency&gt;

AbstractTypeProcessor class: Base class
Let's introduce the base class that has the following abstract method:
public abstract void processType(Trees trees, TypeElement typeElement, TreePath treePath);

import com.sun.source.util.JavacTask;
import com.sun.source.util.TaskEvent;
import com.sun.source.util.TaskListener;
import com.sun.source.util.TreePath;
import com.sun.source.util.Trees;
import java.util.HashSet;
import java.util.List;
import java.util.Set;
import javax.annotation.processing.AbstractProcessor;
import javax.annotation.processing.ProcessingEnvironment;
import javax.annotation.processing.RoundEnvironment;
import javax.lang.model.element.Element;
import javax.lang.model.element.Name;
import javax.lang.model.element.TypeElement;
import javax.lang.model.util.ElementFilter;

// NOTE: It is designed to work only with `@Target(ElementType.TYPE)` annotations!
public abstract class AbstractTypeProcessor extends AbstractProcessor {
    private final AnalyzeTaskListener analyzeTaskListener = new AnalyzeTaskListener(this);
    protected final Set&lt;Name&gt; remainingTypeElementNames = new HashSet&lt;&gt;();
    private Trees trees;

    protected AbstractTypeProcessor() {
    }

    @Override
    public synchronized void init(final ProcessingEnvironment processingEnv) {
        super.init(processingEnv);
        trees = Trees.instance(processingEnv);
        JavacTask.instance(processingEnv).addTaskListener(analyzeTaskListener);
    }

    @Override
    public boolean process(final Set&lt;? extends TypeElement&gt; annotations, final RoundEnvironment roundEnv) {
        for (final TypeElement annotation : annotations) {
            final Set&lt;? extends Element&gt; annotatedElements = roundEnv.getElementsAnnotatedWith(annotation);
            final Set&lt;TypeElement&gt; typeElements = ElementFilter.typesIn(annotatedElements);
            final List&lt;Name&gt; typeElementNames = typeElements.stream()
                .map(TypeElement::getQualifiedName)
                .toList();
            remainingTypeElementNames.addAll(typeElementNames);
        }
        System.out.println(
            String.format(&quot;Remaining type element names: %s&quot;, remainingTypeElementNames)
        );
        return false;
    }

    public abstract void processType(Trees trees, TypeElement typeElement, TreePath treePath);

    private void handleAnalyzedType(final TypeElement typeElement) {
        System.out.println(
            String.format(&quot;Handling analyzed type element: %s&quot;, typeElement)
        );
        if (!remainingTypeElementNames.remove(typeElement.getQualifiedName())) {
            return;
        }

        final TreePath treePath = trees.getPath(typeElement);
        processType(trees, typeElement, treePath);
    }

    private static final class AnalyzeTaskListener implements TaskListener {
        private final AbstractTypeProcessor processor;

        public AnalyzeTaskListener(final AbstractTypeProcessor processor) {
            this.processor = processor;
        }

        @Override
        public void finished(final TaskEvent e) {
            if (e.getKind() != TaskEvent.Kind.ANALYZE) {
                return;
            }

            processor.handleAnalyzedType(e.getTypeElement());
        }
    }
}

CheckMethodBodies class: Annotation class
@Target(ElementType.TYPE)
@Retention(RetentionPolicy.SOURCE)
public @interface CheckMethodBodies {
}

CheckMethodBodiesProcessor class: Annotation processor
import com.google.auto.service.AutoService;
import com.sun.source.tree.BlockTree;
import com.sun.source.tree.CompilationUnitTree;
import com.sun.source.tree.ExpressionTree;
import com.sun.source.tree.MethodTree;
import com.sun.source.tree.StatementTree;
import com.sun.source.tree.SwitchTree;
import com.sun.source.tree.Tree;
import com.sun.source.util.TreePath;
import com.sun.source.util.TreePathScanner;
import com.sun.source.util.Trees;
import javax.annotation.processing.Processor;
import javax.annotation.processing.SupportedAnnotationTypes;
import javax.annotation.processing.SupportedSourceVersion;
import javax.lang.model.SourceVersion;
import javax.lang.model.element.TypeElement;
import javax.lang.model.type.TypeMirror;

@SupportedAnnotationTypes(&quot;org.example.annotation.processor.CheckMethodBodies&quot;)
@SupportedSourceVersion(SourceVersion.RELEASE_8)
@AutoService(Processor.class)
public final class CheckMethodBodiesProcessor extends AbstractTypeProcessor {
    @Override
    public void processType(final Trees trees, final TypeElement typeElement, final TreePath treePath) {
        final CompilationUnitTree compilationUnitTree = treePath.getCompilationUnit();
        final TestMethodTreePathScanner treePathScanner = new TestMethodTreePathScanner(trees, compilationUnitTree);
        treePathScanner.scan(compilationUnitTree, null);
    }

    private static final class TestMethodTreePathScanner extends TreePathScanner&lt;Void, Void&gt; {
        private final Trees trees;
        private final CompilationUnitTree compilationUnitTree;

        public TestMethodTreePathScanner(
            final Trees trees,
            final CompilationUnitTree compilationUnitTree
        ) {
            this.trees = trees;
            this.compilationUnitTree = compilationUnitTree;
        }

        @Override
        public Void visitMethod(final MethodTree node, final Void unused) {
            System.out.println(
                String.format(&quot;Visiting method: %s&quot;, node.getName())
            );

            final BlockTree blockTree = node.getBody();
            for (final StatementTree statementTree : blockTree.getStatements()) {
                if (statementTree.getKind() != Tree.Kind.SWITCH) {
                    continue;
                }

                final SwitchTree switchTree = (SwitchTree) statementTree;
                final ExpressionTree switchTreeExpression = switchTree.getExpression();
                System.out.println(
                    String.format(&quot;Switch tree expression: %s&quot;, switchTreeExpression)
                );

                final TreePath treePath = TreePath.getPath(compilationUnitTree, switchTreeExpression);
                final TypeMirror typeMirror = trees.getTypeMirror(treePath);
                System.out.println(
                    String.format(&quot;Tree mirror: %s&quot;, typeMirror)
                );
            }
            return null;
        }
    }
}

Test project
Maven project
&lt;plugin&gt;
    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
    &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
    &lt;configuration&gt;
        &lt;annotationProcessorPaths&gt;
            &lt;path&gt;
                &lt;groupId&gt;org.example&lt;/groupId&gt;
                &lt;artifactId&gt;annotation-processor&lt;/artifactId&gt;
                &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;
            &lt;/path&gt;
        &lt;/annotationProcessorPaths&gt;
        &lt;showWarnings&gt;true&lt;/showWarnings&gt;
    &lt;/configuration&gt;
&lt;/plugin&gt;

To be able to use the annotation class:
&lt;dependency&gt;
    &lt;groupId&gt;org.example&lt;/groupId&gt;
    &lt;artifactId&gt;annotation-processor&lt;/artifactId&gt;
    &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;
&lt;/dependency&gt;

Switcher class: Using annotation
import org.example.annotation.processor.CheckMethodBodies;

@CheckMethodBodies
public final class Switcher {
    public void theMethod() {
        final Integer value = 1;
        switch (value.toString() + &quot;0&quot; + &quot;0&quot;) {
            case &quot;100&quot;:
                System.out.println(&quot;Hundred!&quot;);
            default:
                System.out.println(&quot;Not hundred!&quot;);
        }
    }
}

Testing
Execute the command for the annotation processor project:
mvn clean install

Execute the command for the test project:
mvn clean compile

Observe the output of the annotation processor:
Remaining type element names: [org.example.annotation.processor.test.Switcher]
Remaining type element names: [org.example.annotation.processor.test.Switcher]
Handling analyzed type element: org.example.annotation.processor.test.Switcher
Visiting method: &lt;init&gt;
Visiting method: theMethod
Switch tree expression: (value.toString() + &quot;00&quot;)
Tree mirror: java.lang.String

Stand-alone program
It is possible to use javac functionality in a stand-alone program.
It seems that it is necessary to get the tree path and then get the type mirror:
final CompilationUnitTree compilationUnitTree = &lt;…&gt;;
final ExpressionTree switchTreeExpression = &lt;…&gt;;

final TreePath treePath = TreePath.getPath(compilationUnitTree, switchTreeExpression);
final TypeMirror typeMirror = trees.getTypeMirror(treePath);

An excerpt from the documentation: TypeMirror (Java Platform SE 8 ):

public interface TypeMirror
extends AnnotatedConstruct
Represents a type in the Java programming language. Types include primitive types, declared types (class and interface types), array types, type variables, and the null type. Also represented are wildcard type arguments, the signature and return types of executables, and pseudo-types corresponding to packages and to the keyword void.

Draft implementation
Input file: Switcher class
public final class Switcher {
    public void theMethod() {
        final Integer value = 1;
        switch (value.toString() + &quot;0&quot; + &quot;0&quot;) {
            case &quot;100&quot;:
                System.out.println(&quot;Hundred!&quot;);
            default:
                System.out.println(&quot;Not hundred!&quot;);
        }
    }
}

Program class
Please, replace the &quot;/path/to/Switcher.java&quot; file path value with the actual file path value.
import com.sun.source.tree.BlockTree;
import com.sun.source.tree.CompilationUnitTree;
import com.sun.source.tree.ExpressionTree;
import com.sun.source.tree.MethodTree;
import com.sun.source.tree.StatementTree;
import com.sun.source.tree.SwitchTree;
import com.sun.source.tree.Tree;
import com.sun.source.util.JavacTask;
import com.sun.source.util.TreePath;
import com.sun.source.util.TreePathScanner;
import com.sun.source.util.Trees;
import java.io.IOException;
import java.net.URI;
import java.nio.charset.StandardCharsets;
import java.nio.file.Files;
import java.nio.file.Path;
import java.util.List;
import javax.lang.model.type.TypeMirror;
import javax.tools.JavaCompiler;
import javax.tools.JavaFileObject;
import javax.tools.SimpleJavaFileObject;
import javax.tools.ToolProvider;

public final class Program {
    public static void main(final String[] args) throws IOException {
        final JavaCompiler compiler = ToolProvider.getSystemJavaCompiler();
        final JavacTask task = (JavacTask) compiler.getTask(
            null,
            null,
            null,
            null,
            null,
            List.of(new TestFileObject())
        );
        final Iterable&lt;? extends CompilationUnitTree&gt; compilationUnitTrees = task.parse();
        task.analyze();
        final Trees trees = Trees.instance(task);

        for (final CompilationUnitTree compilationUnitTree : compilationUnitTrees) {
            final TestMethodTreePathScanner treePathScanner = new TestMethodTreePathScanner(trees, compilationUnitTree);
            treePathScanner.scan(compilationUnitTree, null);
        }
    }

    private static final class TestFileObject extends SimpleJavaFileObject {
        public TestFileObject() {
            super(URI.create(&quot;myfo:/Switcher.java&quot;), JavaFileObject.Kind.SOURCE);
        }

        @Override
        public CharSequence getCharContent(final boolean ignoreEncodingErrors) throws IOException {
            return Files.readString(
                Path.of(&quot;/path/to/Switcher.java&quot;),
                StandardCharsets.UTF_8
            );
        }
    }

    private static final class TestMethodTreePathScanner extends TreePathScanner&lt;Void, Void&gt; {
        private final Trees trees;
        private final CompilationUnitTree compilationUnitTree;

        public TestMethodTreePathScanner(
            final Trees trees,
            final CompilationUnitTree compilationUnitTree
        ) {
            this.trees = trees;
            this.compilationUnitTree = compilationUnitTree;
        }

        @Override
        public Void visitMethod(final MethodTree node, final Void unused) {
            final BlockTree blockTree = node.getBody();
            for (final StatementTree statementTree : blockTree.getStatements()) {
                if (statementTree.getKind() != Tree.Kind.SWITCH) {
                    continue;
                }

                final SwitchTree switchTree = (SwitchTree) statementTree;
                final ExpressionTree switchTreeExpression = switchTree.getExpression();
                System.out.println(
                    String.format(&quot;Switch tree expression: %s&quot;, switchTreeExpression)
                );

                final TreePath treePath = TreePath.getPath(compilationUnitTree, switchTreeExpression);
                final TypeMirror typeMirror = trees.getTypeMirror(treePath);
                System.out.println(
                    String.format(&quot;Tree mirror: %s&quot;, typeMirror)
                );
            }
            return null;
        }
    }
}

The program output:
Switch tree expression: (value.toString() + &quot;00&quot;)
Tree mirror: java.lang.String

"
"I have a sort of list in my JavaFX Application. Said Application is based on the Spring Boot Framework.
That list I want to populate with multiple instances of my own JavaFX Object created with an .fxml.
To do that I am currently loading the .fxml for each Item in the list with
FXMLLoader fxmlLoader = new FXMLLoader(PATH_TO_FXML);
fxmlLoader.setControllerFactory(applicationContext::getBean);

VBox listItem = fxmlLoader.load();
contentArea.getChildren().add(listItem);

But this would lead to all Items sharing the same Controller, wouldn't it?
Said controller is currently annotated with @Component like the other only once initialized .fxml-Controllers
Is there a way to tell spring to create a new instance of that controller each time it gets requested?
Or is there a nicer way to implement this idea of mine?
Spring Version: 3.2.1
JavaFx Version: 19.0.2
Maven Project
If there are more questions, let me know.
","As you surmised with

But this would lead to all Items sharing the same Controller, wouldn't it?

Spring by default creates a single instance of each bean and shares the same instance whenever a request is made for that bean. This behavior is controlled by Spring's scopes, and this default scope is the singleton scope. To create a new instance of a bean class on each request, you need the prototype scope.
You can (and really should, since JavaFX controllers should be created once each time the FXML is loaded) tell Spring to create a new instance each time the controller is requested by annotating the controller class @Scope(&quot;prototype&quot;):
@Component
@Scope(&quot;prototype&quot;)
public class MyControllerClass {
   // ...
}


Here is a very quick example. Here's an FXML, which I'll call Adder.fxml. It contains two Spinner&lt;Integer&gt; and a Label in a HBox. The idea is that the label will display the sum of the two values in the spinners.
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;?import javafx.geometry.Insets?&gt;
&lt;?import javafx.scene.control.Label?&gt;
&lt;?import javafx.scene.control.Spinner?&gt;
&lt;?import javafx.scene.layout.HBox?&gt;
&lt;?import javafx.scene.control.SpinnerValueFactory.IntegerSpinnerValueFactory?&gt;
&lt;HBox spacing=&quot;20.0&quot; xmlns:fx=&quot;http://javafx.com/fxml&quot;
      fx:controller=&quot;org.jamesd.examples.springscope.AddingController&quot;&gt;
    &lt;padding&gt;
        &lt;Insets bottom=&quot;20.0&quot; left=&quot;20.0&quot; right=&quot;20.0&quot; top=&quot;20.0&quot;/&gt;
    &lt;/padding&gt;

    &lt;Spinner fx:id=&quot;firstSummand&quot; onValueChange=&quot;#updateSum&quot;&gt;
        &lt;valueFactory&gt;&lt;SpinnerValueFactory.IntegerSpinnerValueFactory min=&quot;-20&quot; max=&quot;20&quot; initialValue=&quot;0&quot;/&gt;&lt;/valueFactory&gt;
    &lt;/Spinner&gt;
    &lt;Spinner fx:id=&quot;secondSummand&quot; onValueChange=&quot;#updateSum&quot;&gt;
        &lt;valueFactory&gt;&lt;SpinnerValueFactory.IntegerSpinnerValueFactory min=&quot;-20&quot; max=&quot;20&quot; initialValue=&quot;0&quot;/&gt;&lt;/valueFactory&gt;
    &lt;/Spinner&gt;
    &lt;Label fx:id=&quot;sumText&quot; text=&quot;0&quot; HBox.hgrow=&quot;ALWAYS&quot;/&gt;
&lt;/HBox&gt;

Here is the controller. Note is it annotated with both @Component and @Scope(&quot;prototytpe&quot;).
package org.jamesd.examples.springscope;

import javafx.fxml.FXML;
import javafx.scene.control.Label;
import javafx.scene.control.Spinner;
import org.springframework.context.annotation.Scope;
import org.springframework.stereotype.Component;

@Component
@Scope(&quot;prototype&quot;)
public class AddingController {
    @FXML
    private Label sumText;
    @FXML
    private Spinner&lt;Integer&gt; firstSummand;
    @FXML
    private Spinner&lt;Integer&gt; secondSummand;

    @FXML
    public void updateSum() {
        sumText.setText(Integer.toString(firstSummand.getValue() + secondSummand.getValue()));
    }
}

Here is a spring boot application class, designed to launch a JavaFX application:
package org.jamesd.examples.springscope;

import javafx.application.Application;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class SpringApp {
    public static void main(String[] args) {
        Application.launch(HelloApplication.class, args);
    }
}

and the FX Application:
package org.jamesd.examples.springscope;

import javafx.application.Application;
import javafx.fxml.FXMLLoader;
import javafx.geometry.Insets;
import javafx.geometry.Pos;
import javafx.scene.Node;
import javafx.scene.Scene;
import javafx.scene.layout.VBox;
import javafx.stage.Stage;
import org.springframework.boot.builder.SpringApplicationBuilder;
import org.springframework.context.ConfigurableApplicationContext;

import java.io.IOException;

public class HelloApplication extends Application {

    private ConfigurableApplicationContext applicationContext;

    @Override
    public void init() {
        applicationContext = new SpringApplicationBuilder(SpringApp.class).run();
    }
    @Override
    public void start(Stage stage) throws IOException {

        VBox root = new VBox(10);
        root.setAlignment(Pos.CENTER);
        root.setPadding(new Insets(20));
        for (int i = 0 ; i &lt; 3; i++) {
            root.getChildren().add(createAdder());
        }
        Scene scene = new Scene(root, 600, 375);
        stage.setTitle(&quot;Adding&quot;);
        stage.setScene(scene);
        stage.show();
    }

    private Node createAdder() throws IOException {
        FXMLLoader fxmlLoader = new FXMLLoader(HelloApplication.class.getResource(&quot;Adder.fxml&quot;));
        fxmlLoader.setControllerFactory(applicationContext::getBean);
        return fxmlLoader.load();
    }

}

The application creates three instances of the UI defined in Adder.fxml, which work independently.
If you remove the @Scope(&quot;prototype&quot;) annotation in the controller, you'll see it doesn't work correctly. (Since the fields are reassigned to the same controller instance each time the FXML is loaded, only the last instance loaded gets updated.)

Note you can also use Spring's meta-annotations to create your own meta-annotation which represents a @Component which always has @Scope(&quot;prototype&quot;):
@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
@Documented
@Component
@Scope(&quot;prototype&quot;)
public @interface FXController {}

and then you can just do
@FXController
public class AddingController {
    // ...
}

This might be worthwhile, because FX Controllers in a spring-managed application should always be prototype scope.
"
"This chart shows the problem:

I have JavaFX program that calculates data and draws a chart, but why points are not connected properly? I have tried many things, even creating two separate series, but it doesn't work.
public void createScatterChart(){
    final NumberAxis xAxis = new NumberAxis();
    final NumberAxis yAxis = new NumberAxis();

    final SmoothedChart&lt;Number,Number&gt; smoothedChart = new SmoothedChart&lt;&gt;(xAxis, yAxis);

    XYChart.Series series1 = new XYChart.Series();
    XYChart.Series series2 = new XYChart.Series();
    XYChart.Series series3 = new XYChart.Series();

    for(int i = 0 ; i &lt; this.r.size() ; i ++)
    {
        series1.getData().add(new XYChart.Data(this.r.get(i) * Math.cos(Math.toRadians(this.nodes.get(i))),this.r.get(i) * Math.sin(Math.toRadians(this.nodes.get(i)))));
        //series2.getData().add(new XYChart.Data(this.r.get(i) * Math.cos(Math.toRadians(this.nodes.get(i) * this.xArray[i][0])),this.r.get(i) * Math.sin(Math.toRadians(this.nodes.get(i) * this.xArray[i][0]))));
    }


    smoothedChart.getData().add(series1);
    smoothedChart.getData().add(series2);
    Stage stage = new Stage();
    Scene scene  = new Scene(smoothedChart,800,600);

    stage.setScene(scene);
    stage.show();
}

","A similar problem is examined here, in which the solution hinges on the data sort order. Looking at LineChart, SortingPolicy.NONE specifies &quot;The data should be left in the order defined by the list in XYChart.dataProperty().&quot;

I had to change chart from my SmoothChart to standard LineChart.

Depending on your approach to smoothing, you may encounter the kind of cubic spline artifacts examined here, which also occurs in jfreechart-fx. An approach using Bézier curves is adduced here.
As tested using synthetic data:


import javafx.application.Application;
import javafx.collections.FXCollections;
import javafx.collections.ObservableList;
import javafx.geometry.Pos;
import javafx.scene.Scene;
import javafx.scene.chart.LineChart;
import javafx.scene.chart.NumberAxis;
import javafx.scene.chart.XYChart;
import javafx.scene.control.ChoiceBox;
import javafx.scene.control.Tooltip;
import javafx.scene.layout.Pane;
import javafx.scene.layout.StackPane;
import javafx.stage.Stage;

/**
 * @see https://stackoverflow.com/a/72607616/230513
 * @see https://stackoverflow.com/a/2510048/230513
 */
public class ChartTest extends Application {

    private static final int N = 32;

    @Override
    public void start(Stage stage) {
        var xAxis = new NumberAxis();
        var yAxis = new NumberAxis();
        var series = new XYChart.Series();
        series.setName(&quot;Data&quot;);
        for (int i = 0; i &lt;= N; i++) {
            var t = 2 * Math.PI * i / N;
            var x = Math.cos(t);
            var y = Math.sin(t);
            series.getData().add(new XYChart.Data(x, y));
        }
        var chart = new LineChart&lt;Number, Number&gt;(xAxis, yAxis);
        chart.getData().add(series);
        ObservableList&lt;LineChart.SortingPolicy&gt; policies
            = FXCollections.observableArrayList(LineChart.SortingPolicy.values());
        var policy = new ChoiceBox&lt;LineChart.SortingPolicy&gt;(policies);
        policy.setTooltip(new Tooltip(&quot;Choose a data sorting policy.&quot;));
        policy.getSelectionModel().select(chart.getAxisSortingPolicy());
        chart.axisSortingPolicyProperty().bind(policy.valueProperty());
        Pane root = new StackPane(chart, policy);
        StackPane.setAlignment(policy, Pos.TOP_RIGHT);
        stage.setScene(new Scene(root));
        stage.show();
    }

    public static void main(String[] args) {
        launch(args);
    }
}

"
"I want the height of the bottom dialog to expend to match_parent (as empty activity)
Here is my code.
MainActivity
import androidx.appcompat.app.AppCompatActivity;
import android.os.Bundle;
import android.view.LayoutInflater;
import android.view.View;
import android.widget.Button;

import com.google.android.material.bottomsheet.BottomSheetBehavior;

public class MainActivity extends AppCompatActivity {

    @Override
    protected void onCreate(Bundle savedInstanceState)
    {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);

        Button OpenBottomSheet = findViewById(R.id.open_bottom_sheet);

        OpenBottomSheet.setOnClickListener(
                new View.OnClickListener() {
                    @Override
                    public void onClick(View v)
                    {
                        BottomSheetDialog bottomSheet = new BottomSheetDialog();
                        bottomSheet.show(getSupportFragmentManager(),
                                &quot;ModalBottomSheet&quot;);
                    }
        });
     }
}

BottomSheetDialog
import android.os.Bundle;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.Button;
import android.widget.Toast;

import androidx.annotation.Nullable;

import com.google.android.material.bottomsheet.BottomSheetBehavior;
import com.google.android.material.bottomsheet.BottomSheetDialogFragment;

public class BottomSheetDialog extends BottomSheetDialogFragment {

    @Override
    public View onCreateView(LayoutInflater inflater, @Nullable
    ViewGroup container, @Nullable Bundle savedInstanceState)
    {
        View v = inflater.inflate(R.layout.buttom_sheet_layout,
                container, false);

        return v;
    }

}

Here is full code.
","To have full screen BottomSheetDialogFragment, you'd:

Use a full screen window using android:windowFullscreen applied in a custom style by overriding  getTheme()
Use STATE_EXPANDED state of the bottom sheet to expand to the entire dialog window
Set the bottomSheet layout to MATCH_PARENT
Optionally disable the bottom sheet STATE_HALF_EXPANDED using behavior.setSkipCollapsed()

Applying that to your class:
public class BottomSheetDialog extends BottomSheetDialogFragment {

    @Override
    public View onCreateView(LayoutInflater inflater, @Nullable
    ViewGroup container, @Nullable Bundle savedInstanceState) {
        View v = inflater.inflate(R.layout.bottom_sheet_layout,
                container, false);

        Button algo_button = v.findViewById(R.id.algo_button);
        Button course_button = v.findViewById(R.id.course_button);

        algo_button.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                Toast.makeText(getActivity(),
                                &quot;Algorithm Shared&quot;, Toast.LENGTH_SHORT)
                        .show();
                dismiss();
            }
        });

        course_button.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                Toast.makeText(getActivity(),
                                &quot;Course Shared&quot;, Toast.LENGTH_SHORT)
                        .show();
                dismiss();
            }
        });
        
        return v;
    }


    @Override
    public int getTheme() {
        return R.style.dialog_style;
    }

    @Override
    public void onStart() {
        super.onStart();
        
        View bottomSheet =
                ((com.google.android.material.bottomsheet.BottomSheetDialog) 
                getDialog()).findViewById(com.google.android.material.R.id.design_bottom_sheet);

        if (bottomSheet != null) {
            // set the bottom sheet state to Expanded to expand to the entire window
            BottomSheetBehavior.from(bottomSheet).setState(BottomSheetBehavior.STATE_EXPANDED);
            // disable the STATE_HALF_EXPANDED state
            BottomSheetBehavior.from(bottomSheet).setSkipCollapsed(true);
            // set the bottom sheet height to match_parent
            ViewGroup.LayoutParams layoutParams = bottomSheet.getLayoutParams();
            layoutParams.height = WindowManager.LayoutParams.MATCH_PARENT;
            bottomSheet.setLayoutParams(layoutParams);
        }

        // Make the dialog cover the status bar
        getDialog().getWindow().setFlags(
             WindowManager.LayoutParams.FLAG_LAYOUT_NO_LIMITS, 
             WindowManager.LayoutParams.FLAG_LAYOUT_NO_LIMITS);


    }

}

And this is the custom style for full screen:
&lt;resources&gt;

    &lt;style name=&quot;dialog_style&quot; parent=&quot;Theme.MaterialComponents.BottomSheetDialog&quot;&gt;
        &lt;item name=&quot;android:windowFullscreen&quot;&gt;true&lt;/item&gt;
    &lt;/style&gt;

&lt;/resources&gt;

If you're using Material3, then extend the style from Theme.Material3.DayNight.BottomSheetDialog instead.
Edit:

Great ! but the top bar does not hide

You can make the bottom sheet hide the status bar by setting the below window flag; the above class is updated with that:
// Make the dialog cover the status bar
getDialog().getWindow().setFlags(
     WindowManager.LayoutParams.FLAG_LAYOUT_NO_LIMITS, 
     WindowManager.LayoutParams.FLAG_LAYOUT_NO_LIMITS);

Also for devices with notch you need to set android:windowLayoutInDisplayCutoutMode to the style:
&lt;resources&gt;

    &lt;style name=&quot;dialog_style&quot; parent=&quot;Theme.MaterialComponents.BottomSheetDialog&quot;&gt;
        &lt;item name=&quot;android:windowFullscreen&quot;&gt;true&lt;/item&gt;
        &lt;item name=&quot;android:windowLayoutInDisplayCutoutMode&quot;&gt;shortEdges&lt;/item&gt;
    &lt;/style&gt;

&lt;/resources&gt;

"
"I have this following method that takes in a List of a CustomClass and performs filters using streams and returns at each step based on the result of the filter.
I was wondering if there was a way to simplify the code but combining the filters and statements together to make it more concise and efficient.
public String transform(List&lt;CustomObject&gt; listOfObjects) {

       listOfObjects = listOfObjects.stream()
            .filter(object -&gt; object.objectType().equals(&quot;BUSINESS&quot;)).toList();

       // Primary check as all object should be of business type 
       // and if nothing exist we throw an exception
       if (listOfObjects.isEmpty()) {
           throw new RuntimeException(&quot;NO BUSINESS OBJECT FOUND&quot;);

       }

       // All objects are now of business type but we want them to be active
       List&lt;CustomObject&gt; listOfActiveObjects = listOfObjects.stream()
                     .filter(object -&gt; object.objectStatus().equals(&quot;ACTIVE&quot;))
                     .toList();

       // If no active object found just return the first business object sorted url

       if (listOfActiveObjects.isEmpty()) {
           return listOfObjects.stream()
                .sorted(Comparator.comparing(CustomObject::url))
                .toList().get(0).getUrl();
       }

       // Active objects are present so now filtered with proper locale

       List&lt;CustomObject&gt; listOfActiveObjectsWithLocale = listOfActiveObjects.stream()
                    .filter(object -&gt; object.locale().equals(&quot;en-US&quot;))
                    .toList();

       // If no locale was found just return the first sorted business active url

       if (listOfActiveObjectsWithLocale.isEmpty()) {
           return listOfActiveObjects.stream()
                 .sorted(Comparator.comparing(CustomObject::url))
                 .toList().get(0).getUrl();
       }

       // All filters applied, so within these objects return the sorted business/active/locale url
       return listOfActiveObjectsWithLocale.stream()
              .sorted(Comparator.comparing(CustomObject::url))
              .toList().get(0).getUrl();
   }

","Check for Business objekts first then sort by your priority
public static String transform(List&lt;CustomObject&gt; listOfObjects) {

    // Check if there are any &quot;BUSINESS&quot; objects in the list
    if (listOfObjects.stream().noneMatch(object -&gt; object.objectType().equals(&quot;BUSINESS&quot;))) {
        throw new RuntimeException(&quot;NO BUSINESS OBJECT FOUND&quot;);
    }

    // Sort the objects based on the desired criteria and return the first one
    return listOfObjects.stream()
        .sorted(Comparator
            .comparing((CustomObject object) -&gt; !object.objectType().equals(&quot;BUSINESS&quot;)) // BUSINESS first
            .thenComparing(object -&gt; !object.objectStatus().equals(&quot;ACTIVE&quot;)) // then ACTIVE
            .thenComparing(object -&gt; !object.locale().equals(&quot;en-US&quot;)) // then locale en-US
            .thenComparing(CustomObject::getUrl) // finally by URL
        )
        .findFirst()
        .get()  // Since an element is guaranteed, we can safely call get()
        .getUrl();
}

Edit
The now deleted comment was refering to calling Optional.get() Since in the if condition we made sure that there is at least one Business object, I don't think it is bad. But if you prefer you can change it to something like
public String transform(List&lt;CustomObject&gt; listOfObjects) {

if (listOfObjects.stream().noneMatch(object -&gt; object.objectType().equals(&quot;BUSINESS&quot;))) {
    throw new RuntimeException(&quot;NO BUSINESS OBJECT FOUND&quot;);
}

return listOfObjects.stream()
    .sorted(Comparator
        .comparing((CustomObject object) -&gt; !object.objectType().equals(&quot;BUSINESS&quot;))
        .thenComparing(object -&gt; !object.objectStatus().equals(&quot;ACTIVE&quot;))
        .thenComparing(object -&gt; !object.locale().equals(&quot;en-US&quot;))
        .thenComparing(CustomObject::url)
    )
    .findFirst()
    .map(CustomObject::getUrl)
    .orElseThrow(() -&gt; new RuntimeException(&quot;Unexpected error during transformation&quot;));
}

"
"There's a node and I need to dynamically change its color. I also want to do this using CSS variables. The problem is that JavaFX seems to only perform a CSS lookup when a node property (fill) is explicitly bound to the corresponding styleable object property, the value of which should be obtained via CSS. In other words, CSS only works if styleable property is bound to a node property and that node exists in the scene graph.
But if the Node property is already bound, I can't interpolate its value in the Timeline. Is there any workaround here? For example, can I somehow manually trigger the CSS variable lookup before the timeline starts?
Minimal reproducible example:
public class ExampleApp extends Application {

    public static void main(String[] args) {
        launch();
    }

    @Override
    public void start(Stage stage) {
        var r = new AnimatedRect(200, 200);
        // actual: rect flashes red and blue
        // expected: rect flashes green and orange
        r.setStyle(&quot;-color1: green; -color2: orange;&quot;);

        var scene = new Scene(new BorderPane(r), 200, 200);
        stage.setScene(scene);
        stage.show();
    }

    static class AnimatedRect extends Rectangle {

        public AnimatedRect(double width, double height) {
            super(width, height);
            setFill(color1.get());

            // if you bind the color property to the rect fill, the CSS variables will start to work,
            // but the timeline will stop because it's forbidden to change a bound value,
            // ... and unfortunately bidirectional binding won't help here either
            // fillProperty().bind(color1);

            var timeline = new Timeline(
                new KeyFrame(Duration.millis(0),
                    new KeyValue(fillProperty(), color1.get(), LINEAR)
                ),
                new KeyFrame(Duration.millis(1000),
                    new KeyValue(fillProperty(), color2.get(), LINEAR)
                )
            );
            timeline.setCycleCount(Timeline.INDEFINITE);
            timeline.setAutoReverse(false);

            sceneProperty().addListener((obs, o, n) -&gt; {
                if (n != null) {
                    timeline.play();
                } else {
                    timeline.stop();
                }
            });
        }

        final StyleableObjectProperty&lt;Paint&gt; color1 = new SimpleStyleableObjectProperty&lt;&gt;(
            StyleableProperties.COLOR1, AnimatedRect.this, &quot;-color1&quot;, Color.RED
        );

        final StyleableObjectProperty&lt;Paint&gt; color2 = new SimpleStyleableObjectProperty&lt;&gt;(
            StyleableProperties.COLOR2, AnimatedRect.this, &quot;-color2&quot;, Color.BLUE
        );

        static class StyleableProperties {

            private static final CssMetaData&lt;AnimatedRect, Paint&gt; COLOR1 = new CssMetaData&lt;&gt;(
                &quot;-color1&quot;, PaintConverter.getInstance(), Color.RED
            ) {
                @Override
                public boolean isSettable(AnimatedRect c) {
                    return !c.color1.isBound();
                }

                @Override
                public StyleableProperty&lt;Paint&gt; getStyleableProperty(AnimatedRect c) {
                    return c.color1;
                }
            };

            private static final CssMetaData&lt;AnimatedRect, Paint&gt; COLOR2 = new CssMetaData&lt;&gt;(
                &quot;-color2&quot;, PaintConverter.getInstance(), Color.BLUE
            ) {
                @Override
                public boolean isSettable(AnimatedRect c) {
                    return !c.color2.isBound();
                }

                @Override
                public StyleableProperty&lt;Paint&gt; getStyleableProperty(AnimatedRect c) {
                    return c.color2;
                }
            };

            private static final List&lt;CssMetaData&lt;? extends Styleable, ?&gt;&gt; STYLEABLES;

            static {
                final List&lt;CssMetaData&lt;? extends Styleable, ?&gt;&gt; styleables =
                    new ArrayList&lt;&gt;(Rectangle.getClassCssMetaData());
                styleables.add(COLOR1);
                styleables.add(COLOR2);
                STYLEABLES = Collections.unmodifiableList(styleables);
            }
        }

        public static List&lt;CssMetaData&lt;? extends Styleable, ?&gt;&gt; getClassCssMetaData() {
            return StyleableProperties.STYLEABLES;
        }

        @Override
        public List&lt;CssMetaData&lt;? extends Styleable, ?&gt;&gt; getCssMetaData() {
            return getClassCssMetaData();
        }
    }
}

UPDATE:
I found the problem. JavaFX resolves CSS variables after the node is connected to the scene. My previous code creates the timeline before the color values change. So I need to listen for color changes and update the timeline accordingly. Since it's immutable, the only way is to create a new object. It's still not optimal, because if I update both colors, the animation will be played twice, but at least it works now.
static class AnimatedRect extends Rectangle {

SimpleObjectProperty&lt;Timeline&gt; timeline = new SimpleObjectProperty&lt;&gt;();

public AnimatedRect(double width, double height) {
    super(width, height);
    setFill(color1.get());

    color1.addListener((obs, o, v) -&gt; {
        if (timeline.get() != null) {
            timeline.get().stop();
        }
        timeline.set(createTimeline());
        timeline.get().play();
    });

    color2.addListener((obs, o, v) -&gt; {
        if (timeline.get() != null) {
            timeline.get().stop();
        }

        timeline.set(createTimeline());
        timeline.get().play();
    });

    sceneProperty().addListener((obs, o, n) -&gt; {
        if (n != null) {
            if (timeline.get() != null) {
                timeline.get().play();
            }
        } else {
            if (timeline.get() != null) {
                timeline.get().stop();
            }
        }
    });
}

Timeline createTimeline() {
    var timeline = new Timeline(
        new KeyFrame(Duration.millis(0),
            new KeyValue(fillProperty(), color1.getValue(), LINEAR)
        ),
        new KeyFrame(Duration.millis(1000),
            new KeyValue(fillProperty(), color2.getValue(), LINEAR)
        )
    );
    timeline.setCycleCount(Timeline.INDEFINITE);
    timeline.setAutoReverse(false);

    return timeline;
}

// .. the rest of the code

","In the original code, you look up the values of the colors in the constructor of AnimatedRectangle, which will necessarily have their default values at that point, and use those to create the timeline. Once you set the colors via the call to setStyle(...), the timeline is already created and its &quot;endpoint colors&quot; are essentially immutable.
(The problem is exacerbated, as noted in the update to the question, by the fact that the values of the CSS properties will not actually be set until applyCSS() is called on the rectangle, which is typically on the first layout pass after the rectangle is added to a scene.)
A better solution is simply to look up the values of the endpoint colors on each frame of the animation. You can do this, for example, using a Transition instead of a Timeline (another solution would be to use an AnimationTimer). Here is a modified constructor which will work the way you want:
        public AnimatedRect(double width, double height) {
            super(width, height);
            setFill(color1.get());

//            var timeline = new Timeline(
//                    new KeyFrame(Duration.millis(0),
//                            new KeyValue(fillProperty(), color1.get(), Interpolator.LINEAR)
//                    ),
//                    new KeyFrame(Duration.millis(1000),
//                            new KeyValue(fillProperty(), color2.get(), Interpolator.LINEAR)
//                    )
//            );
//            timeline.setCycleCount(Timeline.INDEFINITE);
//            timeline.setAutoReverse(false);

            Transition transition = new Transition() {
                {
                    setCycleDuration(Duration.seconds(1));
                }
                @Override
                protected void interpolate(double v) {
                    Paint p1 = color1.get();
                    Paint p2 = color2.get();
                    // if these are both colors, interpolate them. 
                    // If they're e.g. gradients, just switch halfway:
                    if (p1 instanceof Color c1 &amp;&amp; p2 instanceof Color c2) {
                        setFill(c1.interpolate(c2, v));
                    } else {
                        setFill(v &lt;= 0.5 ? p1 : p2);
                    }
                }
            };
            transition.setCycleCount(Animation.INDEFINITE);
            sceneProperty().subscribe(scene -&gt; {
                if (scene != null) {
                    transition.play();
                } else {
                    transition.stop();
                }
            });
        }

Note this allows you to dynamically change the style of the rectangle while the animation is running.
Also note that I changed sceneProperty().addListener(...) to sceneProperty().subscribe(), which I find slightly cleaner. Calling this would also work if the rectangle were already part of a scene (which is impossible in the current case, but could be possible under some code refactoring). subscribe() was introduced in JavaFX 21.
"
"We would like to have a Java REST API to return files from Google Cloud Storage as attachment. I was able to able to get it to work using the following method. The problem is that the file has to be downloaded locally to the service container (we are deploying on Google Cloud Run) and this is a problem in the case of very large files, and may generally be bad practice. Is there a way to modify this code somehow to skip the creation of a local file?
@GetMapping(path = &quot;/file&quot;, produces = MediaType.APPLICATION_OCTET_STREAM_VALUE)
public ResponseEntity&lt;InputStreamResource&gt; getSpecificFile(@RequestParam String fileName,
        @RequestParam String bucketName, @RequestParam String projectName) {
    Storage storage = StorageOptions.newBuilder().setProjectId(projectId).build().getService();
    Blob blob = storage.get(bucketName, fileName);
    ReadChannel readChannel = blob.reader();
    String outputFileName = tempFileDestination.concat(&quot;\\&quot;).concat(fileName);
    try (FileOutputStream fileOutputStream = new FileOutputStream(outputFileName)) {
        fileOutputStream.getChannel().transferFrom(readChannel, 0, Long.MAX_VALUE);
        String contentType = Files.probeContentType(Paths.get(outputFileName));

        FileInputStream fileInputStream = new FileInputStream(outputFileName);
        return ResponseEntity.ok().contentType(MediaType.valueOf(contentType))
                .header(&quot;Content-Disposition&quot;, &quot;attachment; filename=&quot; + fileName)
                .body(new InputStreamResource(fileInputStream));
    } catch (IOException e) {
        e.printStackTrace();
        return ResponseEntity.internalServerError().body(null);
    } finally {
        // delete the local file as cleanup
        try {
            Files.delete(Paths.get(outputFileName));
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}

","Well, that did not take me long to figure out. I was able to make it work as follows:
@GetMapping(path = &quot;/file&quot;, produces = MediaType.APPLICATION_OCTET_STREAM_VALUE)
public ResponseEntity&lt;InputStreamResource&gt; getSpecificFile(@RequestParam String fileName, @RequestParam String bucketName, @RequestParam String projectName) {
    Storage storage = StorageOptions.newBuilder().setProjectId(projectId).build().getService();
    Blob blob = storage.get(bucketName, fileName);
    ReadChannel readChannel = blob.reader();
    try {
        String contentType = Files.probeContentType(Paths.get(fileName));

        InputStream inputStream = Channels.newInputStream(readChannel);
        return ResponseEntity.ok().contentType(MediaType.valueOf(contentType))
                .header(&quot;Content-Disposition&quot;, &quot;attachment; filename=&quot; + fileName)
                .body(new InputStreamResource(inputStream));
    } catch (IOException e) {
        e.printStackTrace();
        return ResponseEntity.internalServerError().body(null);
    }
}

Basically redirect the InputStream to the readChannel instead of the file.
"
"I'm trying to mock one function in an EntityListener, so the real implementation is not called in my test. The code looks like the following:
@EntityListeners(myEntityListener.class)
@Entity
public class myEntity {
...
}

public class myEntityListener {
    
    public String doThis() {
         // code i want to skip/mock
         return expensiveToCompute;
    }
    
    @PrePersist
    public void myListener(myEntity e) {
         if (complexConditionToTest) e.setSomething(doThis());
    }
}

I would like to mock doThis(), but run the actual myListener() function so I can test that the complex conditioning is correct.
I tried to test using Mockito, with the following code:
public class MyEntityListenerTest {

    @Autowired
    myEntityRepository repo;
    
    @MockBean
    myEntityListener entityListener;
    
    @Test
    public void myListenerTest() throws Exception {
        String expected = &quot;fake-text&quot;;
        Mockito.when(entityListener.doThis()).thenReturn(expected);
        
        myEntity e = new myEntity();
        myEntity stored = repo.save(e);
        assertThat(stored.getSomething()).isEqualTo(expected);
    }
}

The myEntity constructor and doThis call both have more params, removed for simplicity in the example code.
I expected that this would mock the doThis function, but it does not appear to be mocked and the actual implemention is called instead. There are no error messages.
I am guessing that MockBean is not finding an existing myEntityListener bean and instead is creating a new one (which goes unused), but I am not sure how to make the test find the correct myEntityListener.
","I couldn't get mockito to work, so I ended up creating a static field I could change instead:
@EntityListeners(MyEntityListener.class)
@Entity
public class myEntity {
...
}

public class MyEntityListener {
    public static final testing = false;

    public String doThis() {
         if (testing) return &quot;fake-text&quot;;
         // code i want to skip/mock
         return expensiveToCompute;
    }
    
    @PrePersist
    public void myListener(myEntity e) {
         if (complexConditionToTest) e.setSomething(doThis());
    }
}

This makes the test look like:
public class MyEntityListenerTest {

    @Autowired
    myEntityRepository repo;

    @Before
    public final void setup() {
        MyEntityListener.testing = true;
    }
    
    @Test
    public void myListenerTest() throws Exception {
        String expected = &quot;fake-text&quot;;
        myEntity e = new myEntity();
        myEntity stored = repo.save(e);
        assertThat(stored.getSomething()).isEqualTo(expected);
    }
}

If you need the expected text to change you could probably use a string property instead of a boolean, but this was fine for my use case.
"
"I have incoming byte streams probably encoded in H264 from a RTSP camera through a websocket in my spring boot application,
I need to decode the incoming H264 streams to transmit the video to my frontend clients.
I have tried using javaCV/FFMpeg but nothing works.
Any help would be appreciated
this is the part of hex dump received through socket
00000000: 01 00 00 00 04 48 32 36 34 00 00 00 24 38 65 34    .....H264...$8e4
00000010: 32 39 65 37 61 2D 32 66 34 66 2D 34 37 31 61 2D    29e7a-2f4f-471a-
00000020: 39 61 63 30 2D 66 66 62 38 64 64 37 63 37 64 37    9ac0-ffb8dd7c7d7
00000030: 32 00 00 00 D4 7B 22 49 73 49 6E 69 74 22 3A 66    2...T{&quot;IsInit&quot;:f
00000040: 61 6C 73 65 2C 22 49 73 41 75 64 69 6F 22 3A 66    alse,&quot;IsAudio&quot;:f
00000050: 61 6C 73 65 2C 22 54 6F 74 61 6C 53 65 63 6F 6E    alse,&quot;TotalSecon
00000060: 64 73 22 3A 30 2E 30 36 2C 22 46 72 61 6D 65 54    ds&quot;:0.06,&quot;FrameT
00000070: 69 6D 65 22 3A 22 32 30 32 33 2D 30 32 2D 32 33    ime&quot;:&quot;2023-02-23
00000080: 54 30 34 3A 32 31 3A 35 33 2E 35 33 31 5A 22 2C    T04:21:53.531Z&quot;,
00000090: 22 53 65 71 75 65 6E 63 65 49 64 22 3A 31 2C 22    &quot;SequenceId&quot;:1,&quot;
000000a0: 42 61 73 65 44 65 63 6F 64 65 54 69 6D 65 22 3A    BaseDecodeTime&quot;:
000000b0: 32 36 35 38 37 2C 22 4D 65 64 69 61 54 69 6D 65    26587,&quot;MediaTime
000000c0: 22 3A 32 36 35 38 37 2C 22 49 73 46 72 61 6D 65    &quot;:26587,&quot;IsFrame
000000d0: 48 69 64 64 65 6E 22 3A 66 61 6C 73 65 2C 22 49    Hidden&quot;:false,&quot;I
000000e0: 73 4B 65 79 46 72 61 6D 65 22 3A 66 61 6C 73 65    sKeyFrame&quot;:false
000000f0: 2C 22 49 64 22 3A 34 34 35 2C 22 47 65 6E 65 72    ,&quot;Id&quot;:445,&quot;Gener
00000100: 61 74 69 6F 6E 22 3A 31 7D 00 00 3F 50 00 00 00    ation&quot;:1}..?P...
00000110: 68 6D 6F 6F 66 00 00 00 10 6D 66 68 64 00 00 00    hmoof....mfhd...
00000120: 00 00 00 01 BD 00 00 00 50 74 72 61 66 00 00 00    ....=...Ptraf...
00000130: 10 74 66 68 64 00 02 00 00 00 00 00 01 00 00 00    .tfhd...........
00000140: 14 74 66 64 74 01 00 00 00 00 00 00 00 00 00 67    .tfdt..........g
00000150: DB 00 00 00 24 74 72 75 6E 01 00 0F 01 00 00 00    [...$trun.......
00000160: 01 00 00 00 70 00 00 00 3C 00 00 3E E0 00 01 00    ....p...&lt;..&gt;`...
00000170: 00 00 00 00 00 00 00 3E E8 6D 64 61 74 00 00 3E    .......&gt;hmdat..&gt;
00000180: DC 41 E1 81 80 93 BE 16 2B 33 77 3D 4C B6 55 8B    \Aa...&gt;.+3w=L6U.
00000190: D2 55 60 92 05 F7 F7 A4 97 54 4B 6C A6 68 48 84    RU`..ww$.TKl&amp;hH.
000001a0: 68 FF D2 B6 6C 02 31 FC 24 01 78 EA BD 20 AD 15    h.R6l.1|$.xj=.-.
000001b0: F1 73 31 4B EB EF 18 1B 50 B3 13 F2 DC C6 4C E1    qs1Kko..P3.r\FLa
000001c0: 75 8B 94 52 6B C5 09 37 55 1E 45 66 6A 92 39 23    u..RkE.7U.Efj.9#
000001d0: C9 2D FD BB EC AD FD CF C4 30 75 FF 44 66 FA 85    I-};l-}OD0u.Dfz.
000001e0: D9 7C 18 72 AE 63 45 60 DD D7 65 44 84 49 95 8D    Y|.r.cE`]WeD.I..
000001f0: 2C 70 6C 57 8E E9 A9 EB B6 F6 78 BD D6 88 99 F6    ,plW.i)k6vx=V..v
00000200: FC 25 B1 0A FF DF CB 77 6A 67 37 24 A5 3D 8F A1    |%1.._Kwjg7$%=.!
00000210: 27 9B 4F 42 0E CD B8 87 6E C9 99 FC 6F 4C 53 4B    '.OB.M8.nI.|oLSK
00000220: 01 EA B6 AF 99 F8 22 C1 8F 1E C1 66 D6 8A 09 D6    .j6/.x&quot;A..AfV..V
00000230: 99 79 91 F7 C1 2A 08 1F 81 CB 5E DD C3 CA 86 8F    .y.wA*...K^]CJ..
00000240: 57 BF 17 A2 64 6B 69 56 AE 19 1F 57 AD A6 D8 C2    W?.&quot;dkiV...W-&amp;XB
00000250: 06 28 EB 46 D3 E4 85 51 3E E2 A5 40 50 50 85 7D    .(kFSd.Q&gt;b%@PP.}
00000260: 72 6B 20 87 1A 6E 73 E1 B8 88 9E 20 23 48 6D FE    rk...nsa8...#Hm~
00000270: C2 0D 39 ED 24 B2 6D B5 9B 81 B6 BC F4 EE DE A2    B.9m$2m5..6&lt;tn^&quot;
00000280: CF A1 08 D0 D2 5B EE FA 0D DA FD 3B 79 C7 89 E5    O!.PR[nz.Z};yG.e
00000290: 4F 64 73 37 98 D6 2D 47 1D 8B A3 47 DD EA C9 8E    Ods7.V-G..#G]jI.
000002a0: 3E 8C 97 E2 42 15 FB 22 A6 83 A1 34 18 52 5E 35    &gt;..bB.{&quot;&amp;.!4.R^5
000002b0: 2A A6 E2 71 D7 4F 96 0A EC AE 8D 39 27 B8 CF 61    *&amp;bqWO..l..9'8Oa
000002c0: CC ED E9 AF 74 C3 95 D3 E3 96 32 20 E6 31 0B E4    Lmi/tC.Sc.2.f1.d
000002d0: DC F4 FF 41 37 36 E7 DB 87 AE B3 7D BF CA F8 05    \t.A76g[..3}?Jx.
000002e0: 72 2A 38 AB B8 8E 98 43 97 C8 5E 80 57 C6 E7 1E    r*8+8..C.H^.WFg.
000002f0: 86 75 CE CD CE BF CF 10 C9 8A C2 C9 6E 33 41 AC    .uNMN?O.I.BIn3A,
00000300: 91 AC A8 F3 1B E6 D5 0A 22 A1 2C 4C 68 19 51 4D    .,(s.fU.&quot;!,Lh.QM
00000310: 17 DA AE E1 D7 BC 0E 2D F8 14 61 E2 4F BA 26 A3    .Z.aW&lt;.-x.abO:&amp;#
00000320: 0A E4 A6 BE 08 EA 3C 28 E6 C5 6B CA 3A 86 D2 59    .d&amp;&gt;.j&lt;(fEkJ:.RY
00000330: 34 C2 ED 91 72 5A EF 2C BE D7 38 A4 60 D7 F3 97    4Bm.rZo,&gt;W8$`Ws.
00000340: BB E6 FD C2 D0 29 10 B5 A4 79 D8 3E 61 48 8A F9    ;f}BP).5$yX&gt;aH.y
00000350: C6 D8 13 D0 FD DB D6 FA 24 7F CD 5A BF 06 57 49    FX.P}[Vz$.MZ?.WI
00000360: 51 EC ED B2 74 AB 92 1D 37 68 70 A2 A5 31 B5 5F    Qlm2t+..7hp&quot;%15_
00000370: EA CF 9E 3E 6A B1 78 16 B7 94 D1 46 7B 63 C1 67    jO.&gt;j1x.7.QF{cAg
00000380: D2 B0 08 44 64 1E 68 15 39 80 E3 DD EB C0 E1 71    R0.Dd.h.9.c]k@aq
00000390: E8 EE D0 4D DF 4F 41 E0 96 C5 34 AD BC D3 9E 88    hnPM_OA`.E4-&lt;S..
000003a0: 0B 17 D8 7D 3A A8 3B 06 78 79 93 B7 30 92 C8 D8    ..X}:(;.xy.70.HX
000003b0: 5D 27 04 D7 00 9F E3 EA A3 C6 BD B9 05 21 5C 68    ]'.W..cj#F=9.!\h
000003c0: 45 DB 90 2A 05 38 79 D9 84 60 C7 F2 BB DE 1B 5A    E[.*.8yY.`Gr;^.Z
000003d0: 44 0B ED 67 34 DF 07 8B F5 04 27 9E 1A F0 04 CA    D.mg4_..u.'..p.J
000003e0: 86 B1 2C 0B 78 D0 58 86 81 62 D8 70 3D BA 9D 51    .1,.xPX..bXp=:.Q
000003f0: D8 2C 6C 6A 10 88 B9 F8 89 3D 6F 39 C2 52 49 CF    X,lj..9x.=o9BRIO
00000400: 9F C1 50 6A D4 9E A5 96 B2 0A 99 1D 6B BC 63 03    .APjT.%.2...k&lt;c.
00000410: A4 8C 7E 1D BD DF 8B D8 97 EE 9A 59 78 63 FC 74    $.~.=_.X.n.Yxc|t
00000420: 3B 40 75 AF A7 1A B7 F0 56 A5 5F 3E 81 54 83 A0    ;@u/'.7pV%_&gt;.T..
00000430: 7F FC AD 71 CE AF 54 8B 5D DC 27 34 20 A3 0A 73    .|-qN/T.]\'4.#.s
00000440: 76 A5 81 33 22 31 56 6B 1D 82 C4 32 FB 82 15 F6    v%.3&quot;1Vk..D2{..v
00000450: 97 C8 47 29 3C 9E 59 9A C0 83 48 A0 55 CB C8 D6    .HG)&lt;.Y.@.H.UKHV
00000460: 36 92 CC 54 A7 00 E3 28 9E 99 45 B2 E5 7E 88 A7    6.LT'.c(..E2e~.'
00000470: 28 4E CA 75 17 3C D3 B5 6C F5 FD AC 05 55 BF F7    (NJu.&lt;S5lu},.U?w
00000480: 98 61 92 30 D8 0F 0E A5 DD 61 4D 80 27 5B A7 68    .a.0X..%]aM.'['h
00000490: E5 B9 C2 B8 EE 31 F6 63 29 37 C5 C9 11 39 90 8D    e9B8n1vc)7EI.9..
000004a0: D8 00 35 F4 7A 2D 79 D0 6A BB 9C 98 E4 41 CF 3F    X.5tz-yPj;..dAO?
000004b0: DE 9D 8B BF 04 69 1D BC 5C E7 E1 F2 49 01 8D F5    ^..?.i.&lt;\garI..u
000004c0: 41 3E 3F FB AE 54 B2 D9 F2 A0 E8 0A F7 59 47 77    A&gt;?{.T2Yr.h.wYGw
000004d0: 3C 19 C8 7B 81 9B 17 19 E9 81 A0 36 AD C6 62 71    &lt;.H{....i..6-Fbq
000004e0: DB 68 72 8F 6A 37 45 D9 0E 6E DC 2C 5E 52 C2 75    [hr.j7EY.n\,^RBu
000004f0: 51 2F F9 CE 8A 10 12 E9 C8 68 A9 D6 A6 D7 5B 14    Q/yN...iHh)V&amp;W[.
00000500: 11 51 42 FD BE B5 09 56 7F 19 C3 EB A7 A6 DF 6C    .QB}&gt;5.V..Ck'&amp;_l
00000510: 55 A3 11 DC EF 81 C3 CD DD 63 BF 38 F8 5A 4A 45    U#.\o.CM]c?8xZJE
00000520: 33 24 7B A4 55 B3 85 A6 87 75 3B 85 51 5C 03 B7    3${$U3.&amp;.u;.Q\.7

UPDATE TO THE CODE
1st Packet find here
2nd Packet find here
I have updated the code as per one of the comment to read only MDAT box to retrieve H264 stream from the incoming bytes[] through the socket, now I send only MDAT box contents (next byte after MDAT box)
public Map.Entry&lt;Boolean, List&lt;Integer&gt;&gt; hasMdat(byte[] byteArray) {
    for (int i = 0; i &lt; byteArray.length - 3; i++) {
        if (byteArray[i] == (byte) 109 &amp;&amp;
                byteArray[i + 1] == (byte) 100 &amp;&amp;
                byteArray[i + 2] == (byte) 97 &amp;&amp;
                byteArray[i + 3] == (byte) 116) {

            return Map.entry(true, Arrays.asList(i, i + 1, i + 2, i + 3));
        }
    }
    return Map.entry(false, List.of(0));
}

This is my code which handles the byte stream
initSocketConnection(new VideoStreamCallback() {
        @Override
        public void onVideoStreamReceived(byte[] bytes) {
           
Map.Entry&lt;Boolean, List&lt;Integer&gt;&gt; b = hasMdat(bytes);
        if (b.getKey()) {
            byte[] b1 = Arrays.copyOfRange(bytes, b.getValue().get(3) + 1, bytes.length);
  //write b1 back to client using spring SSE
            
        }

        }
    });

","
&quot;I have incoming byte streams probably encoded in H264 from a Genetec
camera...&quot; &quot;I need to decode the incoming H264 streams to transmit the video to my frontend...&quot;

Note: If your &quot;frontend&quot; playback system can play MP4 then you already have a playable file as given. There is no need to extract H.264 bytes from within MP4 bytes, or convert bytes to AnnexB format, or adding Start-codes, or skipping audio frames, etc. Simply remove the starting Genetec header and the rest of data is a playable MP4.
This answer below is for when the playback system accepts chunked MP4 data (H.264 contained in MP4).
If the player expects actual raw H.264 frames then for extraction: see the Answer by Markus Schumann.
 ## Shortest version: 
From each packet:

At offset [49] read a skippable &quot;Size&quot; from the 32-bit integer (is in Big-Endian format).
MP4 data begins at int mp4_data_pos = (49 + skipSize);.
Extract MP4 data parts only, and use them for testing as video playback.
In MP4 data: [moov] is metadata and then chunks are [moof+mdat] ... [moof+mdat].
Find moov as: 0x6D6F6F76, find moof as: 0x6D6F6F66, find mdat as: 0x6D646174.
Get a size of each atom by reading a 32-bit integer from the previous four bytes. (eg: int size_mdat = read_32_bits_from( pos_mdat - 4); since one array slot holds 8 bits).
First [moof + mdat] chunk after the metadata part must contain a keyframe for display.
Find keyframe through checking for a frame type 5 with:
int frame_type = ( byteArray[ mdat_pos + 8 ] &amp; 0x1F );
(NB 1: This will only find a keyframe if it's the first frame in a chunk (eg: a [moof + mdat] package).
(NB 2: It's possible your Genetec device outputs a keyframe later (eg: every 25th frame) or else maybe it only emits a keyframe at device start-up? Check for such an issue if not getting a keyframe.
If testing with an HTML5 video tag, then your codec setup is: avc1.640028.
If testing as file then save N-amount of &quot;MP4 data&quot; parts as one file (ie: file is the joined MP4 chunks).

 ## Short version: 
[ to add later: Image of header sections to skip + with 32-bit integers highlighted ]
In each packet...

Skip ahead, past the first 49 bytes.
At offset 49, read a 32-bit integer (ie: to update some &quot;skipSize&quot; variable).
Skip ahead (past some Object bytes) by the new amount from skipSize variable.
MP4 data begins here (eg: mp4_begins_pos = (49 + skipSize);.

Some notes about MP4 data in packets:

MP4 data is up to end of packet, and may have a larger size than the current packet size.
MP4 data which is larger than the packet's own size will continue in the next packet.
in bytes: moov atom has required MP4 metadata, and moof atom is a playable MP4 chunk.
in bytes: an MP4 atom begins with a SIZE (32-bits or 4 bytes), followed by atom NAME.
in bytes: Copy all atoms (in order of appearance) by their SIZEs into a new bytes array. This array can be saved as a file or sent to a decoder for playback.

(Option A) For online playback (using HTML5 video tag): Send the array of &quot;MP4 data&quot; parts only to your front-end.
(meaning all packet bytes minus the first 49 bytes and also minus the Object bytes (by its size)).
Web playback means choosing either: 

Using MediaSource Extensions API to manually feed received chunks to the browser's MPEG decoder.

Using a server-side script as tag's src, where the script &quot;pipes&quot; the MP4 data back into the video tag.

Pre-saving the chunks on server as they arrive, then using HLS Live Playlist format to serve frontend clients.


From the STSD atom I can see you have: H.264 &quot;High&quot; profile @ level 4.0 (or avc1.640028).
type= video/mp4; codecs= &quot;avc1.640028&quot;;

(Option B) To save as a file for quick testing: Concat (join) your required N-amount of &quot;MP4 data&quot; parts into one long array, then save that array as a file. Test the new MP4 file from your storage folder in a player like VLC Media Player.
 ## Long version: 
The format of Genetec's MP4 header is easy enough to understand:
(1) Each packet begins with some 49 bytes of skippable values.

(a) Each packet's first byte value is 0x01.

Confirm with a if (byteArray[0] == 1) { /* is OK packet */ }.
Else assume there is some packet corruption. Skip/ignore such packets.


(b) Followed by 32-int for size of data, and then data itself: is type String (eg: H264).

example packet-1 bytes: 00 00 00 04 (=4) then 48 32 36 34 (=H264)


(c) Followed by 32-int size for possible GUUID (seems to be enough hex values for 16 bytes).

example packet-1 bytes: 00 00 00 24 (=36) then XXXX-XX-XX-etc style hex values.



This above data seems to be always 49 bytes. Confirm by checking other packets, (is there a pattern of 49 bytes that are: a string of &quot;H264&quot; followed by a sequence of &quot;XXXX-XX-XX-etc&quot; style values?).

Skip past these 49 bytes (of Genetec's custom header bytes).
Skip range is [byte 0] ... [byte 48], since these bytes are not needed for playback.

(2) After skipping, there is an Object (added by Genetec) of side metadata (eg: {&quot;IsAudio&quot;:false} ).

(a) From offset 49 onwards, read a 32-bit integer for the size (bytes length) of the Object.

example packet-1 bytes: 00 00 00 C8 == is size of 36 bytes


(b) Skip the Object bytes by using the found size (to land at example position: offset N).

These custom Object's bytes are not needed for playback.



(3) At offset N, the actual MP4 data begins.
Here is a starting example code to check the packet for MP4 data size to extract for playback.
Use the read_int32 function that I've provided to get a 32-bit integer from some Array position.
import java.util.Arrays;

public class Main 
{
    //# Vars for MP4 data
    public static int size_MP4_data = 0;
    public static int size_expected_MP4_data = 0;
    public static int size_received_MP4_data = 0;
    public static int offset_MP4_data = 0;
    
    public static boolean need_more_MP4_data = false;
     
    public static void main(String[] args) 
    {
        //# Example Array to represent a received input Packet
        //# Array contents are from the first 320 bytes of your first example packet
        //# See full bytes at: https://pastebin.com/embed_js/3Ca8ZDFk 
        int[] bytes_Packet =    {

                                    0x01, 0x00, 0x00, 0x00, 0x04, 0x48, 0x32, 0x36, 0x34, 0x00, 0x00, 0x00, 0x24, 0x39, 0x33, 0x65, 
                                    0x63, 0x35, 0x39, 0x31, 0x30, 0x2D, 0x65, 0x65, 0x35, 0x38, 0x2D, 0x34, 0x39, 0x37, 0x32, 0x2D, 
                                    0x61, 0x30, 0x66, 0x66, 0x2D, 0x32, 0x65, 0x62, 0x33, 0x61, 0x33, 0x61, 0x34, 0x32, 0x66, 0x35, 
                                    0x66, 0x00, 0x00, 0x00, 0xC8, 0x7B, 0x22, 0x49, 0x73, 0x49, 0x6E, 0x69, 0x74, 0x22, 0x3A, 0x74, 
                                    0x72, 0x75, 0x65, 0x2C, 0x22, 0x49, 0x73, 0x41, 0x75, 0x64, 0x69, 0x6F, 0x22, 0x3A, 0x66, 0x61, 
                                    0x6C, 0x73, 0x65, 0x2C, 0x22, 0x54, 0x6F, 0x74, 0x61, 0x6C, 0x53, 0x65, 0x63, 0x6F, 0x6E, 0x64, 
                                    0x73, 0x22, 0x3A, 0x30, 0x2E, 0x30, 0x2C, 0x22, 0x46, 0x72, 0x61, 0x6D, 0x65, 0x54, 0x69, 0x6D, 
                                    0x65, 0x22, 0x3A, 0x22, 0x32, 0x30, 0x32, 0x33, 0x2D, 0x30, 0x32, 0x2D, 0x32, 0x35, 0x54, 0x31, 
                                    0x36, 0x3A, 0x35, 0x30, 0x3A, 0x32, 0x37, 0x2E, 0x32, 0x36, 0x31, 0x5A, 0x22, 0x2C, 0x22, 0x53, 
                                    0x65, 0x71, 0x75, 0x65, 0x6E, 0x63, 0x65, 0x49, 0x64, 0x22, 0x3A, 0x31, 0x2C, 0x22, 0x42, 0x61, 
                                    0x73, 0x65, 0x44, 0x65, 0x63, 0x6F, 0x64, 0x65, 0x54, 0x69, 0x6D, 0x65, 0x22, 0x3A, 0x30, 0x2C, 
                                    0x22, 0x4D, 0x65, 0x64, 0x69, 0x61, 0x54, 0x69, 0x6D, 0x65, 0x22, 0x3A, 0x30, 0x2C, 0x22, 0x49, 
                                    0x73, 0x46, 0x72, 0x61, 0x6D, 0x65, 0x48, 0x69, 0x64, 0x64, 0x65, 0x6E, 0x22, 0x3A, 0x66, 0x61,
                                    0x6C, 0x73, 0x65, 0x2C, 0x22, 0x49, 0x73, 0x4B, 0x65, 0x79, 0x46, 0x72, 0x61, 0x6D, 0x65, 0x22, 
                                    0x3A, 0x66, 0x61, 0x6C, 0x73, 0x65, 0x2C, 0x22, 0x49, 0x64, 0x22, 0x3A, 0x30, 0x2C, 0x22, 0x47,
                                    0x65, 0x6E, 0x65, 0x72, 0x61, 0x74, 0x69, 0x6F, 0x6E, 0x22, 0x3A, 0x31, 0x7D, 0x00, 0x00, 0x02,
                                    0xC4, 0x00, 0x00, 0x00, 0x1C, 0x66, 0x74, 0x79, 0x70, 0x64, 0x61, 0x73, 0x68, 0x00, 0x00, 0x00,
                                    0x00, 0x69, 0x73, 0x6F, 0x6D, 0x64, 0x61, 0x73, 0x68, 0x6D, 0x70, 0x34, 0x31, 0x00, 0x00, 0x02, 
                                    0xA8, 0x6D, 0x6F, 0x6F, 0x76, 0x00, 0x00, 0x00, 0x78, 0x6D, 0x76, 0x68, 0x64, 0x01, 0x00, 0x00, 
                                    0x00, 0x00, 0x00, 0x00, 0x00, 0xE0, 0x1F, 0xEC, 0xD3, 0x00, 0x00, 0x00, 0x00, 0xE0, 0x1F, 0xEC
                                  
                                };

        //# Process the packet to extract MP4 data
        int[] data_MP4 = process_Packet( bytes_Packet ); //# returns a trimmed array
    }
    
    static int[] process_Packet( int[] input ) 
    {
        //# (optional) Confirm function code is running ....
        System.out.println( &quot;## Checking received Packet byte values .... &quot; );
        
        //# NOTE: 
        //# &quot;size_header_Genetec&quot; is the mentioned &quot;skipSize&quot; variable equivalent.
        //# it stores how much bytes length (size) to skip past to reach MP4 data.
        
        int temp_int = 0; //# temp number for counting
        int size_total_packet = input.length; //# using size of &quot;input&quot; packet given to this function
        int size_header_Genetec = 0;
        
        //# first check if this packet's MP4 data needs to be added to another previous packet's data to make a full (uncorrupt) chunk.
        if( need_more_MP4_data == true)
        {
            //## to fix later (if needed)
            //## solution: extract needed remainder and append to an existing array
        }
        
        ///////////////////////////////
        //### Phase 1: Find MP4 attoms
        ///////////////////////////////
        
        //# Account for starting &quot;0x01&quot; byte
        size_header_Genetec = 1;
        
        //# since the size is increased with a &quot;+=&quot; we can re-use
        //# the newly increased &quot;size_header_Genetec&quot; value.
        
        //# get next size (usually String of 4 letters: &quot;H264&quot;)
        temp_int = read_int32( input, size_header_Genetec );
        size_header_Genetec += (temp_int + 4);
        
        //# get next size (usually a GUUID of hex values)
        temp_int = read_int32( input, size_header_Genetec );
        size_header_Genetec += (temp_int + 4);
        
        //# get next size (usually an Object of metadata values)
        temp_int = read_int32( input, size_header_Genetec );
        size_header_Genetec += (temp_int + 4);
        
        //# skip next 4 bytes
        size_header_Genetec += 4;
       
        //# Update offset for later use ...
        offset_MP4_data = size_header_Genetec;
        
        //# final check before next steps
        System.out.println( &quot;- MP4 data begins at offset: [&quot; + offset_MP4_data + &quot;] until end of this packet&quot; );
        
        ////////////////////////////////
        //### Phase 2: Handle MP4 atoms
        ////////////////////////////////
        
        //# check atom NAME
        temp_int = read_int32( input, offset_MP4_data + 4 );
        
        if( 
            //# IF atom &quot;name&quot; is one of these (as expected from your packet MP4 structure)...
            ( temp_int == 0x66747970 )    //# is &quot;ftyp&quot;
            || ( temp_int == 0x6D6F6F76 ) //# is &quot;moov&quot;
            || ( temp_int == 0x6D6F6F66 ) //# is &quot;moof&quot;
            || ( temp_int == 0x6D646174 ) //# is &quot;mdat&quot;
            
        )
        {
            //# THEN check all MP4 atoms in this packet for total size of MP4 data
            //# do this by adding together all the atom sizes
            //# note: if size is bigger than packet, then store data for completion with next packet
            
            //# add sizes of MP4 atoms...
            
            temp_int = offset_MP4_data;
                
            while(true)
            {
                size_expected_MP4_data += read_int32( input, temp_int );
                temp_int = ( offset_MP4_data + size_expected_MP4_data );
                
                //# avoid reading past end of packet
                if( size_expected_MP4_data &gt;= ( size_total_packet - offset_MP4_data ) )
                {
                    break;
                }
            }
        
        }
        
        /////////////////////////////////////////////////////////////////////////////
        //# confirm MP4 data positions are correct (double-check by a hex view of same bytes)...
        System.out.println( &quot;&gt;&gt; MP4 data offset: &quot; + offset_MP4_data );
        System.out.println( &quot;&gt;&gt; MP4 data length is : &quot; + size_expected_MP4_data );
        /////////////////////////////////////////////////////////////////////////////
        
        ///////////////////////////////
        //# Phase 3: Copy the MP4 data
        ///////////////////////////////
        
        int copyStartPos = offset_MP4_data;
        int copyEndPos = (size_total_packet-1);
        
        //# prepare for when getting the next new packet
        
        if( size_expected_MP4_data &gt; ( size_total_packet - offset_MP4_data ) ) { need_more_MP4_data = true; }
        
        if( need_more_MP4_data == false )
        {
            //# reset the count for this new MP4 chunk
            //size_received_MP4_data = 0;
            //size_expected_MP4_data = 0;
        }
        
        //# slice the Array to keep only the MP4 data parts
        return ( Arrays.copyOfRange( input , copyStartPos, copyEndPos) );
        
    }
    
    //# function expects:    
    //# &quot;input&quot; = array to search, 
    //# &quot;pos&quot; = start position of reading a 4-byte sequence 
    static int read_int32( int[] input, int pos ) 
    {
        
        //# integer to hold combined 4-byte values as one result/number
        int temp_int = 0;
        
        //# join the four byte values into &quot;temp_int&quot;
        temp_int = ( input[ pos+0 ] &lt;&lt; 24 );
        temp_int |= ( input[ pos+1 ] &lt;&lt; 16 );
        temp_int |= ( input[ pos+2 ] &lt;&lt; 8 );
        temp_int |= ( input[ pos+3 ] &lt;&lt; 0 );

        return ( temp_int ); 
        
    }
        
}

Options for stream playback:
(a) If your player expects MP4 data:

Your data is already playable in most video players
Send all data of each packet, after skipping past the first 49 bytes and also the { ... } Object bytes.

(b) If your player expects raw H.264 data (ie: It does not play the MP4 data, only H.264 data):

Then you will have to extract each H.264 video frame from inside the MP4 data.
Your H.264 data is in AVCC format (.mp4), which means each frame comes with a &quot;size in bytes&quot; value.
Most players will expect raw H.264 to be in AnnexB format (.h264). Replace all four bytes of frame's size with this four byte sequence 00 00 00 01.
(correctly: Use 00 00 00 01 for SPS, PPS and Keyframes, then 00 00 01 for  other frames eg: P-frames).

"
"Is there any algorithm that search a pattern with some 0s in a target array which any number on 0s in the pattern does not affect determining the result?
This question is similiar to 2D pattern search algorithms but the links provided are not accessible.
Given a m*n array T and u*v array P, u â‰¤ m, v â‰¤ n, 0 â‰¤ P[i][j] &lt; q, where q is a positive integer.
0s in P can be an arbirtary integer in T if P lies in T. For example:
q = 10
P[3][3] = {{2, 3, 0},
           {0, 1, 5}
            9, 0, 2}}
T[5][5] = {{2, 3, 4, 3, 6},
           {4, 1, 5, 7, 8},
           {9, 1, 2, 3, 1},
           {2, 4, 5, 1, 5},
           {3, 1, 9, 0, 2}}

The algorithm that I'm seeking should give (0,0) and (2,2) since the pattern is found and any number in T lies on 0 in P does not affect the output.
I've come across Rabin-Karp algorithm but the 0s are taken into account.
Implementation in Java would be great. Other languages will also do.
Any help would be appreciated.
","The structure of this problem resembles 2D convolution: a kernel is applied at every possible position to a 2D array, some value is calculated based on corresponding elements (equality) and these values are further processed (filtering out incomplete matches). And 2D convolution indeed gives a solution to this problem in  time for any input. (this complexity is a simplification reasonable in practice, more in the last section)
Reduction to bivariate polynomial multiplication over 
Let's define polynomials describing the array to be searched in and the pattern:


P[][] is essentially rotated 180°. Their product is:

What is the coefficient of  equal to? It's a sum of u * v products between P[][] and T[][] elements, where T[i][j] is multiplied with P[i][j] with 0 &lt;= i &lt; u, 0 &lt;= j &lt; v.
Similarly for all 0 &lt;= k &lt;= m - u, 0 &lt;= l &lt;= n - v, the coefficient of  is equal to the sum of u * v products of T[][] and P[][] elements where T[i+k][j+l] is multiplied with P[i][j] with 0 &lt;= i &lt; u, 0 &lt;= j &lt; v.
For other k, l we have sums of products for incomplete overlaps that we don't care about.
If T[][] matches P[][] at some position, we know exactly what the corresponding coefficient will be: it's a sum of squares of P[][] elements, since all non-zero elements have to match others equal to them and it doesn't matter what were zeros matched to. Converse is, of course, not guaranteed: if some coefficient is equal to this value, it might not correspond to a match. Let's ignore this for now, at least we can expect to reduce the number of positions to verify matches at.
For implementation purposes, this convolution can be done over  instead of : all coefficients will be calculated modulo prime number significantly greater than m * n (and greater than q, but we will eliminate this restriction later).
Implementing bivariate polynomial multiplication
There's a standard trick to reduce multidimensional convolution to 1D — helix transform, in other words index projection. In our case we will map  to . Then after performing 1D convolution, the inner index j will not overflow into the outer index and can be separated.
The algorithm for performing 1D convolution over  is based on the Number Theoretic Transform, almost identical to the more popular Fast Fourier Transform, except instead of a primitive root of unity in complex numbers a primitive root modulo p is used.
Increasing robustness
Our current algorithm may generate false positives when sum of products just happens to be equal to what we expect modulo p, and in theory values in T[][] and P[][] can be such that this effect becomes disastrous. Let's remap these values to others, random &quot;keys&quot; in 1..p-1, keeping zeros in P[][] intact. The key for each distinct value will be sampled independently for T[][] and P[][], different values can even happen to be mapped to the same key (with very low probability). This will simplify further analysis.
Such remapping reduces false positive rate per pattern position to 1 / p, rigorously — to 2 / p or less for every input. The sum of products at a position differs from the target sum by , where a counts the difference between the number of cells where tkey_i and pkey_j were matched and were supposed to be matched (making corresponding base values in T[][] and P[][] equal). If we vary any imperfectly matched pkey_j through 1..p-1, the difference will vary through p-1 distinct values, unless the sum of its multipliers, a_ij * tkey_i, is zero, which has probability slightly less than 1 / p as a sum of independent variables.
Obtaining results and detailed complexity
p can be taken sufficiently big to make the total false positive rate through all positions arbitrarily small, the Chinese Remainder Theorem can be used to combine results modulo different p. So it isn't strictly necessary to verify matches, giving the stated complexity, , to obtain correct results with confidence level of, say, (1 - 1e-38)^(n * m) ~= 1 - 1e-38 * n * m ~= 100%.
Alternatively all probable matches can be verified, giving complexity  where c is the amount of true matches, as long as  is maintained.
C++ implementation
#include &lt;algorithm&gt;
#include &lt;cassert&gt;
#include &lt;chrono&gt;
#include &lt;iostream&gt;
#include &lt;random&gt;
#include &lt;unordered_map&gt;
#include &lt;vector&gt;

using Matrix = std::vector&lt;std::vector&lt;uint32_t&gt;&gt;;

namespace Impl {

template &lt;uint32_t mod&gt;
struct ModEnv {
  static constexpr uint32_t prod(uint32_t a, uint32_t b) {
    return uint64_t{a} * b % mod;
  }

  static constexpr uint32_t sum(uint32_t a, uint32_t b) {
    return a + b - (b &gt;= mod - a ? mod : 0);
  }

  static constexpr uint32_t dif(uint32_t a, uint32_t b) {
    return a - b + (a &lt; b ? mod : 0);
  }

  static constexpr uint32_t binpow(uint32_t x, uint32_t p) {
    uint32_t res = 1;
    while (p) {
      if (p % 2) {
        res = prod(res, x);
      }
      x = prod(x, x);
      p /= 2;
    }
    return res;
  }

  static void ntt(std::vector&lt;uint32_t&gt;&amp; v, bool inverse) {
    constexpr uint32_t root = 5555;
    int n = v.size();
    for (int i = 1, j = 0; i &lt; n; ++i) {
      int bit = n;
      do {
        bit /= 2;
        j ^= bit;
      } while (!(j &amp; bit));
      if (i &lt; j) {
        std::swap(v[i], v[j]);
      }
    }
    for (int len = 2; len &lt;= n; len *= 2) {
      uint32_t d = mod / len;
      uint32_t pw = binpow(root, inverse ? mod - 1 - d : d);
      for (int i = 0; i &lt; n; i += len) {
        int half = len / 2;
        uint32_t w = 1;
        for (int j = 0; j &lt; half; ++j) {
          uint32_t s = v[i + j], t = prod(v[i + j + half], w);
          v[i + j] = sum(s, t);
          v[i + j + half] = dif(s, t);
          w = prod(w, pw);
        }
      }
    }
    if (inverse) {
      uint32_t inv = binpow(n, mod - 2);
      for (uint32_t&amp; x : v) {
        x = prod(x, inv);
      }
    }
  }

  static std::vector&lt;uint32_t&gt; conv1d(std::vector&lt;uint32_t&gt; a, std::vector&lt;uint32_t&gt; b) {
    int output_size = a.size() + b.size() - 1, pow2_size = 1;
    while (pow2_size &lt; output_size) {
      pow2_size *= 2;
    }
    a.resize(pow2_size);
    b.resize(pow2_size);
    ntt(a, false);
    ntt(b, false);
    for (int i = 0; i &lt; pow2_size; ++i) {
      a[i] = prod(a[i], b[i]);
    }
    ntt(a, true);
    a.resize(output_size);
    return a;
  }

  static Matrix conv2d_180(Matrix a, Matrix b) {
    int m = a.size(), n = a[0].size(), u = b.size(), v = b[0].size();
    int stride = n + v - 1;
    std::vector&lt;uint32_t&gt; b1d((u - 1) * stride + v);
    for (int i = 0; i &lt; u; ++i) {
      assert(b[i].size() == v);
      copy(b[i].rbegin(), b[i].rend(), b1d.begin() + (u - i - 1) * stride);
    }
    Matrix{}.swap(b);
    std::vector&lt;uint32_t&gt; a1d((m - 1) * stride + n);
    for (int i = 0; i &lt; m; ++i) {
      copy(a[i].begin(), a[i].end(), a1d.begin() + i * stride);
    }
    Matrix{}.swap(a);
    auto res1d = conv1d(std::move(a1d), std::move(b1d));
    int s = m - u + 1, t = n - v + 1;
    Matrix res(s, std::vector&lt;uint32_t&gt;(t));
    for (int i = 0; i &lt; s; ++i) {
      copy_n(res1d.begin() + (i + u - 1) * stride + v - 1, t, res[i].begin());
    }
    return res;
  }

  static std::vector&lt;std::pair&lt;int, int&gt;&gt; pattern_match(Matrix t, Matrix p) {
    std::unordered_map&lt;uint32_t, uint32_t&gt; t_map, p_map;
    std::mt19937 rng(std::chrono::duration_cast&lt;std::chrono::nanoseconds&gt;(
        std::chrono::high_resolution_clock::now().time_since_epoch()).count());
    std::uniform_int_distribution&lt;uint32_t&gt; d(1, mod - 1);
    for (auto&amp; row: t) {
      for (uint32_t&amp; val: row) {
        auto [it, f] = t_map.emplace(val, 0);
        if (f) {
          it-&gt;second = d(rng);
        }
        val = it-&gt;second;
      }
    }
    uint32_t expected_value = 0;
    for (auto&amp; row: p) {
      for (uint32_t&amp; val: row) {
        if (val == 0) {
          continue;
        }
        auto [it, f] = p_map.emplace(val, 0);
        if (f) {
          it-&gt;second = d(rng);
        }
        if (auto t_it = t_map.find(val); t_it != t_map.end()) {
          expected_value = sum(expected_value, prod(t_it-&gt;second, it-&gt;second));
        } else {
          // value in pattern doesn't exist in text
          return {};
        }
        val = it-&gt;second;
      }
    }
    auto conv = conv2d_180(std::move(t), std::move(p));
    std::vector&lt;std::pair&lt;int, int&gt;&gt; res;
    for (int i = 0; i &lt; conv.size(); ++i) {
      for (int j = 0; j &lt; conv[0].size(); ++j) {
        if (conv[i][j] == expected_value) {
          res.emplace_back(i, j);
        }
      }
    }
    return res;
  }
};

template &lt;uint32_t mod&gt;
bool can_use(const Matrix&amp; t, const Matrix&amp; p) {
  int m = t.size(), n = t[0].size(), u = p.size(), v = p[0].size();
  int stride = n + v - 1;
  int poly_size = (m - 1) * stride + n + (u - 1) * stride + v - 1, pow2_size = 1;
  while (pow2_size &lt; poly_size) {
    pow2_size *= 2;
  }
  return (mod - 1 &amp; 1 - mod) % pow2_size == 0;
}

void verify_matrix(const Matrix&amp; m) {
  assert(!m.empty() &amp;&amp; !m[0].empty() &amp;&amp; m.size() * m[0].size() &lt; 1 &lt;&lt; 30);
  for (int i = 1; i &lt; m.size(); ++i) {
    assert(m[i].size() == m[0].size());
  }
}

void verify_text_pattern(const Matrix&amp; t, const Matrix&amp; p) {
  verify_matrix(t);
  verify_matrix(p);
  assert(t.size() &gt;= p.size() &amp;&amp; t[0].size() &gt;= p[0].size());
}

constexpr uint32_t mod1 = (3 &lt;&lt; 30) + 1, mod2 = (13 &lt;&lt; 28) + 1;

}  // namespace Impl

std::vector&lt;std::pair&lt;int, int&gt;&gt; pattern_match_single_mod(Matrix t, Matrix p) {
  Impl::verify_text_pattern(t, p);
  if (Impl::can_use&lt;Impl::mod2&gt;(t, p)) {
    return Impl::ModEnv&lt;Impl::mod2&gt;::pattern_match(std::move(t), std::move(p));
  }
  assert(Impl::can_use&lt;Impl::mod1&gt;(t, p) &amp;&amp; &quot;text and pattern sizes are too big&quot;);
  return Impl::ModEnv&lt;Impl::mod1&gt;::pattern_match(std::move(t), std::move(p));
}

std::vector&lt;std::pair&lt;int, int&gt;&gt; pattern_match_two_mods(Matrix t, Matrix p) {
  Impl::verify_text_pattern(t, p);
  assert(Impl::can_use&lt;Impl::mod2&gt;(t, p) &amp;&amp; &quot;text and pattern sizes are too big&quot;);
  auto m1 = Impl::ModEnv&lt;Impl::mod1&gt;::pattern_match(t, p);
  auto m2 = Impl::ModEnv&lt;Impl::mod2&gt;::pattern_match(std::move(t), std::move(p));
  std::vector&lt;std::pair&lt;int, int&gt;&gt; m(m1.size() + m2.size());
  m.erase(set_intersection(m1.begin(), m1.end(), m2.begin(), m2.end(), m.begin()), m.end());
  return m;
}

std::vector&lt;std::pair&lt;int, int&gt;&gt; pattern_match_naive(const Matrix&amp; t, const Matrix&amp; p) {
  std::vector&lt;std::pair&lt;int, int&gt;&gt; res;
  for (int i = 0; i &lt; t.size() - p.size() + 1; ++i) {
    for (int j = 0; j &lt; t[0].size() - p[0].size() + 1; ++j) {
      bool ok = 1;
      for (int k = 0; k &lt; p.size() &amp;&amp; ok; ++k) {
        for (int l = 0; l &lt; p[0].size() &amp;&amp; ok; ++l) {
          ok &amp;= p[k][l] == 0 || p[k][l] == t[i + k][j + l];
        }
      }
      if (ok) {
        res.emplace_back(i, j);
      }
    }
  }
  return res;
}

void stress(int m, int n, int u, int v, double q, double prob_zero) {
  std::mt19937 rng(std::chrono::duration_cast&lt;std::chrono::nanoseconds&gt;(
      std::chrono::high_resolution_clock::now().time_since_epoch()).count());
  std::uniform_real_distribution&lt;double&gt; du(1, q);
  std::bernoulli_distribution db(prob_zero);
  while (1) {
    Matrix t(m, std::vector&lt;uint32_t&gt;(n));
    Matrix p(u, std::vector&lt;uint32_t&gt;(v));
    for (auto&amp; row: t) {
      for (auto&amp; elem: row) {
        elem = du(rng);
      }
    }
    for (auto&amp; row: p) {
      for (auto&amp; elem: row) {
        elem = db(rng) ? 0 : du(rng);
      }
    }
    int i = rng() % (m - u + 1), j = rng() % (n - v + 1);
    for (int k = 0; k &lt; u; ++k) {
      for (int l = 0; l &lt; v; ++l) {
        if (p[k][l] != 0) {
          t[i + k][j + l] = p[k][l];
        }
      }
    }
    auto naive = pattern_match_naive(t, p);
    assert(naive == pattern_match_single_mod(std::move(t), std::move(p)));
  }
}

int main() {
  auto ans = pattern_match_two_mods(
    {{2, 3, 4, 3, 6},
     {4, 1, 5, 7, 8},
     {9, 1, 2, 3, 1},
     {2, 4, 5, 1, 5},
     {3, 1, 9, 0, 2}},
    {{2, 3, 0},
     {0, 1, 5},
     {9, 0, 2}}
  );
  for (auto [i, j]: ans) {
    std::cout &lt;&lt; '(' &lt;&lt; i &lt;&lt; &quot;, &quot; &lt;&lt; j &lt;&lt; &quot;) &quot;;
  }
  std::cout &lt;&lt; '\n';
}

"
"I want to show data of all children inside all the Categories from the database (added the image below of what my database looks). I am adding data to my RecyclerView using the adapter which needs FirebaseRecyclerOptions object to be passed. I saw one answer where DataSnapshot was used to get child of child data, I tried to get the data using that and it showed me when I logged it in logcat (the commented code is what I tried using), but I do not know how to use that with my Adapter class.
This is what my database looks, I want the data inside of the highlighted fields:
{
    &quot;Category_wise&quot;: {
        &quot;education&quot;: {
            &quot;udemy&quot;: {     &lt;-Return data of this child
                &quot;companyName&quot;: &quot;Udemy&quot;,
                ...
            },
            &quot;khanacademy&quot;: {     &lt;-Return data of this child
                &quot;companyName&quot;: &quot;Khan Academy&quot;,
                ...
            }
        },
        &quot;technology&quot;: {
            &quot;google&quot;: {    &lt;-Return data of this child
                &quot;companyName&quot;: &quot;Google&quot;,
                ...
            },
            &quot;facebook&quot;: {    &lt;-Return data of this child
                &quot;companyName&quot;: &quot;Facebook&quot;,
                ...
            },
        ....
    }   
}

In the below code, SCard is my Model Class and SCardAdapter is my Adapter Class.
This is my Fragment (HomeFragment) where I'm adding data into recyclerview:
public class HomeFragment extends Fragment{
    private RecyclerView recyclerView;
    private Query query;
    private SCardAdapter&lt;SCard, SCardAdapter.ViewHolder&gt; adapter;

    public HomeFragment() {}

    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container,
                             Bundle savedInstanceState) {
        View v = inflater.inflate(R.layout.fragment_home, container, false);
        recyclerView = v.findViewById(R.id.search_recyclerview);
        LinearLayoutManager linearLayoutManager = new LinearLayoutManager(getContext());
        recyclerView.setLayoutManager(linearLayoutManager);
        recyclerView.setHasFixedSize(true);

        setQueryByOrder(&quot;technology&quot;, &quot;totalInvestors&quot;);
        fetchResult(query);

        return v;
    }

//    protected void fetchAll(){
//        final DatabaseReference reference= FirebaseDatabase.getInstance().getReference(&quot;Category_wise&quot;);
//        reference.addValueEventListener(new ValueEventListener() {
//            @Override
//            public void onDataChange(@NonNull DataSnapshot dataSnapshot) {
//                for (DataSnapshot snapshot : dataSnapshot.getChildren()){
//                    Log.i(TAG, &quot;4321: Name of each company: &quot; + Objects.requireNonNull(snapshot.child(&quot;companyName&quot;).getValue()).toString()
//                }
//            }
//            @Override
//            public void onCancelled(@NonNull DatabaseError databaseError) {}
//        });
//    }

    protected void setQueryByOrder(String choice, String order){
        query = FirebaseDatabase.getInstance()
                .getReference()
                .child(&quot;Category_wise&quot;).child(choice).orderByChild(order);
    }

    protected void fetchResult(Query query) {
        FirebaseRecyclerOptions&lt;SCard&gt; options =
                new FirebaseRecyclerOptions.Builder&lt;SCard&gt;()
                        .setQuery(query, new SnapshotParser&lt;SCard&gt;() {
                            @NonNull
                            @Override
                            public SCard parseSnapshot(@NonNull DataSnapshot snapshot) {
                                return new SCard(
                                        Objects.requireNonNull(snapshot.child(&quot;companyName&quot;).getValue()).toString()...);
                            }
                        })
                        .build();
        adapter = new SCardAdapter&lt;&gt;(options);
        adapter.startListening();
        recyclerView.setAdapter(adapter);
    }

    @Override
    public void onStart() {
        super.onStart();
        adapter.startListening();
    }

    @Override
    public void onStop() {
        super.onStop();
        adapter.stopListening();
    }
}

This is my Adapter Class:
public class SCardAdapter&lt;M extends SCard, V extends SCardAdapter.ViewHolder&gt; extends FirebaseRecyclerAdapter&lt;SCard, V&gt; {

    FirebaseRecyclerOptions&lt;SCard&gt; options;
    public SCardAdapter(@Nullable FirebaseRecyclerOptions&lt;SCard&gt; options) {
        super(options);
        this.options = options;
    }

    @Override
    protected void onBindViewHolder(V holder, @SuppressLint(&quot;RecyclerView&quot;) final int position, SCard model) {
        holder.setName(model.getsName());
      ...
    }

    @Override
    public V onCreateViewHolder(ViewGroup parent, int viewType) {
        View view = LayoutInflater.from(parent.getContext())
                .inflate(R.layout.startup_search_card, parent, false);
        return (V) new ViewHolder(view);
    }

    public class ViewHolder extends RecyclerView.ViewHolder {
        ImageView simg2;
        TextView sname, sdesc, senddate, sperraised, snoin, sminam;
        ProgressBar sraisingprogbar;
        public ViewHolder(View itemView) {
            super(itemView);
            sname = itemView.findViewById(R.id.sname);
          ...
        }

        public void setName(String string) {
            sname.setText(string);
        }
      ...
    }
}

","First of all, you need to create 2 loops since your json looks like that and store them inside an arrayList. You are suppose to get all the data there inside the arrayList.
FirebaseDatabase.getInstance().getReference().child(&quot;Category_wise&quot;).addValueEventListener(new ValueEventListener() {
    @Override
    public void onDataChange(@NonNull DataSnapshot dataSnapshot) {
        if (dataSnapshot.exists()) {
            ArrayList&lt;Scard&gt; sCardList = new ArrayList&lt;Scard&gt;();
            for (DataSnapshot snapshot : dataSnapshot.getChildren()) {
                if (snapshot.exists()){
                    for (DataSnapshot shot : snapshot.getChildren()){
                        if (shot.exists()){
                            final Scard scard = shot.getValue(Scard.class);
                            if (scard != null){
                                sCardList.add(scard);
                            }
                        }
                    }
                }
            }
            final ScardAdapter scardAdapter = new ScardAdapter(sCardList);
            recyclerView.setAdapter(scardAdapter);
        }
    }
    @Override
    public void onCancelled(@NonNull DatabaseError databaseError) {
    }
});

Next you need to create your own adapter. You can search on Google also for more information. But you will get the point here.
public class ScardAdapter extends RecyclerView.Adapter&lt;ScardViewHolder&gt; {
    private ArrayList&lt;Scard&gt; sCardList;
    public ScardAdapter(final ArrayList&lt;Scard&gt; sCardList) {
        this.sCardList = sCardList;
    }
    @NonNull
    @Override
    public ScardViewHolder onCreateViewHolder(@NonNull ViewGroup parent, int viewType) {
        View view = LayoutInflater.from(parent.getContext()).inflate(R.layout.scard_item, parent, false);
        return new ScardViewHolder(view);
    }
    @Override
    public void onBindViewHolder(@NonNull final ScardViewHolder holder, final int position) {
        final Scard model = sCardList.get(position);
        holder.getTvCompanyName.setText(model.getCompanyName);
    }
    @Override
    public int getItemCount() {
        return sCardList.size();
    }
}

"
"I came across a strange issue while working on troubleshooting an issue. In my Spring Boot application, I have a GET REST end-point which returns a POJO which looks like:
@GetMapping(value = &quot;/dto&quot;, produces = MediaType.APPLICATION_JSON_VALUE)
  public ResponseEntity&lt;ReportDto&gt; dto() {
    MultiValueMap&lt;String, String&gt; headers = new HttpHeaders();
    headers.add(&quot;Controller processed time&quot;,LocalDateTime.now().toString());
    return new ResponseEntity&lt;ReportDto&gt;(new ReportDto(), headers, HttpStatus.OK);
  }

I have an interceptor which looks like:
@Slf4j
@Component
public class AuditInterceptor implements HandlerInterceptor {
  @Override
  public void afterCompletion
      (HttpServletRequest request, HttpServletResponse response, Object
          handler, Exception exception) throws Exception {
    Thread.sleep(3000);
    log.info((LocalDateTime.now())+&quot; : Audit event stored&quot;);
  }

The Thread.sleep(3000) is to simulate the latency we face from audit event store periodically.
As per design, the audit event is supposed to be written after the response is committed. This is to avoid delay for the client in case of a delay in writing the event.
Surprisingly, the client is receiving the response only after 3 seconds (the added delay), which means the response is committed after the 'afterCompletion' method is executed. When the response type is String, the response is committed before afterCompletion is executed. I had tried the response type Integer, boolean and int as well. Except for String, all other types (the types I tried) are getting written only after the afterCompletion is executed.
I have tried different clients. The behavior for each of them are:

React with axios (both sync and async) - Response body and code are received after the added delay
Postman - Response code 200 is received immediately, but the body was received after the added delay
curl - Response printed immediately, but the connection closed only after the added delay

From the behavior observed from curl, it is clear that the delay is not in the Http Message Converter (Jackson by default in Spring Boot).
I have a workaround solution for my issue, which looks like:
  @GetMapping(value = &quot;/custom&quot;, produces = MediaType.APPLICATION_JSON_VALUE)
  public ResponseEntity&lt;String&gt; custom() throws JsonProcessingException {
    MultiValueMap&lt;String, String&gt; headers = new HttpHeaders();
    headers.add(&quot;Controller processed time&quot;,LocalDateTime.now().toString());
    ObjectMapper objectMapper = new ObjectMapper();
    return new ResponseEntity&lt;String&gt;(objectMapper.writeValueAsString(new ReportDto()), headers,
        HttpStatus.OK);
  }


I prefer to have the right solution for my problem. Appreciate if anyone can help me understand the root cause so that we can come up with the right solution.
Please check the demo code at https://github.com/pkurianjohn/InterceptorAfterCompletion.
Thank you.
","Upon further analysis, it was found that the getContentLength(Object, ContentType) method in AbstractHttpMessageConverter is unable to find the size of the object, hence unable to set the Content-Length header. As the number of bytes to be written is not available, the response will not be written till the output stream is closed, which happens after the after completion.
To fix the issue, registered a custom Http Message Converter extended from MappingJackson2HttpMessageConverter with the overridden method writeInternal(Object object, @Nullable Type type, HttpOutputMessage outputMessage).
The overridden method snippet is as below:
  @Override
  protected void writeInternal(Object object, @Nullable Type type, HttpOutputMessage outputMessage)
      throws IOException, HttpMessageNotWritableException {

    if( outputMessage.getHeaders().getContentLength() &lt; 0 ) {
      ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();

      HttpOutputMessage newOutputMessage = new HttpOutputMessage() {
        @Override
        public OutputStream getBody() {
          return byteArrayOutputStream;
        }

        @Override
        public HttpHeaders getHeaders() {
          return outputMessage.getHeaders();
        }
      };
      super.writeInternal(object, type, newOutputMessage);

      byteArrayOutputStream.flush();
      byteArrayOutputStream.close();


      HttpHeaders headers = outputMessage.getHeaders();
      headers.setContentLength(byteArrayOutputStream.toByteArray().length);
      StreamUtils.copy(byteArrayOutputStream.toByteArray(), outputMessage.getBody());
    } else {
      super.writeInternal(object, type, outputMessage);
    }
  }

The full demo code is available at https://github.com/pkurianjohn/InterceptorAfterCompletion/tree/content-length-fix.
"
"We are using Spring MVC ver 4.3 in one of the projects. The application supports i18n and Locale can be changed using url like http://appurl.com/?locale=en 
The problem is when someone sends random value for locale, it is not rejected, rather Spring sets Content-Language header value = that_random_str 
And since we are using CookieLocaleResolver, Spring also sets localeCookie value to that_random_str.
Is there any way where we can tell Spring to restrict locale values to only few Locales like only English(en) and Hindi(hi) are allowed and IllegalArgumentException shall be thrown for other values
","Couldn't find any standard solution to this so solved it with my own way.  I extended LocaleChangeInterceptor and overridden preHandle method.
public class CustomLocaleChangeInterceptor extends LocaleChangeInterceptor{

    private Set&lt;String&gt; validLanguages;
    
    public CustomLocaleChangeInterceptor() {
        super();
    }
    
    public Set&lt;String&gt; getValidLanguages() {
        return validLanguages;
    }

    public void setValidLanguages(Set&lt;String&gt; validLanguages) {
        this.validLanguages = validLanguages;
    }

    @Override
    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler)
            throws ServletException {

        String newLocale = request.getParameter(getParamName());
        if(newLocale!=null &amp;&amp; !&quot;&quot;.equals(newLocale)) {
            newLocale=newLocale.toLowerCase();
            if(!validLanguages.contains(newLocale)) {
                throw new IllegalArgumentException(&quot;Language/Locale not supported&quot;);
            }
        }
            
        return super.preHandle(request, response, handler);
    }
}


and in xml configuration:
&lt;interceptors&gt;
        &lt;beans:bean
            class=&quot;mypackage.CustomLocaleChangeInterceptor&quot;&gt;
            &lt;beans:property name=&quot;paramName&quot; value=&quot;language&quot; /&gt;
            &lt;beans:property name=&quot;validLanguages&quot;&gt;
                &lt;beans:set&gt;
                    &lt;beans:value&gt;en&lt;/beans:value&gt;
                    &lt;beans:value&gt;hi&lt;/beans:value&gt;
                &lt;/beans:set&gt;
            &lt;/beans:property&gt;
        &lt;/beans:bean&gt;
&lt;/interceptors&gt;

Though the invalid Locale doesn't cause any issue, the security team was persistent that Locale injection is possible so I needed to do this.
"
"I found a lot of answers about it on this site, but most of them are based on modifying the requirements or modifying the code of the parent class to do so.
Without discussing the requirements and modifying the code of the parent class, can we get its constructor and derive it through reflection and other means?
public class Parent {
    private Parent() {
    }
}

","Technically?
Well, we have to look at what prevents doing that:
You can't access a constructor that is not visible - loading a class that tries that would be rejected by the JVM.
Javac will always create a constructor - if you do not explicitly create one, it will create the default constructor. And it's a compile time error if the super class doesn't have a visible constructor without any arguments.
So Javac is out for now.
But what about creating the bytecode yourself?
Well, every constructor either needs to call a constructor of the super class or an other constructor of the same class.
We can't call the constructor of the parent class - because it's not visible.
And calling an other constructor of our class is also not useful - as the other constructor again needs to call an other constructor - which would result in a stack overflow.
But we could simply leave out the constructor.
The downside is - now we can't create any instance of our class.
But we have a subclass.
But is the constructor really not accessible by any other class?
Well - Java 11 introduced Nest-Based Access Controls.
A class in the same nest could access the private constructor.
But the list of nestmates is static - well, was, until Java 15.
Java 15 introduced Lookup.defineHiddenClass - which allows us to load a class as nestmate of an other class.
There is still no way to compile a subclass without changing Parent, so we create the bytecode by hand. In the end:
package test.se17;

import static java.lang.invoke.MethodType.methodType;
import static org.objectweb.asm.Opcodes.*;

import java.lang.invoke.MethodHandles;
import java.lang.invoke.MethodHandles.Lookup.ClassOption;

import org.objectweb.asm.ClassWriter;
import org.objectweb.asm.MethodVisitor;

public class InheritParent {
    
    private static final String PARENT = &quot;test/se17/Parent&quot;;
    
    public static void main(String[] args) throws Throwable {
        ClassWriter cw = new ClassWriter(ClassWriter.COMPUTE_MAXS | ClassWriter.COMPUTE_FRAMES);
        
        cw.visit(V17, ACC_PUBLIC, &quot;test/se17/Child&quot;, null, PARENT, null);
        
        MethodVisitor mv = cw.visitMethod(ACC_PUBLIC, &quot;&lt;init&gt;&quot;, &quot;()V&quot;, null, null);
        mv.visitCode();
        mv.visitVarInsn(ALOAD, 0);
        mv.visitMethodInsn(INVOKESPECIAL, PARENT, &quot;&lt;init&gt;&quot;, &quot;()V&quot;, false);
        mv.visitInsn(RETURN);
        mv.visitMaxs(0, 0);
        mv.visitEnd();
        
        cw.visitEnd();
        
        MethodHandles.Lookup lookup = MethodHandles.privateLookupIn(Parent.class, MethodHandles.lookup());
        MethodHandles.Lookup childLookup = lookup.defineHiddenClass(cw.toByteArray(), true, ClassOption.NESTMATE);
        
        Parent child = (Parent) childLookup.findConstructor(childLookup.lookupClass(), methodType(void.class)).asType(methodType(Parent.class)).invokeExact();
        System.out.println(child);
        System.out.println(child.getClass());
        System.out.println(child instanceof Parent);
    }
}

This will create, load and instantiate a subclass of Parent.
Note: In my code, Parent is in the package test.se17.
So, yes, it is technically possible to create a subclass of Parent.
Is it a good idea? Probably not.
"
"I have a private key as a String and I want to convert it to PrivateKey
String privateKey = &quot;Y2E3YjYwYzRjMDRjMjk1ZDQ5ZTQzM2RlMTdjZjVkNGE0NGFjYzJmM2IzOWExNWZhMjViNGE4ZWJiZDBiMDVkYTIwNGU4MWE3ZWZmMTQ0NGE2ZmM2NjExNzRmNTY4M2I0YmYyMTk5YTkyY2UzOWRkZjdmMzhkNTFjYTNmM2Q3ZDU&quot;;

byte[] pkcs8EncodedBytes = Base64.getDecoder().decode(privateKey);

PKCS8EncodedKeySpec keySpec = new PKCS8EncodedKeySpec(pkcs8EncodedBytes);

PrivateKey pkey = KeyFactory.getInstance(&quot;Ed25519&quot;) //NoSuchAlgorithmException
                            .generatePrivate(keySpec);

But I get this error:
java.security.NoSuchAlgorithmException: Ed25519 KeyFactory not available
at KeyFactory.getInstance(&quot;Ed25519&quot;)
I'm using Java-10
","The posted key is double encoded, first hex, then Base64. Note that the double encoding is not necessary. If the key is Base64 and hex decoded, the result is a 64 bytes key.
From this 64 bytes key the first 32 bytes are the secret key and the following 32 bytes are the public key. More details about this format can be found here.
Import, signing and verification with BouncyCastle are possible with Ed25519PrivateKeyParameters, Ed25519PublicKeyParameters and Ed25519Signer classes:
import java.nio.ByteBuffer;
import java.nio.charset.StandardCharsets;
import java.util.Base64;
import org.bouncycastle.crypto.Signer;
import org.bouncycastle.crypto.params.Ed25519PrivateKeyParameters;
import org.bouncycastle.crypto.params.Ed25519PublicKeyParameters;
import org.bouncycastle.crypto.signers.Ed25519Signer;
import org.bouncycastle.util.encoders.Hex;

// Base64, hex decode
String keyHexBase64 =&quot;Y2E3YjYwYzRjMDRjMjk1ZDQ5ZTQzM2RlMTdjZjVkNGE0NGFjYzJmM2IzOWExNWZhMjViNGE4ZWJiZDBiMDVkYTIwNGU4MWE3ZWZmMTQ0NGE2ZmM2NjExNzRmNTY4M2I0YmYyMTk5YTkyY2UzOWRkZjdmMzhkNTFjYTNmM2Q3ZDU&quot;;
byte[] key = Hex.decode(new String(Base64.getDecoder().decode(keyHexBase64), StandardCharsets.UTF_8));

// Separate secret and public key
ByteBuffer keyBuffer = ByteBuffer.wrap(key);
byte[] secretKey = new byte[32];
keyBuffer.get(secretKey);
byte[] publicKey = new byte[keyBuffer.remaining()];
keyBuffer.get(publicKey);

// Signing
byte[] message = &quot;The quick brown fox jumps over the lazy dog&quot;.getBytes(StandardCharsets.UTF_8);
Ed25519PrivateKeyParameters secretKeyParameters = new Ed25519PrivateKeyParameters(secretKey, 0);
Signer signer = new Ed25519Signer();
signer.init(true, secretKeyParameters);
signer.update(message, 0, message.length);
byte[] signature = signer.generateSignature();
System.out.println(&quot;Signature (hex): &quot; + Hex.toHexString(signature));

// Verification
Ed25519PublicKeyParameters publicKeyParameters = new Ed25519PublicKeyParameters(publicKey, 0);
Signer verifier = new Ed25519Signer();
verifier.init(false, publicKeyParameters);
verifier.update(message, 0, message.length);
boolean verified = verifier.verifySignature(signature); // Signature (hex): 2aa31bb14799a00ac1129bdd6773a8481f0fd7e829d59f6fccc81021bf21e397dc5d17362d342615a5500598542586cad8891f984bdb90ec0c80b48eb638df07
System.out.println(&quot;Verification: &quot; + verified); // Verification: true

Regarding BouncyCastle, bcprov-jdk15on is required, which can be loaded from the Maven repository or from the BouncyCastle website.
"
"I want to parse a proto file. Wanted to check is there any java library available which can parse proto files. Based on my requirement I cannot use descriptor parseFrom method or protoc command. Please suggest thanks in advance.
$ protoc --include_imports --descriptor_set_out temp *.proto // I don't want to do this manual step 
or 
DescriptorProtos.FileDescriptorProto descriptorProto = DescriptorProtos.FileDescriptorProto.parseFrom(proto.getBytes());


Appreciate suggestion thanks
","Possible solution: io.protostuff:protostuff-parser library
Let's consider the 3.1.38 version of the io.protostuff:protostuff-parser library as the current version.

Maven dependency: Maven Repository: io.protostuff » protostuff-parser » 3.1.38.
protostuff-parser Maven module directory in GitHub repository: protostuff-compiler/protostuff-parser at v3.1.38 · protostuff/protostuff-compiler.
GitHub repository: protostuff/protostuff-compiler at v3.1.38.

Example program
Please, consider the below example program as a draft to get started with the library.
Input file
Let's assume the /some/directory/data/test.proto file exist with the following content:
message SearchRequest {
  string query = 1;
  int32 page_number = 2;
  int32 result_per_page = 3;
  enum ContentType {
    WEB = 1;
    IMAGES = 2;
    VIDEO = 3;
  }
  ContentType content_type = 4;
}

pom.xml: Dependencies
&lt;project&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;io.protostuff&lt;/groupId&gt;
            &lt;artifactId&gt;protostuff-parser&lt;/artifactId&gt;
            &lt;version&gt;3.1.38&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.inject&lt;/groupId&gt;
            &lt;artifactId&gt;guice&lt;/artifactId&gt;
            &lt;version&gt;5.1.0&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
&lt;/project&gt;

Program
import com.google.inject.Guice;
import com.google.inject.Injector;
import io.protostuff.compiler.ParserModule;
import io.protostuff.compiler.model.Message;
import io.protostuff.compiler.model.Proto;
import io.protostuff.compiler.parser.Importer;
import io.protostuff.compiler.parser.LocalFileReader;
import io.protostuff.compiler.parser.ProtoContext;
import java.nio.file.Path;
import java.util.List;

public final class Program {
    public static void main(final String[] args) {
        final Injector injector = Guice.createInjector(new ParserModule());
        final Importer importer = injector.getInstance(Importer.class);
        final ProtoContext protoContext = importer.importFile(
            new LocalFileReader(Path.of(&quot;/some/directory/data&quot;)),
            &quot;test.proto&quot;
        );

        final Proto proto = protoContext.getProto();

        final List&lt;Message&gt; messages = proto.getMessages();
        System.out.println(String.format(&quot;Messages: %s&quot;, messages));

        final Message searchRequestMessage = proto.getMessage(&quot;SearchRequest&quot;);
        System.out.println(String.format(&quot;SearchRequest message: %s&quot;, searchRequestMessage));

        final List&lt;Enum&gt; searchRequestMessageEnums = searchRequestMessage.getEnums();
        System.out.println(String.format(&quot;SearchRequest message enums: %s&quot;, searchRequestMessageEnums));
    }
}

The program output:
Messages: [Message{name=SearchRequest, fullyQualifiedName=..SearchRequest, fields=[Field{name=query, typeName=string, tag=1, options=DynamicMessage{fields={}}}, Field{name=page_number, typeName=int32, tag=2, options=DynamicMessage{fields={}}}, Field{name=result_per_page, typeName=int32, tag=3, options=DynamicMessage{fields={}}}, Field{name=content_type, typeName=ContentType, tag=4, options=DynamicMessage{fields={}}}], enums=[Enum{name=ContentType, fullyQualifiedName=..SearchRequest.ContentType, constants=[EnumConstant{name=WEB, value=1, options=DynamicMessage{fields={}}}, EnumConstant{name=IMAGES, value=2, options=DynamicMessage{fields={}}}, EnumConstant{name=VIDEO, value=3, options=DynamicMessage{fields={}}}], options=DynamicMessage{fields={}}}], options=DynamicMessage{fields={}}}]
SearchRequest message: Message{name=SearchRequest, fullyQualifiedName=..SearchRequest, fields=[Field{name=query, typeName=string, tag=1, options=DynamicMessage{fields={}}}, Field{name=page_number, typeName=int32, tag=2, options=DynamicMessage{fields={}}}, Field{name=result_per_page, typeName=int32, tag=3, options=DynamicMessage{fields={}}}, Field{name=content_type, typeName=ContentType, tag=4, options=DynamicMessage{fields={}}}], enums=[Enum{name=ContentType, fullyQualifiedName=..SearchRequest.ContentType, constants=[EnumConstant{name=WEB, value=1, options=DynamicMessage{fields={}}}, EnumConstant{name=IMAGES, value=2, options=DynamicMessage{fields={}}}, EnumConstant{name=VIDEO, value=3, options=DynamicMessage{fields={}}}], options=DynamicMessage{fields={}}}], options=DynamicMessage{fields={}}}
SearchRequest message enums: [Enum{name=ContentType, fullyQualifiedName=..SearchRequest.ContentType, constants=[EnumConstant{name=WEB, value=1, options=DynamicMessage{fields={}}}, EnumConstant{name=IMAGES, value=2, options=DynamicMessage{fields={}}}, EnumConstant{name=VIDEO, value=3, options=DynamicMessage{fields={}}}], options=DynamicMessage{fields={}}}]

"
"I have a multithreaded Java application that uses ThreadLocal fields to keep the threads isolated from each other. As part of this application I also have a requirement to implement timeouts on certain functions to prevent DOS attacks.
I'm looking for a way to time out a Java function that is running in the current thread
I've seen plenty of solutions such as How to timeout a thread which will create a Future to execute some code, launch it in a new thread and and wait for it to complete. I want to make it work the other way round.
Consider the following code, which will be run in a multi-threaded environment:
class MyClass {
    // ThreadLocal is not private so callback can access it
    ThreadLocal&lt;AtomicInteger&gt; counter = ThreadLocal.withInitial(AtomicInteger::new);

    public void entry(Function&lt;?, ?&gt; callback) {
        counter.get().set(10);                      // Calling thread performs set up
        I_need_a_timeout(callback, 110);            // Call a function which might take a long time
        int result = counter.get().get();           // If there is no time out this will be 110
    }

    private void I_need_a_timeout(Function&lt;?, ?&gt; callback, int loop) {
        while (loop-- &gt;= 0) {
            counter.get().incrementAndGet();
            callback.apply(null);                   // This may take some time
        }
    }
}

I need to be able to terminate I_need_a_timeout if it runs for too long, but if I were to execute it in a future then it would have it's own thread and therefore it's own instance of AtomicInteger so the value read by the calling code would always be the value I initialise it to (in this case 10)
Update: I've updated the sample code to be closer to my real application. The client passes a function to I_need_a_timeout that could take any amount of time to return (or potentially may never return), so polling solutions won't work
","You can straight-forwardly create a future whose operation is executed in the current thread and cancel it from another thread. E.g.
public class CancelCurrentThread {
    public static void main(String[] args) {
        try(var canceler = Executors.newScheduledThreadPool(1)) {
            for(long timeout: List.of(500, 1500)) {
                FutureTask&lt;String&gt; task
                    = new FutureTask&lt;&gt;(CancelCurrentThread::operation);
                System.out.println(&quot;Timeout: &quot; + timeout + &quot;ms&quot;);
                canceler.schedule(
                    () -&gt; task.cancel(true), timeout, TimeUnit.MILLISECONDS);
                task.run();
                try {
                    System.out.println(&quot;Result: &quot; + task.get());
                }
                catch(CancellationException ex) {
                    System.out.println(&quot;Canceled&quot;);
                }
                catch(InterruptedException|ExecutionException e) {
                    System.out.println(&quot;Failed: &quot; + e);
                }
            }
        }
    }
  
    static String operation() throws InterruptedException {
        System.out.println(&quot;performing operation in &quot; + Thread.currentThread());
        Thread.sleep(1000);
        return &quot;result&quot;;
    }
}

Timeout: 500ms
performing operation in Thread[#1,main,5,main]
Canceled
Timeout: 1500ms
performing operation in Thread[#1,main,5,main]
Result: result

It’s important to keep in mind that the same restrictions as with futures from an ExecutorService apply. To return quickly upon cancelation, the operation must respond to interruption, either by using operations which abort on interruption (like the sleep in the example) or by polling Thread.interrupted() in reasonable intervals.
"
"i have an ETL pipeline with 5 steps. Each step can be executed on a different thread and different application.
That makes it really hard to pass down the otel trace context through everything because the internals are not available for me so all i have access to are the processings steps.
What iam looking for is a way to construct the trace context just from a single id which uniquely identifies a complete run through all the 5 steps of a single data entry.
Example:



Data package
step
thread
application




id_1234
1
thread_01
app_01


id_1234
2
thread_02
app_01


id_1234
3
thread_10
app_02


id_1234
4
thread_01
app_01


id_1234
5
thread_05
app_02


id_5555
1
thread_05
app_02


id_5555
2
thread_01
app_02


id_5555
3
thread_05
app_01


id_5555
4
thread_06
app_02


id_5555
5
thread_15
app_02



What iam looking for is code which works a little bit like this:
public class Step1 {
    public void execute(DataPackage obj){
         var otelContext = SpanContext.create(
              TraceId.fromBytes(obj.getUniqueId().getBytes()),
              SpanId.fromBytes(processorName.getBytes()),
              TraceFlags.getDefault(),
              TraceState.getDefault()
         );

         var wrap = Span.wrap(otelContext);

         var with = Context.root().with(wrap);
  
         var span = tracer.spanBuilder(&quot;Step1&quot;).setParent(with).startSpan();
         CompletableFuture.runAsync(() -&gt; { /* the code is here*/ }).whenComplete((c1, exception) -&gt; {
         if (exception != null) {
           span.recordException(exception);
         } else {
           span.end();
         }
       });
    }
}

What happens is that the start and the end of the span are separate and not under the same trace context. So something goes wrong here

Iam now trying to construct the context by hand:
    var paddedArray = new byte[16];

    var originalArray = context.getId().getBytes();
    System.arraycopy(originalArray, 0, paddedArray, 16 - originalArray.length, originalArray.length);

    var wrap = Span.wrap(SpanContext.createFromRemoteParent(
        TraceId.fromBytes(paddedArray),
        SpanId.fromBytes(paddedArray),
        TraceFlags.getDefault(),
        TraceState.getDefault())
    );
    var otelContext = Context.root().with(wrap);

    var startSpan = tracer.spanBuilder(context.getId())
        .setParent(otelContext)
        .startSpan();

The issue i had previously was that the input byte array was not of the correct length. Now i fixed that, but the next issue is that this is not appearing in jaeger at all.
I suspect its because the context didnt get created in jaeger because here it always assumes the context exists previously. Is there a way to &quot;upsert&quot; always a context?
I have really no way of knowing when a context should be created or not since etl pipeline 1 can run before etl pipeline 2 or vice versa
","I think the available APIs should be quite good at handling your problem.
Can you try the following code and let me know if it works for you?
Also have a look at the following:
https://opentelemetry.io/docs/languages/java/instrumentation/#get-the-current-span
import io.opentelemetry.api.trace.*;
import io.opentelemetry.context.Scope;
import io.opentelemetry.context.Context;
import java.util.concurrent.CompletableFuture;

public class EtlStep {
    private final Tracer tracer;

    public EtlStep(Tracer tracer) {
        this.tracer = tracer;
    }

    public void execute(DataPackage obj, int step) {
        // Retrieve the current span context if available
        SpanContext parentContext = Span.fromContext(Context.current()).getSpanContext();

        Span span = tracer.spanBuilder(&quot;Step&quot; + step)
            .setParent(Context.current().with(Span.wrap(parentContext)))
            .startSpan();

        try (Scope scope = span.makeCurrent()) {
            CompletableFuture.runAsync(() -&gt; {
                // Your code here
            }).whenComplete((unused, exception) -&gt; {
                if (exception != null) {
                    span.recordException(exception);
                }
                span.end();
            });
        }
    }
}

class DataPackage {
    private final String uniqueId;

    public DataPackage(String uniqueId) {
        this.uniqueId = uniqueId;
    }

    public String getUniqueId() {
        return uniqueId;
    }
}

And here is the test initialization (just a toy example):
// Create EtlStep instances
EtlStep step1 = new EtlStep(tracer);
EtlStep step2 = new EtlStep(tracer);
EtlStep step3 = new EtlStep(tracer);
EtlStep step4 = new EtlStep(tracer);
EtlStep step5 = new EtlStep(tracer);

// Create data packages
DataPackage dataPackage1 = new DataPackage(&quot;id_1234&quot;);
DataPackage dataPackage2 = new DataPackage(&quot;id_5555&quot;);

// Execute ETL steps for each data package
step1.execute(dataPackage1, 1);
step2.execute(dataPackage1, 2);
step3.execute(dataPackage1, 3);
step4.execute(dataPackage1, 4);
step5.execute(dataPackage1, 5);

"
"I am solving leetcode LRU design problem - Leetcode LRU:

Design a data structure that follows the constraints of a Least Recently Used (LRU) cache.
Implement the LRUCache class:

LRUCache(int capacity) Initialize the LRU cache with positive size capacity.
int get(int key) Return the value of the key if the key exists, otherwise return -1.
void put(int key, int value) Update the value of the key if the key exists. Otherwise, add the key-value pair to the cache. If the number of keys exceeds the capacity from this operation, evict the least recently used key.

The functions get and put must each run in O(1) average time complexity.

I designed it with using Queue and HashMap, and I was able to pass 20 out of 22 test cases. However, the remaining test cases are timing out.
On searching, I found that a doubly linked list is the best way to implement it. I am curious as why queue and hash map is timing out and why a doubly linked list is the best way to solve this.
Below is my implementation:
class LRUCache {
    int capacity=0;
    BlockingQueue&lt;Integer&gt; queue;
    Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;();

    public LRUCache(int capacity) {
        this.capacity = capacity;
        queue = new ArrayBlockingQueue&lt;Integer&gt;(capacity);
    }
    
    public int get(int key) {
        if(queue.contains(key)){
            queue.remove(key);
            queue.add(key);
            return map.get(key);
        }
        else
            return -1;
    }
    
    public void put(int key, int value) {
        if(queue.contains(key)){
            queue.remove(key);
            queue.add(key);
            map.put(key, value);
        }
        else if(queue.size()&lt;capacity){
            queue.add(key);
            map.put(key,value);
            
        }
        else{
            int oldKey = queue.remove();
            map.remove(oldKey);
            queue.add(key);
            map.put(key,value);
        }
    }
}

The result is as shown below:

","The method calls queue.remove(key) and queue.contains(key) do not have O(1) time complexity. See the documentation on ArrayBlockingQueue which mentions that this queue is &quot;...backed by an array&quot;, i.e. it needs to scan the array to locate a given value. This has O(𝑛) time complexity. This is enough reason not to use it for this challenge. On top of that, the operations use locking to avoid concurrency problems which make them even slower. The documentation on remove has:

remove
public boolean remove(Object o)
[...] Removal of interior elements in circular array based queues is an intrinsically slow and disruptive operation, so should be undertaken only in exceptional circumstances, ideally only when the queue is known not to be accessible by other threads.

Doubly linked list
You mention doubly linked lists: Java even has a doubly linked list implementation that is combined with a hash map: a LinkedHashMap.
The documentation mentions:

A special constructor is provided to create a linked hash map whose order of iteration is the order in which its entries were last accessed, from least-recently accessed to most-recently (access-order). This kind of map is well-suited to building LRU caches.

And that is exactly what you need here. The implementation could be:
class LRUCache extends LinkedHashMap&lt;Integer, Integer&gt; {
    int capacity;  
    
    public LRUCache(int capacity) {
        // Foresee one more than desired capacity, so no extension is needed
        // when we allow a temporary overrun before deleting the eldest entry
        super(capacity + 1, 1, true); // true will enable the LRU behavior
        this.capacity = capacity;
    }

    // This method is called internally by put, getOrDefault (and similar).
    //    See documentation
    protected boolean removeEldestEntry(Map.Entry&lt;Integer, Integer&gt; entry) {
        return this.size() &gt; this.capacity; // overrun detected: ask for removal
    }
    
    public int get(int key) {
        return getOrDefault(key, -1);
    }
}

"
"Can't figure out how to stop processing Flux on first match.
This what I have right now:
findAll(): Flux&lt;Object&gt;
findStorageId(Relation r): Mono&lt;Long&gt; | Mono.empty()
isPassing(Relation r): boolean

findAll().flatMap(p -&gt; {
  return Flux.fromStream(p.getRelations().stream()).flatMap(r -&gt; {
    return isPassing(r) ? findStorageId(r) : Mono.empty();
  });
})
.handle((Long storageId, SynchronousSink&lt;Long&gt; sink) -&gt; {
  if (storageId != null) {
    sink.next(storageId);
    sink.complete();
  }
})
.next()
.switchIfEmpty(Mono.error(new RuntimeException(&quot;Can't find storageId.&quot;)));

I'm trying to understand how I can interrupt processing of flux when first storageId is found. Right now I see, that first flatMap continues to work after finding first match.
","The problem is that flatmap is using using concurrency and prefetch is more than 1.
In this case if you dont want to call the database many times but one by one you need to use concatmap with prefatch 1.
  public static final String TO_BE_FOUND = &quot;B&quot;;

  @Override
  public void run(String... args) throws Exception {
    Mono&lt;String&gt; storageId =
        Flux.just(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;A&quot;)
            .doOnNext(id -&gt; System.out.printf(&quot;processing: %s\n&quot;, id))
            .concatMap(s -&gt; findStorageId(s),1)
            .next()
            .switchIfEmpty(Mono.error(new RuntimeException(&quot;Can't find storageId.&quot;)));
    storageId.subscribe();
  }

  private static Mono&lt;String&gt; findStorageId(String s) {
    return TO_BE_FOUND.equals(s)
        ? Mono.just(s + UUID.randomUUID()).delayElement(Duration.ofSeconds(1))
        : Mono.delay(Duration.ofSeconds(1)).flatMap(aLong -&gt; Mono.empty());
  }

in this case concatmap with prefetch 1 will request elements 1 by one and it will wait for the response.
"
"I'm trying to open the MainActivity when the user clicks a button in my notification, while the app is only running in the background with a service. When the button is clicked, these lines are triggered in the Service class:
Intent openApp = new Intent(this, MainActivity.class);
openApp.setFlags(Intent.FLAG_ACTIVITY_NEW_TASK);
startActivity(openApp);

I've checked it, and the lines are triggered, so there's no problem in reacting to the button's click, the Activity won't open though.
Any suggestions? Why isn't this working for me and how can I make it work?
Edit
I was asked for some more code, so in my onStartCommand() inside my Service, if it starts with a stop-action within its intent, I call the killService() method, which kills the Service, starts the MainActivity and do some other stuff:
if (action != null &amp;&amp; action.equals(ACTION_STOP_SERVICE)) {
    killService();
}

To set the Notifications button, I use this code:
Intent stopActionIntent = new Intent(this, TimerService.class);
        stopActionIntent.setAction(ACTION_STOP_SERVICE);
        PendingIntent stopActionPendingIntent = PendingIntent.getService(this, 1, stopActionIntent, PendingIntent.FLAG_IMMUTABLE);

timerNotificationBuilder.addAction(R.drawable.stop, &quot;Stop&quot;, stopActionPendingIntent);

And as I said, the button already reacts to the user clicking on it, so that's not the problem.
","You can try to receive the click in a BroadcastReceiver and then open activity from there.

Try this to add a action button o your notification:

timerNotificationBuilder.addAction(createNotificationActionButton(&quot;STOP&quot;);

Where the createNotificationActionButton method is this:
public NotificationCompat.Action createNotificationActionButton(String text){
        Intent intent = new Intent(this, StopwatchNotificationActionReceiver.class);

        @SuppressLint(&quot;InlinedApi&quot;) PendingIntent pendingIntent = PendingIntent.getBroadcast(this, new Random().nextInt(100), intent, PendingIntent.FLAG_IMMUTABLE);

        return new NotificationCompat.Action(0, text, pendingIntent);
    }


Create a class named StopwatchNotificationActionReceiver and make it extent a BroadcastReceiver. This is the code for that class:

import android.content.BroadcastReceiver;
import android.content.Context;
import android.content.Intent;
import android.util.Log;

public class StopwatchNotificationActionReceiver extends BroadcastReceiver {

    @Override
    public void onReceive(Context context, Intent intent) {
        PrefUtil.setIsRunningInBackground(context, false);
        PrefUtil.setTimerSecondsPassed(context, 0);
        PrefUtil.setWasTimerRunning(context, false);
        context.stopService(MainActivity.serviceIntent);
        Intent activityIntent = new Intent(context, MainActivity.class);
        activityIntent.setFlags(Intent.FLAG_ACTIVITY_NEW_TASK);
        context.startActvity(activityIntent);
    }
}

Also you need to register that receiver in your manifest like this:
&lt;receiver android:name=&quot;StopwatchNotificationActionReceiver&quot;/&gt;


Where the MainActivity.serviceIntent is a public static variable which looks like this:

public static Intent serviceIntent;

And this intent is only used to start the service like this:
//In onCreate
serviceIntent = new Intent(this, TimerService.class);

//In onPause
PrefUtil.setTimerSecondsPassed(this,seconds);
            if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.O) {
                startForegroundService(serviceIntent);
            }

Or you can try the simple method:
if (action != null &amp;&amp; action.equals(ACTION_STOP_SERVICE)) {
    Context context = this;
    Intent activityIntent = new Intent(context, MainActivity.class);
        activityIntent.setFlags(Intent.FLAG_ACTIVITY_NEW_TASK);
        context.startActvity(activityIntent);
    killService();
}

Edit

Another solution is here. Again. You need to refer to my repo as I have made changes to the files in order to complete your task.  In the service class, refer to this method. There, I start the activity if the action is reset(r). Or else, it opens the broadcast receiver. Then, in the activity, I receive that extra in the onResume() method. If the reset button is not clicked, it opens the Receiver class.
And as always, you can view the result of the app from here.
I hope that code will do your work.
"
"in HashMap when I pass List of Objects as Key I get different results.
List&lt;NewClass&gt; list1 = new ArrayList&lt;&gt;();
List&lt;NewClass&gt; list2 = new ArrayList&lt;&gt;();

NewClass obj1 = new NewClass(1, &quot;ddd&quot;, &quot;eee@gmail.com&quot;);
NewClass obj2 = new NewClass(2, &quot;ccc&quot;, &quot;kkk@gmail.com&quot;);

list1.add(obj1);
list1.add(obj2);

list2.add(obj1);
list2.add(obj2);

Map&lt;List&lt;NewClass&gt;, Integer&gt; mapClass = new HashMap&lt;&gt;();
mapClass.put(list1, 1234);
mapClass.put(list2, 4567);

System.out.println(mapClass.size());
System.out.println(mapClass.get(list1));

NewClass obj4 = new NewClass(1, &quot;ddd&quot;, &quot;eee@gmail.com&quot;);
NewClass obj5 = new NewClass(2, &quot;ccc&quot;, &quot;kkk@gmail.com&quot;);
List&lt;NewClass&gt; list3 = new ArrayList&lt;&gt;();
list3.add(obj4);
list3.add(obj5);

System.out.println(mapClass.get(list3));

System.out.println(list1.hashCode());
System.out.println(list2.hashCode());
System.out.println(list3.hashCode());

Below is the output I see
hashCode() called - Computed hash: -1704251796
hashCode() called - Computed hash: -587009612
hashCode() called - Computed hash: -1704251796
hashCode() called - Computed hash: -587009612
1
hashCode() called - Computed hash: -1704251796
hashCode() called - Computed hash: -587009612
4567
hashCode() called - Computed hash: -1704251796
hashCode() called - Computed hash: -587009612
**null**
hashCode() called - Computed hash: -1704251796
hashCode() called - Computed hash: -587009612
-1879206775
hashCode() called - Computed hash: -1704251796
hashCode() called - Computed hash: -587009612
-1879206775
hashCode() called - Computed hash: -1704251796
hashCode() called - Computed hash: -587009612
-1879206775

Even though hashcode is same for all the 3 lists, mapClass.get(list3) is retuning null. list3 has same object as list1 / list2. Why is this behaviour ?
","Code Problems
The problem with your code lies in many aspects:

Using lists as keys
Re-using the same references for more than one entry's key
The implementation of NewClass

As others have already pointed out, you're using ArrayList instances as keys, which is a bad design choice, since the hashcode of mutable objects is subject to change, introducing the risk of losing access to the paired values.
Furthermore, since the implementation of ArrayList.equals() and ArrayList.hashCode() is based on the equals() and hashCode() of its elements, you also need to make sure that their class implements the methods properly.
HashMap Implementation
The HashMap class is implemented as an array of buckets, where a bucket is either a linked list or a tree structure. When a key-value pair is added to a HashMap, the key's hashcode is mapped to a bucket's index, and then the entry is added to the bucket. This means that a bucket can contain multiple entries with different keys, since different objects can share the same hashcode (see the hashCode contract). Once a bucket has been selected, the HashMap verifies whether this bucket contains a pair whose key is equal to the key of the new entry. If it does, the pair with the matching key is updated with the new entry's value; if it doesn't, the new entry is simply added to the bucket.
Code Explanation &amp; Answer
As shown from the output above, after adding the second entry with a different list as its key, we can see that the map's size is still 1. This is because you've used the exact same references (obj1 and obj2) to construct both the first and second key. In this scenario, regardless of NewClass providing a definition for equals() and hashCode(), the two lists will always be equal and yield the same hashcode. This is because the hashcode of an ArrayList is computed from the hashcode of its elements (both lists contain the same references). While ArrayList.equals() relies on the Objects.equals method, which first compares the memory address of the two given references, and then invokes the equals() method of the first parameter. Hence, the second entry updating the first one, as explained in the section above.

Even though hashcode is same for all the 3 lists, mapClass.get(list3) is retuning null. list3 has same object as list1 / list2. Why is this behaviour?

Instead, when you're trying to retrieve with list3 the value associated to list1 or list2, you're getting null because NewClass does not provide a proper definition of the equals() and hashCode() methods. These methods should be defined on the same set of attributes as explained in the equals and hashCode contract.
Here, I've attached an implementation of NewClass which honors the equals() and hashCode() contract, and that allows the first entry's value to be returned when list3 is passed.
public class NewClass {
    int id;
    String s1, s2;

    public NewClass(int id, String s1, String s2) {
        this.id = id;
        this.s1 = s1;
        this.s2 = s2;
    }

    public int hashCode() {
        return Objects.hash(id, s1, s2);
    }

    @Override
    public boolean equals(Object obj) {
        if (obj == this) return true;
        if (obj == null || obj.getClass() != getClass()) return false;
        NewClass nc = (NewClass) obj;
        return nc.id == id &amp;&amp; Objects.equals(s1, nc.s1) &amp;&amp; Objects.equals(s2, nc.s2);
    }
}

Conclusions
In conclusion, as others have already said, you shouldn't be using mutable objects as keys for a HashMap. The changes applied to a key's internal state may alter its hashcode, making the paired value unreachable, or even worst in a remote scenario, retrieving another key's value. These are some helpful guidelines on how to design a HashMap key:

https://howtodoinjava.com/java/collections/hashmap/design-good-key-for-hashmap/
https://www.baeldung.com/java-custom-class-map-key

"
"i can display image to pdf when i use local image path with jasper-reports, but i need to get the image from amazon S3,ã€€how can i display amazon S3 image to pdf by java?  should i download the image from amazonS3 first? or link the full image path in jasper report?
for example, i linked the local image path by jasper-reports, if i want to get the image from amazonS3 , how can i do that? please hlpe me.
&lt;imageExpression class=&quot;java.lang.String&quot;&gt;&lt;![CDATA[&quot;image_name.jpg&quot;]]&gt;&lt;/imageExpression&gt;

","You can generate a pre-signed URL of the S3 image object using AWS Java SDK and use the pre-signed URL in the jasper-reports. In this way, you don't have to download the image from S3. Please note that there is an expiry time for the pre-signed which can be set from java.
Below is the code snippet for reference to generate S3 pre-signed URL.
More details in this link. https://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html
public class GenerateS3SignedUrl implements BiFunction&lt;String, String, String&gt; {
    @Override
    @SneakyThrows
    public String apply(String bucketName, String objectKey) {
        String awsRegion = StringUtils.isEmpty(System.getenv(Constant.REGION)) ? Constant.DEFAULT_REGION :
                System.getenv(Constant.REGION);

        log.info(&quot;calculating expiration hrs defaults to 2 hrs&quot;);
        int expirationHrs;
        if (StringUtils.isEmpty(System.getenv(Constant.EXPIRATION_DURATION))) {
            expirationHrs = 2;
        } else if (!StringUtils.isNumeric(System.getenv(Constant.EXPIRATION_DURATION))) {
            expirationHrs = 2;
        } else {
            expirationHrs = Integer.parseInt(System.getenv(Constant.EXPIRATION_DURATION));
        }

        long expirationInMillis = 1000L * 60 * 60 * expirationHrs;
        log.info(&quot;create pre-signed url generate request..&quot;);
        GeneratePresignedUrlRequest generatePresignedUrlRequest =
                new GeneratePresignedUrlRequest(bucketName, objectKey)
                        .withMethod(HttpMethod.GET)
                        .withExpiration(Date.from(Instant.now().plusMillis(expirationInMillis)));

        log.info(&quot;generate pre-signed url..&quot;);
        URL preSignedUrl = AwsCommonConfig.getAmazonS3Client(awsRegion)
                .generatePresignedUrl(generatePresignedUrlRequest);

        log.info(&quot;return pre-signed url for file : {} with expiration in {} hrs.&quot;, objectKey, expirationHrs);
        return preSignedUrl.toString();
    }
}    
public class AwsCommonConfig {
    private static AmazonS3 amazonS3;

    public static AmazonS3 getAmazonS3Client (String awsRegion) {
        if (amazonS3 == null) {
            amazonS3 = AmazonS3ClientBuilder.standard()
                    .withRegion(awsRegion)
                    .withPathStyleAccessEnabled(true)
                    .build();
        }
        return amazonS3;
    }

}

"
"I'm new to Java graphics, so most of this code is stuff I've gathered from the internet and injecting it into my own program. This program is meant to have a red square, controlled by the arrow keys, detect when it collides with a falling blue dot that resets to the top each time it hits the bottom.
import java.awt.*;  
import java.awt.event.*;
import javax.swing.*;  

class Surface extends JPanel implements ActionListener, KeyListener {

    private final int DELAY = 8;
    private Timer timer;
    private Image image;
    private int x, y;
    private final int MOVE_AMOUNT = 5;
    public final int width = 800;
    public final int length = 600;
    private boolean upPressed, downPressed, leftPressed, rightPressed;
    ;

    public Surface() {
        setDoubleBuffered(true);
        initTimer();
        loadImage();
        setFocusable(true);
        requestFocusInWindow();
        addKeyListener(this);
        
        x = 200;
        y = 200;
    }
    
    private Image resizeImage(Image originalImage, int newWidth, int newHeight) {
        return originalImage.getScaledInstance(newWidth, newHeight, Image.SCALE_SMOOTH);
    }
    
    public Rectangle getRedDotBounds() {
        return new Rectangle(x, y, image.getWidth(this), image.getHeight(this));
    }
    
    private void initTimer() {

        timer = new Timer(DELAY, this);
        timer.start();
    }
    
    private void loadImage(){
        ImageIcon ii = new ImageIcon(&quot;Basic_red_dot.png&quot;);
        if (ii.getImageLoadStatus() == MediaTracker.ERRORED) {
            System.out.println(&quot;Image failed to load.&quot;);
        }
        Image originalImage =ii.getImage();
     // Resize the image to the desired dimensions
        int newWidth = 75; // Set the desired width
        int newHeight = 75; // Set the desired height
        image = resizeImage(originalImage, newWidth, newHeight);
    }
    
    @Override
    protected void paintComponent(Graphics g){
        super.paintComponent(g);
        g.clearRect(0, 0, getWidth(), getHeight());
        drawImage(g);
    }
    
    private void drawImage(Graphics g){
        g.drawImage(image, x, y, this);
    }
    
    public Timer getTimer() {
        
        return timer;
    }
    
    public void actionPerformed(ActionEvent e) {
        updatePosition();
        repaint();
    }
    
    
    private void updatePosition(){
        if (leftPressed){
            x = Math.max(x-MOVE_AMOUNT, 0);
        }
        if(rightPressed){
            x = Math.min(x + MOVE_AMOUNT, getWidth() - image.getWidth(this));
        }
        if(upPressed){
            y = Math.max(y - MOVE_AMOUNT, 0);
        }
        if(downPressed){
            y = Math.min(y + MOVE_AMOUNT, getHeight() - image.getHeight(this));
        }
    }
    
    
    @Override
    
    public void keyPressed(KeyEvent e){
        int key = e.getKeyCode();
        
        switch (key) {
            case KeyEvent.VK_LEFT:
                leftPressed = true;
                break;
            case KeyEvent.VK_RIGHT:
                rightPressed = true;
                break;
            case KeyEvent.VK_UP:
                upPressed = true;
                break;
            case KeyEvent.VK_DOWN:
                downPressed = true;
                break;
        }
        
        repaint();
    }
    
    
    @Override
    public void keyReleased(KeyEvent e) {
        int key = e.getKeyCode();
        
        switch (key) {
            case KeyEvent.VK_LEFT:
                leftPressed = false;
                break;
            case KeyEvent.VK_RIGHT:
                rightPressed = false;
                break;
            case KeyEvent.VK_UP:
                upPressed = false;
                break;
            case KeyEvent.VK_DOWN:
                downPressed = false;
                break;
        }
        
        
    }

    @Override
    public void keyTyped(KeyEvent e) {
        // Not used, but required by KeyListener
    }

}


class BlueDot extends JPanel implements ActionListener {
    private int x, y;
    private Image image;
    private final int DOT_SIZE = 10;
    private final int FALL_SPEED = 1;
    private Timer timer;
    private int n = 0;
    
    public BlueDot() {
        setDoubleBuffered(true);
        setPreferredSize(new Dimension(500, 500));
        x = (int) (Math.random()*500);
        y = 0;
        loadImage();
        timer = new Timer(10, this);
        timer.start();
    }
    
    private Image resizeImage(Image originalImage, int newWidth, int newHeight) {
        return originalImage.getScaledInstance(newWidth, newHeight, Image.SCALE_SMOOTH);
    }
    
    private void loadImage() {
        ImageIcon ii = new ImageIcon(&quot;Basic_blue_dot.png&quot;);
        if (ii.getImageLoadStatus() == MediaTracker.ERRORED) {
            System.out.println(&quot;Image failed to load.&quot;);
        }
        Image originalImage =ii.getImage();
     // Resize the image to the desired dimensions
        int newWidth = 200; // Set the desired width
        int newHeight = 200; // Set the desired height
        image = resizeImage(originalImage, newWidth, newHeight);
    }
    
    public Rectangle getBlueDotBounds() {
        return new Rectangle(x, y, DOT_SIZE, DOT_SIZE);
    }
    
    protected void paintComponent(Graphics g) {
        super.paintComponent(g);
        g.clearRect(0, 0, getWidth(), getHeight());
        g.drawImage(image, x, y, DOT_SIZE, DOT_SIZE, this);
    }
    
    public void actionPerformed(ActionEvent e){
        
        y += FALL_SPEED;
        if (y &gt; getHeight()) {
            n++;
            System.out.println(&quot;reset&quot; + n);
            x = (int) (Math.random() * 500);
            y = 0;
        }
        //repaint();
    }
}


public class MyProgram
{
    public static void main(String[] args){
        SwingUtilities.invokeLater(new Runnable(){
            public void run() {
                JFrame frame = new JFrame();
                frame.setTitle(&quot;Image Display&quot;);
                frame.setSize(500,500);
                frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);
                
                
                Surface surface = new Surface();
                BlueDot blueDot = new BlueDot();
                
                
                frame.setLayout(null);
                
                surface.setBounds(0, 0, 500, 500);
                frame.add(surface);
                
                blueDot.setBounds((int) (Math.random() * 450), 0, 500, 500);
                frame.add(blueDot);
                
                frame.setLocationRelativeTo(null);
                frame.setVisible(true);
                
                surface.requestFocusInWindow();
                frame.addWindowListener(new WindowAdapter() {
                    @Override
                    public void windowOpened(WindowEvent e) {
                        surface.requestFocusInWindow();
                    }
                
                });
                
                Timer collisionTimer = new Timer(10, new ActionListener() {
                    public void actionPerformed(ActionEvent e) {
                        Rectangle redBounds = surface.getRedDotBounds();
                        Rectangle blueBounds = blueDot.getBlueDotBounds();
                        
                        if (redBounds.intersects(blueBounds)) {
                            System.out.println(&quot;Collision!!!!&quot;);
                        }
                    }
                });
                collisionTimer.start();
            }
        });
    }
    
}

The collision detection is a little off, mainly because the red square and the blue dot are constantly flickering. I've tried commenting out each of the three repaint statements, but that either results in one of the two elements being invisible, or some other glaring issue. How can I fix the program so this flickering goes away and the collision detection works flawlessly?
","Due to @camickr requesting a more complete answer here it is. Do not stack JPanels that causes the flickering. Use a buffered image. Do not throw everything into one class but seperate code into seperate classes, think of object oriented approach.
This is abstract class ball;
package move;

import java.awt.Rectangle;
import java.awt.event.ActionListener;
import java.awt.image.BufferedImage;
import javax.imageio.ImageIO;
import javax.swing.Timer;

public abstract class Ball
{
   protected BufferedImage bufferedImage;
   private int imageDiameter;
   protected int x;
   protected int y;
   private int timerDelay;
   private Timer timer;
   protected int widthOfPlayingField;

   public Ball(String image, int imageDiameter, int timerDelay, int xDirection,
         int yDirection, int widthOfPlayingField)
   {
      try
      {
         bufferedImage = ImageIO.read(Ball.class.getResource(image));
      }
      catch (Exception e)
      {
         System.out.println(&quot;Image &quot; + image + &quot; failed to load.&quot;);
      }
      x = xDirection;
      y = yDirection;
      this.timerDelay = timerDelay;
      this.imageDiameter = imageDiameter;
      this.widthOfPlayingField = widthOfPlayingField;
   }

   public BufferedImage getBufferedImage()
   {
      return bufferedImage;
   }

   public Rectangle getBounds()
   {
      return new Rectangle(x, y, bufferedImage.getWidth(),
            bufferedImage.getHeight());
   }

   public int getX()
   {
      return x;
   }

   public int getY()
   {
      return y;
   }

   public void initTimer(ActionListener actionListener)
   {
      timer = new Timer(timerDelay, actionListener);
      timer.start();
   }

   public int getImageDiameter()
   {
      return imageDiameter;
   }

   public int getTimerDelay()
   {
      return timerDelay;
   }
}

This is class DropBall.
package move;

import java.util.UUID;

public class DropBall extends Ball
{
   private int deltaSpeed = 1;
   private int resets = 0;
   private UUID uuid;

   public DropBall(String image, int imageDiameter, int timerDelay,
         int xDirection, int yDirection, int widthOfPlayingField)
   {
      super(image, imageDiameter, timerDelay, xDirection, yDirection,
            widthOfPlayingField);
      uuid = UUID.randomUUID();
   }

   public void updatePosition()
   {
      y += deltaSpeed;
   }

   public UUID getUuid()
   {
      return uuid;
   }

   public void reset()
   {
      resets++;
      System.out.println(&quot;resetting ball: &quot; + resets + &quot; Ball Nr. &quot; + uuid);
      x = (int) (Math.random() * widthOfPlayingField);
      y = 0;
   }
}

This is class MoveBall.
package move;

public class MoveBall extends Ball
{
   private final int MOVE_AMOUNT = 5;
   private int heightOfPlayingField;

   public MoveBall(String image, int imageDiameter, int timerDelay,
         int xDirection, int yDirection, int widthOfPlayingField,
         int heightOfPlayingField)
   {
      super(image, imageDiameter, timerDelay, xDirection, yDirection,
            widthOfPlayingField);
      this.heightOfPlayingField = heightOfPlayingField;
   }

   public void updatePosition(boolean leftPressed, boolean rightPressed,
         boolean upPressed, boolean downPressed)
   {
      if (leftPressed)
      {
         x = Math.max(x - MOVE_AMOUNT, 0);
      }
      if (rightPressed)
      {
         x = Math.min(x + MOVE_AMOUNT,
               widthOfPlayingField - bufferedImage.getWidth());
      }
      if (upPressed)
      {
         y = Math.max(y - MOVE_AMOUNT, 0);
      }
      if (downPressed)
      {
         y = Math.min(y + MOVE_AMOUNT,
               heightOfPlayingField - bufferedImage.getHeight());
      }
   }
}

This is class Surface.
package move;

import java.awt.Graphics;
import java.awt.event.ActionEvent;
import java.awt.event.ActionListener;
import java.awt.event.KeyEvent;
import java.awt.event.KeyListener;
import java.util.List;

import javax.swing.JPanel;

class Surface extends JPanel implements ActionListener, KeyListener
{
   private static final long serialVersionUID = -1375861046489286313L;
   private final int HEIGHT;
   private List&lt;DropBall&gt; balls;
   private MoveBall moveBall;
   private boolean upPressed, downPressed, leftPressed, rightPressed;

   public Surface(List&lt;DropBall&gt; balls, MoveBall moveBall, int height)
   {
      HEIGHT = height;
      this.balls = balls;
      this.moveBall = moveBall;
      addKeyListener(this);
      setFocusable(true);
      requestFocusInWindow();

      for (Ball ball : balls)
      {
         ball.initTimer(this);
      }
      
      moveBall.initTimer(this);
   }

   @Override
   protected void paintComponent(Graphics g)
   {
      super.paintComponent(g);
      for (Ball ball : balls)
      {
         g.drawImage(ball.getBufferedImage(), ball.getX(), ball.getY(),
               ball.getImageDiameter(), ball.getImageDiameter(), this);
      }
      g.drawImage(moveBall.getBufferedImage(), moveBall.getX(), moveBall.getY(),
            this);
   }

   public void actionPerformed(ActionEvent e)
   {
      for (DropBall ball : balls)
      {
         ball.updatePosition();
         if (ball.getY() &gt; HEIGHT)
         {
            ball.reset();
         }
      }
      validate();
      repaint();
   }

   @Override
   public void keyPressed(KeyEvent e)
   {
      int key = e.getKeyCode();

      switch (key)
      {
      case KeyEvent.VK_LEFT:
         leftPressed = true;
         break;
      case KeyEvent.VK_RIGHT:
         rightPressed = true;
         break;
      case KeyEvent.VK_UP:
         upPressed = true;
         break;
      case KeyEvent.VK_DOWN:
         downPressed = true;
         break;
      }
      moveBall.updatePosition(leftPressed, rightPressed, upPressed,
            downPressed);
      validate();
      repaint();
   }

   @Override
   public void keyReleased(KeyEvent e)
   {
      int key = e.getKeyCode();

      switch (key)
      {
      case KeyEvent.VK_LEFT:
         leftPressed = false;
         break;
      case KeyEvent.VK_RIGHT:
         rightPressed = false;
         break;
      case KeyEvent.VK_UP:
         upPressed = false;
         break;
      case KeyEvent.VK_DOWN:
         downPressed = false;
         break;
      }
   }

   @Override
   public void keyTyped(KeyEvent e)
   {
      // Not used, but required by KeyListener
   }
}

This is class MyProgram with main method.
package move;

import java.awt.Rectangle;
import java.awt.event.ActionEvent;
import java.awt.event.ActionListener;
import java.util.ArrayList;
import java.util.List;

import javax.swing.JFrame;
import javax.swing.SwingUtilities;
import javax.swing.Timer;

public class MyProgram
{
   private static final int HEIGHT = 500;
   private static final int WIDTH = 500;

   public static void main(String[] args)
   {
      SwingUtilities.invokeLater(new Runnable()
      {
         public void run()
         {
            JFrame frame = new JFrame();
            frame.setTitle(&quot;Image Display&quot;);
            frame.setSize(WIDTH, HEIGHT);
            frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);

            List&lt;DropBall&gt; balls = new ArrayList&lt;&gt;();
            DropBall blueBall = new DropBall(&quot;kugel_blau.png&quot;, 10, 50,
                  (int) (Math.random() * 500), 0, WIDTH);
            balls.add(blueBall);
            // here more dropping ball can be added

            MoveBall moveBall = new MoveBall(&quot;kugel_rot.png&quot;, 50, 0, WIDTH / 2,
                  HEIGHT / 2, WIDTH, HEIGHT);

            Surface surface = new Surface(balls, moveBall, HEIGHT);
            frame.add(surface);

            frame.setLocationRelativeTo(null);
            frame.setVisible(true);

            Timer collisionTimer = new Timer(10, new ActionListener()
            {
               public void actionPerformed(ActionEvent e)
               {
                  Rectangle redBounds = moveBall.getBounds();
                  for (DropBall ball : balls)
                  {
                     Rectangle blueBounds = ball.getBounds();
                     if (redBounds.intersects(blueBounds))
                     {
                        System.out.println(&quot;Collision!!!!&quot;);
                     }
                  }
               }
            });
            collisionTimer.start();
         }
      });
   }
}

Have fun :-)!
"
"I have the following Java code that increases the value of a hex value and returns a string using the following format (where x is an increasing hex value and 7 is a literal):
xxxx-7xxx-xxxx

The value is increasing from left to right like so and consists of 12 characters:
0000-7000-0001
0000-7000-0002
...
0000-7fff-ffff
0001-7000-0000

The code:
public class GeneratorTemplate {
    private static final AtomicLong COUNTER = new AtomicLong(0);

    public static String generateTemplate() {
        // incrementing the counter
        long currentValue = COUNTER.getAndIncrement();

        // get 11 character (not 12 because 7 is a literal and appended) hex value represented as string
        String rawResult = String.format(&quot;%011X&quot;, currentValue &amp; 0xFFFFFFFFFFFL);

        // append and format values
        return (rawResult.substring(0, 4) +
                &quot;-7&quot; +
                rawResult.substring(4, 7) +
                &quot;-&quot; +
                rawResult.substring(7)).toLowerCase();

    }

    public static void main(String[] args) {
        for (int i = 0; i &lt; 3; i++) {
            System.out.println(generateTemplate());
        }
    }
}

It works but I'm sure that this is not efficient.
The performance issue revolves around the fact that I do appending of 7, hyphens and lowercase manually. I think that if I do this in the format() method, I'll have gains in performance. In addition, that code would be clearer
I tried to accomplish this in one line but to no avail. This code is obviously incorrect and I wonder how to use the format() method correctly, so that I won't have to append anything manually:
return String.format(&quot;%04X-7%03X-%04X&quot;, currentValue &amp; 0xFFFFL, currentValue &amp; 0xFFFL, currentValue &amp; 0xFFFFL);

Could anyone clarify of whether this is possible to do using the format() method and what the number of F represents in the parameter (I obviously understand that this is hex but how many F's should be there)?
Thanks
","use the String.format() method, so you can format the hex value directly without having to manually append the segments. You need to use the correct bit manipulation and formatting within String.format().
import java.util.concurrent.atomic.AtomicLong;

public class GeneratorTemplate {
    private static final AtomicLong COUNTER = new AtomicLong(0);

    public static String generateTemplate() {
        // Increment the counter
        long currentValue = COUNTER.getAndIncrement();

        // Use bit manipulation and formatting to generate the desired output
        long part1 = (currentValue &gt;&gt; 28) &amp; 0xFFFF;  // First 4 hex digits
        long part2 = (currentValue &gt;&gt; 16) &amp; 0xFFF;   // Next 3 hex digits
        long part3 = currentValue &amp; 0xFFFF;          // Last 4 hex digits

        return String.format(&quot;%04X-7%03X-%04X&quot;, part1, part2, part3);
    }

    public static void main(String[] args) {
        for (int i = 0; i &lt; 3; i++) {
            System.out.println(generateTemplate());
        }
    }
}

Bit Manipulation:
(&gt;&gt;): moves bits to the right, discarding bits on the right and filling the left side with zeros for unsigned shifts.
First Part: part1 = (currentValue &gt;&gt; 28) &amp; 0xFFFF,shifts currentValue right by 28 bits to extract the first 4 hex digits.
The mask 0xFFFF ensures exactly 4 hex digits are taken.
Second Part: part2 = (currentValue &gt;&gt; 16) &amp; 0xFFF,shifts currentValue right by 16 bits to extract the next 3 hex digits.
The mask 0xFFF ensures exactly 3 hex digits are taken.
Third Part: part3 = currentValue &amp; 0xFFFF uses the mask 0xFFFF to get the last 4 hex digits directly.
Formatting:
 String.format(&quot;%04X-7%03X-%04X&quot;, part1, part2, part3)

%04X formats the first part to 4 hex digits.
%03X formats the middle part to 3 hex digits.
%04X formats the last part to 4 hex digits.
the out put of aforementioned code is:

"
"I'm trying to create a Spinner in JavaFX that accepts only multiples of 0.25 and has positive and negative masks, such as -1,50 and +1,50 and have two decimals places and the max value of -20 to 20. In both cases, I need the mask to show (-) and (+). The TextField field must be editable and follow the same rule.
I managed to create a customizable TextField like this but i dont know how to do in a Spinner:
public class TestPane extends BorderPane {
  public TestPane() {

    TextField textField = new TextField();
    BigDecimalConverter converter = new BigDecimalConverter();
    TextFormatter&lt;BigDecimal&gt; textFormatter = new TextFormatter&lt;&gt;(converter, BigDecimal.ZERO, c -&gt; {
      if (!c.getControl().isFocused()) return null;

      String newText = c.getControlNewText().replace(&quot;.&quot;, &quot;,&quot;);

      if (c.getControlNewText().isEmpty()) {
        return c;
      }
      if (c.getControlNewText().equals(&quot;-&quot;) &amp;&amp; c.getAnchor() == 1) {
        return c;
      }
      if (c.getControlNewText().equals(&quot;+&quot;) &amp;&amp; c.getAnchor() == 1) {
        return c;
      }
      if (c.getControlNewText().startsWith(&quot;-&quot;) &amp;&amp; c.getControlCaretPosition() == 0) {
        return c;
      }
      if (c.getControlNewText().startsWith(&quot;+&quot;) &amp;&amp; c.getControlCaretPosition() == 0) {
        c.setText(c.getText() + &quot; &quot;);
        return c;
      }

      BigDecimal newValue = converter.fromString(c.getControlNewText());
      if (newValue != null) {
        return c;
      } else {
        return null;
      }
    });
    textFormatter.valueProperty().bindBidirectional(valueProperty);
    textField.setTextFormatter(textFormatter);
    setCenter(new VBox(10, new HBox(6, new Text(&quot;TextField 1&quot;), textField)));
  }
}

public static class BigDecimalConverter extends BigDecimalStringConverter {

  @Override
  public String toString(BigDecimal value) {
    if (value == null) return &quot;0&quot;;
    return super.toString(value);
  }

  @Override
  public BigDecimal fromString(String value) {
    if (value == null || value.isEmpty()) return BigDecimal.ZERO;
    return super.fromString(value);
  }
}


edit:
i'm using the solution by @swpalmer and implemented this solution to the editor TextFormatter:
TextField editor = spinner.getEditor();

Pattern validDoubleText = Pattern.compile(&quot;[+-]?\\d{0,2}(\\,\\d{0,2})?&quot;);
UnaryOperator&lt;TextFormatter.Change&gt; filter = c -&gt; {
  if (validDoubleText.matcher(c.getControlNewText()).matches()) {
    return c;
  } else {
    return null;
  }
};
TextFormatter&lt;Double&gt; textFormatter = new TextFormatter&lt;Double&gt;(filter);

but i dont know how to limit to only values of (-20,00 to 20,00), and to put an + with the number is positive
Example:

","General Idea
Here are the general ideas needed to meet each of your individual requirements.
Using a TextFormatter with a Spinner
The Spinner class has an editor property which holds a TextField. A TextFormatter can be set on this field just like you would any other. You should set the formatter's converter to the same one used by the SpinnerValueFactory.
Clamping the Value
Clamping the value between minimum and maximum values is relatively straightforward:
double clamp(double value, double min, double max) {
  return Math.min(max, Math.max(min, value));
}

On Java 21+ that can be replaced with:
Math.clamp(value, min, max);

Rounding the Value
To round the value to the nearest multiple of some other value, you can use:
double round(double value, double step) {
  // Note: Math#round rounds ties to positive infinity (&quot;half ceiling&quot;)
  return Math.round(value / step) * step;
}

Parsing &amp; Formatting
And you can use a DecimalFormat to parse strings into numbers and format numbers into strings. Not only can you define the desired pattern to use, but it also adds some localization to your application. For instance, whether . or , (or some other character) is used as the decimal separator will be dynamically determined based on the specified locale (or the system default locale if one is not explicitly given).

Example
Here is an example using the above concepts. Parsing, formatting, clamping, and rounding the value are all encapsulated in a StringConverter implementation.
Main.java
package com.example.app;

import javafx.application.Application;
import javafx.scene.Scene;
import javafx.scene.control.Spinner;
import javafx.scene.control.SpinnerValueFactory.DoubleSpinnerValueFactory;
import javafx.scene.control.TextFormatter;
import javafx.scene.layout.StackPane;
import javafx.stage.Stage;

public class Main extends Application {

  private static final double MIN = -20.0;
  private static final double MAX = -MIN;
  private static final double STEP = 0.25;

  @Override
  public void start(Stage primaryStage) {
    var converter = new ConstrainedDoubleStringConverter(MIN, MAX, STEP);

    var factory = new DoubleSpinnerValueFactory(MIN, MAX, 0.0, STEP);
    factory.setConverter(converter);

    var formatter = new TextFormatter&lt;&gt;(converter, 0.0, change -&gt; {
      if (change.isContentChange()) {
        var text = change.getControlNewText();
        if (!converter.isParsable(text)) {
          return null;
        }
      }
      return change;
    });

    var spinner = new Spinner&lt;&gt;(factory);
    spinner.getEditor().setTextFormatter(formatter);
    spinner.setEditable(true);

    primaryStage.setScene(new Scene(new StackPane(spinner), 500, 300));
    primaryStage.show();
  }

  public static void main(String[] args) {
    launch(Main.class, args);
  }
}

ConstrainedDoubleStringConverter.java
package com.example.app;

import java.text.DecimalFormat;
import java.text.ParsePosition;
import javafx.util.StringConverter;

// Note: This implementation does not allow parsing +/- infinity or NaN
public class ConstrainedDoubleStringConverter extends StringConverter&lt;Double&gt; {

  private final DecimalFormat format = new DecimalFormat(&quot;+0.00;-#&quot;);

  private final double min;
  private final double max;
  private final double stepAmount;

  public ConstrainedDoubleStringConverter(double min, double max, double stepAmount) {
    this.min = min;
    this.max = max;
    this.stepAmount = stepAmount;
  }

  @Override
  public String toString(Double object) {
    return format.format(object == null ? 0.0 : object);
  }

  @Override
  public Double fromString(String string) {
    double value = parse(string);
    if (!Double.isFinite(value))
      throw new NumberFormatException(&quot;Unable to parse into finite double: &quot; + string);

    // round value to nearest multiple of stepAmount
    value = Math.round(value / stepAmount) * stepAmount;
    // coerce value to be between min and max
    return Math.clamp(value, min, max);
  }

  // A way to test if 'fromString' would succeed without using
  // exceptions for control flow.
  public boolean isParsable(String string) {
    return Double.isFinite(parse(string));
  }

  private double parse(String s) {
    if (s == null || s.isEmpty() || isOnlyPrefix(s)) {
      return 0.0;
    }

    // If there is no prefix then assume positive.
    if (!startsWithPrefix(s)) {
      s = format.getPositivePrefix() + s;
    }

    // This approach avoids ParseException on errors and ensures the entire
    // string was consumed (NumberFormat is capable of partial parsing).
    var position = new ParsePosition(0);
    var number = format.parse(s, position);
    if (position.getErrorIndex() != -1 || position.getIndex() != s.length()) {
      return Double.NaN;
    }
    return number.doubleValue();
  }

  private boolean isOnlyPrefix(String s) {
    return s.length() == 1 &amp;&amp; startsWithPrefix(s);
  }

  private boolean startsWithPrefix(String s) {
    if (s.length() &gt;= 1) {
      var positive = format.getPositivePrefix();
      var negative = format.getNegativePrefix();
      return s.startsWith(positive) || s.startsWith(negative);
    }
    return false;
  }
}

module-info.java (optional)
module com.example.app {
  requires javafx.controls;

  exports com.example.app to
      javafx.graphics;
}


BigDecimal
Your question mentions wanting to use BigDecimal. I agree with swpalmer's answer that BigDecimal is overkill for your range of values and step amount. Using double is sufficient and makes the implementation easier. That said, if you need to use BigDecimal for some reason, then you need to:

Rewrite the code from above to use BigDecimal, which basically entails replacing all Double / double types with BigDecimal.
Clamping:
BigDecimal clamp(BigDecimal value, BigDecimal min, BigDecimal max) {
  return min.max(value.min(max));
}

Rounding:
BigDecimal round(BigDecimal value, BigDecimal step) {
  // Note: This does not 100% match the behavior of the double version above
  return value.divide(step, 0, RoundingMode.HALF_UP).multiply(step);
}

Parsing &amp; Formating:
// Configure parsing BigDecimal
format.setParseBigDecimal(true);

// cast the result
var value = (BigDecimal) format.parse(string, position);


Write a SpinnerValueFactory implementation that works with BigDecimal. This involves implementing an increment(int) and a decrement(int) method (both of which should take into account if the factory is set to &quot;wrap around&quot; the values or not). You can look at the existing implementations for help.

Have the StringConverter&lt;BigDecimal&gt; implementation copy the return value when it's equal to 0. The reason being that BigDecimal caches 0 values at a few different scales, and that interferes with the involved JavaFX properties updating properly (the built-in property implementations only fire invalidation events if the new value and current value are !=, i.e., different objects). 


"
"I've added to my storefront a new extension based on commercewebservices and I've tested several sample services directly through swagger and the ones that doesn't need any kind of authorization works perfect. However, the webservices annotated with @ApiBaseSiteIdAndUserIdParam when I set the userId and siteParam the controller that interecepts this petition doesn't set in session the user I pass, it always returns anonymous user. I've tried creating special OAuth credentials but it doesn't work it always returns anonymous user.
   @Secured({ &quot;ROLE_CUSTOMERGROUP&quot;, &quot;ROLE_TRUSTED_CLIENT&quot;, &quot;ROLE_CUSTOMERMANAGERGROUP&quot; })
@GetMapping(value = &quot;/test&quot;)
@ResponseBody
@ApiBaseSiteIdAndUserIdParam
public TestListWsDTO getTest(
        @RequestParam(required = false, defaultValue = DEFAULT_FIELD_SET) final String fields) {
    final CustomerData customerData = customerFacade.getCurrentCustomer();
    if (userFacade.isAnonymousUser()) {
        throw new AccessDeniedException(&quot;Anonymous user is not allowed&quot;);
    }



The test@test.com is a registered user.
Why the customer I indicate through swagger is not being captured by customerFacade.getCurrentCustomer() and it always return anonymous?
","AS per @Neil it's correct in case of OCC V2 context user is getting determined by the OAuth token.
For OCC there are also configured filters which used to configure or put user users in session if it got found otherwise it will set anonymous.
Please have a look of UserMatchingFilter.
/*
 * [y] hybris Platform
 *
 * Copyright (c) 2017 SAP SE or an SAP affiliate company.  All rights reserved.
 *
 * This software is the confidential and proprietary information of SAP
 * (&quot;Confidential Information&quot;). You shall not disclose such Confidential
 * Information and shall use it only in accordance with the terms of the
 * license agreement you entered into with SAP.
 */
package de.hybris.platform.ycommercewebservices.v2.filter;

import de.hybris.platform.core.model.user.UserModel;
import de.hybris.platform.servicelayer.exceptions.UnknownIdentifierException;
import de.hybris.platform.servicelayer.session.SessionService;
import de.hybris.platform.servicelayer.user.UserService;

import java.io.IOException;

import javax.servlet.FilterChain;
import javax.servlet.ServletException;
import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;

import org.apache.log4j.Logger;
import org.springframework.beans.factory.annotation.Required;
import org.springframework.security.access.AccessDeniedException;
import org.springframework.security.core.Authentication;
import org.springframework.security.core.GrantedAuthority;
import org.springframework.security.core.context.SecurityContextHolder;


/**
 * Filter that puts user from the requested url into the session.
 */
public class UserMatchingFilter extends AbstractUrlMatchingFilter
{
    public static final String ROLE_ANONYMOUS = &quot;ROLE_ANONYMOUS&quot;;
    public static final String ROLE_CUSTOMERGROUP = &quot;ROLE_CUSTOMERGROUP&quot;;
    public static final String ROLE_CUSTOMERMANAGERGROUP = &quot;ROLE_CUSTOMERMANAGERGROUP&quot;;
    public static final String ROLE_TRUSTED_CLIENT = &quot;ROLE_TRUSTED_CLIENT&quot;;
    private static final String CURRENT_USER = &quot;current&quot;;
    private static final String ANONYMOUS_USER = &quot;anonymous&quot;;
    private static final String ACTING_USER_UID = &quot;ACTING_USER_UID&quot;;
    private static final Logger LOG = Logger.getLogger(UserMatchingFilter.class);
    private String regexp;
    private UserService userService;
    private SessionService sessionService;

    @Override
    protected void doFilterInternal(final HttpServletRequest request, final HttpServletResponse response,
            final FilterChain filterChain) throws ServletException, IOException
    {
        final Authentication auth = getAuth();
        if (hasRole(ROLE_CUSTOMERGROUP, auth) || hasRole(ROLE_CUSTOMERMANAGERGROUP, auth))
        {
            getSessionService().setAttribute(ACTING_USER_UID, auth.getPrincipal());
        }

        final String userID = getValue(request, regexp);
        if (userID == null)
        {
            if (hasRole(ROLE_CUSTOMERGROUP, auth) || hasRole(ROLE_CUSTOMERMANAGERGROUP, auth))
            {
                setCurrentUser((String) auth.getPrincipal());
            }
            else
            {
                // fallback to anonymous
                setCurrentUser(userService.getAnonymousUser());
            }
        }
        else if (userID.equals(ANONYMOUS_USER))
        {
            setCurrentUser(userService.getAnonymousUser());
        }
        else if (hasRole(ROLE_TRUSTED_CLIENT, auth) || hasRole(ROLE_CUSTOMERMANAGERGROUP, auth))
        {
            setCurrentUser(userID);
        }
        else if (hasRole(ROLE_CUSTOMERGROUP, auth))
        {
            if (userID.equals(CURRENT_USER) || userID.equals(auth.getPrincipal()))
            {
                setCurrentUser((String) auth.getPrincipal());
            }
            else
            {
                throw new AccessDeniedException(&quot;Access is denied&quot;);
            }
        }
        else
        {
            // could not match any authorized role
            throw new AccessDeniedException(&quot;Access is denied&quot;);
        }

        filterChain.doFilter(request, response);
    }

    protected Authentication getAuth()
    {
        return SecurityContextHolder.getContext().getAuthentication();
    }

    protected String getRegexp()
    {
        return regexp;
    }

    @Required
    public void setRegexp(final String regexp)
    {
        this.regexp = regexp;
    }

    protected UserService getUserService()
    {
        return userService;
    }

    @Required
    public void setUserService(final UserService userService)
    {
        this.userService = userService;
    }

    protected SessionService getSessionService()
    {
        return sessionService;
    }

    @Required
    public void setSessionService(final SessionService sessionService)
    {
        this.sessionService = sessionService;
    }

    protected boolean hasRole(final String role, final Authentication auth)
    {
        if (auth != null)
        {
            for (final GrantedAuthority ga : auth.getAuthorities())
            {
                if (ga.getAuthority().equals(role))
                {
                    return true;
                }
            }
        }
        return false;
    }

    protected void setCurrentUser(final String uid)
    {
        try
        {
            final UserModel userModel = userService.getUserForUID(uid);
            userService.setCurrentUser(userModel);
        }
        catch (final UnknownIdentifierException ex)
        {
            LOG.debug(ex.getMessage());
            throw ex;
        }
    }

    protected void setCurrentUser(final UserModel user)
    {
        userService.setCurrentUser(user);
    }
}

"
"I want to listen to the change of userData of a stage in JavaFX. I have tried to wrap the Object which is returned from getUserData method inside a SimpleObjectProperty, then add a listener to it but it didn't work.
This is my attempt:
SimpleObjectProperty&lt;Object&gt; userDataProperty = new SimpleObjectProperty&lt;&gt;(stage.getUserData());
    userDataProperty.addListener((observable, oldValue, newValue) -&gt; {
    // print when userData is changed
    System.out.println(&quot;new userdata:&quot; + stage.getUserData());
});

// change the userData to test if the listener work
stage.setUserData(2);
System.out.println(stage.getUserData());
stage.setUserData(3);
System.out.println(stage.getUserData());

Output:
2
3

How to do it properly?
","The Window class does not expose a JavaFX property for the user data, which means it is not directly observable. However, the Window#setUserData(Object) method has this somewhat cryptic documentation:

Convenience method for setting a single Object property that can be retrieved at a later date. This is functionally equivalent to calling the getProperties().put(Object key, Object value) method. This can later be retrieved by calling getUserData().

That would seem to indicate the user data is held in the ObservableMap returned by Window#getProperties(). And if you look at the implementation, you will see that is indeed the case. Unfortunately, the key for the user data entry is private. But you can use reflection to get the key or, possibly better, use a MapChangeListener to capture it.
Once you have the key you can use Bindings#valueAt(ObservableMap, K) to create an ObjectBinding that will always report the latest user data value.
Here is an example where a MapChangeListener is used to capture the key.
import javafx.application.Application;
import javafx.beans.binding.Bindings;
import javafx.beans.binding.ObjectBinding;
import javafx.collections.MapChangeListener;
import javafx.scene.Scene;
import javafx.scene.control.TextField;
import javafx.scene.layout.Region;
import javafx.scene.layout.StackPane;
import javafx.stage.Stage;
import javafx.stage.Window;

public class Main extends Application {

  // Make sure to keep a strong reference to the binding for as long as you need it.
  private ObjectBinding&lt;Object&gt; userData;

  @Override
  public void start(Stage primaryStage) {
    userData = userDataBinding(primaryStage);
    // Note: Using _ for unused parameters was standardized in Java 22.
    userData.addListener(
        (_, oldVal, newVal) -&gt; System.out.printf(&quot;User data: %s -&gt; %s%n&quot;, oldVal, newVal));

    // Type something into the TextField then press ENTER to see the example at work.
    var field = new TextField();
    field.setMaxWidth(Region.USE_PREF_SIZE);
    field.setOnAction(
        e -&gt; {
          e.consume();
          primaryStage.setUserData(field.getText());
          field.clear();
        });

    primaryStage.setScene(new Scene(new StackPane(field), 500, 300));
    primaryStage.show();
  }

  private ObjectBinding&lt;Object&gt; userDataBinding(Window window) {
    // Use array because local variables must be (effectively) final when used inside
    // a lambda expression or anonymous class.
    var keyHolder = new Object[1];
    window
        .getProperties()
        .addListener(
            new MapChangeListener&lt;&gt;() {
              @Override
              public void onChanged(Change&lt;? extends Object, ? extends Object&gt; change) {
                keyHolder[0] = change.getKey();
                // Only need to capture the key once.
                change.getMap().removeListener(this);
              }
            });
    var oldUserData = window.getUserData(); // save current value
    window.setUserData(new Object());       // invoke the listener
    window.setUserData(oldUserData);        // restore to old value
    return Bindings.valueAt(window.getProperties(), keyHolder[0]);
  }
}

From looking at the documentation, the same approach can be used with Scene and Node.

Of course, you could use your own key and manipulate the Window.getProperties() map directly. That will likely be easier.
"
"I'm trying to use SDO_GEOMETRY in my SpringBoot entity to store and retrieve Polygon data. Here's the field in my entity:
@Column(name = &quot;shape&quot;,columnDefinition = &quot;MDSYS.SDO_GEOMETRY&quot;)
private Polygon shape;

However, when I try to save the data, I get the following error:
java.sql.SQLSyntaxErrorException: ORA-00932: inconsistent datatypes:
expected MDSYS.SDO_GEOMETRY got BINARY

I'm using hibernate-spatial and my database is Oracle 19c. Here's the relevant dependency in my pom.xml:
    &lt;dependency&gt;
        &lt;groupId&gt;org.hibernate.orm&lt;/groupId&gt;
        &lt;artifactId&gt;hibernate-spatial&lt;/artifactId&gt;
        &lt;version&gt;6.3.0.Final&lt;/version&gt;
    &lt;/dependency&gt;

my application.property:
# Hibernate properties
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.Oracle12cDialect
spring.jpa.properties.hibernate.enable_lazy_load_no_trans=true

# Hibernate Spatial properties
spring.jpa.properties.hibernate.spatial.dialect=org.hibernate.spatial.dialect.oracle.OracleSpatial10gDialect

and this is my service code:
    // set shape of range
    List&lt;Coordinate&gt; coordinates = new ArrayList&lt;&gt;();
    for (RangeSpotsModel spot : rangeModel.getRangeSpotsModel()) {
        coordinates.add(new Coordinate(spot.getLongitude(), spot.getLatitude()));
    }
    GeometryFactory geometry = new GeometryFactory();
    range.setShape(geometry.createPolygon(coordinates.toArray(new Coordinate[0])));

    range = rangeRepository.save(range);

this is full error:
message: could not execute statement; SQL [n/a]; nested exception is org.hibernate.exception.SQLGrammarException: could not execute statement
stackTrace: org.springframework.dao.InvalidDataAccessResourceUsageException: could not execute statement; SQL [n/a]; nested exception is org.hibernate.exception.SQLGrammarException: could not execute statement
    at org.springframework.orm.jpa.vendor.HibernateJpaDialect.convertHibernateAccessException(HibernateJpaDialect.java:259)
    at org.springframework.orm.jpa.vendor.HibernateJpaDialect.translateExceptionIfPossible(HibernateJpaDialect.java:233)
    at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.translateExceptionIfPossible(AbstractEntityManagerFactoryBean.java:551)
    at org.springframework.dao.support.ChainedPersistenceExceptionTranslator.translateExceptionIfPossible(ChainedPersistenceExceptionTranslator.java:61)
    at org.springframework.dao.support.DataAccessUtils.translateIfNecessary(DataAccessUtils.java:242)
    at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:152)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
    at org.springframework.data.jpa.repository.support.CrudMethodMetadataPostProcessor$CrudMethodMetadataPopulatingMethodInterceptor.invoke(CrudMethodMetadataPostProcessor.java:174)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
    at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
    at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215)
    at jdk.proxy2/jdk.proxy2.$Proxy251.save(Unknown Source)
    at com.sheikh.mems.range.business.service.RangeServiceImpl.create(RangeServiceImpl.java:91)
    at com.sheikh.mems.range.business.service.RangeServiceImpl$$FastClassBySpringCGLIB$$208728df.invoke(&lt;generated&gt;)
    at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
    at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:793)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
    at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:763)
    at org.springframework.transaction.interceptor.TransactionInterceptor$1.proceedWithInvocation(TransactionInterceptor.java:123)
    at org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:388)
    at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:119)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
    at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:763)
    at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:708)
    at com.sheikh.mems.range.business.service.RangeServiceImpl$$EnhancerBySpringCGLIB$$b4e88a92.create(&lt;generated&gt;)
    at com.sheikh.mems.range.presentation.RangeControllerBackPanel.create(RangeControllerBackPanel.java:35)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
    at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
    at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
    at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
    at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1067)
    at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:963)
    at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
    at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:681)
    at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:764)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:227)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)
    at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:53)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:327)
    at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:115)
    at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:81)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:122)
    at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:116)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:126)
    at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:81)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:109)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:149)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.security.oauth2.server.resource.web.BearerTokenAuthenticationFilter.doFilterInternal(BearerTokenAuthenticationFilter.java:142)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:103)
    at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:89)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.web.filter.CorsFilter.doFilterInternal(CorsFilter.java:91)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.security.web.header.HeaderWriterFilter.doHeadersAfter(HeaderWriterFilter.java:90)
    at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:75)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:112)
    at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:82)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:55)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.security.web.session.DisableEncodeUrlFilter.doFilterInternal(DisableEncodeUrlFilter.java:42)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:336)
    at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:211)
    at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:183)
    at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:354)
    at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:267)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)
    at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)
    at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)
    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:197)
    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:97)
    at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:541)
    at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:135)
    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:92)
    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:78)
    at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:360)
    at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:399)
    at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:65)
    at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:890)
    at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1743)
    at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49)
    at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
    at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
    at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
    at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.hibernate.exception.SQLGrammarException: could not execute statement
    at org.hibernate.exception.internal.SQLExceptionTypeDelegate.convert(SQLExceptionTypeDelegate.java:63)
    at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:37)
    at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:113)
    at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:99)
    at org.hibernate.engine.jdbc.internal.ResultSetReturnImpl.executeUpdate(ResultSetReturnImpl.java:200)
    at org.hibernate.dialect.identity.GetGeneratedKeysDelegate.executeAndExtract(GetGeneratedKeysDelegate.java:58)
    at org.hibernate.id.insert.AbstractReturningDelegate.performInsert(AbstractReturningDelegate.java:43)
    at org.hibernate.persister.entity.AbstractEntityPersister.insert(AbstractEntityPersister.java:3279)
    at org.hibernate.persister.entity.AbstractEntityPersister.insert(AbstractEntityPersister.java:3885)
    at org.hibernate.action.internal.EntityIdentityInsertAction.execute(EntityIdentityInsertAction.java:84)
    at org.hibernate.engine.spi.ActionQueue.execute(ActionQueue.java:645)
    at org.hibernate.engine.spi.ActionQueue.addResolvedEntityInsertAction(ActionQueue.java:282)
    at org.hibernate.engine.spi.ActionQueue.addInsertAction(ActionQueue.java:263)
    at org.hibernate.engine.spi.ActionQueue.addAction(ActionQueue.java:317)
    at org.hibernate.event.internal.AbstractSaveEventListener.addInsertAction(AbstractSaveEventListener.java:330)
    at org.hibernate.event.internal.AbstractSaveEventListener.performSaveOrReplicate(AbstractSaveEventListener.java:287)
    at org.hibernate.event.internal.AbstractSaveEventListener.performSave(AbstractSaveEventListener.java:193)
    at org.hibernate.event.internal.AbstractSaveEventListener.saveWithGeneratedId(AbstractSaveEventListener.java:123)
    at org.hibernate.event.internal.DefaultPersistEventListener.entityIsTransient(DefaultPersistEventListener.java:185)
    at org.hibernate.event.internal.DefaultPersistEventListener.onPersist(DefaultPersistEventListener.java:128)
    at org.hibernate.event.internal.DefaultPersistEventListener.onPersist(DefaultPersistEventListener.java:55)
    at org.hibernate.event.service.internal.EventListenerGroupImpl.fireEventOnEachListener(EventListenerGroupImpl.java:107)
    at org.hibernate.internal.SessionImpl.firePersist(SessionImpl.java:756)
    at org.hibernate.internal.SessionImpl.persist(SessionImpl.java:742)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at org.springframework.orm.jpa.ExtendedEntityManagerCreator$ExtendedEntityManagerInvocationHandler.invoke(ExtendedEntityManagerCreator.java:362)
    at jdk.proxy2/jdk.proxy2.$Proxy189.persist(Unknown Source)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at org.springframework.orm.jpa.SharedEntityManagerCreator$SharedEntityManagerInvocationHandler.invoke(SharedEntityManagerCreator.java:311)
    at jdk.proxy2/jdk.proxy2.$Proxy189.persist(Unknown Source)
    at org.springframework.data.jpa.repository.support.SimpleJpaRepository.save(SimpleJpaRepository.java:666)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289)
    at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137)
    at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121)
    at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:529)
    at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:285)
    at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:639)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
    at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:163)
    at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:138)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
    at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
    at org.springframework.transaction.interceptor.TransactionInterceptor$1.proceedWithInvocation(TransactionInterceptor.java:123)
    at org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:388)
    at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:119)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
    at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137)
    ... 111 more
Caused by: java.sql.SQLSyntaxErrorException: ORA-00932: inconsistent datatypes: expected MDSYS.SDO_GEOMETRY got BINARY

    at oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:630)
    at oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:564)
    at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1151)
    at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:771)
    at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:299)
    at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:498)
    at oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:152)
    at oracle.jdbc.driver.T4CPreparedStatement.executeForRows(T4CPreparedStatement.java:1052)
    at oracle.jdbc.driver.OracleStatement.executeSQLStatement(OracleStatement.java:1531)
    at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1311)
    at oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3746)
    at oracle.jdbc.driver.OraclePreparedStatement.executeLargeUpdate(OraclePreparedStatement.java:3918)
    at oracle.jdbc.driver.OraclePreparedStatement.executeUpdate(OraclePreparedStatement.java:3897)
    at oracle.jdbc.driver.OraclePreparedStatementWrapper.executeUpdate(OraclePreparedStatementWrapper.java:992)
    at com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61)
    at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeUpdate(HikariProxyPreparedStatement.java)
    at org.hibernate.engine.jdbc.internal.ResultSetReturnImpl.executeUpdate(ResultSetReturnImpl.java:197)

","Your Entity class seems to be correct but please verify one more time to check for any inconsistency if present.
Note: I guess you are creating and sending data in a wrong way to DB via controller. The controller logic looks suspicious. Please verify your controller and entity.
Also, check the DB Table. Drop the existing table and try to create a new table via Hibernate and verify once again.

Let me show how I tested to see it working. I started from the very beginning of setting up the Oracle DB on my system.
Steps that will help you to reach to your solution:
Note: I'm using Oracle DB 23c, MacOS &amp; JDK 17.0.8 to prove my point.
You can use any version of Oracle DB. For Windows, I guess Oracle DB installer is present. So, you can skip the VM based setup.
Oracle doesn't have installer support for MacOS. So, for that reason, I had to do in this way.
For MacOS way:

Start with installing Oracle Virtual Box on the system. You can download from here based on the OS.

Then, download the installation Oracle Database 23c Free - Developer Release VirtualBox Appliance (.vba) from there official website here.
This .vba file provides pre-configured Oracle software for your use.
The above virtual machine contains:
1. Oracle Linux 8.7
2. Oracle Database 23.2 Free - Developer Release for Linux x86-64
3. Oracle REST Data Services 23.1
4. Oracle SQLcl 23.1
5. Oracle APEX 22.2


You have to follow the setup video here from Database Star will help you to setup.

After, you done with this, install Oracle SQL Developer on to your system from here.

Also start the VM containing Oracle DB hosted on linux. It will start the oracle DB as well.

Start the Oracle SQL Developer and connect to the VM oracle DB and test the connection as successful or not.



For Oracle 23c, the default user - hr and password - oracle and pluggable db - freepdb1
You are done now with DB setup.

Now, coming to codebase:
I have created a sample spring boot project name 'demo' with this structure.
.
├── HELP.md
├── mvnw
├── mvnw.cmd
├── pom.xml
├── src
│   ├── main
│   │   ├── java
│   │   │   └── com
│   │   │       └── example
│   │   │           └── demo
│   │   │               ├── CustomConnectionFinderForSpatialSupport.java
│   │   │               ├── DemoApplication.java
│   │   │               ├── MyGeo.java
│   │   │               ├── MyGeoRepository.java
│   │   │               └── Resource.java
│   │   └── resources
│   │       ├── application.properties
│   │       ├── static
│   │       └── templates

application.properties:
spring.datasource.url=jdbc:oracle:thin:@localhost:1521/freepdb1
spring.datasource.username=hr
spring.datasource.password=oracle
spring.jpa.hibernate.ddl-auto=create
spring.jpa.properties.hibernate.spatial.dialect=org.hibernate.spatial.dialect.oracle.OracleSpatial10gDialect
spring.jpa.properties.hibernate.spatial.connection_finder=com.example.demo.CustomConnectionFinderForSpatialSupport
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.Oracle12cDialect

MyGeo.java:
package com.example.demo;

import jakarta.persistence.Column;
import jakarta.persistence.Entity;
import jakarta.persistence.Id;
import lombok.Data;
import org.locationtech.jts.geom.Polygon;

@Data
@Entity
public class MyGeo {

    @Id
    @Column(name = &quot;id&quot;)
    private String id;

    @Column(name = &quot;shape&quot;,columnDefinition = &quot;MDSYS.SDO_GEOMETRY&quot;)
    private Polygon shape;
}

MyGeoRepository.java:
package com.example.demo;

import org.springframework.data.jpa.repository.JpaRepository;

public interface MyGeoRepository extends JpaRepository&lt;MyGeo, String&gt; {

}

CustomConnectionFinderForSpatialSupport.java:
public class CustomConnectionFinderForSpatialSupport implements ConnectionFinder {
    @SneakyThrows
    @Override
    public Connection find(Connection connection) {
        return ((HikariProxyConnection) connection).unwrap(OracleConnection.class);
    }
}

Resource.java:
package com.example.demo;

import org.locationtech.jts.geom.Coordinate;
import org.locationtech.jts.geom.GeometryFactory;
import org.locationtech.jts.io.ParseException;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;

import java.util.ArrayList;
import java.util.List;

@RestController
public class Resource {

    @Autowired
    private MyGeoRepository myGeoRepository;

    @GetMapping(&quot;/save&quot;)
    public void save() {
        List&lt;Coordinate&gt; coordinates = new ArrayList&lt;&gt;();
        coordinates.add(new Coordinate(0, 1));
        coordinates.add(new Coordinate(2, 5));
        coordinates.add(new Coordinate(2, 7));
        coordinates.add(new Coordinate(0, 7));
        coordinates.add(new Coordinate(0, 1));
        GeometryFactory geometry = new GeometryFactory();
        MyGeo geo = new MyGeo();
        geo.setId(&quot;ID&quot;);
        geo.setShape(geometry.createPolygon(coordinates.toArray(new Coordinate[0])));
        geo = myGeoRepository.save(geo);
        System.out.println(geo);
    }

}

pom.xml:
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
    xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;3.1.3&lt;/version&gt;
        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;
    &lt;groupId&gt;com.example&lt;/groupId&gt;
    &lt;artifactId&gt;demo&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;demo&lt;/name&gt;
    &lt;description&gt;Demo project for Spring Boot&lt;/description&gt;
    &lt;properties&gt;
        &lt;java.version&gt;17&lt;/java.version&gt;
        &lt;spring-cloud.version&gt;2022.0.4&lt;/spring-cloud.version&gt;
    &lt;/properties&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;!-- https://mvnrepository.com/artifact/jakarta.validation/jakarta.validation-api --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;jakarta.validation&lt;/groupId&gt;
            &lt;artifactId&gt;jakarta.validation-api&lt;/artifactId&gt;
            &lt;version&gt;3.0.2&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt;
            &lt;optional&gt;true&lt;/optional&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.oracle.database.jdbc&lt;/groupId&gt;
            &lt;artifactId&gt;ojdbc8&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.hibernate.orm&lt;/groupId&gt;
            &lt;artifactId&gt;hibernate-spatial&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;
            &lt;artifactId&gt;lombok&lt;/artifactId&gt;
            &lt;optional&gt;true&lt;/optional&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
    &lt;dependencyManagement&gt;
        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
                &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
                &lt;version&gt;${spring-cloud.version}&lt;/version&gt;
                &lt;type&gt;pom&lt;/type&gt;
                &lt;scope&gt;import&lt;/scope&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;
    &lt;/dependencyManagement&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;excludes&gt;
                        &lt;exclude&gt;
                            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;
                            &lt;artifactId&gt;lombok&lt;/artifactId&gt;
                        &lt;/exclude&gt;
                    &lt;/excludes&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;

As soon as I start the application, it will create the table 'MY_GEO' in the oracle db due to this property spring.jpa.hibernate.ddl-auto=create.
Click on refresh button in SQL Developer to bring the newly created table.

Table columns:

Now, I have an endpoint to save the data. So, as soon as I hit the url - http://localhost:8080/save
It successfully save the data without any issue.
Spring Boot Console Logs (got the end of this log to see the printed data from that MyGeo Object):
/Library/Java/JavaVirtualMachines/jdk-17.jdk/Contents/Home/bin/java -agentlib:jdwp=transport=dt_socket,address=127.0.0.1:50764,suspend=y,server=n -javaagent:/Users/anish/Library/Caches/JetBrains/IdeaIC2022.3/captureAgent/debugger-agent.jar -Dfile.encoding=UTF-8 -classpath /Users/anish/Downloads/demo/target/classes:/Users/anish/.m2/repository/org/springframework/boot/spring-boot-starter-web/3.1.3/spring-boot-starter-web-3.1.3.jar:/Users/anish/.m2/repository/org/springframework/boot/spring-boot-starter/3.1.3/spring-boot-starter-3.1.3.jar:/Users/anish/.m2/repository/org/springframework/boot/spring-boot/3.1.3/spring-boot-3.1.3.jar:/Users/anish/.m2/repository/org/springframework/boot/spring-boot-autoconfigure/3.1.3/spring-boot-autoconfigure-3.1.3.jar:/Users/anish/.m2/repository/org/springframework/boot/spring-boot-starter-logging/3.1.3/spring-boot-starter-logging-3.1.3.jar:/Users/anish/.m2/repository/ch/qos/logback/logback-classic/1.4.11/logback-classic-1.4.11.jar:/Users/anish/.m2/repository/ch/qos/logback/logback-core/1.4.11/logback-core-1.4.11.jar:/Users/anish/.m2/repository/org/apache/logging/log4j/log4j-to-slf4j/2.20.0/log4j-to-slf4j-2.20.0.jar:/Users/anish/.m2/repository/org/apache/logging/log4j/log4j-api/2.20.0/log4j-api-2.20.0.jar:/Users/anish/.m2/repository/org/slf4j/jul-to-slf4j/2.0.7/jul-to-slf4j-2.0.7.jar:/Users/anish/.m2/repository/jakarta/annotation/jakarta.annotation-api/2.1.1/jakarta.annotation-api-2.1.1.jar:/Users/anish/.m2/repository/org/yaml/snakeyaml/1.33/snakeyaml-1.33.jar:/Users/anish/.m2/repository/org/springframework/boot/spring-boot-starter-json/3.1.3/spring-boot-starter-json-3.1.3.jar:/Users/anish/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.15.2/jackson-databind-2.15.2.jar:/Users/anish/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.15.2/jackson-annotations-2.15.2.jar:/Users/anish/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.15.2/jackson-core-2.15.2.jar:/Users/anish/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jdk8/2.15.2/jackson-datatype-jdk8-2.15.2.jar:/Users/anish/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jsr310/2.15.2/jackson-datatype-jsr310-2.15.2.jar:/Users/anish/.m2/repository/com/fasterxml/jackson/module/jackson-module-parameter-names/2.15.2/jackson-module-parameter-names-2.15.2.jar:/Users/anish/.m2/repository/org/springframework/boot/spring-boot-starter-tomcat/3.1.3/spring-boot-starter-tomcat-3.1.3.jar:/Users/anish/.m2/repository/org/apache/tomcat/embed/tomcat-embed-core/10.1.12/tomcat-embed-core-10.1.12.jar:/Users/anish/.m2/repository/org/apache/tomcat/embed/tomcat-embed-el/10.1.12/tomcat-embed-el-10.1.12.jar:/Users/anish/.m2/repository/org/apache/tomcat/embed/tomcat-embed-websocket/10.1.12/tomcat-embed-websocket-10.1.12.jar:/Users/anish/.m2/repository/org/springframework/spring-web/6.0.11/spring-web-6.0.11.jar:/Users/anish/.m2/repository/org/springframework/spring-beans/6.0.11/spring-beans-6.0.11.jar:/Users/anish/.m2/repository/org/springframework/spring-webmvc/6.0.11/spring-webmvc-6.0.11.jar:/Users/anish/.m2/repository/org/springframework/spring-context/6.0.11/spring-context-6.0.11.jar:/Users/anish/.m2/repository/org/springframework/spring-expression/6.0.11/spring-expression-6.0.11.jar:/Users/anish/.m2/repository/org/springframework/cloud/spring-cloud-starter-circuitbreaker-resilience4j/3.0.3/spring-cloud-starter-circuitbreaker-resilience4j-3.0.3.jar:/Users/anish/.m2/repository/org/springframework/cloud/spring-cloud-starter/4.0.4/spring-cloud-starter-4.0.4.jar:/Users/anish/.m2/repository/org/springframework/cloud/spring-cloud-context/4.0.4/spring-cloud-context-4.0.4.jar:/Users/anish/.m2/repository/org/springframework/security/spring-security-crypto/6.1.3/spring-security-crypto-6.1.3.jar:/Users/anish/.m2/repository/org/springframework/cloud/spring-cloud-commons/4.0.4/spring-cloud-commons-4.0.4.jar:/Users/anish/.m2/repository/org/springframework/security/spring-security-rsa/1.0.12.RELEASE/spring-security-rsa-1.0.12.RELEASE.jar:/Users/anish/.m2/repository/org/bouncycastle/bcpkix-jdk18on/1.73/bcpkix-jdk18on-1.73.jar:/Users/anish/.m2/repository/org/bouncycastle/bcprov-jdk18on/1.73/bcprov-jdk18on-1.73.jar:/Users/anish/.m2/repository/org/bouncycastle/bcutil-jdk18on/1.73/bcutil-jdk18on-1.73.jar:/Users/anish/.m2/repository/org/springframework/cloud/spring-cloud-circuitbreaker-resilience4j/3.0.3/spring-cloud-circuitbreaker-resilience4j-3.0.3.jar:/Users/anish/.m2/repository/io/github/resilience4j/resilience4j-spring-boot3/2.0.2/resilience4j-spring-boot3-2.0.2.jar:/Users/anish/.m2/repository/io/github/resilience4j/resilience4j-spring6/2.0.2/resilience4j-spring6-2.0.2.jar:/Users/anish/.m2/repository/io/github/resilience4j/resilience4j-annotations/2.0.2/resilience4j-annotations-2.0.2.jar:/Users/anish/.m2/repository/io/github/resilience4j/resilience4j-consumer/2.0.2/resilience4j-consumer-2.0.2.jar:/Users/anish/.m2/repository/io/github/resilience4j/resilience4j-circularbuffer/2.0.2/resilience4j-circularbuffer-2.0.2.jar:/Users/anish/.m2/repository/io/github/resilience4j/resilience4j-framework-common/2.0.2/resilience4j-framework-common-2.0.2.jar:/Users/anish/.m2/repository/io/github/resilience4j/resilience4j-micrometer/2.0.2/resilience4j-micrometer-2.0.2.jar:/Users/anish/.m2/repository/io/github/resilience4j/resilience4j-retry/2.0.2/resilience4j-retry-2.0.2.jar:/Users/anish/.m2/repository/io/github/resilience4j/resilience4j-ratelimiter/2.0.2/resilience4j-ratelimiter-2.0.2.jar:/Users/anish/.m2/repository/io/github/resilience4j/resilience4j-circuitbreaker/2.0.2/resilience4j-circuitbreaker-2.0.2.jar:/Users/anish/.m2/repository/io/github/resilience4j/resilience4j-core/2.0.2/resilience4j-core-2.0.2.jar:/Users/anish/.m2/repository/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar:/Users/anish/.m2/repository/io/github/resilience4j/resilience4j-timelimiter/2.0.2/resilience4j-timelimiter-2.0.2.jar:/Users/anish/.m2/repository/org/springframework/boot/spring-boot-starter-aop/3.1.3/spring-boot-starter-aop-3.1.3.jar:/Users/anish/.m2/repository/org/springframework/spring-aop/6.0.11/spring-aop-6.0.11.jar:/Users/anish/.m2/repository/org/aspectj/aspectjweaver/1.9.20/aspectjweaver-1.9.20.jar:/Users/anish/.m2/repository/org/springframework/boot/spring-boot-starter-actuator/3.1.3/spring-boot-starter-actuator-3.1.3.jar:/Users/anish/.m2/repository/org/springframework/boot/spring-boot-actuator-autoconfigure/3.1.3/spring-boot-actuator-autoconfigure-3.1.3.jar:/Users/anish/.m2/repository/org/springframework/boot/spring-boot-actuator/3.1.3/spring-boot-actuator-3.1.3.jar:/Users/anish/.m2/repository/io/micrometer/micrometer-observation/1.11.3/micrometer-observation-1.11.3.jar:/Users/anish/.m2/repository/io/micrometer/micrometer-commons/1.11.3/micrometer-commons-1.11.3.jar:/Users/anish/.m2/repository/io/micrometer/micrometer-core/1.11.3/micrometer-core-1.11.3.jar:/Users/anish/.m2/repository/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar:/Users/anish/.m2/repository/org/latencyutils/LatencyUtils/2.0.3/LatencyUtils-2.0.3.jar:/Users/anish/.m2/repository/jakarta/validation/jakarta.validation-api/3.0.2/jakarta.validation-api-3.0.2.jar:/Users/anish/.m2/repository/com/github/msarhan/ummalqura-calendar/2.0.2/ummalqura-calendar-2.0.2.jar:/Users/anish/.m2/repository/org/springframework/boot/spring-boot-configuration-processor/3.1.3/spring-boot-configuration-processor-3.1.3.jar:/Users/anish/.m2/repository/org/springframework/boot/spring-boot-starter-data-jpa/3.1.3/spring-boot-starter-data-jpa-3.1.3.jar:/Users/anish/.m2/repository/org/springframework/boot/spring-boot-starter-jdbc/3.1.3/spring-boot-starter-jdbc-3.1.3.jar:/Users/anish/.m2/repository/com/zaxxer/HikariCP/5.0.1/HikariCP-5.0.1.jar:/Users/anish/.m2/repository/org/springframework/spring-jdbc/6.0.11/spring-jdbc-6.0.11.jar:/Users/anish/.m2/repository/org/hibernate/orm/hibernate-core/6.2.7.Final/hibernate-core-6.2.7.Final.jar:/Users/anish/.m2/repository/jakarta/persistence/jakarta.persistence-api/3.1.0/jakarta.persistence-api-3.1.0.jar:/Users/anish/.m2/repository/jakarta/transaction/jakarta.transaction-api/2.0.1/jakarta.transaction-api-2.0.1.jar:/Users/anish/.m2/repository/org/hibernate/common/hibernate-commons-annotations/6.0.6.Final/hibernate-commons-annotations-6.0.6.Final.jar:/Users/anish/.m2/repository/io/smallrye/jandex/3.0.5/jandex-3.0.5.jar:/Users/anish/.m2/repository/com/fasterxml/classmate/1.5.1/classmate-1.5.1.jar:/Users/anish/.m2/repository/net/bytebuddy/byte-buddy/1.14.6/byte-buddy-1.14.6.jar:/Users/anish/.m2/repository/org/glassfish/jaxb/jaxb-runtime/4.0.3/jaxb-runtime-4.0.3.jar:/Users/anish/.m2/repository/org/glassfish/jaxb/jaxb-core/4.0.3/jaxb-core-4.0.3.jar:/Users/anish/.m2/repository/org/eclipse/angus/angus-activation/2.0.1/angus-activation-2.0.1.jar:/Users/anish/.m2/repository/org/glassfish/jaxb/txw2/4.0.3/txw2-4.0.3.jar:/Users/anish/.m2/repository/com/sun/istack/istack-commons-runtime/4.1.2/istack-commons-runtime-4.1.2.jar:/Users/anish/.m2/repository/jakarta/inject/jakarta.inject-api/2.0.1/jakarta.inject-api-2.0.1.jar:/Users/anish/.m2/repository/org/antlr/antlr4-runtime/4.10.1/antlr4-runtime-4.10.1.jar:/Users/anish/.m2/repository/org/springframework/data/spring-data-jpa/3.1.3/spring-data-jpa-3.1.3.jar:/Users/anish/.m2/repository/org/springframework/data/spring-data-commons/3.1.3/spring-data-commons-3.1.3.jar:/Users/anish/.m2/repository/org/springframework/spring-orm/6.0.11/spring-orm-6.0.11.jar:/Users/anish/.m2/repository/org/springframework/spring-tx/6.0.11/spring-tx-6.0.11.jar:/Users/anish/.m2/repository/org/springframework/spring-aspects/6.0.11/spring-aspects-6.0.11.jar:/Users/anish/.m2/repository/com/oracle/database/jdbc/ojdbc8/21.9.0.0/ojdbc8-21.9.0.0.jar:/Users/anish/.m2/repository/org/hibernate/orm/hibernate-spatial/6.2.7.Final/hibernate-spatial-6.2.7.Final.jar:/Users/anish/.m2/repository/org/geolatte/geolatte-geom/1.8.2/geolatte-geom-1.8.2.jar:/Users/anish/.m2/repository/org/locationtech/jts/jts-core/1.18.2/jts-core-1.18.2.jar:/Users/anish/.m2/repository/org/jboss/logging/jboss-logging/3.5.3.Final/jboss-logging-3.5.3.Final.jar:/Users/anish/.m2/repository/org/projectlombok/lombok/1.18.28/lombok-1.18.28.jar:/Users/anish/.m2/repository/jakarta/xml/bind/jakarta.xml.bind-api/4.0.0/jakarta.xml.bind-api-4.0.0.jar:/Users/anish/.m2/repository/jakarta/activation/jakarta.activation-api/2.1.2/jakarta.activation-api-2.1.2.jar:/Users/anish/.m2/repository/org/springframework/spring-core/6.0.11/spring-core-6.0.11.jar:/Users/anish/.m2/repository/org/springframework/spring-jcl/6.0.11/spring-jcl-6.0.11.jar:/Applications/IntelliJ IDEA CE.app/Contents/lib/idea_rt.jar com.example.demo.DemoApplication
Connected to the target VM, address: '127.0.0.1:50764', transport: 'socket'

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::                (v3.1.3)

2023-09-24T13:09:07.592+05:30  INFO 5615 --- [           main] com.example.demo.DemoApplication         : Starting DemoApplication using Java 17.0.8 with PID 5615 (/Users/anish/Downloads/demo/target/classes started by anish in /Users/anish/Downloads/demo)
2023-09-24T13:09:07.595+05:30  INFO 5615 --- [           main] com.example.demo.DemoApplication         : No active profile set, falling back to 1 default profile: &quot;default&quot;
2023-09-24T13:09:08.785+05:30  INFO 5615 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2023-09-24T13:09:08.859+05:30  INFO 5615 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 57 ms. Found 1 JPA repository interfaces.
2023-09-24T13:09:09.224+05:30  INFO 5615 --- [           main] o.s.cloud.context.scope.GenericScope     : BeanFactory id=fbabf9c4-2b1a-389f-80b2-693443647993
2023-09-24T13:09:10.046+05:30  INFO 5615 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
2023-09-24T13:09:10.057+05:30  INFO 5615 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2023-09-24T13:09:10.058+05:30  INFO 5615 --- [           main] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.12]
2023-09-24T13:09:10.189+05:30  INFO 5615 --- [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2023-09-24T13:09:10.190+05:30  INFO 5615 --- [           main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2530 ms
2023-09-24T13:09:10.785+05:30  INFO 5615 --- [           main] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2023-09-24T13:09:10.858+05:30  INFO 5615 --- [           main] org.hibernate.Version                    : HHH000412: Hibernate ORM core version 6.2.7.Final
2023-09-24T13:09:10.860+05:30  INFO 5615 --- [           main] org.hibernate.cfg.Environment            : HHH000406: Using bytecode reflection optimizer
2023-09-24T13:09:10.871+05:30  INFO 5615 --- [           main] o.h.spatial.integration.SpatialService   : HHH80000001: hibernate-spatial integration enabled : true
2023-09-24T13:09:11.047+05:30  INFO 5615 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2023-09-24T13:09:11.469+05:30  INFO 5615 --- [           main] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection oracle.jdbc.driver.T4CConnection@3c7d8a4
2023-09-24T13:09:11.471+05:30  INFO 5615 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2023-09-24T13:09:11.501+05:30  WARN 5615 --- [           main] org.hibernate.orm.deprecation            : HHH90000026: Oracle12cDialect has been deprecated; use org.hibernate.dialect.OracleDialect instead
2023-09-24T13:09:11.695+05:30  INFO 5615 --- [           main] org.hibernate.spatial                    : HHH80000003: hibernate-spatial adding type contributions from : org.hibernate.spatial.dialect.oracle.OracleDialectContributor
2023-09-24T13:09:11.696+05:30  INFO 5615 --- [           main] org.hibernate.spatial                    : HHH80000002: hibernate-spatial using Connection Finder for creating Oracle types : com.example.demo.CustomConnectionFinderForSpatialSupport
2023-09-24T13:09:11.725+05:30  INFO 5615 --- [           main] o.h.b.i.BytecodeProviderInitiator        : HHH000021: Bytecode provider name : bytebuddy
2023-09-24T13:09:11.908+05:30  INFO 5615 --- [           main] o.s.o.j.p.SpringPersistenceUnitInfo      : No LoadTimeWeaver setup: ignoring JPA class transformer
2023-09-24T13:09:11.939+05:30  INFO 5615 --- [           main] org.hibernate.spatial                    : HHH80000003: hibernate-spatial adding type contributions from : org.hibernate.spatial.dialect.oracle.OracleDialectContributor
2023-09-24T13:09:11.939+05:30  INFO 5615 --- [           main] org.hibernate.spatial                    : HHH80000002: hibernate-spatial using Connection Finder for creating Oracle types : com.example.demo.CustomConnectionFinderForSpatialSupport
2023-09-24T13:09:12.121+05:30  INFO 5615 --- [           main] o.h.b.i.BytecodeProviderInitiator        : HHH000021: Bytecode provider name : bytebuddy
2023-09-24T13:09:12.229+05:30  INFO 5615 --- [           main] org.hibernate.spatial                    : HHH80000004: hibernate-spatial adding function contributions from : org.hibernate.spatial.dialect.oracle.OracleDialectContributor
2023-09-24T13:09:12.889+05:30  INFO 5615 --- [           main] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-09-24T13:09:13.205+05:30  INFO 5615 --- [           main] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2023-09-24T13:09:13.650+05:30  WARN 5615 --- [           main] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2023-09-24T13:09:14.837+05:30  INFO 5615 --- [           main] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 1 endpoint(s) beneath base path '/actuator'
2023-09-24T13:09:14.938+05:30  INFO 5615 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8080 (http) with context path ''
2023-09-24T13:09:14.973+05:30  INFO 5615 --- [           main] com.example.demo.DemoApplication         : Started DemoApplication in 7.939 seconds (process running for 8.645)
2023-09-24T13:15:29.478+05:30  INFO 5615 --- [nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2023-09-24T13:15:29.481+05:30  INFO 5615 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2023-09-24T13:15:29.506+05:30  INFO 5615 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : Completed initialization in 24 ms
MyGeo(id=ID, shape=POLYGON ((0 1, 2 5, 2 7, 0 7, 0 1)))

My DB updated with the data that I have sent:

That's all.
"
"I'm quite new in the reactive world
My code looks like this:
    Flux.fromIterable(list)
                    .collectMap(a -&gt; a.getName(),
                            b-&gt; functionReturningMonoOfC(b)
                            .map(C::url)
                    .block();

The result is of type Map&lt;String, Mono&lt;String&gt;&gt; . I would like it to be of type Map&lt;String, String&gt;. Any ideas?
","I suggest to use flatMap operator before collecting the elements into a Map
public class ReactorApp {

    record Person(String name){}

    public static Mono&lt;String&gt; functionReturningMono(Person person) {
        return Mono.just(&quot;Hello &quot; + person.name());
    }

    public static void main(String[] args) {
        List&lt;Person&gt; persons = List.of(
                new Person(&quot;John&quot;),
                new Person(&quot;Mike&quot;),
                new Person(&quot;Stacey&quot;)
        );

        Map&lt;String, String&gt; result = Flux.fromIterable(persons)
                .flatMap(person -&gt; functionReturningMono(person)
                        .map(String::toUpperCase)
                        .map(message -&gt; Map.entry(person.name(), message)))
                .collectMap(Map.Entry::getKey, Map.Entry::getValue)
                .block();

        System.out.println(&quot;Result : &quot; + result);
        // Result : {Mike=HELLO MIKE, Stacey=HELLO STACEY, John=HELLO JOHN}
    }

}

"
"As they describe us here, the WebSecurityConfigurerAdapter will deprecated in a while.
I try to refactor the implementation of WebSecurityConfigurerAdapter with SecurityFilterChain due to I want to implement an JWT pattern.
The main consideration which I faced is that the configure in returns void.
@Override
protected void configure(AuthenticationManagerBuilder auth) throws Exception {
    auth.userDetailsService(userDetailsService).passwordEncoder(bCryptPasswordEncoder);
}

@Override
protected void configure(HttpSecurity http) throws Exception {
    CustomAuthenticationFilter customAuthenticationFilter = new CustomAuthenticationFilter(authenticationManagerBean(), accessTokenExpiredInDays, refreshTokenExpiredInDays, jwtSecret);
    customAuthenticationFilter.setFilterProcessesUrl(&quot;/api/login&quot;);
    http
        .csrf().disable();
    http
        .sessionManagement()
            .sessionCreationPolicy(SessionCreationPolicy.STATELESS);
    http
        .authorizeRequests()
            .antMatchers(&quot;/error&quot;).permitAll();
    http
        .authorizeRequests()
            .antMatchers(&quot;/api/login/**&quot;, &quot;/api/token/refresh/**&quot;).permitAll();
    http
        .authorizeRequests()
            .anyRequest().authenticated();
    http
        .addFilter(customAuthenticationFilter);
    http
        .addFilterBefore(new CustomAuthorizationFilter(jwtSecret), UsernamePasswordAuthenticationFilter.class);
}

@Bean
@Override
public AuthenticationManager authenticationManagerBean() throws Exception{
    return super.authenticationManagerBean();
}

","Note that Spring Security has built-in support for JWT authentication and there is no need to create a custom filter.
You can find an example provided by the Spring Security team here.
However, if you do choose to create a custom filter, the recommended way to configure it is by creating a custom DSL.
This is the same way that Spring Security does it internally.
I've rewritten your configuration below using a custom DSL.
@Bean
public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception {
    http
        .csrf().disable();
    http
        .sessionManagement()
        .sessionCreationPolicy(SessionCreationPolicy.STATELESS);
    http
        .authorizeRequests()
        .antMatchers(&quot;/error&quot;).permitAll();
    http
        .authorizeRequests()
        .antMatchers(&quot;/api/login/**&quot;, &quot;/api/token/refresh/**&quot;).permitAll();
    http
        .authorizeRequests()
        .anyRequest().authenticated();
    // apply the custom DSL which adds the custom filter
    http
        .apply(customDsl());
    http
        .addFilterBefore(new CustomAuthorizationFilter(jwtSecret), UsernamePasswordAuthenticationFilter.class);

    return http.build();
}

public class MyCustomDsl extends AbstractHttpConfigurer&lt;MyCustomDsl, HttpSecurity&gt; {
    @Override
    public void configure(HttpSecurity http) throws Exception {
        AuthenticationManager authenticationManager =
                http.getSharedObject(AuthenticationManager.class);
        CustomAuthenticationFilter filter = 
                new CustomAuthenticationFilter(authenticationManager, accessTokenExpiredInDays, refreshTokenExpiredInDays, jwtSecret);
        filter.setFilterProcessesUrl(&quot;/api/login&quot;);
        http.addFilter(filter);
    }

    public static MyCustomDsl customDsl() {
        return new MyCustomDsl();
    }
}

This configuration, as well as other examples, are described in the Spring blog post on migrating away from the WebSecurityConfigurerAdapter.
"
"I'm trying to connect to Redis using TLS, and it works fine for a keystore that has only a single cert inside of it.
The problem is, if I have multiple certs imported to my keystore, how does it know to choose the correct alias to pull the correct key?
I implemented my own X509KeyManager to see how it works, and the chooseClientAlias(String[] strings, Principal[] prncpls, Socket socket)
method appears to be passed an empty array for prncples, which I'd presume would be how it could tell what cert to use.
But since that is empty, it simply returns whatever the first alias is that matches the keytype specified in the strings input, aka RSA, and that first alias might not be the correct one (which then ends up with it picking the incorrect key, and the ssl connection fails).
Is there something I'm misunderstanding about how this should be working to choose the correct alias for the connection, like do I need to be creating a different SSL Socket Factory &amp; KeyManager for every SSL application I interface with, and explicitly specify the alias to use? Sorry, I'm not super well versed in TLS with java. Thanks.

Commands I used to generate the certs (ran this twice to create the real test cert, and a random fake cert which I imported after the real one to test if it would pick the right alias):
Create CA:
===
&quot;C:\Program Files\Git\mingw64\bin\openssl.exe&quot; genrsa -out ca.key 2048
&quot;C:\Program Files\Git\mingw64\bin\openssl.exe&quot; req -new -x509 -sha256 -key ca.key -out ca.crt

Create Redis Server Cert:
===
&quot;C:\Program Files\Git\mingw64\bin\openssl.exe&quot; genrsa -out redis.key
&quot;C:\Program Files\Git\mingw64\bin\openssl.exe&quot; req -new -sha256 -key redis.key -out redis.csr
&quot;C:\Program Files\Git\mingw64\bin\openssl.exe&quot; x509 -req -in redis.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out redis.crt -days 1000 -sha256

Create Client:
===
&quot;C:\Program Files\Git\mingw64\bin\openssl.exe&quot; genrsa -out client1.key 2048
&quot;C:\Program Files\Git\mingw64\bin\openssl.exe&quot; req -new -sha256 -key client1.key -out client1.csr
&quot;C:\Program Files\Git\mingw64\bin\openssl.exe&quot; x509 -req -in client1.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client1.crt -days 1000 -sha256

Commands I used to import the certs to a keystore:
Add ca to truststore:
=====
keytool -import -alias redisCA -keystore keystore.jks -file ca.crt

generate pkcs12:
=====
openssl pkcs12 -export -in client1.crt -inkey client1.key -out keystore.p12 -name my_cert

Import pkcs12 cert/key to keystore:
=====
keytool -importkeystore -destkeystore keystore.jks -srckeystore keystore.p12 -srcstoretype PKCS12 -alias my_cert

Code I used to interface with Redis (taken basically straight off their websites example):
public void testWithTls() throws IOException, GeneralSecurityException {
        HostAndPort address = new HostAndPort(&quot;localhost&quot;, 6379);
        
        SSLSocketFactory sslFactory = createSslSocketFactory(
                &quot;D:\\tmp\\keystore.jks&quot;,
                &quot;123456&quot;,
                &quot;D:\\tmp\\keystore.jks&quot;,
                &quot;123456&quot;
        );

        JedisClientConfig config = DefaultJedisClientConfig.builder()
                .ssl(true).sslSocketFactory(sslFactory)
                .build();

        JedisPooled jedis = new JedisPooled(address, config);
        jedis.set(&quot;foo&quot;, &quot;bar&quot;);
        System.out.println(jedis.get(&quot;foo&quot;)); // prints bar
}
   
private static SSLSocketFactory createSslSocketFactory(
            String caCertPath, String caCertPassword, String userCertPath, String userCertPassword)
            throws IOException, GeneralSecurityException {

        KeyStore keyStore = KeyStore.getInstance(KeyStore.getDefaultType());
        keyStore.load(new FileInputStream(userCertPath), userCertPassword.toCharArray());

        KeyStore trustStore = KeyStore.getInstance(KeyStore.getDefaultType());
        trustStore.load(new FileInputStream(caCertPath), caCertPassword.toCharArray());

        TrustManagerFactory trustManagerFactory = TrustManagerFactory.getInstance(&quot;SunX509&quot;);
        trustManagerFactory.init(trustStore);

        KeyManagerFactory keyManagerFactory = KeyManagerFactory.getInstance(&quot;SunX509&quot;);
        keyManagerFactory.init(keyStore, userCertPassword.toCharArray());

        SSLContext sslContext = SSLContext.getInstance(&quot;TLS&quot;);
        sslContext.init(keyManagerFactory.getKeyManagers(), trustManagerFactory.getTrustManagers(), null);

        return sslContext.getSocketFactory();
}

Information:
Jedis version: 4.4.3
Redis Docker container version: redis:7.0.10
Redis Docker container run command: `redis-server --tls-port 6379 --port 0 --tls-cert-file /tls/redis.crt --tls-key-file /tls/redis.key --tls-ca-cert-file /tls/ca.crt --loglevel warning`
Why am I using a jks store and not the p12: Because thats what the company I work at uses

","There is no build in solution in the JDK for this kind of use case. I would either suggest to create 2 SSLContext. For every keystore 1 and use that to call the Jedis server.
If you are willing to use a library, then this option is possible. I have created this kind of option which will do the trick. See below for the code snippet:
import nl.altindag.ssl.SSLFactory;
import nl.altindag.ssl.util.CertificateUtils;

import javax.net.ssl.SSLSocketFactory;
import java.nio.file.Paths;
import java.security.cert.Certificate;
import java.util.List;

public class App {
    public static void main(String[] args) {
        String caCertPath = &quot;D:\\tmp\\ca-certs.crt&quot;;
        List&lt;Certificate&gt; certificates = CertificateUtils.loadCertificate(Paths.get(caCertPath));

        SSLFactory sslFactory = SSLFactory.builder()
                .withIdentityMaterial(Paths.get(&quot;D:\\tmp\\keystore-one.jks&quot;), &quot;123456&quot;.toCharArray())
                .withIdentityMaterial(Paths.get(&quot;D:\\tmp\\keystore-two.jks&quot;), &quot;123456&quot;.toCharArray())
                .withIdentityRoute(&quot;client-alias-one&quot;, &quot;https://localhost:6379/&quot;, &quot;https://localhost:6380/&quot;)
                .withIdentityRoute(&quot;client-alias-two&quot;, &quot;https://localhost:6381/&quot;, &quot;https://localhost:6382/&quot;)
                .withTrustMaterial(certificates)
                .build();

        SSLSocketFactory sslSocketFactory = sslFactory.getSslSocketFactory();

        JedisClientConfig config = DefaultJedisClientConfig.builder()
                .ssl(true)
                .sslSocketFactory(sslSocketFactory)
                .build();
    }
    
}

In this case you need to specify the alias which is named in the keystore file for the key and map that to the specific host and port as an identity route aka key material route. In that way it will use a specific key for a list of target servers. The library can be found here: sslcontext-kickstart
"
"I want to get the current code line number when instrumenting the java bytecode. Instrumentation is achieved through ASM. Insert the bytecode corresponding to getLineNumber after the visitcode, the return value is -1, but the return value obtained by instrumentation in other locations is normal.
for example,the source code is as follows
public static int add(int a, int b){
        int sum = a + b;
        return sum;
    }

According to the logic of ASM, the bytecode to obtain the line number information should be inserted after the add method.
But when I call the function in the main method, the line number obtained is -1
At the same time, I also analyzed the assembly code before and after instrumentation, as follows
//this is before instrumentation
public static int add(int, int);
    Code:
       0: iload_0
       1: iload_1
       2: iadd
       3: istore_2
       4: iload_2
       5: ireturn

//this is after instrumentation
public static int add(int, int);
    Code:
       0: new           #33                 // class java/lang/StringBuilder
       3: dup
       4: invokespecial #34                 // Method java/lang/StringBuilder.&quot;&lt;init&gt;&quot;:()V
       7: ldc           #36                 // String _
       9: invokevirtual #40                 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder;
      12: invokestatic  #46                 // Method java/lang/Thread.currentThread:()Ljava/lang/Thread;
      15: invokevirtual #50                 // Method java/lang/Thread.getStackTrace:()[Ljava/lang/StackTraceElement;
      18: iconst_1
      19: aaload
      20: invokevirtual #56                 // Method java/lang/StackTraceElement.getLineNumber:()I
      23: invokevirtual #59                 // Method java/lang/StringBuilder.append:(I)Ljava/lang/StringBuilder;
      26: invokevirtual #63                 // Method java/lang/StringBuilder.toString:()Ljava/lang/String;
      29: invokestatic  #69                 // Method afljava/logger/Logger.writeToLogger:(Ljava/lang/String;)V
      32: iload_0
      33: iload_1
      34: iadd
      35: istore_2
      36: iload_2
      37: ireturn

As you can see, I get not only the line number, but also the class name and method name. Among them, the class name and method name are obtained normally, and the line number is obtained as -1.
Additionally, Only inserting after the visitcode position will let the line number be -1, and inserting the same bytecode at other positions will not have this problem.
And this is one part of my instrumentation code
private void instrument(){
            mv.visitTypeInsn(Opcodes.NEW, &quot;java/lang/StringBuilder&quot;);
            mv.visitInsn(Opcodes.DUP);

            mv.visitMethodInsn(Opcodes.INVOKESPECIAL, &quot;java/lang/StringBuilder&quot;, &quot;&lt;init&gt;&quot;, &quot;()V&quot;, false);
            mv.visitMethodInsn(Opcodes.INVOKESTATIC, &quot;java/lang/Thread&quot;, &quot;currentThread&quot;, &quot;()Ljava/lang/Thread;&quot;, false);
            mv.visitMethodInsn(Opcodes.INVOKEVIRTUAL, &quot;java/lang/Thread&quot;, &quot;getName&quot;, &quot;()Ljava/lang/String;&quot;, false);
            mv.visitMethodInsn(Opcodes.INVOKEVIRTUAL, &quot;java/lang/StringBuilder&quot;, &quot;append&quot;, &quot;(Ljava/lang/String;)Ljava/lang/StringBuilder;&quot;, false);
            mv.visitLdcInsn(&quot;_&quot; + classAndMethodName + &quot;_&quot;);

            mv.visitMethodInsn(Opcodes.INVOKEVIRTUAL, &quot;java/lang/StringBuilder&quot;, &quot;append&quot;, &quot;(Ljava/lang/String;)Ljava/lang/StringBuilder;&quot;, false);
            mv.visitMethodInsn(Opcodes.INVOKESTATIC, &quot;java/lang/Thread&quot;, &quot;currentThread&quot;, &quot;()Ljava/lang/Thread;&quot;, false);
            mv.visitMethodInsn(Opcodes.INVOKEVIRTUAL, &quot;java/lang/Thread&quot;, &quot;getStackTrace&quot;, &quot;()[Ljava/lang/StackTraceElement;&quot;, false);
            mv.visitInsn(Opcodes.ICONST_1);
            mv.visitInsn(Opcodes.AALOAD);
            
            mv.visitMethodInsn(Opcodes.INVOKEVIRTUAL, &quot;java/lang/StackTraceElement&quot;, &quot;getLineNumber&quot;, &quot;()I&quot;, false);
            mv.visitMethodInsn(Opcodes.INVOKEVIRTUAL, &quot;java/lang/StringBuilder&quot;, &quot;append&quot;, &quot;(I)Ljava/lang/StringBuilder;&quot;, false);
            mv.visitMethodInsn(Opcodes.INVOKEVIRTUAL, &quot;java/lang/StringBuilder&quot;, &quot;toString&quot;, &quot;()Ljava/lang/String;&quot;, false);
            mv.visitMethodInsn(Opcodes.INVOKESTATIC, &quot;afljava/logger/Logger&quot;, &quot;writeToLogger&quot;, &quot;(Ljava/lang/String;)V&quot;, false);
        }

        @Override
        public void visitCode() {
            super.visitCode();
            instrument();
        }


Like Holger's code,instead I insert code by using visitcode.
","The line numbers are given by the LineNumberTable Attribute which maps bytecode locations to source code lines. When you transform code with the ASM library, it will take care to adapt code locations to reflect changes.
This implies that when you inject code before any original code, the location of the first code associated with a line number gets adapted too, so your new code is not covered by the line numbers.
Instead of injecting the code on visitCode, you may inject it after the first line number has been reported through visitLineNumber. In the best case, this still is before any executable code (it may not, if synthetic code has been injected by other means already).
This way, the new code gets associated with the first recorded line number. However, you don’t need to deal with stack traces to reconstitute this information, as it is already known at this point of code injection. Since class and method name are known too, there is not even a need to generate string concatenation code. You can assemble the string beforehand.
package com.example;

import java.lang.invoke.MethodHandles;

import org.objectweb.asm.*;

public class AsmExample {
    static class Test {
        public static int add(int a, int b){
            int sum = a + b;
            return sum;
        }
    }

    public static void main(String[] args) throws Exception {
        ClassReader cr = new ClassReader(AsmExample.class.getName()+&quot;$Test&quot;);
        ClassWriter cw = new ClassWriter(cr, ClassWriter.COMPUTE_MAXS);
        cr.accept(new ClassVisitor(Opcodes.ASM9, cw) {
            String className;
            @Override
            public void visit(int ver,
                int acc, String name, String sig, String superName, String[] ifs) {

                super.visit(ver, acc, name, sig, superName, ifs);
                className = name.replace('/', '.');
            }
            @Override
            public MethodVisitor visitMethod(
                int acc, String name, String desc, String sig, String[] ex) {

                MethodVisitor mv = super.visitMethod(acc, name, desc, sig, ex);
                if(name.equals(&quot;add&quot;)) mv = new Injector(mv, className + '_' + name);
                return mv;
            }
        }, 0);

        MethodHandles.lookup().defineClass(cw.toByteArray());

        System.out.println(&quot;return value: &quot; + Test.add(30, 12));
    }

    static class Injector extends MethodVisitor {
        private final String classAndMethodName;
        private boolean logStatementAdded;

        public Injector(MethodVisitor methodVisitor, String classAndMethod) {
            super(Opcodes.ASM9, methodVisitor);
            classAndMethodName = classAndMethod;
        }

        @Override
        public void visitLineNumber(int line, Label start) {
            super.visitLineNumber(line, start);
            if(!logStatementAdded) {
                logStatementAdded = true;
                visitFieldInsn(Opcodes.GETSTATIC,
                    &quot;java/lang/System&quot;, &quot;out&quot;, &quot;Ljava/io/PrintStream;&quot;);
                visitLdcInsn(classAndMethodName + &quot;_&quot; + line);
                visitMethodInsn(Opcodes.INVOKEVIRTUAL,
                    &quot;java/io/PrintStream&quot;, &quot;println&quot;, &quot;(Ljava/lang/String;)V&quot;, false);
            }
        }
    }
}

com.example.AsmExample$Test_add_10
return value: 42

I used a simple print statement instead of your logger, but the example should be easy to adapt.

As an alternative, if you want to stay with your original logic as much as possible, you may just alter the bytecode location of the first reported line number association, to cover your injected code:
static class Injector extends MethodVisitor {
    private final String classAndMethodName;
    Label newStart = new Label();

    public Injector(MethodVisitor methodVisitor, String classAndMethod) {
        super(Opcodes.ASM9, methodVisitor);
        classAndMethodName = classAndMethod;
    }

    @Override
    public void visitCode() {
        super.visitCode();
        visitLabel(newStart);
        instrument();
    }

    @Override
    public void visitLineNumber(int line, Label start) {
        if(newStart != null) {
            start = newStart;
            newStart = null;
        }
        super.visitLineNumber(line, start);
    }

    …

Keep in mind that a line number reported for a code location is associated with all following instructions, until the next line number is reported. While ASM will invoke the visitor methods in the order of the code locations, we don’t need to be as strict when calling into the class writer.
So we can associate a Label with the beginning of the method by calling visitLabel(newStart); before instrument();, without knowing the line number. By the time, visitLineNumber is called for the first time, we replace the label start, which represents the original start of the method, with our new label, representing the new start. ASM doesn’t mind that we didn’t call visitLineNumber before instrument();, as only the code location associated with the Label matters.
"
"I have a thread pool with 8 threads
private static final ExecutorService SERVICE = Executors.newFixedThreadPool(8);

My mechanism emulating the work of 100 user (100 Tasks):
List&lt;Callable&lt;Boolean&gt;&gt; callableTasks = new ArrayList&lt;&gt;();
for (int i = 0; i &lt; 100; i++) { // Number of users == 100
    callableTasks.add(new Task(client));
}
SERVICE.invokeAll(callableTasks);
SERVICE.shutdown();

The user performs the Task of generating a document.

Get UUID of Task;
Get Task status every 10 seconds;
If Task is ready get document.

public class Task implements Callable&lt;Boolean&gt; {

    private final ReportClient client;

    public Task(ReportClient client) {
        this.client = client;
    }

    @Override
    public Boolean call() {
        final var uuid = client.createDocument(documentId);
        GetStatusResponse status = null;
        do {
            try {
                Thread.sleep(10000); // This stop current thread, but not a Task!!!!
            } catch (InterruptedException e) {
                return Boolean.FALSE;
            }
            status = client.getStatus(uuid);
        } while (Status.PENDING.equals(status.status()));
        final var document = client.getReport(uuid);
        return Boolean.TRUE;
    }
}

I want to give the idle time (10 seconds) to another task. But when the command Thread.sleep(10000); is called, the current thread suspends its execution. First 8 Tasks are suspended and 92 Tasks are pending 10 seconds. How can I do 100 Tasks in progress at the same time?
","EDIT: I just posted this answer and realized that you seem to be using that code to emulate real user interactions with some system. I would strongly recommend just using a load testing utility for that, rather than trying to come up with your own. However, in that case just using a CachedThreadPool might do the trick, although probably not a very robust or scalable solution.
Thread.sleep() behavior here is working as intended: it suspends the thread to let the CPU execute other threads.
Note that in this state a thread can be interrupted for a number of reasons unrelated to your code, and in that case your Task returns false: I'm assuming you actually have some retry logic down the line.
So you want two mutually exclusive things: on the one hand, if the document isn't ready, the thread should be free to do something else, but should somehow return and check that document's status again in 10 seconds.
That means you have to choose:

You definitely need that once-every-10-seconds check for each document - in that case, maybe use a cachedThreadPool and have it generate as many threads as necessary, just keep in mind that you'll carry the overhead for numerous threads doing virtually nothing.

Or, you can first initiate that asynchronous document creation process and then only check for status in your callables, retrying as needed.


Something like:
public class Task implements Callable&lt;Boolean&gt; {
    private final ReportClient client;
    private final UUID uuid;
    // all args constructor omitted for brevity
    @Override
    public Boolean call() {
        GetStatusResponse status = client.getStatus(uuid);
        if (Status.PENDING.equals(status.status())) {
            final var document = client.getReport(uuid);
            return Boolean.TRUE;
        } else {
            return Boolean.FALSE; //retry next time
        }
    }
}

List&lt;Callable&lt;Boolean&gt;&gt; callableTasks = new ArrayList&lt;&gt;();
for (int i = 0; i &lt; 100; i++) { 
    var uuid = client.createDocument(documentId); //not sure where documentId comes from here in your code
    callableTasks.add(new Task(client, uuid));
}

List&lt;Future&lt;Boolean&gt;&gt; results = SERVICE.invokeAll(callableTasks); 
// retry logic until all results come back as `true` here

This assumes that createDocument is relatively efficient, but that stage can be parallelized just as well, you just need to use a separate list of Runnable tasks and invoke them using the executor service.
Note that we also assume that the document's status will indeed eventually change to something other than PENDING, and that might very well not be the case. You might want to have a timeout for retries.
"
"I'm using a JTextArea in a JFrame. I would like the tab key to insert four spaces instead of a tab.
The method setTabSize does not work, as it puts a tab ('\t') in the contents of the text area.
How can I have JTextArea insert four spaces instead of a tab whenever I press the tab key? That way the getText() method will return indentations of four spaces for every tab.
","I would avoid using KeyListeners (as a general rule with JTextComponents) and even Key Bindings, since while Key Bindings would work for keyboard input, it wouldn't work for copy-and-paste.
In my mind, the best way is to use a DocumentFilter set on the JTextArea's Document (which is a PlainDocument, by the way). This way, even if you copy and paste text into the JTextAreas, one with tabs, then all the tabs will automatically be converted to 4 spaces on insertion.
For example:
import javax.swing.*;
import javax.swing.text.AttributeSet;
import javax.swing.text.BadLocationException;
import javax.swing.text.DocumentFilter;
import javax.swing.text.PlainDocument;

public class TestTextArea {
    public static void main(String[] args) {
        SwingUtilities.invokeLater(() -&gt; {
            JTextArea textArea = new JTextArea(20, 50);
            JScrollPane scrollPane = new JScrollPane(textArea);

            int spaceCount = 4;
            ((PlainDocument) textArea.getDocument()).setDocumentFilter(new ChangeTabToSpacesFilter(spaceCount));

            JFrame frame = new JFrame(&quot;GUI&quot;);
            frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);
            frame.add(scrollPane);
            frame.pack();
            frame.setLocationRelativeTo(null);
            frame.setVisible(true);
        });
    }
    
    private static class ChangeTabToSpacesFilter extends DocumentFilter {
        private int spaceCount;
        private String spaces = &quot;&quot;;
        
        public ChangeTabToSpacesFilter(int spaceCount) {
            this.spaceCount = spaceCount;
            for (int i = 0; i &lt; spaceCount; i++) {
                spaces += &quot; &quot;;
            }
        }

        @Override
        public void insertString(FilterBypass fb, int offset, String string, AttributeSet attr)
                throws BadLocationException {
            string = string.replace(&quot;\t&quot;, spaces);
            super.insertString(fb, offset, string, attr);
        }
        
        @Override
        public void remove(FilterBypass fb, int offset, int length) throws BadLocationException {
            super.remove(fb, offset, length);
        }
        
        @Override
        public void replace(FilterBypass fb, int offset, int length, String text, AttributeSet attrs)
                throws BadLocationException {
            text = text.replace(&quot;\t&quot;, spaces);
            super.replace(fb, offset, length, text, attrs);
        }
        
    }

}

So now, even if I copy and paste a document with tabs within it, into the JTextArea, all tabs will be automatically replaced with spaceCount spaces.
"
"I am using Jackson XML annotation to serialize/deserialize xml output of my spring boot APIs. This API is meant to provide legacy support. Because of that, it's required to have the exact same response structure as a new API as well. I encountered a peculiar situation for which I could not find any solution. I tried customSerilizer but that also doesn't seem to solve the problem.
I need to serialize A and B tags into pairs without any parent tags. The existing XML serilizes data in this format.
&lt;Item&gt;
  &lt;SNO&gt;22656565&lt;/SNO&gt;
  &lt;Weight&gt;0.0&lt;/Weight&gt;
  &lt;A&gt;data1&lt;/A&gt;
  &lt;B&gt;foo1&lt;/B&gt;
  &lt;A&gt;data2&lt;/A&gt;
  &lt;A&gt;data3&lt;/A&gt;
  &lt;B&gt;foo3&lt;/B&gt;
  &lt;A&gt;data4&lt;/A&gt;
  &lt;A&gt;data5&lt;/A&gt;
  &lt;A&gt;data6&lt;/A&gt;
  &lt;B&gt;foo6&lt;/B&gt;
&lt;/Item&gt;

The java pojo look like this:

@Data
@JacksonXmlRootElement(localName = &quot;Item&quot;)
@JsonPropertyOrder({&quot;SNO&quot;, &quot;Weight&quot;, &quot;A&quot;, &quot;B&quot;})
public class Item {

    @JacksonXmlProperty(localName = &quot;SNO&quot;)
    private String sNo;
    
    @JacksonXmlProperty(localName = &quot;Weight&quot;)
    private Float weight;
    
    @JacksonXmlProperty(localName = &quot;A&quot;)
    @JacksonXmlElementWrapper(useWrapping = false)
    private List&lt;String&gt; aList;
    
    @JacksonXmlProperty(localName = &quot;B&quot;)
    @JacksonXmlElementWrapper(useWrapping = false)
    private List&lt;String&gt; bList;
}

Note: through a CustomSerializer I was able to read this data but serilization cannot be done. Whatever I try at the end all of A and B tags are grouped after serialization.
Could you please help in finding any solution to serialize this case? Thanks in advance.
","It is possible to serialize your Item object with the alternate A and B tags in your xml file with a custom serializer, so if you use the provided Item class you can directly add a JsonSerialize annotation over your class and it will work fine with the constraint of the same size of the two not null lists like you have specified:
@JsonSerialize(using = ItemSerializer.class) //&lt;-- here the new annotation
@Data
@JacksonXmlRootElement(localName = &quot;Item&quot;)
@JsonPropertyOrder({&quot;SNO&quot;, &quot;Weight&quot;, &quot;A&quot;, &quot;B&quot;})
public class Item {

    @JacksonXmlProperty(localName = &quot;SNO&quot;)
    private String sNo;

    @JacksonXmlProperty(localName = &quot;Weight&quot;)
    private Float weight;

    @JacksonXmlProperty(localName = &quot;A&quot;)
    @JacksonXmlElementWrapper(useWrapping = false)
    private List&lt;String&gt; aList;

    @JacksonXmlProperty(localName = &quot;B&quot;)
    @JacksonXmlElementWrapper(useWrapping = false)
    private List&lt;String&gt; bList;
}

So you can define a new class extending the StdSerializer providing the alternate writing of A and B tags for your lists:
public class ItemSerializer extends StdSerializer&lt;Item&gt; {

    public ItemSerializer() {
        super(Item.class);
    }

    @Override
    public void serialize(Item value, JsonGenerator jgen, SerializerProvider provider) throws IOException {
        jgen.writeStartObject();
        jgen.writeStringField(&quot;SNO&quot;, value.getSNo());
        jgen.writeNumberField(&quot;Weight&quot;, value.getWeight());
        List&lt;String&gt; aList = value.getAList();
        for (int i = 0; i &lt; aList.size(); ++i) {
            jgen.writeStringField(&quot;A&quot;, aList.get(i));
            jgen.writeStringField(&quot;B&quot;, value.getBList().get(i));
        }
        jgen.writeEndObject();
    }
}

The expected result will be produced like below:
Item item = new Item();
item.setSNo(&quot;22656565&quot;);
item.setWeight(0.0f);
List&lt;String&gt; aList = List.of(&quot;data1&quot;, &quot;data2&quot;, &quot;data3&quot;);
item.setAList(aList);
List&lt;String&gt; bList = List.of(&quot;foo1&quot;, &quot;foo2&quot;, &quot;foo3&quot;);
item.setBList(bList);
String s = xmlMapper.writeValueAsString(item);
System.out.println(s);

Output:
&lt;Item&gt;
  &lt;SNO&gt;22656565&lt;/SNO&gt;
  &lt;Weight&gt;0.0&lt;/Weight&gt;
  &lt;A&gt;data1&lt;/A&gt;
  &lt;B&gt;foo1&lt;/B&gt;
  &lt;A&gt;data2&lt;/A&gt;
  &lt;B&gt;foo2&lt;/B&gt;
  &lt;A&gt;data3&lt;/A&gt;
  &lt;B&gt;foo3&lt;/B&gt;
&lt;/Item&gt;

"
"Question
What is the proper way to create a Java annotation processor, which makes use of annotations that it itself generates?
Context
I'm looking at annotation processing as a means of generating repetitive/boilerplate code and currently in my crosshair are annotations that use an enum. From what I understand only enums which are explicitly referenced can be used, however I'd like to be able to use any client Enum (thus not something that is known to the annotation processor at its compile time).
public @interface GenericEnumAnnotation() {
    Enum&lt;?&gt; value();
}

doesn't work, rather this has to be done as
public @interface MyEnumAnnotation() {
    MyEnum value();
}

So code generation to the rescue! Rather than having the client create a custom annotation for each Enum, I have it setup to generate this annotation based on a @GenerateAnnotation annotation. Thus
@GenerateAnnotation
public enum MyEnum {...}

will generate the valid MyEnumAnnotation
@EnumAnnotation
public @interface MyEnumAnnotation() {
    MyEnum value();
}

Client code can then make use of the generated @MyEnumAnnotation. Now that the enum is generated, I want to now use this @MyEnumAnnotation to generate some additional code for client code that is annotated with it. The newly generated annotation becomes available in the second pass of the annotation processor, and thanks to the @EnumAnnotation I can tell that this is the annotation that I want to use for code generation, however when I make the attempt no usages are found.
@SupportedAnnotationTypes(&quot;com.company.generator.EnumAnnotation&quot;)
@AutoService(Processor.class)
public class EnumAnnotationProcessor extends AbstractProcessor {

    @Override
    public boolean process(Set&lt;? extends TypeElement&gt; annotations, RoundEnvironment env) {
        annotations.forEach(enumAnnotation -&gt; { //@EnumAnnotation
            env.getElementsAnnotatedWith(enumAnnotation).forEach(customAnnontation -&gt; { //@MyEnumAnnotation
                env.getElementsAnnotatedWith(customAnnotation -&gt; { // Elements using the @MyEnumAnnotation
                    // Never entered - nothing annotated is found
                });
            });
        });
    }
}

From experimentation I've determined that this is due to the second pass only looking at the &quot;new files&quot; rather than the full scope/scale of the classes. The client code (which uses the annotation) is only processed during the initial pass and as such it is no longer searchable/accessible in the second pass when the annotation processor actually knows of this generated annotation.
The only method that I have found that allows me to go back and &quot;reprocess&quot; the original file set is by means of a separate processor which just purely holds on to the environment from the first pass, and using it rather than the environment from subsequent passes.
@SupportedAnnotationTypes(&quot;*&quot;)
@SupportedSourceVersion(SourceVersion.RELEASE_21)
@AutoService(Processor.class)
public class FirstPassCollector extends AbstractProcessor {
    
    public static RoundEnvironment firstPassEnvironment = null;

    @Override
    public boolean process(Set&lt;? extends TypeElement&gt; annotations, RoundEnvironment roundEnv) {
        if (firstPassEnvironment == null)
            FirstPassCollector.firstPassEnvironment = roundEnv;
        return false;
    }

}


@SupportedAnnotationTypes(&quot;com.company.generator.EnumAnnotation&quot;)
@AutoService(Processor.class)
public class EnumAnnotationProcessor extends AbstractProcessor {

    @Override
    public boolean process(Set&lt;? extends TypeElement&gt; annotations, RoundEnvironment env) {
        annotations.forEach(enumAnnotation -&gt; {
            env.getElementsAnnotatedWith(enumAnnotation).forEach(customAnnontation -&gt; {
                FirstPassCollector.firstPassEnvironment.getElementsAnnotatedWith(customAnnotation -&gt; {
                    // Now searching the files from the first pass, and annotated classes are now found!
                });
            });
        });
    }
}


I know there are deficiencies in the code as written (i.e.: no null check on the firstPassEnvironment when using it), however as a concept this is something that works, but feels like a rather brittle/hacked solution. Is there a better way of accomplishing this end goal?
","The problem:
I would be excited to tackle the question:

What is the proper way to create a Java annotation processor, which makes use of annotations that it itself generates?

This particular answer however addresses the following (slightly changed) question:

How to create a Java annotation processor, which makes use of annotations that itself generates?

The difference is that I am not 100% certain (or able to guarantee) the &quot;proper&quot; part.
Note though that the original problem you are presenting seems to be another one:

I'd like to be able to use any client Enum

...which I will give a shot at another answer here later.
Concept
By the problem statement itself one can devise the following sequence of steps needed to fulfil (in order):

Generate annotations for specific enums.
Generate code for elements annotated with the generated annotations of the previous step.

That's only two steps (hence the name of the processor in the implementation below). One could extend this concept to more steps, for example if in step 2 the generated code needs further processing, such as when step 2 would generate more annotations (and so on until as many steps as needed are complete). This answer addresses this only-two step concept.
Long story short, one more detailed sequence of steps for solving the problem is the following:

Find annotated enums which we want to generate annotations for.
Generate the annotations of the enums found at step 1.
Find elements annotated with any annotation generated at step 2.
Generate any code required for the elements found at step 3.

...which is intended to be repeated per round and I am going to use it as reference in the answer parts which follow.
Materializing a solution:
For step 1, we can just create an annotation which will annotate each enum the user wants to fit into the category. We will then find these enums through the getElementsAnnotatedWith method of RoundEnvironment (on each round). This idea is taken from your GenerateAnnotation (and is named as GenerateEnumAnnotation in this answer).
For step 2, we should use the Filer to generate the annotations. We want to create them through the Filer so that they are taken into account for the compilation.
Then, according to the documentation of Processor:

On each round, a processor may be asked to process a subset of the annotations found on the source and class files produced by a prior round.

as well as also according to the documentation of Filer:

This interface supports the creation of new files by an annotation processor. ... Source and class files so created will be considered for processing by the tool in a subsequent round of processing after the close method has been called on the Writer or OutputStream used to write the contents of the file.

So when we generate source files (which includes generated annotations) through the Filer interface, we get their corresponding Elements as the root ones in subsequent rounds. We can then use these to find which other Elements seen so far are annotated with them (fulfilling step 3).
Step 4 now is ready to generate the code you are requiring, since we now have each enum annotation at our disposal from the 3rd step.
Implementation:
Annotation to generate the enum annotations:
package annotations.soq78648395;

import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;

@Target(ElementType.TYPE)
@Retention(RetentionPolicy.SOURCE)
public @interface GenerateEnumAnnotation {
}

Processor:
package annotations.soq78648395;

import java.io.IOException;
import java.io.PrintStream;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Map;
import java.util.Objects;
import java.util.Set;
import javax.annotation.processing.AbstractProcessor;
import javax.annotation.processing.Messager;
import javax.annotation.processing.ProcessingEnvironment;
import javax.annotation.processing.RoundEnvironment;
import javax.annotation.processing.SupportedAnnotationTypes;
import javax.annotation.processing.SupportedSourceVersion;
import javax.lang.model.SourceVersion;
import javax.lang.model.element.AnnotationMirror;
import javax.lang.model.element.Element;
import javax.lang.model.element.ElementKind;
import javax.lang.model.element.TypeElement;
import javax.lang.model.util.Elements;
import javax.tools.Diagnostic;
import javax.tools.JavaFileObject;

@SupportedSourceVersion(SourceVersion.RELEASE_8)
@SupportedAnnotationTypes(&quot;annotations.soq78648395.GenerateEnumAnnotation&quot;)
public class TwoStepEnumAnnotationProcessor extends AbstractProcessor {
    
    /**
     * We need an {@code Element} representation which can be independent of the round. That's
     * because {@code Element} information is populated as new rounds are coming (for example for
     * the generated annotations), so we need to reload {@code Element}s on every round.
     */
    private static final class InterRoundElement {
        
        //All public final to avoid getters and setters in order to save space for the answer post itself.
        public final String simpleName, packageName, qualifiedName;
        
        public InterRoundElement(final Elements elementUtils,
                                 final Element element) {
            this(element.getSimpleName().toString(), elementUtils.getPackageOf(element).getQualifiedName().toString());
        }
        
        public InterRoundElement(final String simpleName,
                                 final String packageName) {
            this.simpleName = simpleName;
            this.packageName = packageName;
            qualifiedName = packageName.isEmpty()? simpleName: (packageName + '.' + simpleName);
        }

        @Override
        public String toString() {
            return qualifiedName;
        }

        @Override
        public int hashCode() {
            return Objects.hashCode(qualifiedName);
        }

        @Override
        public boolean equals(final Object obj) {
            if (this == obj)
                return true;
            if (obj == null || getClass() != obj.getClass())
                return false;
            return Objects.equals(qualifiedName, ((InterRoundElement) obj).qualifiedName);
        }
    }
    
    private boolean isAnnotatedWith(final AnnotationMirror annotationMirror,
                                    final TypeElement annotation) {
        final TypeElement other = (TypeElement) annotationMirror.getAnnotationType().asElement();
        //Note here: 'other.getKind()' may actually be 'CLASS' rather than 'ANNOTATION_TYPE' (it happens for annotations generated by annotation processing).
        return Objects.equals(annotation.getQualifiedName().toString(), other.getQualifiedName().toString());
    }
    
    /**
     * As &lt;i&gt;early elements&lt;/i&gt; are named the {@code Element}s which are potentially annotated with
     * an enum annotation which is going to be generated. For example root elements of the first
     * round will not appear again in the following rounds, but they may be already annotated with
     * an enum annotation which is not yet generated, so we need to maintain them until we find out
     * what happens.
     */
    private final Set&lt;InterRoundElement&gt; earlyElements = new HashSet&lt;&gt;();
    
    /**
     * A {@code Map} from generated enum annotations to the {@code Element}s being annotated with
     * them. If an enum annotation is registered as a key of this map, then its code is already
     * generated even if no {@code Elements} are found to be annotated with it (ie for empty map
     * value).
     */
    private final Map&lt;InterRoundElement, Set&lt;InterRoundElement&gt;&gt; processedElements = new HashMap&lt;&gt;();
    
    /**
     * Just a zero based index of the processing round.
     */
    private int roundSerial = -2;
    
    /**
     * For debugging messages.
     * @param tokens
     */
    private void debug(final Object... tokens) {
        System.out.print(String.format(&quot;&gt;&gt;&gt;&gt; [Round %2d]&quot;, roundSerial));
        for (final Object token: tokens) {
            System.out.print(' ');
            System.out.print(token);
        }
        System.out.println();
    }
    
    /**
     * Opens a {@code PrintStream} for writing/generating code.
     * @param interRoundElement
     * @param originatingElements
     * @return
     * @throws IOException
     */
    private PrintStream create(final InterRoundElement interRoundElement,
                               final Element... originatingElements) throws IOException {
        debug(&quot;Will generate output for&quot;, interRoundElement);
        final JavaFileObject outputFileObject = processingEnv.getFiler().createSourceFile(interRoundElement.qualifiedName, originatingElements);
        return new PrintStream(outputFileObject.openOutputStream());
    }
    
    /**
     * Generates an enum annotation.
     * @param origin
     * @param output
     * @param originatingElements
     * @return {@code true} for success, otherwise {@code false}.
     */
    private boolean generateEnumAnnotation(final InterRoundElement origin,
                                           final InterRoundElement output,
                                           final Element... originatingElements) {
        try (final PrintStream outputFileOutput = create(output, originatingElements)) {
            if (!output.packageName.isEmpty()) { //The default package is represented as an empty 'packageName'.
                outputFileOutput.println(&quot;package &quot; + output.packageName + &quot;;&quot;);
                outputFileOutput.println();
            }
            for (final Object line: new Object[]{ //We obviously here need to utilize text blocks of the newer Java versions...
                &quot;@java.lang.annotation.Target(java.lang.annotation.ElementType.TYPE)&quot;,
                &quot;@java.lang.annotation.Retention(java.lang.annotation.RetentionPolicy.SOURCE)&quot;,
                &quot;public @interface &quot; + output.simpleName + &quot; {&quot;,
                &quot;    &quot; + origin.qualifiedName + &quot; value();&quot;,
                &quot;}&quot;
            })
                outputFileOutput.println(line);
            return true;
        }
        catch (final IOException ioe) {
            ioe.printStackTrace(System.out);
            return false;
        }
    }
    
    private void reset() {
        processedElements.clear();
        earlyElements.clear();
        roundSerial = -1;
    }
    
    @Override
    public synchronized void init(final ProcessingEnvironment processingEnv) {
        super.init(processingEnv);
        if (super.isInitialized())
            reset(); //Initialize, prepare for new processes.
    }
    
    /**
     * This method handles elements annotated with {@code GenerateEnumAnnotation}.
     * @param annotatedElement
     */
    private void handleGenerationAnnotation(final Element annotatedElement) {
        final Messager messager = processingEnv.getMessager();
        if (annotatedElement.getKind() != ElementKind.ENUM)
            messager.printMessage(Diagnostic.Kind.ERROR, &quot;Only enums are supported.&quot;, annotatedElement);
        else {
            final InterRoundElement origin = new InterRoundElement(processingEnv.getElementUtils(), (TypeElement) annotatedElement);
            final String pack = (origin.packageName.isEmpty()? &quot;&quot;: (origin.packageName + '.')) + &quot;enum_annotations&quot;;
            final InterRoundElement output = new InterRoundElement(origin.simpleName + &quot;Annotation&quot;, pack); //The enum annotation element to generate...
            if (generateEnumAnnotation(origin, output, annotatedElement))
                processedElements.computeIfAbsent(output, dejaVu -&gt; new HashSet&lt;&gt;()); //Store the generated enum annotation information.
            else
                messager.printMessage(Diagnostic.Kind.ERROR, &quot;Failed to create annotation &quot; + output + &quot; (for &quot; + origin + &quot;).&quot;, annotatedElement);
        }
    }
    
    /**
     * Handles {@code Element}s annotated with a generated enum annotation. Modify this according to
     * your requirements. It assumes that generated enum annotations are not repeatable (otherwise
     * it should take a {@code Collection} of {@code AnnotationMirror}s instead of a single one).
     * @param enumAnnotation The generated enum annotation.
     * @param annotatedElement The {@code Element} annotated with {@code enumAnnotation}.
     * @param annotationMirror The mirror of annotating {@code annotatedElement} with {@code enumAnnotation}.
     */
    private void handleEnumAnnotation(final TypeElement enumAnnotation,
                                      final Element annotatedElement,
                                      final AnnotationMirror annotationMirror) {
        //The current implementation just prints some messages of the annotation we've found...
        debug(&quot;Processing&quot;, annotatedElement.getKind(), &quot;element&quot;, annotatedElement);
        debug(&quot;    Annotated by enum annotation&quot;, enumAnnotation);
        debug(&quot;    With the following applicable mirror:&quot;, annotationMirror);
    }
    
    /**
     * Find out if any &quot;early elements&quot; are annotated with a generated enum annotation, and handle them.
     */
    private void processEarlyElements() {
        final Elements elementUtils = processingEnv.getElementUtils();
        //We need a defensive shallow copy of 'earlyElements' because it is going to be modified inside the loop:
        final Set&lt;InterRoundElement&gt; defensiveCopiedEarlyElements = new HashSet&lt;&gt;(earlyElements);
        processedElements.forEach((annotationInterRoundElement, annotatedElements) -&gt; {
            final TypeElement generatedEnumAnnotation = elementUtils.getTypeElement(annotationInterRoundElement.qualifiedName);
            //'roundEnv.getElementsAnnotatedWith(generatedEnumAnnotation)' will actually return an empty List here, so we have to find elements annotated with 'generatedEnumAnnotation' via its annotation mirrors...
            defensiveCopiedEarlyElements.forEach(interRoundElement -&gt; {
                final TypeElement annotatedElement = elementUtils.getTypeElement(interRoundElement.qualifiedName); //Reload annotated element for the current round (ie don't rely on its previous Element occurences), because its annotations may not yet be ready.
                //The following code assumes generated enum annotations are not repeatable...
                annotatedElement.getAnnotationMirrors().stream()
                        .filter(annotationMirror -&gt; isAnnotatedWith(annotationMirror, generatedEnumAnnotation)) //Continue only for mirrors of type generatedEnumAnnotation.
                        .filter(annotationMirror -&gt; annotatedElements.add(interRoundElement)) //If we've seen the early element before then skip it, otherwise add it to annotatedElements and process it...
                        .findAny()
                        .ifPresent(annotationMirror -&gt; {
                            earlyElements.remove(interRoundElement); //No need to store it any more.
                            handleEnumAnnotation(generatedEnumAnnotation, annotatedElement, annotationMirror);
                        });
            });
        });
    }
    
    @Override
    public boolean process(final Set&lt;? extends TypeElement&gt; annotations,
                           final RoundEnvironment roundEnv) {
        ++roundSerial;
        final Elements elementUtils = processingEnv.getElementUtils();
        final Set&lt;? extends Element&gt; rootElements = roundEnv.getRootElements();
        
        //Store only root elements of type class in 'earlyElements':
        rootElements.stream()
                .filter(rootElement -&gt; rootElement.getKind() == ElementKind.CLASS)
                .map(rootElement -&gt; new InterRoundElement(elementUtils, rootElement))
                .forEachOrdered(earlyElements::add);

        debug(&quot;Annotations:&quot;, annotations);
        debug(&quot;Root elements:&quot;, rootElements);
        
        /*First process early elements and then generate enum annotations. The sequence of these two
        calls is ought to how their methods' body is implemented, and we know we won't loose any
        enum annotations because any generated enum annotations in the current round will be
        supplied as root elements in the next round and we already add &quot;early elements&quot; in the
        beginning of the processor's 'process' method.*/
        processEarlyElements();
        roundEnv.getElementsAnnotatedWith(GenerateEnumAnnotation.class).forEach(this::handleGenerationAnnotation);

        debug(&quot;Current early elements:&quot;, earlyElements);
        processedElements.forEach((annotation, elements) -&gt; debug(&quot;Created annotation&quot;, annotation, &quot;with the following elements processed for it so far:&quot;, elements));

        //Cleanup, prepare for later processes (if reused):
        if (roundEnv.processingOver())
            reset();
        
        /*GenerateEnumAnnotations are always consumed. Even if there is a failure in generating the
        resulting code from a GenerateEnumAnnotation annotated element, this doesn't mean that
        repeating the handleGenerationAnnotation has the potential in succeeding in later rounds,
        so we are not repeating the attempt (ie handling is finished, errors are handled, we move on).*/
        return true;
    }
}

How to use it:
On the user's code, annotate the enums you need with GenerateEnumAnnotation:
package soq78648395;

import annotations.soq78648395.GenerateEnumAnnotation;

@GenerateEnumAnnotation
public enum MyEnum {
    MY_A, MY_B;
}

...and then use the generated enum annotation like so:
package soq78648395;

import soq78648395.enum_annotations.MyEnumAnnotation;

@MyEnumAnnotation(MyEnum.MY_A)
public class MyClass {
}

Assumptions and lack of feature support:

No other processors will interfere with the annotations your processor generates. I haven't considered such scenarios in the code of this answer.
Only enums are supported to be annotated with GenerateEnumAnnotation.
Only classes are currently supported to be annotated with generated enum annotations. This can easily be extended to support all TypeElement kinds through a simple condition check, but for all other Element kinds more modifications are required (depends on the kinds you would want).
Generated enum annotations are not repeatable.
Elements annotated with more than one of the generated enum annotations will only be processed once. The good news is that you can get the annotation mirrors for these elements and inpect them (in handleEnumAnnotation method) to find out if they are annotated with more than one generated enum annotation, but only if you generate all the enum annotations at the same round. For more rounds, you would need to extend the implementation. I think this should potentially also solve the case of the generated enum annotations being repeatable.

Notes:

Since I am only using Java SE version 8 throughout this answer's code, then adaptations may be required to run in newer versions. This answer was a long journey for me (with respect to my other answers in this site and the corresponding research/testing) so I chose the version which I feel most confortable with and know what to expect. But I understand that you need a Java 21 version instead (judging from your code), so I apologize for this. I hope the algorithmic solution of the concept will remain mostly the same and that this code will kickstart your efforts in supporting your newer Java version.
Since we are implicitly processing the generated annotations (that is, we are not declaring support for the generated annotations in any way), then it is likely more appropriate to declare supporting any annotation (&quot;*&quot;) and then claiming only the ones we generate (along with GenerateEnumAnnotation of course). The only thing I think that enables us to verify the correctness of the implementation of this answer (as far as the implicit claiming of generated annotations goes) is whether my first assumption holds true. If you go down the supporting any annotation path then try to be careful not to claim annotations other than the ones you really generate and know (eg through maintaining a data structure between rounds with these annotations).

Update for addressing comments:
According to my experimental observations, the following steps are how processing happens (in the case of the code in this answer at least):

In the first round everything known to the compiler at that point is represented through RoundEnvironment and nothing more. This includes the fact that MyClass is annotated with MyEnumAnnotation, but its corresponding AnnotationMirror currently indicates the annotation type is of kind CLASS, rather than ANNOTATION_TYPE and it seems like there is more information missing (eg you can't obtain its parameters), which should be normal judging from the fact that the code for MyEnumAnnotation is not yet generated and processed. In this same round we are fully generating enum annotations (ie MyEnumAnnotation) (that is, we create the source file and close it's OutputStream, thus making the code available to the compiler tool).
Our processor is called for the second round, where we can now find the generated enum annotations at the root Elements (suggesting that the compiler has now processed the representation of the generated annotations from the previous round, so that they are now reachable as Elements). We are not getting MyClass in root elements in the second and subsequent rounds, but they are reachable through the RoundEnvironment (we need to find them, eg with elementUtils.getTypeElement(...) method). At this point we know enum annotation Elements so one could say we should just use roundEnv.getElementsAnnotatedWith(myEnumAnnotationElement);, but according to my testing this just returns an empty Set here (maybe going with @SupportedAnnotationTypes(&quot;*&quot;) could fix the issue, but I haven't tested it). Another option to find AnnotationMirrors would be to go the other way around, which I mean is to obtain the AnnotationMirrors of the annotated Element and find the ones of interest. This option is what I used, so, if we hadn't maintained a set of seen client Elements then we wouldn't know what to look for.
For your 3rd comment, yes, you understand correctly. According to my experiments, Element information may become outdated as new rounds are coming (for example in the case of @MyEnumAnnotation's type). I don't know the whole annotation processing documentation by heart, but I can't find information if the above experimental results can be backed by the documentation (nor contradicted by it), so I assumed that Elements kept by reference between rounds may not be up to date with their latest internal state in upcoming rounds. Hence the need for an Element representation which can endure changing rounds. Obviously, in this context, it is not practical to maintain all the information about an Element (because it's too much information to maintain, we don't need it all and finally it is expected to change between rounds), and the only information needed is an identifier of the Element to find it on each upcoming round. Since we are specifically looking into TypeElements then their qualified names seem enough for such identification and there is also the handy method elementUtils.getTypeElement to spare us from more involved searches. This identifying information is kept in InterRoundElements in the context of this answer.
For your 4th comment, I am not entirely certain with what you mean to compare maintainability and performance of this code with. To be honest, I haven't benchmarked this code for performance (I only implemented the concept and verified its implementation and runtime to some extent), nor do I know how to compare maintainability (I guess cleaner code wins a maintainability comparison with a less clean one, but I am not really an expert on clean code). I am also uncertain on which functionality of RoundEnvironment you are referring to as being reimplemented here. So, if possible, let me know if there is anything more you could clarify in your 4th comment.

"
"I dont have access to a mac and I need to detect if the MacOS currently running is ARM64 or x64/x86 in java programmatically. Its going to be used to download chromedriver from chrome-for-testing.
Currently, I'm using this code to download:
String osName = System.getProperty(&quot;os.name&quot;, &quot;&quot;).toLowerCase();
if(osName.contains(&quot;darwin&quot;) || osName.contains(&quot;mac&quot;)) {
    // FIXME: no check for ARM64 arch
    _zipName = chromeForTesting ? &quot;chromedriver-mac-x64.zip&quot; : &quot;chromedriver_mac64.zip&quot;;
}

","something along the following (edited and tested on macpro)
cat GetSystemProperty.java 
public class GetSystemProperty {

    public static void main(String[] args) {
        String osName = System.getProperty(&quot;os.name&quot;);
        String osArch = System.getProperty(&quot;os.arch&quot;);

        System.out.println(&quot;OS name: &quot; + osName);
        System.out.println(&quot;OS architecture: &quot; + osArch);
    }
}

java GetSystemProperty
OS name: Mac OS X
OS architecture: x86_64

MacBook-Pro-15: glennstevenson$ 
uname -a
MacBook-Pro-15: 20.6.0 Darwin Kernel Version 20.6.0: Thu Jul  6 22:12:47 PDT 2023; root:xnu-7195.141.49.702.12~1/RELEASE_X86_64 x86_64


NB: a case-insensitive test may be wisest!
As always, test before use !
"
"I am trying to parse the following JSON to POJO, specifically the payload I want to extract as String[] or List of String without losing the JSON format.
{
  &quot;payLoad&quot;: [
    {
      &quot;id&quot;: 1,
      &quot;userName&quot;: null,
      &quot;arName&quot;: &quot;A1&quot;,
      &quot;areas&quot;: []
    },
    {
      &quot;id&quot;: 2,
      &quot;userName&quot;: &quot;alpha2&quot;,
      &quot;arName&quot;: &quot;A2&quot;,
      &quot;areas&quot;: []
    }
  ],
  &quot;count&quot;: 2,
  &quot;respCode&quot;: 200
}

Here is the POJO that I am using -
public class Response {

    @JsonProperty(&quot;count&quot;)
    private int totalCount;

    @JsonProperty(&quot;respCode&quot;)
    private int responseCode;

    @JsonProperty(&quot;payLoad&quot;)
    @JsonFormat(with = JsonFormat.Feature.ACCEPT_SINGLE_VALUE_AS_ARRAY)
    private String[] transactionsList;

    public String[] getTransactionsList() {
        return transactionsList;
    }

    public void setTransactionsList(String[] transactionsList) {
        this.transactionsList = transactionsList;
    }
..
}

This is method I am using with springboot to parse it automatically to
public void transactionsReceived() throws JsonProcessingException {
    ObjectMapper objectMapper = new ObjectMapper();
    Response responseRcvd = objectMapper.readValue(jsonString, Response.class); 
}

Here is an error I am getting -
    Exception in thread &quot;main&quot; com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot deserialize value of type `[Ljava.lang.String;` from Object value (token `JsonToken.START_OBJECT`)
 at [Source: (String)&quot;{&quot;payLoad&quot;: [{&quot;id&quot;: 1,&quot;userName&quot;: null,&quot;arName&quot;: &quot;A1&quot;,&quot;areas&quot;: []},{&quot;id&quot;: 2,&quot;userName&quot;: &quot;alpha2&quot;,&quot;arName&quot;: &quot;A2&quot;,&quot;areas&quot;: []}],&quot;count&quot;: 2,&quot;respCode&quot;: 200}&quot;; line: 1, column: 14] (through reference chain: com.example.demo.model.Response[&quot;payLoad&quot;]-&gt;java.lang.Object[][0])
    at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:59)
    at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1741)..

","You can write custom deserializer:
  public class JsonObjectListDeserializer extends StdDeserializer&lt;List&lt;String&gt;&gt; {

    public JsonObjectListDeserializer() {
      super(List.class);
    }

    @Override
    public List&lt;String&gt; deserialize(JsonParser parser, DeserializationContext context) throws IOException, JacksonException {
      JsonNode node = parser.getCodec().readTree(parser);
      List&lt;String&gt; result = new ArrayList&lt;&gt;();
      if (node.isArray()) {
        for (JsonNode element : node) {
          result.add(element.toString());
        }
      } else if (node.isObject()) {
        result.add(node.toString());
      } else {
        //maybe nothing?
      }
      return result;
    }
  }

JsonNode.toString() returns json representation of the node, like this you revert the node back to json to save in list.
Then register the deserializer only for this particular field.
public static class Response {

    @JsonProperty(&quot;count&quot;)
    private int totalCount;

    @JsonProperty(&quot;respCode&quot;)
    private int responseCode;

    @JsonProperty(&quot;payLoad&quot;)
    @JsonDeserialize(using = JsonObjectListDeserializer.class)
    private List&lt;String&gt; transactionsList;

    //getters, setters, etc.
  }

"
"I have custom StdDeserializer&lt;Date&gt;, how can i unit test the overridden deserialize method here?
or how can i prepare or mock JsonParser here for unit testing desterilize method?
public class StringToDateDeserializer extends StdDeserializer&lt;Date&gt; {

    protected StdDateFormat df = new StdDateFormat();

    public StringToDateDeserializer() {
        this(null);
    }

    protected StringToDateDeserializer(Class&lt;?&gt; T) {
        super(T);
    }

    @Override
    public Date deserialize(JsonParser jsonParser, DeserializationContext ctxt) throws IOException {
        String dateStr = jsonParser.getText();
        if (StringUtils.isEmpty(dateStr)) {
            return null;
        }
        try {
            return df.parse(dateStr);
        } catch (ParseException e) {
            throw new MyCustomException(&quot;Invalid date passed, ISO 8601 is expected&quot;);
        }
    }
}

","Example of test for StringToDateDeserializer with 100% coverage.
public class TestClass {
    private ObjectMapper mapper;
    private StringToDateDeserializer deserializer;

    @Before
    public void setup() {
        mapper = new ObjectMapper();
        deserializer = new StringToDateDeserializer();
    }

    @Test
    public void dateTest() throws IOException {
        Date date = deserializer.deserialize(prepareParser(&quot;{ \&quot;value\&quot;:\&quot;2020-07-10T15:00:00.000\&quot; }&quot;), mapper.getDeserializationContext());

        Assert.assertNotNull(date);
        Assert.assertEquals(1594393200000L, date.getTime());
    }

    @Test(expected = MyCustomException.class)
    public void exceptionalTest() throws IOException {
        deserializer.deserialize(prepareParser(&quot;{ \&quot;value\&quot;:\&quot;2020-07\&quot; }&quot;), mapper.getDeserializationContext());
    }

    @Test
    public void nullTest() throws IOException {
        Date date = deserializer.deserialize(prepareParser(&quot;{ \&quot;value\&quot;:\&quot;\&quot; }&quot;), mapper.getDeserializationContext());

        Assert.assertNull(date);
    }

    private JsonParser prepareParser(String json) throws IOException {
        JsonParser parser = mapper.getFactory().createParser(json);
        while (parser.nextToken() != JsonToken.VALUE_STRING);
        return parser;
    }
}

"
"I am trying to use Java Selenium WebDriver capture all javascripts errors of a webpage.
Here a sample of my code :
import org.openqa.selenium.WebDriver;
import org.openqa.selenium.firefox.FirefoxDriver;
import org.openqa.selenium.firefox.FirefoxOptions;
import org.openqa.selenium.logging.LogEntries;
import org.openqa.selenium.logging.LogType;

public class MainExample {
    public static void main(String[] args) {
        System.setProperty(&quot;webdriver.gecko.driver&quot;, &quot;path_to_driver/geckodriver&quot;);
        FirefoxOptions options = new FirefoxOptions();
        WebDriver driver = new FirefoxDriver(options);
        driver.get(&quot;https://www.google.com&quot;);
        LogEntries entries = driver.manage().logs().get(LogType.BROWSER);
    }
}

As Firefox driver I am using this version : geckodriver-v0.30.0-linux64.tar.gz
Here is my Selenium version :
&lt;dependency&gt;
    &lt;groupId&gt;org.seleniumhq.selenium&lt;/groupId&gt;
    &lt;artifactId&gt;selenium-java&lt;/artifactId&gt;
    &lt;version&gt;4.1.1&lt;/version&gt;
&lt;/dependency&gt;

My problem is that when running the previous code I get the following exception :

Driver info: driver.version: RemoteWebDriver  at
org.openqa.selenium.json.JsonInput.peek(JsonInput.java:122)   at
org.openqa.selenium.json.JsonTypeCoercer.lambda$null$6(JsonTypeCoercer.java:140)
at
org.openqa.selenium.json.JsonTypeCoercer.coerce(JsonTypeCoercer.java:126)
at org.openqa.selenium.json.Json.toType(Json.java:69)   at
org.openqa.selenium.json.Json.toType(Json.java:55)    at
org.openqa.selenium.json.Json.toType(Json.java:50)    at
org.openqa.selenium.remote.http.W3CHttpResponseCodec.decode(W3CHttpResponseCodec.java:87)
at
org.openqa.selenium.remote.http.W3CHttpResponseCodec.decode(W3CHttpResponseCodec.java:49)
at
org.openqa.selenium.remote.HttpCommandExecutor.execute(HttpCommandExecutor.java:158)
at
org.openqa.selenium.remote.service.DriverCommandExecutor.execute(DriverCommandExecutor.java:83)
at
org.openqa.selenium.remote.RemoteWebDriver.execute(RemoteWebDriver.java:552)
at
org.openqa.selenium.remote.RemoteExecuteMethod.execute(RemoteExecuteMethod.java:35)
at
org.openqa.selenium.remote.RemoteLogs.getRemoteEntries(RemoteLogs.java:81)
at org.openqa.selenium.remote.RemoteLogs.get(RemoteLogs.java:77)    at
MainExample.main(MainExample.java:17)

If I run the code on a custom page that have some Java script error I do see them in the logs :

JavaScript error: http://localhost/js/app.js?version=625f9736, line 1:
TypeError: e is undefined

but I am not able to retrieve them using
driver.manage().logs().get(LogType.BROWSER);

I have tried the different codes of this related subject but I am each time getting this error.
I also have tried to downgrade my selenium version to 3.141.59 but I am still getting the same error.
","Using WebDriver log endopints (not supported)
There is no get-logs endpoint defined by W3C WebDriver yet..
https://www.w3.org/TR/webdriver/#endpoints
And this still opened:
https://github.com/w3c/webdriver/issues/406
So, unfortunately, driver.manage().logs() is not implemented by Firefox.
From geckodriver team:

This isn't in the W3C spec at this time, so we are delaying support until the behaviour is well specified. But your request is noted.

See

(2016) https://bugzilla.mozilla.org/show_bug.cgi?id=1453962

(2016) https://github.com/mozilla/geckodriver/issues/284

(2018) https://github.com/mozilla/geckodriver/issues/1292



Using DevTools (seems to work)
I was able to see the console output with selenium-4.1.1 and devtools.v85
package org.example.getlogs

import org.openqa.selenium.WebDriver
import org.openqa.selenium.devtools.DevTools
import org.openqa.selenium.devtools.v85.log.Log
import org.openqa.selenium.firefox.FirefoxDriver
import org.openqa.selenium.firefox.FirefoxOptions

class GetLogsTest {

    public static void main(String[] args) {
        FirefoxOptions options = new FirefoxOptions();
        WebDriver driver = new FirefoxDriver(options);
        DevTools devTools = driver.getDevTools();
        devTools.createSession();
        devTools.send(Log.enable());
        devTools.addListener(Log.entryAdded(),
                logEntry -&gt; {
                    System.out.println(&quot;&quot; + logEntry.getLevel()+ &quot;: &quot; + logEntry.getText());
                });
        driver.get(&quot;https://stackoverflow.com/questions/70787924/how-to-capture-java-script-errors-using-selenium-java-webdriver&quot;);
        driver.quit();
    }
}

"
"Just testing some Spring Boot 3.0.0 with GraalVM Native and got some queries regarding it as I could not find properly documentation regarding it.
So, I've started a new project with GraalVM Native Support and Spring Web depedencies on Spring Initializr (https://start.spring.io/).
Then, for testing native image purposes I have my DemoApplication class like as follows:
package com.example.demo;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

import jakarta.servlet.http.HttpServletRequest;

@SpringBootApplication(proxyBeanMethods = false)
public class DemoApplication {

    @Autowired
    private HttpServletRequest request;

    public static void main(String[] args) {
       SpringApplication.run(DemoApplication.class, args);
    }   

}

As a result to build a native image, have used command as follows:
mvn -Pnative spring-boot:build-image

The image was successfully compiled and created:
docker images

REPOSITORY                 TAG              IMAGE ID       CREATED        SIZE
paketobuildpacks/run       tiny-cnb         c71fb787280a   3 days ago     17.3MB
paketobuildpacks/builder   tiny             cf7ea4946a20   42 years ago   588MB
demo                       0.0.1-SNAPSHOT   7794949d07ce   42 years ago   96.9MB

When I run this &quot;demo&quot; image using:
docker run demo:0.0.1-SNAPSHOT

It shows the following exception:
.   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::                (v3.0.0)

2022-12-16T21:23:41.386Z  INFO 1 --- [           main] com.example.demo.DemoApplication         : Starting AOT-processed DemoApplication using Java 17.0.5 with PID 1 (/workspace/com.example.demo.DemoApplication started by cnb in /workspace)
2022-12-16T21:23:41.386Z  INFO 1 --- [           main] com.example.demo.DemoApplication         : No active profile set, falling back to 1 default profile: &quot;default&quot;
2022-12-16T21:23:41.395Z  INFO 1 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
2022-12-16T21:23:41.396Z  INFO 1 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2022-12-16T21:23:41.396Z  INFO 1 --- [           main] o.apache.catalina.core.StandardEngine    : Starting Servlet engine: [Apache Tomcat/10.1.1]
2022-12-16T21:23:41.399Z  INFO 1 --- [           main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2022-12-16T21:23:41.400Z  INFO 1 --- [           main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 14 ms
2022-12-16T21:23:41.403Z  WARN 1 --- [           main] w.s.c.ServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'demoApplication': Instantiation of supplied bean failed
2022-12-16T21:23:41.403Z  INFO 1 --- [           main] o.apache.catalina.core.StandardService   : Stopping service [Tomcat]
2022-12-16T21:23:41.404Z ERROR 1 --- [           main] o.s.boot.SpringApplication               : Application run failed

org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'demoApplication': Instantiation of supplied bean failed
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.obtainInstanceFromSupplier(AbstractAutowireCapableBeanFactory.java:1236) ~[com.example.demo.DemoApplication:6.0.2]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.obtainFromSupplier(AbstractAutowireCapableBeanFactory.java:1210) ~[com.example.demo.DemoApplication:6.0.2]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1157) ~[com.example.demo.DemoApplication:6.0.2]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:561) ~[com.example.demo.DemoApplication:6.0.2]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:521) ~[com.example.demo.DemoApplication:6.0.2]
        at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:326) ~[com.example.demo.DemoApplication:6.0.2]
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234) ~[com.example.demo.DemoApplication:6.0.2]
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:324) ~[com.example.demo.DemoApplication:6.0.2]
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200) ~[com.example.demo.DemoApplication:6.0.2]
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:961) ~[com.example.demo.DemoApplication:6.0.2]
        at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:915) ~[com.example.demo.DemoApplication:6.0.2]
        at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:584) ~[com.example.demo.DemoApplication:6.0.2]
        at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146) ~[com.example.demo.DemoApplication:3.0.0]
        at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:730) ~[com.example.demo.DemoApplication:3.0.0]
        at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:432) ~[com.example.demo.DemoApplication:3.0.0]
        at org.springframework.boot.SpringApplication.run(SpringApplication.java:308) ~[com.example.demo.DemoApplication:3.0.0]
        at org.springframework.boot.SpringApplication.run(SpringApplication.java:1302) ~[com.example.demo.DemoApplication:3.0.0]
        at org.springframework.boot.SpringApplication.run(SpringApplication.java:1291) ~[com.example.demo.DemoApplication:3.0.0]
        at com.example.demo.DemoApplication.main(DemoApplication.java:16) ~[com.example.demo.DemoApplication:na]
Caused by: com.oracle.svm.core.jdk.UnsupportedFeatureError: Proxy class defined by interfaces [interface jakarta.servlet.http.HttpServletRequest] not found. Generating proxy classes at runtime is not supported. Proxy classes need to be defined at image build time by specifying the list of interfaces that they implement. To define proxy classes use -H:DynamicProxyConfigurationFiles=&lt;comma-separated-config-files&gt; and -H:DynamicProxyConfigurationResources=&lt;comma-separated-config-resources&gt; options.
        at com.oracle.svm.core.util.VMError.unsupportedFeature(VMError.java:89) ~[na:na]
        at com.oracle.svm.core.reflect.proxy.DynamicProxySupport.getProxyClass(DynamicProxySupport.java:171) ~[na:na]
        at java.base@17.0.5/java.lang.reflect.Proxy.getProxyConstructor(Proxy.java:47) ~[com.example.demo.DemoApplication:na]
        at java.base@17.0.5/java.lang.reflect.Proxy.newProxyInstance(Proxy.java:1037) ~[com.example.demo.DemoApplication:na]
        at org.springframework.beans.factory.support.AutowireUtils.resolveAutowiringValue(AutowireUtils.java:134) ~[na:na]
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:1576) ~[com.example.demo.DemoApplication:6.0.2]
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1368) ~[com.example.demo.DemoApplication:6.0.2]
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1325) ~[com.example.demo.DemoApplication:6.0.2]
        at org.springframework.beans.factory.aot.AutowiredFieldValueResolver.resolveValue(AutowiredFieldValueResolver.java:189) ~[na:na]
        at org.springframework.beans.factory.aot.AutowiredFieldValueResolver.resolveAndSet(AutowiredFieldValueResolver.java:167) ~[na:na]
        at com.example.demo.DemoApplication__Autowiring.apply(DemoApplication__Autowiring.java:14) ~[na:na]
        at org.springframework.beans.factory.support.InstanceSupplier$1.get(InstanceSupplier.java:82) ~[na:na]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.obtainInstanceFromSupplier(AbstractAutowireCapableBeanFactory.java:1225) ~[com.example.demo.DemoApplication:6.0.2]
        ... 18 common frames omitted

I assume that must be something related to inform a implementation for interface jakarta.servlet.http.HttpServletRequest, however I don't how to inform/configure it.
Do you guys have any suggestion(s)?
Thanks in advance.
","After several readings on Spring Boot 3.0.0 (https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/) user guide, have figured out that some classes Spring AOT needs a runtime hint as a result to compile it into a native program.
If the class is not found you need to create a custom hint (https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#native-image.advanced.custom-hints).
So to have it worked I have created a HTTPServletRequest runtime hint (bear in mind that must implements interface org.springframework.aot.hint.RuntimeHintsRegistrar) class as follows:
public class HttpServletRequestRuntimeHint implements RuntimeHintsRegistrar{

    @Override
    public void registerHints(RuntimeHints hints, ClassLoader classLoader) {
    try {
        ProxyHints proxies = hints.proxies();
        proxies.registerJdkProxy(HttpServletRequest.class);
    } catch (Exception e) {
        throw new RuntimeException(&quot;Could not register RuntimeHint: &quot; + e.getMessage());
    }
    }

}

And then it needs to be informed on a @Configuration class or, for example,  your @SpringBootApplication annotated application class to activate those hints:
@SpringBootApplication
@ImportRuntimeHints(value = {HttpServletRequestRuntimeHint.class})
public class DemoApplication {       
    
    @Autowired   
    private HttpServletRequest request;

    public static void main(String[] args) {
        SpringApplication.run(ApplicationConfig.class, args);
    }

}

And done!
Hope that helps!
Thank you.
"
"I have an auth-server + resource server in one app. I've spent a lot of time searching and debugging, but there aren't many updated pages or topics about Spring Boot 3.+ related to this. So, I had this working and wanted to add a custom secret that will be shared between my client and server. And here is where the problems started...
This is my auth+resource server config:
@Configuration
@EnableWebSecurity
public class SecurityConfig {

@Value(&quot;${security.jwt.secret}&quot;)
private String jwtSecret;

@Bean
SecurityFilterChain authorizationServerSecurityFilterChain(HttpSecurity http,
                                                           CorsConfigurationSource corsConfigurationSource) throws Exception {
    OAuth2AuthorizationServerConfiguration.applyDefaultSecurity(http);
    http.getConfigurer(OAuth2AuthorizationServerConfigurer.class).oidc(Customizer.withDefaults());

    http.exceptionHandling((exceptions) -&gt; exceptions.defaultAuthenticationEntryPointFor(
                    new LoginUrlAuthenticationEntryPoint(&quot;/login&quot;), new MediaTypeRequestMatcher(MediaType.TEXT_HTML)))
            .oauth2ResourceServer((resourceServer) -&gt; resourceServer.jwt(Customizer.withDefaults()));

    http.cors(customizer -&gt; customizer.configurationSource(corsConfigurationSource));
    return http.build();
}

@Bean
SecurityFilterChain defaultSecurityFilterChain(HttpSecurity http) throws Exception {
    http.authorizeHttpRequests(
                    authorize -&gt; authorize.requestMatchers(&quot;/oauth2/authorize&quot;).permitAll().anyRequest().authenticated())
            .formLogin(formLogin -&gt; formLogin.loginPage(&quot;/login&quot;).permitAll())
            .oauth2ResourceServer(oauth2 -&gt; oauth2.jwt(Customizer.withDefaults()));
    http.csrf(csrf -&gt; csrf.csrfTokenRepository(CookieCsrfTokenRepository.withHttpOnlyFalse()));
    return http.build();
}

@Bean
PasswordEncoder passwordEncoder() {
    return new BCryptPasswordEncoder();
}

@Bean
public JwtEncoder jwtEncoder() {
    byte[] keyBytes = Base64.getDecoder().decode(jwtSecret);
    SecretKeySpec secretKeySpec = new SecretKeySpec(keyBytes, &quot;HmacSHA256&quot;);
    OctetSequenceKey octetKey = new OctetSequenceKey.Builder(secretKeySpec)
            .keyID(&quot;customKey&quot;)
            .build();
    JWKSet jwkSet = new JWKSet(octetKey);
    JWKSource&lt;SecurityContext&gt; jwkSource = (jwkSelector, context) -&gt; {
        List&lt;JWK&gt; keys = jwkSelector.select(jwkSet);
        if (keys.isEmpty()) {
            System.out.println(&quot;No keys found matching selection criteria!&quot;);
        } else {
            System.out.println(&quot;Keys selected: &quot; + keys.stream().map(JWK::getKeyID).collect(Collectors.joining(&quot;, &quot;)));
        }
        return keys;
    };

    return new NimbusJwtEncoder(jwkSource);
}

@Bean
JwtDecoder jwtDecoder() {
    byte[] keyBytes = Base64.getDecoder().decode(jwtSecret);
    SecretKeySpec secretKeySpec = new SecretKeySpec(keyBytes, &quot;HmacSHA256&quot;);
    return NimbusJwtDecoder.withSecretKey(secretKeySpec).build();
}
}

And i have in my app.properties:
security.jwt.secret=r26BoWWyTQMp/8rkD3RnRKsbHkRsmQWjTvJTfmhrQxU=

I had everything working with asymmetric way (private and public key), but I wanted to try this wat too...
Now, when logging in with the client, I always receive:
org.springframework.security.oauth2.jwt.JwtEncodingException: An error occurred while attempting to encode the Jwt: Failed to select a JWK signing key

What am i missing in the server?

","I have fixed the issue with this:
@Slf4j
@Configuration
@RequiredArgsConstructor
@EnableWebSecurity
public class SecurityConfig {

@Value(&quot;${jwt.key}&quot;)
private String jwtKey;

private final TokenService tokenService;

@Bean
SecurityFilterChain authorizationServerSecurityFilterChain(HttpSecurity http,
                                                           CorsConfigurationSource corsConfigurationSource) throws Exception {
    OAuth2AuthorizationServerConfiguration.applyDefaultSecurity(http);
    http.getConfigurer(OAuth2AuthorizationServerConfigurer.class).oidc(Customizer.withDefaults());

    http.exceptionHandling((exceptions) -&gt; exceptions.defaultAuthenticationEntryPointFor(
                    new LoginUrlAuthenticationEntryPoint(&quot;/login&quot;), new MediaTypeRequestMatcher(MediaType.TEXT_HTML)))
            .oauth2ResourceServer((resourceServer) -&gt; resourceServer.jwt(jwtSpec -&gt; {
                jwtSpec.decoder(jwtDecoder());
            }));

    http.cors(customizer -&gt; customizer.configurationSource(corsConfigurationSource));
    return http.build();

}

@Bean
SecurityFilterChain defaultSecurityFilterChain(HttpSecurity http) throws Exception {
    http
            .authorizeHttpRequests(authz -&gt; authz
                    .requestMatchers(&quot;/hello&quot;).authenticated()
                    .anyRequest().permitAll())
            .oauth2ResourceServer(oauth2 -&gt; oauth2
                    .jwt(jwt -&gt; jwt.decoder(jwtDecoder())))
            .formLogin(Customizer.withDefaults());

    return http.build();
}

@Bean
AuthorizationServerSettings authorizationServerSettings() {
    return AuthorizationServerSettings.builder().build();
}

@Bean
WebSecurityCustomizer webSecurityCustomizer() {
    return (web) -&gt; web.ignoring().requestMatchers(new AntPathRequestMatcher(&quot;/h2-console/**&quot;));
}

@Bean
PasswordEncoder passwordEncoder() {
    return new BCryptPasswordEncoder();
}

@Bean
OAuth2TokenCustomizer&lt;JwtEncodingContext&gt; jwtCustomizer() {
    return tokenService.jwtCustomizer();
}

@Bean
public JwtEncoder jwtEncoder() {
    return tokenService.jwtEncoder();
}

@Bean
public JwtDecoder jwtDecoder() {
    byte[] keyBytes = Base64.getDecoder().decode(jwtKey);
    SecretKeySpec keySpec = new SecretKeySpec(keyBytes, &quot;HmacSHA256&quot;);
    return NimbusJwtDecoder.withSecretKey(keySpec).build();
}
}

And the TokenService class:
@Service
public class TokenService {

@Value(&quot;${jwt.key}&quot;)
private String jwtKey;

public OAuth2TokenCustomizer&lt;JwtEncodingContext&gt; jwtCustomizer() {
    return context -&gt; {
        if (OAuth2TokenType.ACCESS_TOKEN.equals(context.getTokenType())) {
            context.getJwsHeader().algorithm(MacAlgorithm.HS256);
            Date expirationDate = 
Date.from(Instant.now().plus(Duration.ofHours(5)));
            Date issueDate = Date.from(Instant.now());
            context.getClaims().claims(claims -&gt; {
                claims.put(&quot;exp&quot;, expirationDate);
                claims.put(&quot;iat&quot;, issueDate);
                claims.put(&quot;custom&quot;, &quot;custom&quot;);
            });
        }
    };
}

public JwtEncoder jwtEncoder() {
    return parameters -&gt; {
        byte[] secretKeyBytes = Base64.getDecoder().decode(jwtKey);
        SecretKeySpec secretKeySpec = new SecretKeySpec(secretKeyBytes, &quot;HmacSHA256&quot;);

        try {
            MACSigner signer = new MACSigner(secretKeySpec);

            JWTClaimsSet.Builder claimsSetBuilder = new JWTClaimsSet.Builder();
            parameters.getClaims().getClaims().forEach((key, value) -&gt;
                    claimsSetBuilder.claim(key, value instanceof Instant ? Date.from((Instant) value) : value)
            );
            JWTClaimsSet claimsSet = claimsSetBuilder.build();

            JWSHeader header = new JWSHeader(JWSAlgorithm.HS256);

            SignedJWT signedJWT = new SignedJWT(header, claimsSet);
            signedJWT.sign(signer);

            return Jwt.withTokenValue(signedJWT.serialize())
                    .header(&quot;alg&quot;, header.getAlgorithm().getName())
                    .subject(claimsSet.getSubject())
                    .issuer(claimsSet.getIssuer())
                    .claims(claims -&gt; claims.putAll(claimsSet.getClaims()))
                    .issuedAt(claimsSet.getIssueTime().toInstant())
                    .expiresAt(claimsSet.getExpirationTime().toInstant())
                    .build();
        } catch (Exception e) {
            throw new IllegalStateException(&quot;Error while signing the JWT&quot;, e);
        }
    };
}
}

"
"I am new to Spring, I was working with @Value annotation and found out that it can be applied to fields or constructor/method parameters, but as I was trying to inject value using parameters it was not injecting the value for my parameters and I was getting values as null
.
I have used @Value in the parameter of the method below
public void setName(@Value(&quot;Adventure of War&quot;) String name) {
        System.out.println(&quot;Setting Company Name&quot;);
        this.name = name;
}

Complete code(Company.java)
package gd.rf.anuragsaini.stereotype;

import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Component;

@Component
public class Company {
    String name;
    String motive;

    public void setName(@Value(&quot;Adventure of War&quot;) String name) {
        System.out.println(&quot;Setting Company Name&quot;);
        this.name = name;
    }

    public void setMotive(@Value(&quot;A place for War&quot;) String motive) {
        System.out.println(&quot;Setting Company Motive&quot;);
        this.motive = motive;
    }

    @Override
    public String toString() {
        return &quot;Company{&quot; +
                &quot;name='&quot; + name + '\'' +
                &quot;, motive='&quot; + motive + '\'' +
                '}';
    }
}

Main File(App.java)
package gd.rf.anuragsaini.stereotype;

import org.springframework.context.ApplicationContext;
import org.springframework.context.support.ClassPathXmlApplicationContext;

public class App {
    public static void main(String[] args) {
        ApplicationContext IOC = new ClassPathXmlApplicationContext(&quot;config.xml&quot;);
        Company c1 = IOC.getBean(&quot;company&quot;, Company.class);
        System.out.println(c1);
    }
}

Output
Company{name='null', motive='null'}

","Like that the setters will not be invoked.When constructing the bean of type Company, the frameWork use by default the default constructor of the class Company.
So you should use @Autowired for the setters to garantee that the setters will be invoked by the frameWork when constructing those beans :
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Component;

@Component
public class Company {
    String name;
    String motive;

    @Autowired
    public void setName(@Value(&quot;Adventure of War&quot;) String name) {
        System.out.println(&quot;Setting Company Name&quot;);
        this.name = name;
    }

    @Autowired
    public void setMotive(@Value(&quot;A place for War&quot;) String motive) {
        System.out.println(&quot;Setting Company Motive&quot;);
        this.motive = motive;
    }

    @Override
    public String toString() {
        return &quot;Company{&quot; + &quot;name='&quot; + name + '\'' + &quot;, motive='&quot; + motive + '\'' + '}';
    }
}


Output :
Company{name='Adventure of War', motive='A place for War'}

"
"I am currently refactoring the security configuration removing WebSecurityConfigurerAdapter and am currently stuck on a config using two Basic Auth configurations with different user stores on different paths.
Current configuration looks like this and works fine:
@EnableWebSecurity
public class SecurityConfig {

    @Order(1)
    @Configuration
    public static class BasicSpecialAuth extends WebSecurityConfigurerAdapter {

        // some code

        @Override
        protected void configure(AuthenticationManagerBuilder auth) throws Exception {
            // some code
 auth.inMemoryAuthentication().withUser(specialUser.getId()).password(passwordEncoder().encode(specialUser.getPassword())).roles(&quot;SPECIALROLE&quot;);
        }


        @Override
        protected void configure(HttpSecurity http) throws Exception {
            http.httpBasic()
                    .and()
                    .antMatcher(&quot;/very-special-path/**&quot;)
                    //. more code
                    .authorizeRequests(r -&gt; r
                            .anyRequest().authenticated());
        }
    }

    @Order(2)
    @Configuration
    public static class BasicAppAuth extends WebSecurityConfigurerAdapter {

        // some code

        @Bean
        public CustomUserDetailsService customUserDetailsService() {
            return new CustomUserDetailsService(userRepository);
        }

        @Override
        protected void configure(final AuthenticationManagerBuilder auth) throws Exception {
            auth.userDetailsService(customUserDetailsService())
                    .passwordEncoder(encoder());
        }

        @Override
        protected void configure(HttpSecurity http) throws Exception {
            http.httpBasic()
                    .and()
                    //. more code
                    .authorizeRequests(auth -&gt; auth
                            .anyRequest().authenticated());
        }
    }
}

As can be seen, /very-special-path uses InMemoryAuthentication set up at start by configuration.
All other paths should be authenticated using users from local database. Due to possible duplicates on usernames I am not able to use the database for /very-special-path users too. Requirement is to have these separated.
Following documentation it was quite simple to change this on our apps providing Basic Auth and JWT Auth on different path. But with both using Basic Auth and different user stores, I have no idea how to set up configuration properly.
Any help would be appreciated.
Edit, the current config:
@Configuration
public class SecurityConfig {

    // some code

    @Bean
    public PasswordEncoder passwordEncoder() {
        return new BCryptPasswordEncoder();
    }

    @Bean
    public UserDetailsService customUserDetailsService() {
        return new CustomUserDetailsService(userRepository);
    }

    @Bean
    public InMemoryUserDetailsManager inMemoryUserDetailsService() {
        // more code

        UserDetails healthUser = User.withUsername(specialUser.getId())
                .password(passwordEncoder().encode(specialUser.getPassword()))
                .roles(&quot;SPECIALROLE&quot;)
                .build();
        return new InMemoryUserDetailsManager(healthUser);
    }

    @Bean
    @Order(1)
    public SecurityFilterChain specialFilterChain(HttpSecurity http) throws Exception {
        http.httpBasic()
                .and()
                .antMatcher(&quot;/very-special-path/**&quot;)
                .authorizeRequests(auth -&gt; auth
                        .anyRequest().authenticated());
        return http.build();
    }

    @Bean
    public SecurityFilterChain filterChain(HttpSecurity http) throws Exception {
        http.httpBasic()
                .and()
                .authorizeRequests(auth -&gt; auth
                        .anyRequest().authenticated());
        return http.build();
    }
}

The app starts without any Warning or Error.
Both chains are mentioned in the log:
o.s.s.web.DefaultSecurityFilterChain     : Will secure any request with ..
o.s.s.web.DefaultSecurityFilterChain     : Will secure Ant [pattern='/very-special-path/**'] with ..
But authentication does not work. Checked for different endpoints and with different users. Every request gets an 401.
This config misses the assignment of the UserDetails to the specific filter chain. Is there a way to do so?
","Based on the extra information you provided, there are only a couple of tweaks needed to get your configuration working. Take a look at this blog post that details common patterns for replacing WebSecurityConfigurerAdapter with the component-based approach, specifically the local AuthenticationManager.
For the special/health user, you can manually construct a local authentication manager so as not to collide with the global one declared as an @Bean. Here's a full example:
@EnableWebSecurity
public class SecurityConfig {

    // ...

    @Bean
    public PasswordEncoder passwordEncoder() {
        return new BCryptPasswordEncoder();
    }

    @Bean
    public UserDetailsService customUserDetailsService() {
        return new CustomUserDetailsService(userRepository);
    }

    @Bean
    @Order(1)
    public SecurityFilterChain specialFilterChain(HttpSecurity http, PasswordEncoder passwordEncoder) throws Exception {
        http
            .mvcMatcher(&quot;/very-special-path/**&quot;)
            .authorizeRequests((authorize) -&gt; authorize
                .anyRequest().authenticated()
            )
            .httpBasic(Customizer.withDefaults())
            .authenticationManager(specialAuthenticationManager(passwordEncoder));

        return http.build();
    }

    private AuthenticationManager specialAuthenticationManager(PasswordEncoder passwordEncoder) {
        UserDetailsService userDetailsService = specialUserDetailsService(passwordEncoder);
        DaoAuthenticationProvider authenticationProvider = new DaoAuthenticationProvider();
        authenticationProvider.setPasswordEncoder(passwordEncoder);
        authenticationProvider.setUserDetailsService(userDetailsService);

        return new ProviderManager(authenticationProvider);
    }

    private UserDetailsService specialUserDetailsService(PasswordEncoder passwordEncoder) {
        UserDetails specialUser = User.withUsername(&quot;specialuser&quot;)
            .password(passwordEncoder.encode(&quot;specialpassword&quot;))
            .roles(&quot;SPECIALROLE&quot;)
            .build();

        return new InMemoryUserDetailsManager(specialUser);
    }

    @Bean
    @Order(2)
    public SecurityFilterChain filterChain(HttpSecurity http) throws Exception {
        http
            .authorizeRequests((authorize) -&gt; authorize
                .anyRequest().authenticated()
            )
            .httpBasic(Customizer.withDefaults());

        return http.build();
    }

}

"
"I am writing a little polling mechanism using Mutiny, part of me learning the library and i am kinda stuck in cancelling the polling when result is found.
I tried using the tick() and what i came up with looks like
Multi.createFrom().ticks().every(Duration.ofSeconds(5))
    .onItem().transformToMultiAndMerge(tick -&gt; {
      System.out.println(&quot;Tick:&quot; + tick);
      return Multi.createFrom()
          .&lt;Transaction&gt;emitter(
              emitter -&gt; {
                service.getTransactions().toMulti()
                    .onItem().transformToMultiAndMerge(
                        transactions -&gt; Multi.createFrom().iterable(transactions))
                    .subscribe().with(transaction -&gt; {
                      if (!verification.isOngoing()) {
                        emitter.fail(new TransactionVerificationException());
                      } else {
                        boolean transactionFound = transaction.getAmount().stream().anyMatch(
                            amount -&gt; amount.getQuantity()
                                .equals(&quot;test&quot;));
                        if (transactionFound) {
                          emitter.emit(transaction);
                          emitter.complete();
                        } 
                      }
                    });
              });
    })
    .subscribe()
    .with(transaction -&gt; log.info(transaction),
        x -&gt; x.printStackTrace());

Problem here is that the Multi from ticks() is running forever and the only way i think of to cancel it would be to propagate somehow that the emitter has completed.
The case here is that i want to emit, and process only if certain conditions are met.
","You approach is almost correct, though,

there is no need to create a custom MultiEmitter out of an existing Multi (or transformed Uni) as you can leverage the different Multi operators on top of your source service#getTransaction result
you missed the EmptyMulti source which will automatically complete downstream subscriber chain and which you can use to signal an absence of valid item (i.e. Transaction)
you need to select the first valid item (being non-null) then transform your Multi to Uni which will result in the upstream subscription being cancelled automatically once an item is received

Here down what the stream pipeline would look like:
Multi.createFrom()
        .ticks()
        .every(Duration.ofSeconds(5))
        .onItem()
        // flat map the ticks to the `service#getTransactions` result
        .transformToMultiAndMerge(tick -&gt; service.getTransactions()
                .toMulti()
                .onItem()
                // flatten Collection&lt;Transaction&gt; to Multi&lt;Transaction&gt;
                .transformToIterable(Function.identity())
                .onItem()
                .transformToMultiAndMerge(transaction -&gt; {
                    if (!verification.isOngoing()) {
                        return Multi.createFrom().failure(new TransactionVerificationException());
                    } else {
                        boolean transactionFound = transaction.getAmount()
                                .stream()
                                .anyMatch(amount -&gt; amount.getQuantity().equals(&quot;test&quot;));
                        if (transactionFound) {
                            return Multi.createFrom().item(transaction);
                        } else {
                            return Multi.createFrom().empty();
                        }
                    }
                })
        )
        .select()
        .first(Objects::nonNull)
        .toUni()
        .subscribe()
        .with(transaction -&gt; log.info(transaction), x -&gt; x.printStackTrace());

"
"I am trying to convert the below Command in SSL to Java
openssl enc -in &lt;inputfilename&gt; -out &lt;file_to_encrypt&gt; -e -aes256 -k s_key

s_key is the file provided which contains the key that will be used to encrypt and decrypt
Steps to be done:
1 - Read the key file
2 - Use it to AES encryption to encrypt file inputfilename
3 - Use the key to  decrypt the same.
I am new to encryption and below is the code i have written so far to encrypt but I am getting issue.
Path path = Paths.get(&quot;/home/debashishd/Downloads/s_key&quot;);
String content = new String(Files.readAllBytes(Paths.get(&quot;/home/debashishd/Downloads/s_key&quot;)));
    
String Test_message = &quot;Hello this is Roxane&quot;;
    
byte[] keyValue = Files.readAllBytes(path);
ByteArrayInputStream byteIS = new ByteArrayInputStream(keyValue);
    
OpenSSLPBEParametersGenerator gen = new OpenSSLPBEParametersGenerator();
OpenSSLPBEParametersGenerator gen1 = gen;
byte[] saltBytes = Hex.decode(salt.getBytes());
gen1.init(keyValue);
CipherParameters cp = gen1.generateDerivedParameters(256);

byte[] keyBytes = ((KeyParameter)cp);           
SecretKeySpec secretKey = new SecretKeySpec(keyBytes,&quot;AES&quot;);
System.out.println(secretKey);
    
Cipher cipher;
Cipher decryptCipher;
cipher = Cipher.getInstance(&quot;AES/CBC/PKCS5Padding&quot;);
cipher.init(Cipher.ENCRYPT_MODE, secretKey,new IvParameterSpec(new byte[16]));

String encrypt_value = getEncoder().encodeToString(cipher.doFinal(Test_message.getBytes(StandardCharsets.UTF_8)));
    
System.out.println(&quot;Encrypted value: &quot; + encrypt_value);
    
decryptCipher = Cipher.getInstance(&quot;AES/CBC/PKCS5Padding&quot;);
decryptCipher.init(Cipher.DECRYPT_MODE, secretKey,new IvParameterSpec(new byte[16]));
String Decrypt_result = new String(decryptCipher.doFinal(getDecoder().decode(encrypt_value)));
System.out.println(&quot;Decrypted value: &quot; + Decrypt_result);

Is there any changes need to be done to achieve the above encrypt and decrypt
Expected output:
Encrypted value: jvggHDPa58+/zQ+HyGUEk/ypndXbatE+b+hBBqiinABOIwxJ7FXqnDb5j813fPwwm/D6d2Y2uh+k4qD77QMqOg==
Decrypted value: Hello this is Roxane

","For compatibility with the OpenSSL statement:

a random 8 bytes of salt must be generated
a 32 bytes key and 16 bytes IV must be derived using EVP_BytesToKey() and the salt
the result must be given in OpenSSL format:
&lt;ASCII Encoding of Salted__&gt;|&lt;salt&gt;|&lt;ciphertext&gt;

For EVP_BytesToKey() you can apply the OpenSSLPBEParametersGenerator class you already suggested.
EVP_BytesToKey() uses a digest. In earlier versions of OpenSSL MD5 was applied by default, from v1.1.0 SHA256. The digest can be set with the -md5 option. Code and OpenSSL statement must both use the same digest to be compatible. OpenSSLPBEParametersGenerator allows the specification of the digest in the constructor, default is MD5.
The following code, is based on your code, i.e. uses OpenSSLPBEParametersGenerator for EVP_BytesToKey() but additionally considers above points. Instead of encrypting the entire data, streams are applied and the data is encrypted chunk by chunk, so that even large files can be processed:
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.nio.charset.StandardCharsets;
import java.security.SecureRandom;
import org.bouncycastle.crypto.digests.MD5Digest;
import org.bouncycastle.crypto.engines.AESEngine;
import org.bouncycastle.crypto.generators.OpenSSLPBEParametersGenerator;
import org.bouncycastle.crypto.io.CipherOutputStream;
import org.bouncycastle.crypto.modes.CBCBlockCipher;
import org.bouncycastle.crypto.paddings.PaddedBufferedBlockCipher;
import org.bouncycastle.crypto.params.ParametersWithIV;

...

String inputPath = &quot;...&quot;;
String outputPath = &quot;...&quot;;
String passwordStr = &quot;...&quot;;

// Generate random 8 bytes salt
SecureRandom random = new SecureRandom();
byte salt[] = new byte[8];
random.nextBytes(salt);

// Derive 32 bytes key (AES_256) and 16 bytes IV
byte[] password = passwordStr.getBytes(StandardCharsets.UTF_8);
OpenSSLPBEParametersGenerator pbeGenerator = new OpenSSLPBEParametersGenerator(new MD5Digest()); // SHA256 as of v1.1.0 (if in OpenSSL the default digest is applied)
pbeGenerator.init(password, salt);
ParametersWithIV parameters = (ParametersWithIV) pbeGenerator.generateDerivedParameters(256, 128); // keySize, ivSize in bits

// Encrypt with AES-256, CBC using streams
try (FileOutputStream fos = new FileOutputStream(outputPath)) {

    // Apply OpenSSL format
    fos.write(&quot;Salted__&quot;.getBytes(StandardCharsets.UTF_8));
    fos.write(salt);

    // Encrypt chunkwise (for large data)
    PaddedBufferedBlockCipher cipher = new PaddedBufferedBlockCipher(new CBCBlockCipher(new AESEngine()));
    cipher.init(true, parameters);
    try (FileInputStream fis = new FileInputStream(inputPath);
         CipherOutputStream cos = new CipherOutputStream(fos, cipher)) {
        int bytesRead = -1;
        byte[] buffer = new byte[64 * 1024 * 1024]; // chunksize, e.g. 64 MiB
        while ((bytesRead = fis.read(buffer)) != -1) {
            cos.write(buffer, 0, bytesRead);
        }    
    }
}

A file encrypted with this code can be decrypted with OpenSSL as follows:
openssl enc -d -aes256 -k &lt;passpharse&gt; -in &lt;enc file&gt; -out &lt;dec file&gt;

Therefore the code is the programmatic analogue of the OpenSSL statement posted at the beginning of your question (whereby the ambiguity regarding the digest still has to be taken into account, i.e. for an OpenSSL version from v1.1.0 SHA256 has to be used instead of MD5).
Note that because of the random salt, different key/IV pairs are generated for each encryption, so there is no reuse, which also removes the vulnerability mentioned in the comment.
"
"I want to disable just one arrow-button of the JavaFX Spinner component, so that they cannot assume illegal values:
I have 2 components spinnerMin and spinnerMax with [2-6] as range of values, as in this picture; the behaviour I want is that when they get to the same value (e.g. Min: 3, Max: 3) the up arrow of Min becomes disabled, aswell as the down arrow of Max.

Anyone knows if this is possible or how can I achieve that in the smoothest way possible?
Edit:
Thank jewelsea for the suggestion. I've added a listener to the valueProperty and set the valueFactory to change the range and it works as expected, even though it still doesn't disable and &quot;gray out&quot; the arrow, which is the behaviour I would like to achieve (but at this point I'm wondering if it is even possible).
spinnerMin.valueProperty().addListener((changed, oldval, newval) -&gt; {
    spinnerMax.setValueFactory(new SpinnerValueFactory.IntegerSpinnerValueFactory(newval, 6, spinnerMax.getValue()));
});

spinnerMax.valueProperty().addListener((changed, oldval, newval) -&gt; {
    spinnerMin.setValueFactory(new SpinnerValueFactory.IntegerSpinnerValueFactory(2, newval, spinnerMin.getValue()));
});

","It is definitely possible to style the buttons as per the value in the spinner.
Below is one way you can accomplish the required behavior.
The general idea is to set some pseudo states to the Spinner when the min/max values are reached. And style the arrow buttons based on the pseudo states.
Below is the sample demo of the above approach.

import javafx.application.Application;
import javafx.css.PseudoClass;
import javafx.geometry.Insets;
import javafx.scene.Scene;
import javafx.scene.control.Spinner;
import javafx.scene.control.SpinnerValueFactory;
import javafx.scene.layout.StackPane;
import javafx.stage.Stage;
public class SpinnerDemo extends Application {
    @Override
    public void start(Stage stage) throws Exception {
        PseudoClass minPseudo = PseudoClass.getPseudoClass(&quot;minvalue&quot;);
        PseudoClass maxPseudo = PseudoClass.getPseudoClass(&quot;maxvalue&quot;);

        Spinner&lt;Integer&gt; spinner = new Spinner&lt;&gt;();
        spinner.getStyleClass().add(Spinner.STYLE_CLASS_SPLIT_ARROWS_VERTICAL);
        SpinnerValueFactory.IntegerSpinnerValueFactory valueFactory = new SpinnerValueFactory.IntegerSpinnerValueFactory(2, 5);
        spinner.valueProperty().addListener((obs, old, val) -&gt; {
            spinner.pseudoClassStateChanged(minPseudo, val == valueFactory.getMin());
            spinner.pseudoClassStateChanged(maxPseudo, val == valueFactory.getMax());
        });
        spinner.setValueFactory(valueFactory);

        StackPane root = new StackPane(spinner);
        root.setPadding(new Insets(15));
        Scene sc = new Scene(root, 250, 200);
        sc.getStylesheets().add(getClass().getResource(&quot;spinner.css&quot;).toString());
        stage.setScene(sc);
        stage.setTitle(&quot;Spinner&quot;);
        stage.show();
    }
}

CSS code:
.spinner:maxvalue .increment-arrow-button {
    -fx-background-color: -fx-outer-border, #999999;
}
.spinner:minvalue .decrement-arrow-button {
     -fx-background-color: -fx-outer-border, #999999;
}

There may be other ways as well. But I think with this approach you have better control over the styling.
"
"When the button counter reaches the targeted clicks, I show an interstitial ad. The interstitial ad was working fine until I left the project for quite a while and today when I returned to it, it wasn't working. I have two different interstitial ads that I want to show on different amounts of clicks, but neither of them are working.
Home.java
    int countClicksNext = 0;
    int countClicksCopy = 0;
    
    int triggerClicksNav = 7;
    int triggerClicks = 3; 

 protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.home_activity);
         
         // Ads

        MobileAds.initialize(this, initializationStatus -&gt; {
        });

        AdView mAdView = findViewById(R.id.adView);

        AdRequest adRequest = new AdRequest.Builder().build();
        mAdView.loadAd(adRequest);
        mAdView.setAdListener(new AdListener() {
            @Override
            public void onAdLoaded() {
                super.onAdLoaded();
            }

            @Override
            public void onAdFailedToLoad(@NotNull LoadAdError adError) {
                super.onAdFailedToLoad(adError);
                mAdView.loadAd(adRequest);
            }

            @Override
            public void onAdOpened() {
                super.onAdOpened();
            }

            @Override
            public void onAdClicked() {
                super.onAdClicked();
            }

            @Override
            public void onAdClosed() {
                super.onAdClosed();
            }
        });

 }



    @SuppressLint(&quot;SetTextI18n&quot;)
    private void next() {
        countClicksNext++;
        position = (position + 1) % quotes_list.size();
        quotesTxt.setText(quotes_list.get(position));
        countTxt.setText(position + &quot;/&quot; + quotes_list.size());
        if (mInterstitialAd != null &amp;&amp; countClicksNext &gt;= triggerClicksNav) {
           showAd();
        }
    }

   


    private void copy() {
        countClicksCopy++;
        ClipboardManager clipboardManager = (ClipboardManager) getSystemService(Context.CLIPBOARD_SERVICE);
        ClipData clipData = ClipData.newPlainText(&quot;text&quot;, quotesTxt.getText());
        if (clipboardManager != null) {
            clipboardManager.setPrimaryClip(clipData);
        }
        Toast.makeText(getApplicationContext(), &quot;Copied&quot;, Toast.LENGTH_SHORT).show();
        if (mInterstitialAd != null &amp;&amp; countClicksCopy &gt;= triggerClicks) {
           showAdOth();
        }

    }

   

 private void showAd() {
        mInterstitialAd.show(HomeActivity.this);
        mInterstitialAd.setFullScreenContentCallback(new FullScreenContentCallback() {
            @Override
            public void onAdDismissedFullScreenContent() {
                super.onAdDismissedFullScreenContent();
                mInterstitialAd = null;
                intertitalAd();
              
               countClicksNext = 0;
             
            }
        });
    }
    private void showAdOth() {
        mInterstitialAd.show(HomeActivity.this);
        mInterstitialAd.setFullScreenContentCallback(new FullScreenContentCallback() {
            @Override
            public void onAdDismissedFullScreenContent() {
                super.onAdDismissedFullScreenContent();
                mInterstitialAd = null;
                intertitalAdOth();
                countClicksCopy = 0;
                
            }
        });
    }

    public void intertitalAd() {

        AdRequest adRequest = new AdRequest.Builder().build();


        InterstitialAd.load(this, &quot;ca-app-pub-3940256099942544/1033173712&quot;, adRequest,
                new InterstitialAdLoadCallback() {
                    @Override
                    public void onAdLoaded(@NonNull InterstitialAd interstitialAd) {
                        // The mInterstitialAd reference will be null until
                        // an ad is loaded.
                        mInterstitialAd = interstitialAd;
                    }

                    @Override
                    public void onAdFailedToLoad(@NonNull LoadAdError loadAdError) {
                        // Handle the error
                        mInterstitialAd = null;
                    }
                });

    }
    public void intertitalAdOth() { 

        AdRequest adRequest = new AdRequest.Builder().build();


        InterstitialAd.load(this, &quot;ca-app-pub-3940256099942544/8691691433&quot;, adRequest,
                new InterstitialAdLoadCallback() {
                    @Override
                    public void onAdLoaded(@NonNull InterstitialAd interstitialAd) {
                        // The mInterstitialAd reference will be null until
                        // an ad is loaded.
                        mInterstitialAd = interstitialAd;
                    }

                    @Override
                    public void onAdFailedToLoad(@NonNull LoadAdError loadAdError) {
                        // Handle the error
                        mInterstitialAd = null;
                    }
                });

    }

","This is my old app I implemented InterstitialAd on it
first, create two vars one for interstitial ad and the other to store the number of the clicks
private static byte clickNumber = 0;
private InterstitialAd interstitialAd;

second in onCreate prepare the advertisement
interstitialAd = new InterstitialAd(this);
interstitialAd.setAdUnitId(&quot;ca-app-pub-3940256099942544/1033173712&quot;);
interstitialAd.loadAd(new AdRequest.Builder().build());

the ad method shows the ad when the targeted click is reached e.g 3 clicks
  public static void detailsActivityInterstitial(){
        interstitialAd.setAdListener(new AdListener() {
            @Override
            public void onAdLoaded() {
                // Code to be executed when an ad finishes loading.
                if(clickNumber == 3){
                    interstitialAd.show();
                }else {
                    clickNumber++;
                }

            }

            @Override
            public void onAdFailedToLoad(int errorCode) {
                // Code to be executed when an ad request fails.
                Log.e(&quot;onAdFailedToLoad&quot;,&quot;detailsActivityInterstitial&quot; + errorCode);
            }

            @Override
            public void onAdOpened() {
                // Code to be executed when the ad is displayed.
            }

            @Override
            public void onAdLeftApplication() {
                // Code to be executed when the user has left the app.
                clickNumber = 0;
            }

            @Override
            public void onAdClosed() {
                // Code to be executed when the interstitial ad is closed.
                clickNumber = 0;

            }
        });
    }

and finally, call the method in onStart method when user is connected
 if(Utils.hasNetworkAccess(this)) {
MobileAds.initialize(this, getString(R.string.ADMOB_APP_ID));
detailsActivityInterstitial();
}

Also, there are Google sample ads on GitHub for every type of AdMob ads, you can use it as a reference, the following one is for interstitial, It simulates the display of an ad in a game
the layout
&lt;RelativeLayout xmlns:android=&quot;http://schemas.android.com/apk/res/android&quot;
    xmlns:tools=&quot;http://schemas.android.com/tools&quot;
    android:id=&quot;@+id/container&quot;
    android:layout_width=&quot;match_parent&quot;
    android:layout_height=&quot;match_parent&quot;
    tools:context=&quot;.MyActivity&quot;
    tools:ignore=&quot;MergeRootFrame&quot;&gt;

    &lt;TextView
        android:id=&quot;@+id/game_title&quot;
        android:layout_width=&quot;wrap_content&quot;
        android:layout_height=&quot;wrap_content&quot;
        android:layout_centerHorizontal=&quot;true&quot;
        android:layout_marginTop=&quot;50dp&quot;
        android:text=&quot;@string/impossible_game&quot;
        android:textAppearance=&quot;?android:attr/textAppearanceLarge&quot; /&gt;

    &lt;TextView
        android:id=&quot;@+id/timer&quot;
        android:layout_width=&quot;wrap_content&quot;
        android:layout_height=&quot;wrap_content&quot;
        android:layout_below=&quot;@+id/game_title&quot;
        android:layout_centerHorizontal=&quot;true&quot;
        android:textAppearance=&quot;?android:attr/textAppearanceLarge&quot; /&gt;

    &lt;Button
        android:id=&quot;@+id/retry_button&quot;
        android:layout_width=&quot;wrap_content&quot;
        android:layout_height=&quot;wrap_content&quot;
        android:layout_centerHorizontal=&quot;true&quot;
        android:layout_centerVertical=&quot;true&quot;
        android:text=&quot;Retry&quot; /&gt;

&lt;/RelativeLayout&gt;

The activity class
/**
 * Main Activity. Inflates main activity xml.
 */
@SuppressLint(&quot;SetTextI18n&quot;)
public class MyActivity extends AppCompatActivity {

    private static final long GAME_LENGTH_MILLISECONDS = 3000;
    private static final String AD_UNIT_ID = &quot;ca-app-pub-3940256099942544/1033173712&quot;;
    private static final String TAG = &quot;MyActivity&quot;;

    private InterstitialAd interstitialAd;
    private CountDownTimer countDownTimer;
    private Button retryButton;
    private boolean gameIsInProgress;
    private long timerMilliseconds;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_my);

        // Log the Mobile Ads SDK version.
        Log.d(TAG, &quot;Google Mobile Ads SDK Version: &quot; + MobileAds.getVersion());

        // Initialize the Mobile Ads SDK.
        MobileAds.initialize(this, new OnInitializationCompleteListener() {
            @Override
            public void onInitializationComplete(InitializationStatus initializationStatus) {}
        });

    loadAd();

        // Create the &quot;retry&quot; button, which tries to show an interstitial between game plays.
        retryButton = findViewById(R.id.retry_button);
        retryButton.setVisibility(View.INVISIBLE);
        retryButton.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                showInterstitial();
            }
        });

        startGame();
    }

  public void loadAd() {
    AdRequest adRequest = new AdRequest.Builder().build();
    InterstitialAd.load(
        this,
        AD_UNIT_ID,
        adRequest,
        new InterstitialAdLoadCallback() {
          @Override
          public void onAdLoaded(@NonNull InterstitialAd interstitialAd) {
            // The mInterstitialAd reference will be null until
            // an ad is loaded.
            MyActivity.this.interstitialAd = interstitialAd;
            Log.i(TAG, &quot;onAdLoaded&quot;);
            Toast.makeText(MyActivity.this, &quot;onAdLoaded()&quot;, Toast.LENGTH_SHORT).show();
            interstitialAd.setFullScreenContentCallback(
                new FullScreenContentCallback() {
                  @Override
                  public void onAdDismissedFullScreenContent() {
                    // Called when fullscreen content is dismissed.
                    // Make sure to set your reference to null so you don't
                    // show it a second time.
                    MyActivity.this.interstitialAd = null;
                    Log.d(&quot;TAG&quot;, &quot;The ad was dismissed.&quot;);
                  }

                  @Override
                  public void onAdFailedToShowFullScreenContent(AdError adError) {
                    // Called when fullscreen content failed to show.
                    // Make sure to set your reference to null so you don't
                    // show it a second time.
                    MyActivity.this.interstitialAd = null;
                    Log.d(&quot;TAG&quot;, &quot;The ad failed to show.&quot;);
                  }

                  @Override
                  public void onAdShowedFullScreenContent() {
                    // Called when fullscreen content is shown.
                    Log.d(&quot;TAG&quot;, &quot;The ad was shown.&quot;);
                  }
                });
          }

          @Override
          public void onAdFailedToLoad(@NonNull LoadAdError loadAdError) {
            // Handle the error
            Log.i(TAG, loadAdError.getMessage());
            interstitialAd = null;

            String error =
                String.format(
                    &quot;domain: %s, code: %d, message: %s&quot;,
                    loadAdError.getDomain(), loadAdError.getCode(), loadAdError.getMessage());
            Toast.makeText(
                    MyActivity.this, &quot;onAdFailedToLoad() with error: &quot; + error, Toast.LENGTH_SHORT)
                .show();
          }
        });
  }

    private void createTimer(final long milliseconds) {
        // Create the game timer, which counts down to the end of the level
        // and shows the &quot;retry&quot; button.
        if (countDownTimer != null) {
            countDownTimer.cancel();
        }

        final TextView textView = findViewById(R.id.timer);

        countDownTimer = new CountDownTimer(milliseconds, 50) {
            @Override
            public void onTick(long millisUnitFinished) {
                timerMilliseconds = millisUnitFinished;
                textView.setText(&quot;seconds remaining: &quot; + ((millisUnitFinished / 1000) + 1));
            }

            @Override
            public void onFinish() {
                gameIsInProgress = false;
                textView.setText(&quot;done!&quot;);
                retryButton.setVisibility(View.VISIBLE);
            }
        };
    }

    @Override
    public void onResume() {
        // Start or resume the game.
        super.onResume();

        if (gameIsInProgress) {
            resumeGame(timerMilliseconds);
        }
    }

    @Override
    public void onPause() {
        // Cancel the timer if the game is paused.
        countDownTimer.cancel();
        super.onPause();
    }

    private void showInterstitial() {
    // Show the ad if it's ready. Otherwise toast and restart the game.
    if (interstitialAd != null) {
      interstitialAd.show(this);
        } else {
            Toast.makeText(this, &quot;Ad did not load&quot;, Toast.LENGTH_SHORT).show();
            startGame();
        }
    }

    private void startGame() {
    // Request a new ad if one isn't already loaded, hide the button, and kick off the timer.
    if (interstitialAd == null) {
      loadAd();
        }

        retryButton.setVisibility(View.INVISIBLE);
        resumeGame(GAME_LENGTH_MILLISECONDS);
    }

    private void resumeGame(long milliseconds) {
        // Create a new timer for the correct length and start it.
        gameIsInProgress = true;
        timerMilliseconds = milliseconds;
        createTimer(milliseconds);
        countDownTimer.start();
    }
}

This video from the Google Admob channel shows the Best practices to win with AdMob interstitial ads
"
"I am doing a very simple replacement on an XML template below:
&lt;?xmlÂ version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;Â Â 
&lt;note&gt;Â Â 
    &lt;to&gt;
        ARABIC: [[${TEST_AR}]]
        HEBREW: [[${TEST_HE}]]  
        CHINESE (MANDARIN): [[${TEST_CH}]]
    &lt;/to&gt;
&lt;/note&gt;

But it seems like thymeleaf has a UTF8 xml bug, Or Im missing something.
Here is what I tried so far:

Template is coded in UTF-8
Java source code is saved in UTF-8
encoding for OutputStreamWriter is UTF-8
ClassLoaderTemplateResolver is set to UTF-8    Maven is set to UTF-8
project.build.sourceEncoding UTF-8
coded the XML as UTF-8  


Seems like Thymeleaf wont write UTF-8 text correctly to XML.
The code example below work faultlessly (except Chinese not sure why but its not import atm) as long as I am opening a text template (just the file extension) .
If I use this line , It works ok and output UTF-8 no Issues.
 templateEngine.process(&quot;test_template.txt&quot;, ct,out);

works great:
&lt;?xmlÂ version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;Â Â 
&lt;note&gt;Â Â 
    &lt;to&gt;
        ARABIC: ÙƒØªØ§Ø¨Ø© Ù…ÙÙ‡ÙˆÙ…Ø© Ù…Ù† Ù‚Ø¨Ù„ Ø§ØºÙ„
        HEBREW: × ×™×¡×™×•×Ÿ  
        CHINESE (MANDARIN): 
    &lt;/to&gt;
&lt;/note&gt;

Once I modify this line (and rename the template accordingly)  to this:
templateEngine.process(&quot;test_template.xml&quot;, ct,out);

thymeleaf will crap out the Uincode fonts and export them as HEX representation.
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;Â 
&lt;note&gt;Â Â 
    &lt;to&gt;
        ARABIC: &amp;#x643;&amp;#x62a;&amp;#x627;&amp;#x628;&amp;#x629; &amp;#x645;&amp;#x641;&amp;#x647;&amp;#x648;&amp;#x645;&amp;#x629; &amp;#x645;&amp;#x646; &amp;#x642;&amp;#x628;&amp;#x644; &amp;#x627;&amp;#x63a;&amp;#x644;
        HEBREW: &amp;#x5e0;&amp;#x5d9;&amp;#x5e1;&amp;#x5d9;&amp;#x5d5;&amp;#x5df;  
        CHINESE (MANDARIN): 
    &lt;/to&gt;
&lt;/note&gt;

Full isolated working example just create the template (test_template.txt) and put it under src/main/resources
package com.xerox;
import java.io.BufferedWriter;
import java.io.FileOutputStream;
import java.io.OutputStreamWriter;
import java.nio.charset.StandardCharsets;
import java.time.LocalDateTime;

import org.thymeleaf.TemplateEngine;
import org.thymeleaf.context.Context;
import org.thymeleaf.templatemode.TemplateMode;
import org.thymeleaf.templateresolver.ClassLoaderTemplateResolver;

public class TestThymeleafUTF8 {
    public static void main(String[] args) {
        try {
               TemplateEngine templateEngine = new TemplateEngine();
                ClassLoaderTemplateResolver resolver = new ClassLoaderTemplateResolver();
                resolver.setCharacterEncoding(&quot;UTF-8&quot;);     
                resolver.setTemplateMode(TemplateMode.TEXT);
                templateEngine.setTemplateResolver(resolver);
                Context ct = new Context();             
                ct.setVariable(&quot;TEST_AR&quot;, &quot;ÙƒØªØ§Ø¨Ø© Ù…ÙÙ‡ÙˆÙ…Ø© Ù…Ù† Ù‚Ø¨Ù„ Ø§ØºÙ„&quot;);
                ct.setVariable(&quot;TEST_HE&quot;, &quot;× ×™×¡×™×•×Ÿ&quot;);
                ct.setVariable(&quot;TEST_CN&quot;, &quot;çŽ‹æ˜Žæ˜¯ä¸­å›½äººã€‚&quot;);
                ct.setVariable(&quot;currentDate&quot;, LocalDateTime.now().toString());
                BufferedWriter out = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(&quot;test_output.txt&quot;), StandardCharsets.UTF_8));
            
                templateEngine.process(&quot;test_template.txt&quot;, ct,out);
        } catch (Exception e) {
            System.out.println(e);
        }
    }
}

Pom.xml:
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
  &lt;groupId&gt;com.xerox&lt;/groupId&gt;
  &lt;artifactId&gt;testUTF&lt;/artifactId&gt;
  &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
  &lt;name&gt;testUTF&lt;/name&gt;
  &lt;description&gt;thymeleaf testUTF&lt;/description&gt;
  
    &lt;properties&gt;
            &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
            &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
            &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
    &lt;/properties&gt;   
  &lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.thymeleaf&lt;/groupId&gt;
        &lt;artifactId&gt;thymeleaf&lt;/artifactId&gt;
        &lt;version&gt;3.1.0.RELEASE&lt;/version&gt;
    &lt;/dependency&gt;

  &lt;/dependencies&gt;
&lt;/project&gt;

","It is not a bug, but the programmed behavior.
The setTemplateMode documentation indicates:

Sets the template mode to be applied to templates resolved by this
resolver.
If template mode patterns (see setHtmlTemplateModePatterns(Set),
setXmlTemplateModePatterns(Set), etc.) are also set, they have higher
priority than the template mode set here (this would act as a default).
Note that this template mode also may not be applied if the template
resource name ends in a known file name suffix: .html, .htm, .xhtml, .xml,  .js, .json, .css, .rss, .atom, .txt. If this behaviour needs to be
overridden so that template name is always applied, the
setForceTemplateMode(boolean) will need to be set.

Note the note that indicates that for a well known file name suffix the template mode will be overwritten. That explains the behavior you are facing: when you use test_template.xml Thymeleaf will no longer use TemplateMode.TEXT as configured but TemplateMode.XML instead, and the text will be then escaped.
As advised in the aforementioned javadoc, you can force Thymeleaf to obbey the configured TemplateMode and your desired behavior using setForceTemplateMode:
package com.xerox;

import java.io.BufferedWriter;
import java.io.FileOutputStream;
import java.io.OutputStreamWriter;
import java.nio.charset.StandardCharsets;
import java.time.LocalDateTime;

import org.thymeleaf.TemplateEngine;
import org.thymeleaf.context.Context;
import org.thymeleaf.templatemode.TemplateMode;
import org.thymeleaf.templateresolver.ClassLoaderTemplateResolver;

public class TestThymeleafUTF8 {
  public static void main(String[] args) {
    try {
      TemplateEngine templateEngine = new TemplateEngine();
      ClassLoaderTemplateResolver resolver = new ClassLoaderTemplateResolver();
      resolver.setCharacterEncoding(&quot;UTF-8&quot;);
      resolver.setTemplateMode(TemplateMode.TEXT); // TemplateMode.HTML should work as well
      resolver.setForceTemplateMode(true);
      templateEngine.setTemplateResolver(resolver);
      Context ct = new Context();
      ct.setVariable(&quot;TEST_AR&quot;, &quot;كتابة مفهومة من قبل اغل&quot;);
      ct.setVariable(&quot;TEST_HE&quot;, &quot;ניסיון&quot;);
      ct.setVariable(&quot;TEST_CH&quot;, &quot;王明是中国人。&quot;);
      ct.setVariable(&quot;currentDate&quot;, LocalDateTime.now().toString());
      BufferedWriter out = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(&quot;test_output.xml&quot;), StandardCharsets.UTF_8));

      templateEngine.process(&quot;test_template.xml&quot;, ct,out);
    } catch (Exception e) {
      System.out.println(e);
    }
  }
}


"
"I have JSON like below
{
    &quot;name&quot; : &quot;sahal&quot;,
    &quot;address&quot; : [
        {
           &quot;state&quot; : &quot;FL&quot;
        },
        {
           &quot;country&quot; : &quot;FL&quot;,
           &quot;city&quot; : {
               &quot;type&quot; : &quot;municipality&quot;,
               &quot;value&quot; : &quot;California City&quot;
           }
        },
        {
           &quot;pin&quot; : &quot;87876&quot;
        }
    ]
}

None of the key:value is constant. Name can change any time. And some time name comes like FirstName.
I tried with  @JsonAnySetter  and  @JsonNode. These works with only one hierarchy
Any other way I can do this and reuse it for other JSON structure without writing the
Pojo for each projects ?
","Consider to use JsonNode and fuse a hierarchy loop to extract all the keys and values dynamically regardless of patterns and relations and values,
    public static void main(String[] args) throws JsonProcessingException {
        //which hierarchy is your json object
        JsonNode node = nodeGenerator(hierarchy);
        JSONObject flat = new JSONObject();
        node.fields().forEachRemaining(o -&gt; {
            if (!o.getValue().isContainerNode())
                flat.put(o.getKey(), o.getValue());
            else {
                parser(o.getValue(), flat);
            }
        });
        System.out.println(flat);
        System.out.println(flat.get(&quot;type&quot;));
    }

    public static JsonNode nodeGenerator(JSONObject input) throws JsonProcessingException {
        ObjectMapper mapper = new ObjectMapper();
        return mapper.readTree(input.toString());
    }

    public static void parser(JsonNode node, JSONObject flat) {
        if (node.isArray()) {
            ArrayNode array = node.deepCopy();
            array.forEach(u -&gt;
                    u.fields().forEachRemaining(o -&gt; {
                        if (!o.getValue().isContainerNode())
                            flat.put(o.getKey(), o.getValue());
                        else {
                            parser(o.getValue(), flat);
                        }
                    }));
        } else {
            node.fields().forEachRemaining(o -&gt; flat.put(o.getKey(), o.getValue()));
        }
    }

Which in your case extracted json will be as follow :

"
"Recently, I have upgraded my cloud dataflow application from Java 11 to Java 17 and its corresponding dependencies. The application works fine and even the test cases work fine. I have also upgraded my apache beam version from 2.35.0 to 2.49.0.
However, in one of the custom classes, RedisWriteIO, there are some changes and now the tests are not passing in the new code coverage.
RedisWriteIO
package com.example.dataflow.io.redis;

import com.google.auto.value.AutoValue;
import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.PTransform;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.values.KV;
import org.apache.beam.sdk.values.PCollection;
import org.apache.beam.sdk.values.PDone;
import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions;
import org.checkerframework.checker.nullness.qual.Nullable;
import redis.clients.jedis.Jedis;
import redis.clients.jedis.Transaction;

public class RedisWriteIO {

    public static Write write() {
        return (new AutoValue_RedisWriteIO_Write.Builder())
                .setConnectionConfiguration(CustomRedisConfigurations.create()).build();
    }

    @AutoValue
    public abstract static class Write extends PTransform&lt;PCollection&lt;KV&lt;String,String&gt;&gt;, PDone&gt; {
        public Write() {
        }

        @Nullable
        abstract CustomRedisConfigurations connectionConfiguration();

        @Nullable
        abstract Long expireTime();

        abstract Builder toBuilder();

        public Write withEndpoint(String host, int port) {
            Preconditions.checkArgument(host != null, &quot;host can not be null&quot;);
            Preconditions.checkArgument(port &gt; 0, &quot;port can not be negative or 0&quot;);
            return this.toBuilder().setConnectionConfiguration(this.connectionConfiguration().withHost(host).withPort(port)).build();
        }

        public Write withAuth(String auth) {
            Preconditions.checkArgument(auth != null, &quot;auth can not be null&quot;);
            return this.toBuilder().setConnectionConfiguration(this.connectionConfiguration().withAuth(auth)).build();
        }

        public Write withTimeout(int timeout) {
            Preconditions.checkArgument(timeout &gt;= 0, &quot;timeout can not be negative&quot;);
            return this.toBuilder().setConnectionConfiguration(this.connectionConfiguration().withTimeout(timeout)).build();
        }

        public Write withConnectionConfiguration(CustomRedisConfigurations connection) {
            Preconditions.checkArgument(connection != null, &quot;connection can not be null&quot;);
            return this.toBuilder().setConnectionConfiguration(connection).build();
        }

        public Write withExpireTime(Long expireTimeMillis) {
            Preconditions.checkArgument(expireTimeMillis != null, &quot;expireTimeMillis can not be null&quot;);
            Preconditions.checkArgument(expireTimeMillis &gt; 0L, &quot;expireTimeMillis can not be negative or 0&quot;);
            return this.toBuilder().setExpireTime(expireTimeMillis).build();
        }

        public PDone expand(PCollection&lt;KV&lt;String, String&gt;&gt; input) {
            Preconditions.checkArgument(this.connectionConfiguration() != null, &quot;withConnectionConfiguration() is required&quot;);
            input.apply(ParDo.of(new WriteFn(this)));
            return PDone.in(input.getPipeline());
        }

        private static class WriteFn extends DoFn&lt;KV&lt;String, String&gt;, Void&gt;{
            private static final int DEFAULT_BATCH_SIZE = 1000;
            private final RedisWriteIO.Write spec;
            private transient Jedis jedis;
            private transient @Nullable Transaction transaction;

            private int batchCount;

            public WriteFn(RedisWriteIO.Write spec) {
                this.spec = spec;
            }

            @Setup
            public void setup() {
                jedis = spec.connectionConfiguration().connect();
            }

            @StartBundle
            public void startBundle() {
                transaction = jedis.multi();
                batchCount = 0;
            }
            @ProcessElement
            public void processElement(DoFn&lt;KV&lt;String, String&gt;, Void&gt;.ProcessContext c) {

                KV&lt;String, String&gt; record = c.element();

                String fieldKey = record.getKey();
                String fieldValue = record.getValue();

                transaction.sadd(fieldKey,fieldValue);

                batchCount++;

                if (batchCount &gt;= DEFAULT_BATCH_SIZE) {
                    transaction.exec();
                    transaction.multi();
                    batchCount = 0;
                }
            }

            @FinishBundle
            public void finishBundle() {
                if (batchCount &gt; 0) {
                    transaction.exec();
                }
                if (transaction != null) {
                    transaction.close();
                }
                transaction = null;
                batchCount = 0;
            }

            @Teardown
            public void teardown() {
                jedis.close();
            }
        }

        @AutoValue.Builder
        abstract static class Builder {
            Builder() {
            }

            abstract Builder setConnectionConfiguration(CustomRedisConfigurations connectionConfiguration);

            abstract Builder setExpireTime(Long expireTimeMillis);

            abstract Write build();

        }
    }
}

The test class is as follows:
package com.example.dataflow.io.redis;

import com.github.fppt.jedismock.RedisServer;
import org.apache.beam.sdk.io.TextIO;
import org.apache.beam.sdk.testing.PAssert;
import org.apache.beam.sdk.testing.TestPipeline;
import org.apache.beam.sdk.transforms.Create;
import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.transforms.Wait;
import org.apache.beam.sdk.values.KV;
import org.apache.beam.sdk.values.PCollection;
import org.apache.beam.sdk.values.PDone;
import org.junit.*;
import org.mockito.Mock;
import org.mockito.MockitoAnnotations;
import redis.clients.jedis.Jedis;
import redis.clients.jedis.Transaction;

import javax.net.ssl.SSLSocketFactory;
import java.io.IOException;

import static org.junit.Assert.assertNotNull;
import static org.mockito.Mockito.*;


public class RedisWriteIOTest {

    private static final String REDIS_HOST = &quot;localhost&quot;;
    private static final String[] INPUT_DATA = new String[]{
            &quot;123456789&quot;,
            &quot;Bruce&quot;,
            &quot;Wayne&quot;
    };

    @Mock
    static SSLSocketFactory socketFactory;
    private static RedisServer server;
    private static int port;

    @Mock
    private static Jedis jedis;

    @Mock
    private Transaction transaction;

    private int batchCount;

    @Rule
    public TestPipeline pipeline = TestPipeline.create();
    @Mock
    CustomRedisConfigurations connection;

    @Mock
    DoFn.OutputReceiver&lt;KV&lt;String, String&gt;&gt; out;

    @Before
    public void setUp() {
        MockitoAnnotations.openMocks(this);
        when(connection.connect()).thenReturn(jedis);
        when(jedis.multi()).thenReturn(transaction);
        batchCount = 0;
    }


    @BeforeClass
    public static void beforeClass() throws Exception {
        server = RedisServer.newRedisServer(8000);
        server.start();
        port = server.getBindPort();
        jedis = new Jedis(server.getHost(), server.getBindPort());
    }

    @AfterClass
    public static void afterClass() throws IOException {
        jedis.close();
        server.stop();
    }

    @Test
    public void WriteMemoryStoreWithEmptyAuth() {
        RedisWriteIO.write()
                .withEndpoint(REDIS_HOST, port).withAuth(&quot;&quot;);
    }

    @Test
    public void WriteMemoryStoreWithAuth() {
        RedisWriteIO.write()
                .withAuth(&quot;AuthString&quot;);
    }

    @Test
    public void WriteTimeOut() {
        RedisWriteIO.write()
                .withTimeout(10);
    }

    @Test
    public void WriteMemoryStoreWithExpireTime() {
        RedisWriteIO.Write write = RedisWriteIO.write();
        write = write.withExpireTime(1000L);
        assertNotNull(write);
    }

    @Test(expected = IllegalArgumentException.class)
    public void WriteMemoryStoreWithoutExpireTime() {
        RedisWriteIO.write()
                .withExpireTime(0L);
    }


    @Test(expected = IllegalArgumentException.class)
    public void WriteMemoryStoreWithNegativeExpireTime() {
        RedisWriteIO.write()
                .withExpireTime(-10L);
    }

    @Test
    public void WriteMemoryStoryWithConnectionConfiguration() {
        connection = CustomRedisConfigurations.create().withHost(REDIS_HOST).withPort(port);
        RedisWriteIO.Write write = RedisWriteIO.write()
                .withConnectionConfiguration(connection);
        assertNotNull(write);
    }

    @Test(expected = IllegalArgumentException.class)
    public void WriteMemoryStoryWithNullConnectionConfiguration() {
        RedisWriteIO.Write write = RedisWriteIO.write()
                .withConnectionConfiguration(null);
    }


    @Test
    public void testBatchProcessingWithTransactionExecuted() {
        RedisWriteIO.Write spec = RedisWriteIO.write().withConnectionConfiguration(connection);
        PCollection&lt;String&gt; flushFlag = pipeline.apply(&quot;Read File&quot;, TextIO.read().from(&quot;files/fileHavingFiveThousandRecords.txt&quot;));

        List&lt;KV&lt;String, String&gt;&gt; recordEntries = new ArrayList&lt;&gt;();
        for (int i = 0; i &lt;= 10000; i++) {
            // adding unique entries 10000 times
            recordEntries.add(KV.of(&quot;Bruce:Wayne&quot; + i, &quot;123456789&quot; + i));
        }

        // outputData will be written to Redis (memorystore)
        PCollection&lt;KV&lt;String, String&gt;&gt; outputData = pipeline.apply(Create.of(recordEntries));

        outputData.apply(&quot;Waiting until clearing Redis database&quot;, Wait.on(flushFlag))
               .apply(&quot;Writing the data into Redis database&quot;, RedisWriteIO.write()
                    .withConnectionConfiguration(CustomRedisConfigurations
                            .create(REDIS_HOST, port)
                            .withTimeout(100)
                            .withAuth(&quot;credentials&quot;)
                            .enableSSL()));
        pipeline.run();

    }

}

RedisWriteIO is a utility class that would write the data from files into Redis database. It works as expected, and the test cases written are working as expected. However, the below block of code is not getting covered by SonarQube.
if (batchCount &gt;= DEFAULT_BATCH_SIZE) {
     transaction.exec();
     transaction.multi();
     batchCount = 0;
}

When the file is having more than 1000 records, the above block should execute. It doesn't work in the test class. I have tried covering this block of code in the testBatchProcessingWithTransactionExecuted() method with a test file having 5000 records but still the block of code doesn't execute.
I need help in writing the test case covering all the lines.
","I was able to write the test case for covering all the lines. I just increased the size of the list to 20000, by doing so, the RedisWriteIO class is functioning as intended to handle even larger datasets.
The batchcount of 1000 acts as the threshold as specified by DEFAULT_BATCH_SIZE, which when it reaches that, then the transaction is executed(transaction.exec()) and a new transaction is started (transaction.multi()).
    @Test
    public void testBatchProcessingWithTransactionExecuted() {
        RedisWriteIO.Write spec = RedisWriteIO.write().withConnectionConfiguration(connection);
        PCollection&lt;String&gt; flushFlag = pipeline.apply(&quot;Read File&quot;, TextIO.read().from(&quot;files/fileHavingFiveThousandRecords.txt&quot;));

        List&lt;KV&lt;String, String&gt;&gt; recordEntries = new ArrayList&lt;&gt;();
        for (int i = 0; i &lt;= 20000; i++) {
            // adding unique entries 20000 times
            recordEntries.add(KV.of(&quot;Bruce:Wayne&quot; + i, &quot;123456789&quot; + i));
        }

        // outputData will be written to Redis (memorystore)
        PCollection&lt;KV&lt;String, String&gt;&gt; outputData = pipeline.apply(Create.of(recordEntries));

        outputData.apply(&quot;Waiting until clearing Redis database&quot;, Wait.on(flushFlag))
               .apply(&quot;Writing the data into Redis database&quot;, RedisWriteIO.write()
                    .withConnectionConfiguration(CustomRedisConfigurations
                            .create(REDIS_HOST, port)
                            .withTimeout(100)
                            .withAuth(&quot;credentials&quot;)
                            .enableSSL()));
        pipeline.run();

    }

"
"Suppose I have 2 ComboBoxes like these, and they show prompt text.

Then let's say I choose 2 items corresponding to those 2 comboboxes

However, upon reselecting the Front-end, the second combobox lost its prompt text, despite me having set the prompt text using setPromptText()

How can I do that? Here is my Application file
package com.example.demo;

import javafx.application.Application;
import javafx.fxml.FXMLLoader;
import javafx.scene.Parent;
import javafx.scene.Scene;
import javafx.stage.Stage;

public class HelloApplication extends Application {

    public static void main(String[] args) {
        launch(args);
    }

    @Override
    public void start(Stage primaryStage) {
        try{
            Parent root = FXMLLoader.load(this.getClass().getResource(&quot;test.fxml&quot;));
            Scene scene = new Scene(root);
            primaryStage.setScene(scene);
            primaryStage.show();
        }catch (Exception e){
            System.out.println(e);
        }
    }
}

Here is the Controller file
package com.example.demo;

import javafx.collections.FXCollections;
import javafx.collections.ObservableList;
import javafx.fxml.FXML;
import javafx.fxml.Initializable;
import javafx.scene.control.ComboBox;

import java.net.URL;
import java.util.ResourceBundle;

public class Test implements Initializable {
    @FXML
    public ComboBox&lt;String&gt; DevelopmentType;
    @FXML
    public ComboBox&lt;String&gt; LanguageProgramming;

    ObservableList&lt;String&gt; listDevelopmentType = FXCollections.observableArrayList(&quot;Back-end&quot;, &quot;Front-end&quot;);
    ObservableList&lt;String&gt; listLanguageProgrammingBackEnd = FXCollections.observableArrayList(&quot;Java&quot;, &quot;PHP&quot;);
    ObservableList&lt;String&gt; listLanguageProgrammingFrontEnd = FXCollections.observableArrayList(&quot;HTML&quot;, &quot;CSS&quot;, &quot;Javascript&quot;);

    @Override
    public void initialize(URL location, ResourceBundle resources) {
        DevelopmentType.setItems(listDevelopmentType);
    }
    public void DevelopmentTypeEvent(){
        if(DevelopmentType.getValue().equals(&quot;Back-end&quot;)){
            LanguageProgramming.setPromptText(&quot;Select a language programming&quot;);//Not working
            LanguageProgramming.setItems(listLanguageProgrammingBackEnd);
        }
        else if(DevelopmentType.getValue().equals(&quot;Front-end&quot;)){
            LanguageProgramming.setPromptText(&quot;Select a language programming&quot;);//Not working
            LanguageProgramming.setItems(listLanguageProgrammingFrontEnd);
        }
    }
}

Here is the FXML file
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;?import javafx.scene.control.ComboBox?&gt;
&lt;?import javafx.scene.layout.AnchorPane?&gt;


&lt;AnchorPane prefHeight=&quot;400.0&quot; prefWidth=&quot;600.0&quot; xmlns=&quot;http://javafx.com/javafx/20.0.1&quot; xmlns:fx=&quot;http://javafx.com/fxml/1&quot; fx:controller=&quot;com.example.demo.Test&quot;&gt;
   &lt;children&gt;
      &lt;ComboBox fx:id=&quot;DevelopmentType&quot; layoutX=&quot;75.0&quot; layoutY=&quot;62.0&quot; onAction=&quot;#DevelopmentTypeEvent&quot; prefHeight=&quot;26.0&quot; prefWidth=&quot;218.0&quot; promptText=&quot;Select a development type&quot; /&gt;
      &lt;ComboBox fx:id=&quot;LanguageProgramming&quot; layoutX=&quot;75.0&quot; layoutY=&quot;98.0&quot; prefHeight=&quot;26.0&quot; prefWidth=&quot;217.0&quot; promptText=&quot;Select a language programming&quot; /&gt;
   &lt;/children&gt;
&lt;/AnchorPane&gt;

","The use of prompt text in a combo box is not well documented. The API documentation says, somewhat vaguely:

Prompt text is not displayed in all circumstances, it is dependent upon the subclasses of ComboBoxBase to clarify when promptText will be shown. For example, in most cases prompt text will never be shown when a combo box is non-editable

The implementation seems patchy, as the question notes. The prompt text appears not to be supported in non-editable mode, with the exception of the initial display.
The (mostly) more robust approach here is to use a custom button cell (which is used to configure the display of the combo box button):
public class Test implements Initializable {
    @FXML
    public ComboBox&lt;String&gt; DevelopmentType;
    @FXML
    public ComboBox&lt;String&gt; LanguageProgramming;

    ObservableList&lt;String&gt; listDevelopmentType = FXCollections.observableArrayList(&quot;Back-end&quot;, &quot;Front-end&quot;);
    ObservableList&lt;String&gt; listLanguageProgrammingBackEnd = FXCollections.observableArrayList(&quot;Java&quot;, &quot;PHP&quot;);
    ObservableList&lt;String&gt; listLanguageProgrammingFrontEnd = FXCollections.observableArrayList(&quot;HTML&quot;, &quot;CSS&quot;, &quot;Javascript&quot;);

    @Override
    public void initialize(URL location, ResourceBundle resources) {
        DevelopmentType.setItems(listDevelopmentType);
        LanguageProgramming.setButtonCell(new ListCell&lt;String&gt;() {
            @Override
            protected void updateItem(String item, boolean empty) {
                super.updateItem(item, empty);
                if (empty || item == null || item.isEmpty()) {
                    setText(&quot;Select a programming language&quot;);
                } else {
                    setText(item);
                }
            }
        });
    }
    public void DevelopmentTypeEvent(){
        if(DevelopmentType.getValue().equals(&quot;Back-end&quot;)){
            LanguageProgramming.setItems(listLanguageProgrammingBackEnd);
        }
        else if(DevelopmentType.getValue().equals(&quot;Front-end&quot;)){
            LanguageProgramming.setItems(listLanguageProgrammingFrontEnd);
        }
    }
}

This also seems slightly broken: it doesn't by itself properly set the text on the initial display. If you keep the initial setting of the prompt text in the FMXL, it seems to work the way you want.
"
"I'm trying to handle the event when a user presses &quot;ok&quot; or &quot;cancel&quot; on the automatic permission dialog presented when I connect a &quot;known&quot; USB device to the android phone.
I'm using the android.usb.host library and can send and receive between the android phone and the device. Futhermore do I handle the &quot;USB_DEVICE_ATTACHED&quot; and &quot;USB_DEVICE_DETACHED&quot; using a BroadcastReceiver without any problems.
I want to enable a sort of &quot;autoconnect&quot; feature and therefore I need to know when the user has pressed &quot;ok&quot; in the automatically displayed permission dialog, but I can't find anything online at all. All I find is &quot;bypass dialog&quot;, but this is not what I want or need.
When I connect the usb device to the android phone, a permission dialog is automatically displayed because I use the &quot;device_filter.xml&quot; solution from androids documentation which can be seen here Android Usb Docs.
This is how I handle the USB_DEVICE_ATTATCHED and USB_DEVICE_DETACHED events:

  public NativeUsbService(ReactApplicationContext reactContext) {
    ...
    // register device attached/detached event listeners
    IntentFilter filter = new IntentFilter();
    filter.addAction(UsbManager.ACTION_USB_DEVICE_ATTACHED);
    filter.addAction(UsbManager.ACTION_USB_DEVICE_DETACHED);

    reactContext.registerReceiver(usbReceiver, filter);
    ...
  }

And then the Broadcast Receiver:
private final BroadcastReceiver usbReceiver = new BroadcastReceiver() {

    public void onReceive(Context context, Intent intent) {
      String action = intent.getAction();
      if (action.equals(UsbManager.ACTION_USB_DEVICE_ATTACHED)) {
        synchronized (this) {
          UsbDevice device = (UsbDevice) intent.getParcelableExtra(UsbManager.EXTRA_DEVICE);

          if(device != null){
            usbDevice = device;
          } else {
            Log.d(TAG, &quot;onReceive: DEVICE WAS ATTACHED AND WAS NULL :(&quot;);
          }
        }
      } else if (action.equals(UsbManager.ACTION_USB_DEVICE_DETACHED)) {
        Log.d(TAG, &quot;onReceive: Device was detached!&quot;);
        if(connection != null) {
          connection.releaseInterface(usbDeviceInterface);
          connection.close();
        }
        connection = null;
        usbDevice = null;
        endpointIn = null;
        endpointOut = null;
      }
    }
  };

I have tried multiple different approaches, but nothing has worked.
I have tried getting the user response in from the intent, like with a manual permission request like below:
private final BroadcastReceiver usbReceiver = new BroadcastReceiver() {

    public void onReceive(Context context, Intent intent) {
      String action = intent.getAction();
      if (action.equals(UsbManager.ACTION_USB_DEVICE_ATTACHED)) {
        synchronized (this) {
          UsbDevice device = (UsbDevice) intent.getParcelableExtra(UsbManager.EXTRA_DEVICE);

          if(device != null){
            usbDevice = device;

            // THIS DOES NOT WORK â†“â†“â†“
            if(intent.getBooleanExtra(UsbManager.EXTRA_PERMISSION_GRANTED, false)) {
              // The code never gets here...
            }

          } else {
              Log.d(TAG, &quot;onReceive: DEVICE WAS ATTACHED AND WAS NULL :(&quot;);
              sendEvent(&quot;onDeviceAttached&quot;, false);
          }

        }
      } else if (action.equals(UsbManager.ACTION_USB_DEVICE_DETACHED)) {
        if(connection != null) {
          connection.releaseInterface(usbDeviceInterface);
          connection.close();
        }
        connection = null;
        usbDevice = null;
        endpointIn = null;
        endpointOut = null;
      }
    }
  };
 

I have also tried by adding a usb permission listener to the broadcast receiver by first adding the action name to my class variables:
private static final String ACTION_USB_PERMISSION = &quot;com.android.example.USB_PERMISSION&quot;;

Then adding the action to my intent filter like so:
 public NativeUsbService(ReactApplicationContext reactContext) {
    // register device attached/detached event listeners
    IntentFilter filter = new IntentFilter();
    filter.addAction(UsbManager.ACTION_USB_DEVICE_ATTACHED);
    filter.addAction(UsbManager.ACTION_USB_DEVICE_DETACHED);
    filter.addAction(ACTION_USB_PERMISSION); // added action to my intent filter

    reactContext.registerReceiver(usbReceiver, filter);
  }

And finally reacting to the action like so:
  private final BroadcastReceiver usbReceiver = new BroadcastReceiver() {

    public void onReceive(Context context, Intent intent) {
      String action = intent.getAction();
      if (action.equals(UsbManager.ACTION_USB_DEVICE_ATTACHED)) {
        synchronized (this) {
          UsbDevice device = (UsbDevice) intent.getParcelableExtra(UsbManager.EXTRA_DEVICE);

          if(device != null){
            usbDevice = device;
          } 
        }
      } else if (action.equals(UsbManager.ACTION_USB_DEVICE_DETACHED)) {
        Log.d(TAG, &quot;onReceive: Device was detached!&quot;);
        if(connection != null) {
          connection.releaseInterface(usbDeviceInterface);
          connection.close();
        }
        connection = null;
        usbDevice = null;
        endpointIn = null;
        endpointOut = null;

        sendEvent(&quot;onDeviceDetached&quot;, true);
      }
      else if (action.equals(ACTION_USB_PERMISSION)) {
        Log.d(TAG, &quot;onReceive: ACTION_USB_PERMISSION&quot;);
        if(intent.getBooleanExtra(UsbManager.EXTRA_PERMISSION_GRANTED, false)) {
          Log.d(TAG, &quot;onReceive: EXTRA_PERMISSION_GRANTED = true&quot;);
        } else Log.d(TAG, &quot;onReceive: EXTRA_PERMISSION_GRANTED = false&quot;);
      }
    }
  };

Please make me aware of any missing information.
Any help is greatly appreciated.
","Answering my own question in case someone else is facing the same issue.
I though about manually requesting the permission again, after permission was granted, since it is possible to handle this manual permission request when user presses an option in the dialog. I discarded this idea, not because it wouldn't work, but because I saw it as unecessary for the user to also have to press another dialog after the initial (automatic dialog).
I must add that I have not implemented this solution, so I do not know with certainty that it would prompt the user again, but I have had trouble with the manual permission request previously. If you want to try this approach the method belongs to the UsbManager class and is invoke like so usbManger.requestPermission(usbDevice).
I ended up with a solution where I start a thread which runs a loop calling usbManager.hasPermission(usbDevice) until it has permission and then emits an event (emitting this event is my use case, implement it how you like).
The solution can be seen here:
  import android.hardware.usb.UsbDevice;
  import android.hardware.usb.UsbDeviceConnection;
  import android.hardware.usb.UsbManager;
  import com.facebook.react.bridge.ReactApplicationContext;
  ...

  private static volatile boolean permissionThreadShouldStop = false;
  private static Thread activePermissionThread = null;

  ...

  public static void usbPermissionEventEmitter(ReactApplicationContext reactContext, UsbManager usbManager, UsbDevice usbDevice) {

    if((activePermissionThread != null &amp;&amp; activePermissionThread.isAlive())) {
      activePermissionThread.interrupt();
    }

    permissionThreadShouldStop = false;

    activePermissionThread = new Thread(new Runnable() {
      @Override
      public void run() {

        while (!usbManager.hasPermission(usbDevice) &amp;&amp; !permissionThreadShouldStop) {
          try {
            Thread.sleep(30);
          } catch (InterruptedException e) {
            e.printStackTrace();
            break;
          }
        }

        if(usbManager.hasPermission(usbDevice)) {
          sendEvent(reactContext, &quot;onUsbPermissionGranted&quot;, true);
        }
      }
    });

    activePermissionThread.start();
  }

The ReactApplicationContext can be swapped with the normal android context. But this is for a react native module, so I use the reactContext.
I hope this will be helpful for someone, because i'm honestly really surpriced how scarse the android documentation is in regards to implementing Usb functionality using the android.hardware.usb library.
Also in general when searching for information online, I have often found myself lost since there is very little information on this subject.
"
"I am getting an error regarding accessibility issue while running the application.
The decorateTask() method is a protected method of the ScheduledThreadPoolExecutor class, which means it can only be accessed by subclasses of ScheduledThreadPoolExecutor, or by other classes in the same package as ScheduledThreadPoolExecutor. If you are not accessing the method from a subclass or from the same package, you will need to modify your code to either extend ScheduledThreadPoolExecutor or move your code into the same package as ScheduledThreadPoolExecutor.
Here is my springboot version:
&lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;2.7.2&lt;/version&gt;
        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;

This is my java version and the spring cloud version:
&lt;java.version&gt;17&lt;/java.version&gt;
&lt;spring-cloud.version&gt;2021.0.3&lt;/spring-cloud.version&gt;

I am getting this exception while executing it.
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.boot.actuate.autoconfigure.metrics.task.TaskExecutorMetricsAutoConfiguration': Injection of autowired dependencies failed; nested exception is java.lang.reflect.InaccessibleObjectException: Unable to make protected java.util.concurrent.RunnableScheduledFuture java.util.concurrent.ScheduledThreadPoolExecutor.decorateTask(java.lang.Runnable,java.util.concurrent.RunnableScheduledFuture) accessible: module java.base does not &quot;opens java.util.concurrent&quot; to unnamed module @525b461a
    at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:405)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1431)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:619)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542)
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:955)
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:918)
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:583)
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:147)
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:734)
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:408)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:308)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1306)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1295)
    at io.armadillo.aftfileuploads.AftFileUploadsApplication.main(AftFileUploadsApplication.java:23)
Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make protected java.util.concurrent.RunnableScheduledFuture java.util.concurrent.ScheduledThreadPoolExecutor.decorateTask(java.lang.Runnable,java.util.concurrent.RunnableScheduledFuture) accessible: module java.base does not &quot;opens java.util.concurrent&quot; to unnamed module @525b461a
    at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)
    at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)
    at java.base/java.lang.reflect.Method.checkCanSetAccessible(Method.java:199)
    at java.base/java.lang.reflect.Method.setAccessible(Method.java:193)
    at org.springframework.util.ReflectionUtils.makeAccessible(ReflectionUtils.java:577)
    at org.springframework.cloud.sleuth.instrument.async.LazyTraceScheduledThreadPoolExecutor.makeAccessibleIfNotNull(LazyTraceScheduledThreadPoolExecutor.java:121)
    at org.springframework.cloud.sleuth.instrument.async.LazyTraceScheduledThreadPoolExecutor.&lt;init&gt;(LazyTraceScheduledThreadPoolExecutor.java:205)
    at org.springframework.cloud.sleuth.instrument.async.LazyTraceThreadPoolTaskScheduler.getScheduledThreadPoolExecutor(LazyTraceThreadPoolTaskScheduler.java:181)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at org.springframework.cloud.sleuth.instrument.async.ExecutorMethodInterceptor.invoke(ExecutorBeanPostProcessor.java:356)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
    at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:763)
    at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:708)
    at org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler$$EnhancerBySpringCGLIB$$19449eea.getScheduledThreadPoolExecutor(&lt;generated&gt;)
    at org.springframework.boot.actuate.autoconfigure.metrics.task.TaskExecutorMetricsAutoConfiguration.safeGetThreadPoolExecutor(TaskExecutorMetricsAutoConfiguration.java:83)
    at org.springframework.boot.actuate.autoconfigure.metrics.task.TaskExecutorMetricsAutoConfiguration.lambda$bindTaskExecutorsToRegistry$0(TaskExecutorMetricsAutoConfiguration.java:61)
    at java.base/java.util.LinkedHashMap.forEach(LinkedHashMap.java:721)
    at org.springframework.boot.actuate.autoconfigure.metrics.task.TaskExecutorMetricsAutoConfiguration.bindTaskExecutorsToRegistry(TaskExecutorMetricsAutoConfiguration.java:56)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredMethodElement.inject(AutowiredAnnotationBeanPostProcessor.java:724)
    at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:119)
    at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:399)
    ... 17 common frames omitted

","This solved the issue:
Issue with using schedular in Spring cloud. It has different configuration:
reference: https://www.baeldung.com/spring-cloud-sleuth-single-application
@Configuration
@EnableAsync
@EnableScheduling
public class ThreadConfig extends AsyncConfigurerSupport
  implements SchedulingConfigurer {
 
    //...
    
    @Override
    public void configureTasks(ScheduledTaskRegistrar scheduledTaskRegistrar) {
        scheduledTaskRegistrar.setScheduler(schedulingExecutor());
    }

    @Bean(destroyMethod = &quot;shutdown&quot;) // I had to remove this line. because it gives error
    public Executor schedulingExecutor() {
        return Executors.newScheduledThreadPool(1);
    }
}

So, in my case proper dependency was not taken and during autoconfiguration it was giving an error.
"
"I have a simple rest API and I am testing springdoc swagger documentation.
The rest controller:
@RestController
public class UserController {

    private final UserService userService;

    public UserController(final UserService userService) {
        this.userService = userService;
    }

    @PostMapping(&quot;/users&quot;)
    @PreAuthorize(&quot;hasAuthority('create:user')&quot;)
    public ResponseEntity&lt;UserDto&gt; create(final @RequestBody @Valid CreateUserCommand command) {
        return ResponseEntity
                .status(HttpStatus.CREATED)
                .body(userService.create(command));
    }
}

Then I wrap all the API response objects in a ResponseControllerAdvice:
@RestControllerAdvice
public class CustomResponseBodyAdvice implements ResponseBodyAdvice&lt;Object&gt; {
    @Override
    public boolean supports(final @NotNull MethodParameter returnType,
                            final @NotNull Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt; converterType) {
        return true;
    }

    @Override
    public Object beforeBodyWrite(final Object body,
                                  final @NotNull MethodParameter returnType,
                                  final @NotNull MediaType selectedContentType,
                                  final @NotNull Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt; selectedConverterType,
                                  final @NotNull ServerHttpRequest request,
                                  final @NotNull ServerHttpResponse response) {
        if (body instanceof ResponseEnvelope || body instanceof Resource) {
            return body;
        }
        if (body instanceof final ResponseEntity&lt;?&gt; responseEntity) {
            response.setStatusCode(responseEntity.getStatusCode());
        }
        return ResponseEnvelope.builder().success(true).result(body).build();
    }

But I am struggling to find a way to make Springdoc take into consideration this ResponseEnvelope wrapper object. Any ideas?
using Spring boot 2.6.2 + Java 17:
            &lt;dependency&gt;
                &lt;groupId&gt;org.springdoc&lt;/groupId&gt;
                &lt;artifactId&gt;springdoc-openapi-ui&lt;/artifactId&gt;
                &lt;version&gt;1.6.4&lt;/version&gt;
            &lt;/dependency&gt;

What I want:
{
 &quot;status&quot;: &quot;OK&quot;,
 &quot;result&quot;: {
   &quot;username&quot;: &quot;johndoe&quot;
   }
 }

What I get:
{
   &quot;username&quot;:  &quot;johndoe&quot;
}

","Fixed it by using the OperationCustomizer interface:
@Configuration
class ApiDocsOperationCustomizer implements OperationCustomizer {

@Override
public Operation customize(Operation operation,
                           HandlerMethod handlerMethod) {
    final Content content = operation.getResponses().get(&quot;200&quot;).getContent();
    content.keySet().forEach(mediaTypeKey -&gt; {
        final MediaType mediaType = content.get(mediaTypeKey);
        mediaType.schema(this.customizeSchema(mediaType.getSchema()));
    });
    return operation;
}

private Schema&lt;?&gt; customizeSchema(final Schema&lt;?&gt; objSchema) {
    final Schema&lt;?&gt; wrapperSchema = new Schema&lt;&gt;();
    wrapperSchema.addProperties(&quot;success&quot;, new BooleanSchema()._default(true));
    wrapperSchema.addProperties(&quot;result&quot;, objSchema);
    return wrapperSchema;
}}

"
"I'm trying to use a custom interceptor following the documentation present here. I just have a simple interceptor showed bellow:
package interceptorserver;

import io.grpc.Metadata;
import io.grpc.ServerCall;
import io.grpc.ServerCall.Listener;
import io.grpc.ServerCallHandler;
import io.grpc.ServerInterceptor;

public class Interceptor implements ServerInterceptor{

    @Override
    public &lt;ReqT, RespT&gt; Listener&lt;ReqT&gt; interceptCall(ServerCall&lt;ReqT, RespT&gt; call, Metadata headers, ServerCallHandler&lt;ReqT, RespT&gt; next) {
        System.out.println(&quot;Hello world&quot;);
        return next.startCall(call, headers);
    }
}


However, when I compile this code and send this to spark connect with the following command:
./start-connect-server.sh \
    --packages org.apache.spark:spark-connect_2.12:3.4.1 \
    --jars Interceptor.jar \
    --conf spark.connect.grpc.interceptor.classes=interceptorserver.Interceptor

I get the following error:
23/07/29 01:17:00 ERROR SparkConnectServer: Error starting Spark Connect server
org.apache.spark.SparkException: [CONNECT.INTERCEPTOR_RUNTIME_ERROR] Generic Spark Connect error. Error instantiating GRPC interceptor: class interceptorserver.Interceptor cannot be cast to class org.sparkproject.connect.grpc.ServerInterceptor (interceptorserver.Interceptor and org.sparkproject.connect.grpc.ServerInterceptor are in unnamed module of loader org.apache.spark.util.MutableURLClassLoader @a5272be)
    at org.apache.spark.sql.connect.service.SparkConnectInterceptorRegistry$.createInstance(SparkConnectInterceptorRegistry.scala:99)
    at org.apache.spark.sql.connect.service.SparkConnectInterceptorRegistry$.$anonfun$createConfiguredInterceptors$4(SparkConnectInterceptorRegistry.scala:67)
    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
    at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
    at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
    at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
...

First I thought that org.sparkproject.connect.grpc.ServerInterceptor is different than io.grpc.ServerInterceptor but when I've checked the code and I saw that Spark is indeed using io.grpc.ServerInterceptor, besides that, the documentation itself says to use io.grpc.ServerInterceptor so my second thought was: &quot;Does my class really implement the io.grpc.ServerInterceptor interface?&quot;, then I did the following dummy test
/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package interceptorserver;

import org.junit.Test;

import org.junit.Assert;

public class LibraryTest {
    @Test public void someLibraryMethodReturnsTrue() {
        Interceptor classUnderTest = new Interceptor();
        Assert.assertTrue(classUnderTest instanceof io.grpc.ServerInterceptor);
    }
}

And my test passed. So my question is: What I'm doing wrong? Why my class can't be casted to the needed one?
","I'm not sure If you made it work.
I have succeeded it, thanks to the comment from hage. Although, due to my own lack of knowledge, it took me a bit to understand it.
I will share it in case it helps someone.
As hage mentioned, Spark shades the GRPC classes.
I copied the configuration of the maven-shade-plugin from the spark-connect-server project to my own pom.xml.
https://github.com/apache/spark/blob/master/connector/connect/server/pom.xml#L284
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;org.example&lt;/groupId&gt;
    &lt;artifactId&gt;interceptor&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

    &lt;properties&gt;
        &lt;maven.compiler.source&gt;11&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;11&lt;/maven.compiler.target&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;spark.shade.packageName&gt;org.sparkproject&lt;/spark.shade.packageName&gt;
        &lt;scala.binary.version&gt;2.12&lt;/scala.binary.version&gt;
        &lt;spark.version&gt;3.5.0&lt;/spark.version&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-connect-common_${scala.binary.version}&lt;/artifactId&gt;
            &lt;version&gt;${spark.version}&lt;/version&gt;
            &lt;scope&gt;provided&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.5.1&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;shadedArtifactAttached&gt;false&lt;/shadedArtifactAttached&gt;
                    &lt;artifactSet&gt;
                        &lt;includes&gt;
                            &lt;include&gt;org.example:interceptor&lt;/include&gt;
                            &lt;include&gt;com.google.guava:*&lt;/include&gt;
                            &lt;include&gt;io.grpc:*:&lt;/include&gt;
                            &lt;include&gt;com.google.protobuf:*&lt;/include&gt;

                            &lt;!--
                              The dependencies below are not added in SBT because SBT add them all
                              as assembly build.
                            --&gt;
                            &lt;include&gt;com.google.android:annotations&lt;/include&gt;
                            &lt;include&gt;com.google.api.grpc:proto-google-common-protos&lt;/include&gt;
                            &lt;include&gt;io.perfmark:perfmark-api&lt;/include&gt;
                            &lt;include&gt;org.codehaus.mojo:animal-sniffer-annotations&lt;/include&gt;
                            &lt;include&gt;com.google.errorprone:error_prone_annotations&lt;/include&gt;
                            &lt;include&gt;com.google.j2objc:j2objc-annotations&lt;/include&gt;
                            &lt;include&gt;org.checkerframework:checker-qual&lt;/include&gt;
                            &lt;include&gt;com.google.code.gson:gson&lt;/include&gt;
                            &lt;include&gt;org.apache.spark:spark-connect-common_${scala.binary.version}&lt;/include&gt;
                        &lt;/includes&gt;
                    &lt;/artifactSet&gt;
                    &lt;relocations&gt;
                        &lt;relocation&gt;
                            &lt;pattern&gt;com.google.common&lt;/pattern&gt;
                            &lt;shadedPattern&gt;${spark.shade.packageName}.connect.guava&lt;/shadedPattern&gt;
                            &lt;includes&gt;
                                &lt;include&gt;com.google.common.**&lt;/include&gt;
                            &lt;/includes&gt;
                        &lt;/relocation&gt;
                        &lt;relocation&gt;
                            &lt;pattern&gt;com.google.thirdparty&lt;/pattern&gt;
                            &lt;shadedPattern&gt;${spark.shade.packageName}.connect.guava&lt;/shadedPattern&gt;
                            &lt;includes&gt;
                                &lt;include&gt;com.google.thirdparty.**&lt;/include&gt;
                            &lt;/includes&gt;
                        &lt;/relocation&gt;
                        &lt;relocation&gt;
                            &lt;pattern&gt;com.google.protobuf&lt;/pattern&gt;
                            &lt;shadedPattern&gt;${spark.shade.packageName}.connect.protobuf&lt;/shadedPattern&gt;
                            &lt;includes&gt;
                                &lt;include&gt;com.google.protobuf.**&lt;/include&gt;
                            &lt;/includes&gt;
                        &lt;/relocation&gt;
                        &lt;relocation&gt;
                            &lt;pattern&gt;io.grpc&lt;/pattern&gt;
                            &lt;shadedPattern&gt;${spark.shade.packageName}.connect.grpc&lt;/shadedPattern&gt;
                        &lt;/relocation&gt;

                        &lt;relocation&gt;
                            &lt;pattern&gt;android.annotation&lt;/pattern&gt;
                            &lt;shadedPattern&gt;${spark.shade.packageName}.connect.android_annotation&lt;/shadedPattern&gt;
                        &lt;/relocation&gt;
                        &lt;relocation&gt;
                            &lt;pattern&gt;io.perfmark&lt;/pattern&gt;
                            &lt;shadedPattern&gt;${spark.shade.packageName}.connect.io_perfmark&lt;/shadedPattern&gt;
                        &lt;/relocation&gt;
                        &lt;relocation&gt;
                            &lt;pattern&gt;org.codehaus.mojo.animal_sniffer&lt;/pattern&gt;
                            &lt;shadedPattern&gt;${spark.shade.packageName}.connect.animal_sniffer&lt;/shadedPattern&gt;
                        &lt;/relocation&gt;
                        &lt;relocation&gt;
                            &lt;pattern&gt;com.google.j2objc.annotations&lt;/pattern&gt;
                            &lt;shadedPattern&gt;${spark.shade.packageName}.connect.j2objc_annotations&lt;/shadedPattern&gt;
                        &lt;/relocation&gt;
                        &lt;relocation&gt;
                            &lt;pattern&gt;com.google.errorprone.annotations&lt;/pattern&gt;
                            &lt;shadedPattern&gt;${spark.shade.packageName}.connect.errorprone_annotations&lt;/shadedPattern&gt;
                        &lt;/relocation&gt;
                        &lt;relocation&gt;
                            &lt;pattern&gt;org.checkerframework&lt;/pattern&gt;
                            &lt;shadedPattern&gt;${spark.shade.packageName}.connect.checkerframework&lt;/shadedPattern&gt;
                        &lt;/relocation&gt;
                        &lt;relocation&gt;
                            &lt;pattern&gt;com.google.gson&lt;/pattern&gt;
                            &lt;shadedPattern&gt;${spark.shade.packageName}.connect.gson&lt;/shadedPattern&gt;
                        &lt;/relocation&gt;

                        &lt;!--
                          For `com.google.api.grpc:proto-google-common-protos`, do not directly define pattern
                          as `common.google`, otherwise, otherwise, the relocation result may be uncertain due
                          to the change of rule order.
                        --&gt;
                        &lt;relocation&gt;
                            &lt;pattern&gt;com.google.api&lt;/pattern&gt;
                            &lt;shadedPattern&gt;${spark.shade.packageName}.connect.google_protos.api&lt;/shadedPattern&gt;
                        &lt;/relocation&gt;
                        &lt;relocation&gt;
                            &lt;pattern&gt;com.google.cloud&lt;/pattern&gt;
                            &lt;shadedPattern&gt;${spark.shade.packageName}.connect.google_protos.cloud&lt;/shadedPattern&gt;
                        &lt;/relocation&gt;
                        &lt;relocation&gt;
                            &lt;pattern&gt;com.google.geo&lt;/pattern&gt;
                            &lt;shadedPattern&gt;${spark.shade.packageName}.connect.google_protos.geo&lt;/shadedPattern&gt;
                        &lt;/relocation&gt;
                        &lt;relocation&gt;
                            &lt;pattern&gt;com.google.logging&lt;/pattern&gt;
                            &lt;shadedPattern&gt;${spark.shade.packageName}.connect.google_protos.logging&lt;/shadedPattern&gt;
                        &lt;/relocation&gt;
                        &lt;relocation&gt;
                            &lt;pattern&gt;com.google.longrunning&lt;/pattern&gt;
                            &lt;shadedPattern&gt;${spark.shade.packageName}.connect.google_protos.longrunning&lt;/shadedPattern&gt;
                        &lt;/relocation&gt;
                        &lt;relocation&gt;
                            &lt;pattern&gt;com.google.rpc&lt;/pattern&gt;
                            &lt;shadedPattern&gt;${spark.shade.packageName}.connect.google_protos.rpc&lt;/shadedPattern&gt;
                        &lt;/relocation&gt;
                        &lt;relocation&gt;
                            &lt;pattern&gt;com.google.type&lt;/pattern&gt;
                            &lt;shadedPattern&gt;${spark.shade.packageName}.connect.google_protos.type&lt;/shadedPattern&gt;
                        &lt;/relocation&gt;
                    &lt;/relocations&gt;
                    &lt;transformers&gt;
                        &lt;transformer
                                implementation=&quot;org.apache.maven.plugins.shade.resource.ServicesResourceTransformer&quot;/&gt;
                    &lt;/transformers&gt;
                &lt;/configuration&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;shade&lt;/goal&gt;
                        &lt;/goals&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;

My interceptor is like Matheus's.
package org.example;

import io.grpc.Metadata;
import io.grpc.ServerCall;
import io.grpc.ServerCallHandler;
import io.grpc.ServerInterceptor;

public class Interceptor implements ServerInterceptor {


    public Interceptor() {
    }

    @Override
    public &lt;ReqT, RespT&gt; ServerCall.Listener&lt;ReqT&gt; interceptCall(ServerCall&lt;ReqT, RespT&gt; serverCall, Metadata metadata, ServerCallHandler&lt;ReqT, RespT&gt; serverCallHandler) {
        System.out.println(&quot;+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++&quot;);
        System.out.println(&quot;HELLO WORLD&quot;);
        System.out.println(&quot;-------------------------------------------------------------------------------------------&quot;);
        return serverCallHandler.startCall(serverCall, metadata);
    }

}

This way it has worked for me without any issues.
"
"I have a method
    private void positionMagican() {
        int x;
        int y;
        boolean magicanIsCreated;
        magicanIsCreated = false;
        while (!magicanIsCreated){
            x = random.nextInt(sizeX);
            y = random.nextInt(sizeY);
            if(field.getFieldable(x,y) instanceof Empty){
                mag = new Magician(x,y,sizeX,sizeY,field,player,this);
                field.setFieldable(x,y,mag);
                magicanIsCreated = true;
            }
        }
    }

And exactly the same methods, but instead of Magican there is a snake, barrels, etc.
Here's an example
    private void positionGoblin() {
        int x;
        int y;
        boolean goblinIsCreated;
        goblinIsCreated = false;
        while (!goblinIsCreated){
            x = random.nextInt(sizeX);
            y = random.nextInt(sizeY);
            if(field.getFieldable(x,y) instanceof Empty){
                goblin = new Goblin(x,y,player,field,this,sizeX,sizeY);
                field.setFieldable(x,y,goblin);
                goblinIsCreated = true; 
            }
        } 
     } ``` 

Here the differences are only in the class of the object and in its parameters, because of this there are many of the same methods in the project, and I donâ€™t understand how to create one method into which the desired parameter could be entered. Is it possible to create a method that combines these methods? I don't understand. How to ensure that an object is created of the required class with the required parameters.
","Wrap your constructors in a single interface.
Your Magician and Goblin constructors take their parameters in a different order, but that’s okay, because that is one of the things an interface can solve:
private interface CharacterGenerator {
    GameCharacter createCharacter(int x,
                                  int y,
                                  int sizeX,
                                  int sizeY,
                                  Field field,
                                  Player player,
                                  Game game);
}

(I am going to assume that Magician and Goblin inherit a common parent, either a common superclass or a common interface—which, for the sake of example, I have named GameCharacter.)
I chose to use the order that corresponds to the Magician constructor, but it doesn’t matter, because each implementation of that interface can choose to do with the constructors as it pleases, including reordering them:
private static class MagicianGenerator
implements CharacterGenerator {
    @Override
    public GameCharacter createCharacter(int x,
                                         int y,
                                         int sizeX,
                                         int sizeY,
                                         Field field,
                                         Player player,
                                         Game game) {

        return new Magician(x, y, sizeX, sizeY, field, player, game);
    }
}

private static class GoblinGenerator
implements CharacterGenerator {
    @Override
    public GameCharacter createCharacter(int x,
                                         int y,
                                         int sizeX,
                                         int sizeY,
                                         Field field,
                                         Player player,
                                         Game game) {

        return new Goblin(x, y, player, field, game, sizeX, sizeY);
    }
}

Now you can create one positioning method instead of two methods, and pass an object that implements CharacterGenerator to that one positioning method:
private void positionCharacter(CharacterGenerator generator) {
    while (true) {
        int x = random.nextInt(sizeX);
        int y = random.nextInt(sizeY);
        if (field.getFieldable(x,y) instanceof Empty) {
            GameCharacter c = generator.createCharacter(
                x, y, sizeX, sizeY, field, player, this);
            field.setFieldable(x, y, c);
            break;
        }
    }
}

And that method would be called using one of these invocations:
positionCharacter(new MagicianGenerator());
positionCharacter(new GoblinGenerator());

Notice that I removed your boolean ‘created’ variable.  You don’t need it.  Just use break to exit your loop.
As others have pointed out, you could make this much more concise if you were to make the Magician constructor and the Goblin constructor take the same arguments in exactly the same order, since that would allow you to treat the CharacterGenerator interface as a functional interface, and it would allow you to use method references instead of explicit implementing classes:
positionCharacter(Magician::new);
positionCharacter(Goblin::new);

"
"I was able to create an elasticsearch 8.5.3 server as a docker image, but with security completely disabled, and in my springboot application I am using ElasticsearchRepository to perform insert,update, and delete and ElasticsearchOperations to perform selection and search, both of these classes/interfaces are included in the Spring Boot Starter Data Elasticsearch 3.0.1 dependency, and I am also using the following application.yaml property to tell both where the server is at
spring:
 elasticsearch:
  uris = 
   - http://localhost:9700
# username: elastic
# password: 123

Now, here is my issue:
I set up another elasticsearch server with complete security features to test my springboot code in a real life scenario, but I can't figure out how to change the application.yaml to add the certificate portion of the security options, I've been stuck on this portion for a week now, I know it contains options like spring.elasticsearch.username and spring.elasticsearch.password, which aren't the issue, but where is the option for the certificate, and how can I make the certificate work on both ElasticsearchRepository and ElasticsearchOperation? I gathered from the majority of tutorials that I need to construct a @configuration class, however the point is that, most, if not all of the tutorials use deprecated methods(I am stuck in a 'This is deprecated' loop), like for example High Level Rest Client. I'm confused as to how to make ElasticsearchRepository and ElasticsearchOperation utilize the specified @Configuration, and what is the alternative to the High Level Rest Client
(I think its RestClient based on what I read on the official documentations, but I cant figure out how to implement it with spring boot elasticsearch data starter)
","What you can do is to extend ElasticsearchConfiguration and override clientConfiguration method. There you can use usingSsl method and pass SSLContext :
@Configuration
class ElasitcSearchConfig extends ElasticsearchConfiguration {

    @Value(&quot;${spring.elasticsearch.client.certificate}&quot;)
    private String certificateBase64;

    @Override
    ClientConfiguration clientConfiguration() {
        final ClientConfiguration clientConfiguration = ClientConfiguration.builder()
                .connectedTo(&quot;localhost:9200&quot;)
                .usingSsl(getSSLConetxt())
                .withBasicAuth(&quot;elastic&quot;, &quot;changeme&quot;)
                .build();
        return clientConfiguration;
    }

    private SSLContext getSSLContext() {
        byte[] decode = Base64.decoder.decode(certificateBase64)

        CertificateFactory cf = CertificateFactory.getInstance(&quot;X.509&quot;);

        Certificate ca;
        try (InputStream certificateInputStream = new ByteArrayInputStream(decode)) {
            ca = cf.generateCertificate(certificateInputStream);
        }

        String keyStoreType = KeyStore.getDefaultType();
        KeyStore keyStore = KeyStore.getInstance(keyStoreType);
        keyStore.load(null, null);
        keyStore.setCertificateEntry(&quot;ca&quot;, ca);

        String tmfAlgorithm = TrustManagerFactory.getDefaultAlgorithm();
        TrustManagerFactory tmf = 
   TrustManagerFactory.getInstance(tmfAlgorithm);
        tmf.init(keyStore);

        SSLContext context = SSLContext.getInstance(&quot;TLS&quot;);
        context.init(null, tmf.getTrustManagers(), null);
        return context;
    }

}

certificateBase64 will hold elasticsearch certificate encoded in base64 format and can be injected through properties file or environment variable (name of the property spring.elasticsearch.client.certificate). The code to create ssl context was originally taken from this answer.
"
"I have a task that will run many times with different values. I'd like to prevent it from executing 2 of the same tasks (Based on the string value) at the same time. Below is an example of the strings. These values will change, but for simplicity I have included these values below in the example. I submit these tasks via an ExecutorService The tasks run, but the 2nd hi blocks the other tasks from running. So 4/5 tasks run concurrently. Once the lock is released from the first hi the 5th tasks continues and the other tasks continue fine. Is there a way to prevent this type of blocking of the task so that the other 3 tasks can run before it so there is no queuing until there is actually 5 tasks running concurrently.
Submission of the tasks:
executor.submit(new Task(&quot;hi&quot;));
executor.submit(new Task(&quot;h&quot;));
executor.submit(new Task(&quot;u&quot;));
executor.submit(new Task(&quot;y&quot;));
executor.submit(new Task(&quot;hi&quot;));
executor.submit(new Task(&quot;p&quot;));
executor.submit(new Task(&quot;o&quot;));
executor.submit(new Task(&quot;bb&quot;));

The Task is simple. It just prints out the string:
Lock l = getLock(x);
try {
l.lock();

System.out.println(x);

try {
Thread.sleep(5000);
} catch (InterruptedException ex) {
Logger.getLogger(Task.class.getName()).log(Level.SEVERE, null, ex);
}

} finally {
l.unlock();

}

I've updated the post to allow for things to be more clearly understood...
","To avoid blocking a thread, you have to ensure that the action doesn’t even run before the other. For example, you can use a CompletableFuture to chain an action, to be scheduled when the previous has been completed:
public static void main(String[] args) {
    ExecutorService es = Executors.newFixedThreadPool(2);
    for(int i = 0; i &lt; 5; i++) submit(&quot;one&quot;, task(&quot;one&quot;), es);
    for(int i = 0; i &lt; 5; i++) submit(&quot;two&quot;, task(&quot;two&quot;), es);
    LockSupport.parkNanos(TimeUnit.SECONDS.toNanos(26));
    es.shutdown();
}

static Runnable task(String x) {
    return () -&gt; {
        System.out.println(x);
        LockSupport.parkNanos(TimeUnit.SECONDS.toNanos(5));
    };
}

static final ConcurrentHashMap&lt;String, CompletableFuture&lt;Void&gt;&gt; MAP
    = new ConcurrentHashMap&lt;&gt;();

static final void submit(String key, Runnable task, Executor e) {
    CompletableFuture&lt;Void&gt; job = MAP.compute(key,
        (k, previous) -&gt; previous != null?
            previous.thenRunAsync(task, e): CompletableFuture.runAsync(task, e));
    job.whenComplete((v,t) -&gt; MAP.remove(key, job));
}

The ConcurrentHashMap allows us to handle the cases as atomic updates

If no previous future exists for a key, just schedule the action, creating the future

If a previous future exists, chain the action, to be scheduled when the previous completed; the dependent action becomes the new future

If a job completed, the two-arg remove(key, job) will remove it if and only if it is still the current job


The example in the main method demonstrates how two independent actions can run with a thread pool of two threads, never blocking at thread.
"
"I am building an application where authentication is done by spring security for HTTP handlers, for HTTP I've disabled csrf protection, and now I want to disable csrf for spring web socket, but I can't figure out how to accomplish this, I've already tried many different approaches but no one seems to be working. If it is impossible to disable csrf for WebSocket how to get a csrf token? (I tried setting up the csrf endpoint to obtain a token but it is not work, and all tutorials I've found are outdated)
Thanks in advance!
web socket security config:
@Configuration
@EnableWebSocketSecurity
public class WebSocketSecurityConfig extends    AbstractSecurityWebSocketMessageBrokerConfigurer {
@Bean
AuthorizationManager&lt;Message&lt;?&gt;&gt; messageAuthorizationManager(
  MessageMatcherDelegatingAuthorizationManager.Builder messages)   {
messages.anyMessage().permitAll();

return messages.build();
}

@Override
    protected boolean sameOriginDisabled() {
    return true;
  }
}

security config:
@Configuration
@EnableWebSecurity(debug = true)
public class SecurityConfig {

  @Autowired
  private JwtFilter jwtFilter;

  @Bean
  SecurityFilterChain securityFilterChain(HttpSecurity HTTP)  throws Exception {
    return http.addFilterBefore(jwtFilter,   BasicAuthenticationFilter.class)
        .cors(AbstractHttpConfigurer::disable)
        .csrf(AbstractHttpConfigurer::disable)
        .authorizeHttpRequests(auth -&gt; auth
        .requestMatchers(&quot;/authenticate&quot;).permitAll()
        .requestMatchers(&quot;/createchatroom&quot;).authenticated()
        .requestMatchers(&quot;/public/*&quot;).permitAll()
        .requestMatchers(&quot;/private/*&quot;).permitAll()
        .requestMatchers(&quot;/ws/**&quot;).authenticated()
        .requestMatchers(&quot;/register&quot;).permitAll()
        .requestMatchers(&quot;/csrf&quot;).authenticated()
         .requestMatchers(&quot;/addEmployeeToFavorites&quot;).hasAnyAuthority(EMPLOYEE.name(),
            ADMIN.name())
        .requestMatchers(&quot;/addChatRoomToFavorites&quot;)
        .hasAnyAuthority(EMPLOYEE.name(), ADMIN.name())
        .requestMatchers(&quot;/home&quot;).hasAnyAuthority(EMPLOYEE.name(), ADMIN.name()))
    .build();
  }
}

","By default, Spring Security requires the CSRF token in any CONNECT message type. This ensures that only a site that has access to the CSRF token can connect. Since only the same origin can access the CSRF token, external domains are not allowed to make a connection.
Spring Security 4.0 has introduced authorization support for WebSockets through the Spring Messaging abstraction.
In Spring Security 5.8, this support has been refreshed to use the AuthorizationManager API.
To configure authorization using Java Configuration, simply include the @EnableWebSocketSecurity annotation and publish an AuthorizationManager&lt;Message&lt;?&gt;&gt; bean or in XML use the use-authorization-manager attribute. One way to do this is by using the AuthorizationManagerMessageMatcherRegistry to specify endpoint patterns like so:
@Configuration
@EnableWebSocketSecurity

public class WebSocketSecurityConfig {
@Bean
AuthorizationManager&lt;Message&lt;?&gt;&gt; messageAuthorizationManager(MessageMatcherDelegatingAuthorizationManager.Builder messages) {
    messages
            .simpDestMatchers(&quot;/user/**&quot;).authenticated()

    return messages.build();
    }
}


Any inbound CONNECT message requires a valid CSRF token to enforce the Same Origin Policy.
The SecurityContextHolder is populated with the user within the simpUser header attribute for any inbound request.
Our messages require the proper authorization. Specifically, any inbound message that starts with &quot;/user/&quot; will require ROLE_USER. Additional details on authorization can be found in [websocket-authorization]


At this point, CSRF is not configurable when using @EnableWebSocketSecurity, though this will likely be added in a future release.

To disable CSRF, instead of using @EnableWebSocketSecurity, you can use XML support or add the Spring Security components yourself, like so:
Java
@Configuration
public class WebSocketSecurityConfig implements WebSocketMessageBrokerConfigurer {
    @Override
    public void addArgumentResolvers(List&lt;HandlerMethodArgumentResolver&gt; argumentResolvers) {
        argumentResolvers.add(new AuthenticationPrincipalArgumentResolver());
    }

    @Override
    public void configureClientInboundChannel(ChannelRegistration registration) {
        AuthorizationManager&lt;Message&lt;?&gt;&gt; myAuthorizationRules = AuthenticatedAuthorizationManager.authenticated();
        AuthorizationChannelInterceptor authz = new AuthorizationChannelInterceptor(myAuthorizationRules);
        AuthorizationEventPublisher publisher = new SpringAuthorizationEventPublisher(this.context);
        authz.setAuthorizationEventPublisher(publisher);
        registration.interceptors(new SecurityContextChannelInterceptor(), authz);
   }
}

web.xml
&lt;websocket-message-broker use-authorization-manager=&quot;true&quot; same-origin-disabled=&quot;true&quot;&gt;
    &lt;intercept-message pattern=&quot;/**&quot; access=&quot;authenticated&quot;/&gt;
&lt;/websocket-message-broker&gt;

On the other hand, if you are using the legacy-websocket-configuration and you want to allow other domains to access your site, you can disable Spring Security’s protection. For example, in Java Configuration you can use the following:
@Configuration
public class WebSocketSecurityConfig extends AbstractSecurityWebSocketMessageBrokerConfigurer {

...

    @Override
    protected boolean sameOriginDisabled() {
        return true;
    }
}


References

WebSocket Security - Spring Security

"
"After upgrading to Spring Boot 3.0.4 webflux from 2.7.5, I am getting &quot;415 UNSUPPORTED_MEDIA_TYPE&quot;. Service was accepting and returning the xml content. It was working fine before.
    @PostMapping(
      value = &quot;/sides/Request&quot;,
      consumes = MediaType.APPLICATION_XML_VALUE,
      produces = MediaType.APPLICATION_XML_VALUE)
  public Mono&lt;ResponseModel&gt; getSsi(@RequestBody Mono&lt;RequestModel&gt; requestModelMono) {...
}

POM:
 &lt;dependency&gt;
            &lt;groupId&gt;javax.xml.bind&lt;/groupId&gt;
            &lt;artifactId&gt;jaxb-api&lt;/artifactId&gt;
            &lt;version&gt;2.3.1&lt;/version&gt;
        &lt;/dependency&gt;

        

        &lt;dependency&gt;
            &lt;groupId&gt;com.sun.xml.bind&lt;/groupId&gt;
            &lt;artifactId&gt;jaxb-core&lt;/artifactId&gt;
 
        &lt;/dependency&gt;

    

        &lt;dependency&gt;
            &lt;groupId&gt;com.sun.xml.bind&lt;/groupId&gt;
            &lt;artifactId&gt;jaxb-impl&lt;/artifactId&gt;
          
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;

","The issue is that javax.xml.bind in Java 11 has been replaced with jakarta.xml.bind in Java 17.
Fix this by updating the dependencies by removing javax.xml.bind:jaxb-api:2.3.1 and replacing with jakarta.xml.bind:jakarta.xml.bind-api (version managed from spring-boot-starter-parent).
All the import javax.xml.bind must be changed to import jakarta.xml.bind in your POJOs if they are handcoded.
Alternatively, if the XML backing classes are generated the jaxb refs in the XSDSchema must changed as follows:
&lt;xsd:schema xmlns:xsd=&quot;http://www.w3.org/2001/XMLSchema&quot;
    xmlns:jxb=&quot;http://java.sun.com/xml/ns/jaxb&quot;
    jxb:version=&quot;1.0&quot;

must be changed to
&lt;xsd:schema xmlns:xsd=&quot;http://www.w3.org/2001/XMLSchema&quot;
    xmlns:jxb=&quot;https://jakarta.ee/xml/ns/jaxb&quot;
    jxb:version=&quot;3.0&quot;

Here is source from a small project that works correctly:
pom
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
    xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

&lt;parent&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
    &lt;version&gt;3.0.5&lt;/version&gt; &lt;!--or 2.7.10 --&gt;
    &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
&lt;/parent&gt;
&lt;groupId&gt;com.example&lt;/groupId&gt;
&lt;artifactId&gt;monostuff&lt;/artifactId&gt;
&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
&lt;name&gt;monostuff&lt;/name&gt;
&lt;description&gt;Demo project for Spring Boot&lt;/description&gt;
&lt;properties&gt;
    &lt;java.version&gt;17&lt;/java.version&gt;&lt;!--or 11 --&gt;
&lt;/properties&gt;
&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-webflux&lt;/artifactId&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;
        &lt;artifactId&gt;lombok&lt;/artifactId&gt;
        &lt;optional&gt;true&lt;/optional&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;jakarta.xml.bind&lt;/groupId&gt;
        &lt;artifactId&gt;jakarta.xml.bind-api&lt;/artifactId&gt;
    &lt;/dependency&gt;
&lt;!-- Java 11 &lt;dependency&gt; --&gt;
&lt;!--            &lt;groupId&gt;javax.xml.bind&lt;/groupId&gt; --&gt;
&lt;!--            &lt;artifactId&gt;jaxb-api&lt;/artifactId&gt; --&gt;
&lt;!--            &lt;version&gt;2.3.0&lt;/version&gt; --&gt;
&lt;!--        &lt;/dependency&gt; --&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;com.sun.xml.bind&lt;/groupId&gt;
        &lt;artifactId&gt;jaxb-core&lt;/artifactId&gt;
        &lt;!-- or Java 11 &lt;version&gt;2.3.0&lt;/version&gt; --&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.sun.xml.bind&lt;/groupId&gt;
        &lt;artifactId&gt;jaxb-impl&lt;/artifactId&gt;
        &lt;!-- or Java 11 &lt;version&gt;2.3.0&lt;/version&gt; --&gt;
        &lt;scope&gt;runtime&lt;/scope&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
        &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.projectreactor&lt;/groupId&gt;
        &lt;artifactId&gt;reactor-test&lt;/artifactId&gt;
        &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;

&lt;build&gt;
    &lt;plugins&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
        &lt;/plugin&gt;
    &lt;/plugins&gt;
&lt;/build&gt;


Controller
package com.example.demo;

import org.springframework.http.MediaType;
import org.springframework.web.bind.annotation.PostMapping;
import org.springframework.web.bind.annotation.RequestBody;
import org.springframework.web.bind.annotation.RestController;

import reactor.core.publisher.Mono;

@RestController
public class MyController {
   @PostMapping(
              value = &quot;/sides/Request&quot;,
              consumes = MediaType.APPLICATION_XML_VALUE,
              produces = MediaType.APPLICATION_XML_VALUE)
          public Mono&lt;ResponseModel&gt; getSsi(@RequestBody Mono&lt;RequestModel&gt; requestModelMono) { 
   
       
         ResponseModel responseModel = new ResponseModel();
        return Mono.just(responseModel);
        }
}

RequestModel
package com.example.demo;

import java.util.Date;

// were javax.xml.bind in java 11 / string boot 2
import jakarta.xml.bind.annotation.XmlAttribute;
import jakarta.xml.bind.annotation.XmlElement;
import jakarta.xml.bind.annotation.XmlRootElement;
import jakarta.xml.bind.annotation.XmlTransient;
import jakarta.xml.bind.annotation.XmlType;

import lombok.Data;
import lombok.NoArgsConstructor;

@XmlRootElement(name = &quot;book&quot;)
@XmlType()
@NoArgsConstructor
@Data
public class RequestModel {

private Long id;
private String name;
private String author;
private Date date;

@XmlAttribute
public void setId(Long id) {
    this.id = id;
}

@XmlElement(name = &quot;title&quot;)
public void setName(String name) {
    this.name = name;
}

@XmlTransient
public void setAuthor(String author) {
    this.author = author;
}

}

ResponseModel
package com.example.demo;

import java.util.Date;

// were javax.xml.bind in java 11 / string boot 2 
import jakarta.xml.bind.annotation.XmlAttribute;
import jakarta.xml.bind.annotation.XmlElement;
import jakarta.xml.bind.annotation.XmlRootElement;
import jakarta.xml.bind.annotation.XmlTransient;
import jakarta.xml.bind.annotation.XmlType;

import lombok.Data;

@XmlRootElement(name = &quot;book&quot;)
@XmlType()
@Data
public class ResponseModel {

private Long id;
private String name;
private String author;
private Date date;

@XmlAttribute
public void setId(Long id) {
    this.id = id;
}

@XmlElement(name = &quot;title&quot;)
public void setName(String name) {
    this.name = name;
}

@XmlTransient
public void setAuthor(String author) {
    this.author = author;
}

}

Sample CURL:
curl http://localhost:8080/sides/Request --header &quot;Content-Type: application/xml&quot; --data &quot;&lt;?xml version=\&quot;1.0\&quot; encoding=\&quot;UTF-8\&quot; standalone=\&quot;yes\&quot;?&gt;&lt;book id=\&quot;1\&quot;&gt;&lt;title&gt;Book1&lt;/title&gt;&lt;date&gt;2016-11-12T11:25:12.227+07:00&lt;/date&gt;&lt;/book&gt;&quot;

"
"I'm trying to create a custom component in JavaFX and import it to SceneBuilder. I created a project that contains only that custom component, and I want to be able to import that component into SceneBuilder. I expected to see CustomComponent in the SceneBuilder selection. However, the custom component selection in SceneBuilder turned out to be blank! How can I solve that? Note that if I use the example described in JavaFX custom component usage in SceneBuilder, it works perfectly.
I don't have a specific FXML file that I want SceneBuilder to show correctly, I just want to import this custom component (that is named CustomComponent) into SceneBuilder.
Here are all my project files. (Note: The Artifact ID of this project is custom-component)
src/main/java/com/remmymilkyway/customcomponent/CustomComponent.java
package com.remmymilkyway.customcomponent;

import javafx.scene.layout.Region;
import javafx.scene.layout.VBox;
import javafx.scene.web.WebEngine;
import javafx.scene.web.WebView;

import java.net.URL;


public class CustomComponent extends Region {
    private final WebView webView;
    private final WebEngine webEngine;

    public CustomComponent() {
        this.webView = new WebView();
        this.webEngine = webView.getEngine();

        URL url = getClass().getResource(&quot;/monaco_editor.html&quot;);
        if (url != null) {
            webEngine.load(url.toExternalForm());
        }

        this.getChildren().add(webView);
    }
    public String getEditorContent() {
        return (String) webEngine.executeScript(&quot;getEditorValue()&quot;);
    }

    public void setEditorContent(String newValue) {
        String escapedContent = newValue.replace(&quot;'&quot;, &quot;\\'&quot;).replace(&quot;\n&quot;, &quot;\\n&quot;);
        webEngine.executeScript(&quot;setEditorValue('&quot; + escapedContent + &quot;');&quot;);
    }

    public void setFontFamily(String fontFamily) {
        webEngine.executeScript(&quot;setFontFamily('&quot; + fontFamily + &quot;');&quot;);
    }

    public void setFontSize(int fontSize) {
        webEngine.executeScript(&quot;setFontSize(&quot; + fontSize + &quot;);&quot;);
    }

    public void setLanguage(String languageIdentifier) {
        webEngine.executeScript(&quot;setLanguage('&quot; + languageIdentifier + &quot;');&quot;);
    }

    @Override
    protected void layoutChildren() {
        webView.setPrefSize(getWidth(), getHeight());
        webView.resize(getWidth(), getHeight());
    }
}

src/main/java/resources/monaco_editor.html
&lt;!DOCTYPE html&gt;
&lt;html lang=&quot;en&quot;&gt;
&lt;head&gt;
    &lt;meta charset=&quot;UTF-8&quot;&gt;
    &lt;title&gt;Monaco Editor in JavaFX&lt;/title&gt;
    &lt;script src=&quot;monaco-editor/min/vs/loader.js&quot;&gt;&lt;/script&gt;
    &lt;style&gt;
        body, html {
            margin: 0;
            padding: 0;
            height: 100%;
            width: 100%;
        }
        #container {
            height: 100%;
            width: 100%;
        }
    &lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;div id=&quot;container&quot;&gt;&lt;/div&gt;
&lt;script&gt;
    require.config({ paths: { 'vs': 'monaco-editor/min/vs' }});
    require(['vs/editor/editor.main'], function () {
        var editor = monaco.editor.create(document.getElementById('container'), {
            language: 'cpp',
            automaticLayout: true
        });

        window.getEditorValue = function () {
            return editor.getValue();
        }
        window.setEditorValue = function (newValue) {
            editor.setValue(newValue);
        }

        window.setFontFamily = function(fontFamily) {
            editor.updateOptions({
                fontFamily: fontFamily
            });
        };

        window.setFontSize = function(fontSize) {
            editor.updateOptions({
                fontSize: fontSize
            });
        };

        window.setLanguage = function(language) {
            monaco.editor.setModelLanguage(editor.getModel(), language);
        };
    });
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;

pom.xml
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;com.remmymilkyway&lt;/groupId&gt;
    &lt;artifactId&gt;custom-component&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;custom-component&lt;/name&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;junit.version&gt;5.10.2&lt;/junit.version&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
            &lt;artifactId&gt;javafx-controls&lt;/artifactId&gt;
            &lt;version&gt;22.0.1&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
            &lt;artifactId&gt;javafx-fxml&lt;/artifactId&gt;
            &lt;version&gt;22.0.1&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
            &lt;artifactId&gt;javafx-web&lt;/artifactId&gt;
            &lt;version&gt;22.0.1&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt;
            &lt;artifactId&gt;junit-jupiter-api&lt;/artifactId&gt;
            &lt;version&gt;${junit.version}&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt;
            &lt;artifactId&gt;junit-jupiter-engine&lt;/artifactId&gt;
            &lt;version&gt;${junit.version}&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.13.0&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;source&gt;22&lt;/source&gt;
                    &lt;target&gt;22&lt;/target&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
                &lt;artifactId&gt;javafx-maven-plugin&lt;/artifactId&gt;
                &lt;version&gt;0.0.8&lt;/version&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;!-- Default configuration for running with: mvn clean javafx:run --&gt;
                        &lt;id&gt;default-cli&lt;/id&gt;
                        &lt;configuration&gt;
                            &lt;mainClass&gt;
                                com.remmymilkyway.customcomponent/com.remmymilkyway.customcomponent.HelloApplication
                            &lt;/mainClass&gt;
                            &lt;launcher&gt;app&lt;/launcher&gt;
                            &lt;jlinkZipName&gt;app&lt;/jlinkZipName&gt;
                            &lt;jlinkImageName&gt;app&lt;/jlinkImageName&gt;
                            &lt;noManPages&gt;true&lt;/noManPages&gt;
                            &lt;stripDebug&gt;true&lt;/stripDebug&gt;
                            &lt;noHeaderFiles&gt;true&lt;/noHeaderFiles&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;

The Monaco Editor distribution files are downloaded in the folder src/main/resources/monaco-editor.
I ran the command mvn install and tried to import the project into SceneBuilder by clicking on the Manually add Library from repository button and imported version 1.0-SNAPSHOT. As shown in this picture:

And I an empty selection when I clicked on the ADD JAR button.

","SceneBuilder 22 does not support WebView in custom components
I tried this and was also unable to get imports of components that rely on WebView to work with SceneBuilder 22.
Even though SceneBuilder will work with the in-built WebView control (it has a component that works with that), it won't work with a custom component that includes a WebView, at least from my testing.  You could file a feature request with Gluon to add the support if you wish.
Test project
You can use a much simpler case to get it to fail.
This test project, roughly follows the example at:

Transforming java slider switch code to Scene Builder control

A project with these two components will only see for import the ButtonComponent, not the WebViewComponent.
src/main/java/com/example/customcomponent/ButtonComponent.java
package com.example.customcomponent;

import javafx.scene.control.Button;
import javafx.scene.layout.Pane;

public class ButtonComponent extends Pane {
    public ButtonComponent() {
        super(new Button(&quot;Button&quot;));
    }
}

src/main/java/com/example/customcomponent/WebViewComponent.java
package com.example.customcomponent;

import javafx.scene.layout.Pane;
import javafx.scene.web.WebView;

public class WebViewComponent extends Pane {
    public WebViewComponent() {
        super(new WebView());
    }
}

src/main/java/module-info.java
module com.example.customcomponent {
    requires javafx.web;
    exports com.example.customcomponent;
}

pom.xml
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;com.example&lt;/groupId&gt;
    &lt;artifactId&gt;editor-component&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;editor-component&lt;/name&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;maven.compiler.source&gt;22&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;22&lt;/maven.compiler.target&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
            &lt;artifactId&gt;javafx-web&lt;/artifactId&gt;
            &lt;version&gt;22.0.2&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
&lt;/project&gt;

SceneBuilder 22 import dialog
Importing into SceneBuilder from the local maven repository (after running a mvn install on the project):
&lt;groupId&gt;com.example&lt;/groupId&gt;
&lt;artifactId&gt;editor-component&lt;/artifactId&gt;
&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

Available components:

"
"I'm trying to make a soap request to a Brazilian government endpoint and I'm facing some trouble.
They make the following wsdl available: https://mdfe-homologacao.svrs.rs.gov.br/ws/MDFeRecepcaoSinc/MDFeRecepcaoSinc.asmx?wsdl
I then generated the corresponding stub using wsimport tool, which consists on the following:

MDFeRecepcaoSinc.java
MdfeRecepcaoResult.java
MDFeRecepcaoSincSoap12.java (interface)
ObjectFactory.java
package-info.java

Then, on my Java application, I did the following:
            ObjectFactory of = new ObjectFactory();
            JAXBElement&lt;String&gt; jaxb = of.createMdfeDadosMsg(&quot;&lt;soap:Envelope xmlns:soap=\&quot;http://www.w3.org/2003/05/soap-envelope\&quot; xmlns:mdf=\&quot;http://www.portalfiscal.inf.br/mdfe/wsdl/MDFeRecepcaoSinc\&quot;&gt;&lt;soap:Header/&gt;&lt;soap:Body&gt;&lt;mdf:mdfeDadosMsg&gt;?&lt;/mdf:mdfeDadosMsg&gt;&lt;/soap:Body&gt;&lt;/soap:Envelope&gt;&quot;);
            MDFeRecepcaoSinc recepcao = new MDFeRecepcaoSinc();
            MDFeRecepcaoSincSoap12 soap = recepcao.getMDFeRecepcaoSincSoap12(
//                  new AddressingFeature(true),
//                  new MTOMFeature(false),
//                  new RespectBindingFeature(true)
            );
            System.out.println(soap.mdfeRecepcao(jaxb.getValue()).getContent());

Although the only result I'm getting, independent of the body text, is [[retMDFe: null]].
I managed to make it work on SoapUI with this exact same request envelope and it returns a correct xml with a few tags inside retMDFe.
It appears to be connecting to their server from my Java client since the tag retMDFe isn't present in the WSDL file or any stub I generated, and since I don't receive the 403 - Forbidden error anymore (configured the system keystore correctly).
Unfortunately, this webservice only allows connections issued with a digital certificate.
I'm suspecting the error may be from the mapping from the endpoint to the MdfeRecepcaoResult class.
I've tried a few things:

enabling different WebServiceFeatures on the constructor of recepcao.getMDFeRecepcaoSincSoap12, although only MTOMFeature as true returned something different: Client received SOAP Fault from server: Server was unable to process request. ---&gt; Data at the root level is invalid. Line 1, position 1. Please see the server log to find more detail regarding exact cause of the failure.;
changing mdfeRecepcao return type from MdfeRecepcaoResult to String, which gave me an empty string;
commenting annotations on mdfeRecepcao, which continued to give me the [[retMDFe: null]] response;
also tried passing different xml strings directly to soap.mdfeRecepcao() method, but got the same results.

What am I possibly doing wrong here? Thank you for your time!
Edit 1:

Declaration of mdfeRecepcao inside MDFeRecepcaoSincSoap12 interface:

    /**
     * 
     * @param mdfeDadosMsg
     * @return
     *     returns br.inf.portalfiscal.mdfe.wsdl.mdferecepcaosinc.MdfeRecepcaoResult
     */
    @WebMethod(action = &quot;http://www.portalfiscal.inf.br/mdfe/wsdl/MDFeRecepcaoSinc/mdfeRecepcao&quot;)
    @WebResult(name = &quot;mdfeRecepcaoResult&quot;, targetNamespace = &quot;http://www.portalfiscal.inf.br/mdfe/wsdl/MDFeRecepcaoSinc&quot;, partName = &quot;mdfeRecepcaoResult&quot;)
    public MdfeRecepcaoResult mdfeRecepcao(
        @WebParam(name = &quot;mdfeDadosMsg&quot;, targetNamespace = &quot;http://www.portalfiscal.inf.br/mdfe/wsdl/MDFeRecepcaoSinc&quot;, partName = &quot;mdfeDadosMsg&quot;)
        String mdfeDadosMsg);


Declaration of createMdfeDadosMsg inside ObjectFactory class

    /**
     * Create an instance of {@link JAXBElement }{@code &lt;}{@link String }{@code &gt;}}
     * 
     */
    @XmlElementDecl(namespace = &quot;http://www.portalfiscal.inf.br/mdfe/wsdl/MDFeRecepcaoSinc&quot;, name = &quot;mdfeDadosMsg&quot;)
    public JAXBElement&lt;String&gt; createMdfeDadosMsg(String value) {
        return new JAXBElement&lt;String&gt;(_MdfeDadosMsg_QNAME, String.class, null, value);
    }

Edit 2:

wsimport version: wsimport version &quot;2.2.9&quot;

wsimport generated files:


br/inf/portalfiscal/mdfe/wsdl/mdferecepcaosinc/MDFeRecepcaoSinc.java
br/inf/portalfiscal/mdfe/wsdl/mdferecepcaosinc/MDFeRecepcaoSincSoap12.java
br/inf/portalfiscal/mdfe/wsdl/mdferecepcaosinc/MdfeRecepcaoResult.java
br/inf/portalfiscal/mdfe/wsdl/mdferecepcaosinc/ObjectFactory.java
br/inf/portalfiscal/mdfe/wsdl/mdferecepcaosinc/package-info.java

I use the following to produce the stubs: wsimport -extension -keep -verbose MDFeRecepcaoSinc.wsdl
and it only gives a single warning: [WARNING] a porta SOAP \&quot;MDFeRecepcaoSincSoap12\&quot;: usa um bind de SOAP 1.2 nÃ£o padrÃ£o. linha 40 de file:/home/teste-progra/tiago/backup/mdfe/wsimport-test/MDFeRecepcaoSinc.wsdl (which means that the port used by the web service does not use a conventional (or default) bind for SOAP 1.2, and has to do with the following line in the wsdl:
    &lt;wsdl:port name=&quot;MDFeRecepcaoSincSoap12&quot; binding=&quot;tns:MDFeRecepcaoSincSoap12&quot;&gt;

I'm not sure if that's of any use though, hence the connection is effectively being held on.
Edit 3: I'm able to successfully read the HTTP request and response with System.setProperty(&quot;com.sun.xml.internal.ws.transport.http.client.HttpTransportPipe.dump&quot;, &quot;true&quot;);
and its content is the following:
---[HTTP request - https://mdfe.svrs.rs.gov.br/ws/MDFeRecepcaoSinc/MDFeRecepcaoSinc.asmx]---
Accept: application/soap+xml, multipart/related
Content-Type: application/soap+xml; charset=utf-8;action=&quot;http://www.portalfiscal.inf.br/mdfe/wsdl/MDFeRecepcaoSinc/mdfeRecepcao&quot;
User-Agent: JAX-WS RI 2.2.9-b130926.1035 svn-revision#5f6196f2b90e9460065a4c2f4e30e065b245e51e
&lt;?xml version='1.0' encoding='UTF-8'?&gt;&lt;S:Envelope xmlns:S=&quot;http://www.w3.org/2003/05/soap-envelope&quot;&gt;&lt;S:Body&gt;&lt;mdfeDadosMsg xmlns=&quot;http://www.portalfiscal.inf.br/mdfe/wsdl/MDFeRecepcaoSinc&quot;&gt;&amp;lt;soap:Envelope xmlns:soap=&quot;http://www.w3.org/2003/05/soap-envelope&quot; xmlns:mdf=&quot;http://www.portalfiscal.inf.br/mdfe/wsdl/MDFeRecepcaoSinc&quot;&amp;gt;&amp;lt;soap:Header/&amp;gt;&amp;lt;soap:Body&amp;gt;&amp;lt;mdf:mdfeDadosMsg&amp;gt;?&amp;lt;/mdf:mdfeDadosMsg&amp;gt;&amp;lt;/soap:Body&amp;gt;&amp;lt;/soap:Envelope&amp;gt;&lt;/mdfeDadosMsg&gt;&lt;/S:Body&gt;&lt;/S:Envelope&gt;--------------------

---[HTTP response - https://mdfe.svrs.rs.gov.br/ws/MDFeRecepcaoSinc/MDFeRecepcaoSinc.asmx - 200]---
null: HTTP/1.1 200 OK
Cache-Control: private, max-age=0
Content-Length: 586
Content-Type: application/soap+xml; charset=utf-8
Date: Fri, 27 Sep 2024 18:54:54 GMT
Server: Microsoft-IIS/10.0
X-AspNet-Version: 4.0.30319
X-Powered-By: ASP.NET
&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;soap:Envelope xmlns:soap=&quot;http://www.w3.org/2003/05/soap-envelope&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:xsd=&quot;http://www.w3.org/2001/XMLSchema&quot;&gt;&lt;soap:Body&gt;&lt;mdfeRecepcaoResult xmlns=&quot;http://www.portalfiscal.inf.br/mdfe/wsdl/MDFeRecepcaoSinc&quot;&gt;&lt;retMDFe xmlns=&quot;http://www.portalfiscal.inf.br/mdfe&quot; versao=&quot;3.00&quot;&gt;&lt;tpAmb&gt;1&lt;/tpAmb&gt;&lt;cUF&gt;43&lt;/cUF&gt;&lt;verAplic&gt;RS20240710093839&lt;/verAplic&gt;&lt;cStat&gt;244&lt;/cStat&gt;&lt;xMotivo&gt;RejeiÃ§Ã£o: Falha na descompactaÃ§Ã£o da Ã¡rea de dados&lt;/xMotivo&gt;&lt;/retMDFe&gt;&lt;/mdfeRecepcaoResult&gt;&lt;/soap:Body&gt;&lt;/soap:Envelope&gt;--------------------

So I'm actually receiving content under retMDFe tag! I just can't get it mapped properly to MdfeRecepcaoResult.
I've tried setting the WS return as String so I could print it directly, which gave me an empty result.
Per instance, which class actually implements this interface?
","You are not correctly marshalling the element here.
// Create the ObjectFactory
ObjectFactory of = new ObjectFactory();

// Create the JAXBElement with the correct inner XML content
String innerXmlContent = &quot;&lt;mdfeDadosMsg xmlns=\&quot;http://www.portalfiscal.inf.br/mdfe/wsdl/MDFeRecepcaoSinc\&quot;&gt;...&lt;/mdfeDadosMsg&gt;&quot;;
JAXBElement&lt;String&gt; jaxb = of.createMdfeDadosMsg(innerXmlContent);

// Create the service and get the SOAP port
MDFeRecepcaoSinc recepcao = new MDFeRecepcaoSinc();
MDFeRecepcaoSincSoap12 soap = recepcao.getMDFeRecepcaoSincSoap12(
        new AddressingFeature(true),
        new MTOMFeature(false),
        new RespectBindingFeature(true)
);

MdfeRecepcaoResult response = soap.mdfeRecepcao(jaxb.getValue()); 
List&lt;Element&gt; list = response.getContent(); 
for(Element element : list) {
    System.out.println(element.getTextContent());
}

"
"I'm making a Spring Boot application. I want to generate PDF from HTML code:
        String htmlString = &quot;&lt;!DOCTYPE html&gt;\n&quot; +
                &quot;&lt;html lang=\&quot;ru\&quot;&gt;\n&quot; +
                &quot;&lt;head&gt;\n&quot; +
                &quot;    &lt;meta charset=\&quot;UTF-8\&quot;/&gt;\n&quot; +
                &quot;    &lt;meta http-equiv=\&quot;X-UA-Compatible\&quot; content=\&quot;IE=edge\&quot;/&gt;\n&quot; +
                &quot;    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1.0\&quot;/&gt;\n&quot; +
                &quot;&lt;/head&gt;\n&quot; +
                &quot;&lt;body&gt;\n&quot; +
                &quot;    &lt;h3&gt;ÐŸÐ Ð•Ð”Ð¡Ð¢ÐÐ’Ð›Ð•ÐÐ˜Ð•&lt;/h3&gt;\n&quot; +
                &quot;&lt;/body&gt;\n&quot; +
                &quot;&lt;/html&gt;&quot;;

        ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();
        String path = FileSystemView.getFileSystemView().getDefaultDirectory().getPath() + &quot;/A.pdf&quot;;
        OutputStream outputStream = new FileOutputStream(path);

        ITextRenderer renderer = new ITextRenderer();
        renderer.setDocumentFromString(htmlString);
        renderer.layout();
        renderer.createPDF(outputStream);

        byteArrayOutputStream.writeTo(outputStream);

As you can see there is a h3 tag with cyrillic symbols. The problem is that after conversion and  saving the symbols are not presented in PDF (it's simply empty, because there is nothing more in html code to be visible). Other symbols are being displayed properly btw.
For html-to-pdf conversion i use:
&lt;dependency&gt;
    &lt;groupId&gt;org.xhtmlrenderer&lt;/groupId&gt;
    &lt;artifactId&gt;flying-saucer-pdf-itext5&lt;/artifactId&gt;
    &lt;version&gt;9.0.1&lt;/version&gt;
&lt;/dependency&gt;

I suppose there is a problem with charset, fonts etc. How can I fix it?
","This worked for me!
public static void main(String[] args) throws DocumentException, IOException, SAXException, ParserConfigurationException {
        String htmlString = &quot;&lt;!DOCTYPE html&gt;\n&quot; + &quot;&lt;html lang=\&quot;ru\&quot;&gt;\n&quot; + &quot;&lt;head&gt;\n&quot;
                + &quot;    &lt;meta charset=\&quot;UTF-8\&quot;/&gt;\n&quot; + &quot;    &lt;meta http-equiv=\&quot;Content-Type\&quot; content=\&quot;text/html\&quot;/&gt;\n&quot;
                + &quot;    &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1.0\&quot;/&gt;\n&quot; 
                + &quot;    &lt;style type='text/css'&gt; &quot;
                + &quot;        * { font-family: Verdana; }/n&quot;
                + &quot;    &lt;/style&gt;/n&quot;
                + &quot;&lt;/head&gt;\n&quot;
                + &quot;&lt;body&gt;\n&quot; + &quot;    &lt;h3&gt;ПРЕДСТАВЛЕНИЕ&lt;/h3&gt;\n&quot; + &quot;&lt;/body&gt;\n&quot; + &quot;&lt;/html&gt;&quot;;


    String path = FileSystemView.getFileSystemView().getDefaultDirectory().getPath() + &quot;/A.pdf&quot;;
    OutputStream os = new FileOutputStream(path);
    ITextRenderer renderer = new ITextRenderer();
    renderer.getFontResolver().addFont(&quot;c:/windows/fonts/verdana.ttf&quot;, BaseFont.IDENTITY_H, BaseFont.NOT_EMBEDDED);
    renderer.setDocumentFromString(htmlString);
    renderer.layout();
    renderer.createPDF(os);
    os.close();
}


I think the trick is to add the CSS to the HTML and the font must match what you set on the PDF.
"
"How can I convert a double array of type String to a double array of type int ?
    @PostMapping(&quot;/hole/coordinate&quot;)
    @ResponseBody
    public String saveCoordinate(@RequestBody Map&lt;String, Object&gt; params) {
        System.out.println(&quot;params = &quot; + params);
        System.out.println(&quot;params = &quot; + params.get(&quot;coordinate&quot;));
        
        return &quot;success&quot;;
    }

System.out.println(params.get(&quot;coordinate&quot;)); store
[[445, 292], [585, 331], [612, 223], [205, 532]]
There are m 2 elements of the double array.
ex) [a,b],[c,d].....m
At this time, I want to receive the result in the data type of int[][], not String.
I was wondering how can I convert from String to int[][].
I tried like below
int[] arr= Stream.of(str.replaceAll(&quot;[\\[\\]\\, ]&quot;, &quot;&quot;).split(&quot;&quot;)).mapToInt(Integer::parseInt).toArray();
for (int i : arr) {
    System.out.println(&quot;i = &quot; + i);
}

but it give me
4
4
5
2
9
2
...

Best Regards!
","You could parse it manually, as done in other answers, but since you are using spring, you should use the tools it offers you.
Spring uses jackson's ObjectMapper for serialization and deserialization by default. A bean of this type is preconfigured for you, you can autowire it in your controller method and use it. Then the entire parsing is this:
int[][] parsedCoordinates = objectMapper.readValue(coordinates, int[][].class);

And your controller method looks like this:
@PostMapping(&quot;/hole/coordinate&quot;)
@ResponseBody
public String saveCoordinate(@RequestBody Map&lt;String, Object&gt; params, ObjectMapper objectMapper) {
    System.out.println(&quot;params = &quot; + params);
    System.out.println(&quot;params = &quot; + params.get(&quot;coordinate&quot;));
    //get string from your params
    String coordinates = &quot;[[445, 292], [585, 331], [612, 223], [205, 532]]&quot;;
    int[][] parsedCoordinates;
    try {
        parsedCoordinates = objectMapper.readValue(coordinates, int[][].class);
    } catch (JsonProcessingException exc) {
        //that's a bad way to handle error, but it's an example
        //you might return error message like - invalid coordinate format
        //or whatever you need
        exc.printStackTrace();
        throw new RuntimeException(exc);
    }
    //printing parsed result to check it
    for (int i = 0; i &lt; parsedCoordinates.length; i++) {
        int[] inner = parsedCoordinates[i];
        for (int j = 0; j &lt; inner.length; j++) {
            System.out.printf(&quot;pos %d-%d, value %d%n&quot;, i, j, parsedCoordinates[i][j]);
        }
    }
    return &quot;success&quot;;
}

"
"I want to shuffle an array of Objects in a card game simulation.
I scrolled through many posts on here and almost all of them mention transforming the array into a list, then shuffling it using an implementation of Collections.shuffle() and then transforming it back into an array.
However, since I actually want to understand what is going on while the shuffling is happening, I want to implement it myself. I wrote this code for my array of Card objects in the array unshuffledDeck[]:
Random shuffleRandom = new Random();
Card[] shuffledDeck = new Card[cardAmount];
for (int i = 0; i &lt; cardAmount; i++) {
    int j = (int) (shuffleRandom.nextFloat() * cardAmount);
    shuffledDeck[i] = unshuffledDeck[j];
}

However, depending on the random number, multiple entries in the shuffledDeck output array can have the same Card in it, which I don't want.
Now I have thought about just adding an if statement to check if the card is already in one of the other entries, something like
Random shuffleRandom = new Random();
Card[] shuffledDeck = new Card[cardAmount];
for (int i = 0; i &lt; cardAmount; i++) {
    int j = (int) (shuffleRandom.nextFloat() * cardAmount);
    boolean cardIsNotYetPresent = true;
    for (int k = 0; k &lt; cardAmount; k++) {
        if (k != i &amp;&amp; shuffledDeck[k] == unshuffledDeck[j]) {
            cardIsNotYetPresent = false;
            break;
        }
    }
    if (cardIsNotYetPresent) {
        shuffledDeck[i] = unshuffledDeck[j];
    } else {
        i--;
    }
}

, but that increase the duration drastically, which is not what I want. How would I approach this problem without adding another O(n) to the runtime of the algorithm?
","Consider implementing the Fisher–Yates shuffle.
Instead of randomly picking an index and checking for duplicates, it works by iterating through the array from the last element to the first and swapping the current element with a randomly chosen earlier element (including itself). This ensures the time complexity is O(n).
import java.util.Random;

class Card {
    private final String suit;
    private final String rank;

    public Card(String rank, String suit) {
        this.rank = rank;
        this.suit = suit;
    }

    @Override
    public String toString() {
        return rank + &quot; of &quot; + suit;
    }
}

public class CardShuffler {
    public static void fisherYatesShuffle(Card[] deck) {
        Random random = new Random();
        int n = deck.length;
        for (int i = n - 1; i &gt; 0; i--) {
            int j = random.nextInt(i + 1);
            swap(deck, i, j);
        }
    }

    private static void swap(Card[] deck, int i, int j) {
        Card temp = deck[i];
        deck[i] = deck[j];
        deck[j] = temp;
    }
    
    private static void printDeck(Card[] deck) {
        for (Card card : deck) {
            System.out.println(card);
        }
    }

    public static void main(String[] args) {
        String[] suits = {&quot;Hearts&quot;, &quot;Diamonds&quot;, &quot;Clubs&quot;, &quot;Spades&quot;};
        String[] ranks = {&quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;10&quot;, &quot;Jack&quot;, &quot;Queen&quot;, &quot;King&quot;, &quot;Ace&quot;};
        Card[] deck = new Card[52];
        int index = 0;
        for (String suit : suits) {
            for (String rank : ranks) {
                deck[index++] = new Card(rank, suit);
            }
        }
        fisherYatesShuffle(deck);
        System.out.println(&quot;Shuffled Deck:&quot;);
        printDeck(deck);
    }
}

Example Output:
Shuffled Deck:
4 of Clubs
8 of Spades
2 of Diamonds
King of Hearts
3 of Diamonds
3 of Clubs
10 of Clubs
4 of Spades
10 of Hearts
Queen of Spades
6 of Hearts
8 of Diamonds
Jack of Spades
King of Spades
5 of Spades
Queen of Diamonds
3 of Hearts
Queen of Hearts
8 of Hearts
2 of Hearts
10 of Spades
2 of Clubs
Jack of Clubs
Ace of Hearts
5 of Hearts
King of Diamonds
10 of Diamonds
5 of Diamonds
7 of Clubs
9 of Clubs
5 of Clubs
6 of Spades
6 of Clubs
9 of Spades
9 of Diamonds
Jack of Hearts
2 of Spades
7 of Hearts
4 of Diamonds
King of Clubs
9 of Hearts
Ace of Diamonds
Ace of Clubs
7 of Diamonds
4 of Hearts
Ace of Spades
8 of Clubs
Queen of Clubs
Jack of Diamonds
3 of Spades
6 of Diamonds
7 of Spades

Try on JDoddle
"
"I am looking to migrate a JUnit 4 test suite to JUnit 5. The JUnit 4 test suite currently looks something like this:
@RunWith(Suite.class)
@SuiteClasses({FirstTest.class, SecondTest.class})
public class JUnit4Suite {
  @ClassRule
  public static JUnit4Server MY_SERVER = new JUnit4Server();
}

where MY_SERVER is an ExternalResource that all the tests in the suite use, to, say publish something (JUnit4Suite.MY_SERVER.publish(...)):
public class JUnit4Server extends ExternalResource {
  @Override
  protected final void before() throws Throwable {
    // start the server
  }

  @Override
  protected final void after() {
    // stop the server
  }
}

The server needs to be initialized only once, at the start of the suite run, before any test runs, and stopped once all tests have finished executing. This currently works fine.
Using JUnit 5, I am coming up with something like this:
@Suite
@SelectClasses({FirstTest.class, SecondTest.class})
public class JUnit5Suite {
  @RegisterExtension
  public static JUnit5Server MY_SERVER = new JUnit5Server();
}

where MY_SERVER now looks like this:
public class JUnit5Server implements BeforeAllCallback, AfterAllCallback {
  @Override
  public void beforeAll(ExtensionContext context) throws Exception {
    ...
  }
}

However, when I run the JUnit5Suite, the server instance gets created fine, however the beforeAll method in the server does not get executed. Is there something missing?
","According to javadoc

@BeforeAll is used to signal that the annotated method should be executed before all tests in the current test class.
In contrast to @BeforeEach methods, @BeforeAll methods are only executed once for a given test class.

Because a test suite probably contains multiple test classes, it's reasonable to change callbacks' behaviour from test class level to test suite level.
Fortunately, JUnit Platform Suite API 1.11 introduced @BeforeSuite and @AfterSuite annotations.

@BeforeSuite is used to signal that the annotated method should be executed before all tests in the current test suite.

and

@AfterSuite is used to signal that the annotated method should be executed after all tests in the current test suite.

With this soulion the server will start before (and will stop after) all tests in the current test suite.
@Suite
@SelectClasses({FirstTest.class, SecondTest.class})
public class SampleSuite {
    static SampleServer server = new SampleServer();

    @BeforeSuite
    static void beforeSuite() {
        System.out.println(&quot;Before Suite&quot;);
        server.start();
    }

    @AfterSuite
    static void afterSuite() {
        server.shutdown();
        System.out.println(&quot;After Suite&quot;);
    }
}

public class SampleServer {
    public void start() {
        System.out.println(&quot;Starting SampleServer&quot;);
    }

    public void shutdown() {
        System.out.println(&quot;SampleServer shutdown&quot;);
    }
}

After running mvn clean test command the output should be like this:
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running io.github.zforgo.jupiter.SampleSuite
Before Suite
Starting SampleServer
[INFO] Running io.github.zforgo.jupiter.FirstTest
FirstTest.bar
FirstTest.foo
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.050 s -- in io.github.zforgo.jupiter.FirstTest
[INFO] Running io.github.zforgo.jupiter.SecondTest
SecondTest.bar
SecondTest.foo
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.007 s -- in io.github.zforgo.jupiter.SecondTest
SampleServer shutdown
After Suite
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.122 s -- in io.github.zforgo.jupiter.SampleSuite

"
"I created the below method to find an Analysis object, update the results field on it and then lastly save the result in the database but not wait for a return.
public void updateAnalysisWithResults(String uuidString, String results) {
        findByUUID(uuidString).subscribe(analysis -&gt; {
            analysis.setResults(results);
            computeSCARepository.save(analysis).subscribe();
        });
    }

This feels poorly written to subscribe within a subscribe.
Is this a bad practice?
Is there a better way to write this?
UPDATE:
entry point
@PatchMapping(&quot;compute/{uuid}/results&quot;)
    public Mono&lt;Void&gt; patchAnalysisWithResults(@PathVariable String uuid, @RequestBody String results) {
        return computeSCAService.updateAnalysisWithResults(uuid,results);
    }

    public Mono&lt;Void&gt; updateAnalysisWithResults(String uuidString, String results) {
//        findByUUID(uuidString).subscribe(analysis -&gt; {
//            analysis.setResults(results);
//            computeSCARepository.save(analysis).subscribe();
//        });
        return findByUUID(uuidString)
                .doOnNext(analysis -&gt; analysis.setResults(results))
                .doOnNext(computeSCARepository::save)
                .then();
    }

","Why it is not working is because you have misunderstood what doOnNext does.
Lets start from the beginning.
A Flux or Mono are producers, they produce items. Your application produces things to the calling client, hence it should always return either a Mono or a Flux. If you don't want to return anything you should return a Mono&lt;Void&gt;.
When the client subscribes to your application what reactor will do is call all operators in the opposite direction until it finds a producer. This is what is called the assembly phase. If all your operators don't chain together you are what i call breaking the reactive chain.
When you break the chain, the things broken from the chain wont be executed.
If we look at your example but in a more exploded version:
@Test
void brokenChainTest() {
    updateAnalysisWithResults(&quot;12345&quot;, &quot;Foo&quot;).subscribe();
}

public Mono&lt;Void&gt; updateAnalysisWithResults(String uuidString, String results) {
    return findByUUID(uuidString)
            .doOnNext(analysis -&gt; analysis.setValue(results))
            .doOnNext(this::save)
            .then();
}

private Mono&lt;Data&gt; save(Data data) {
    return Mono.fromCallable(() -&gt; {
        System.out.println(&quot;Will not print&quot;);
        return data;
    });
}

private Mono&lt;Data&gt; findByUUID(String uuidString) {
    return Mono.just(new Data());
}

private static class Data {
    private String value;

    public void setValue(String value) {
        this.value = value;
    }
}

in the above example save is a callable function that will return a producer. But if we run the above function you will notice that the print will never be executed.
This has to do with the usage of doOnNext. If we read the docs for it it says:

Add behavior triggered when the Mono emits a data successfully.
The Consumer is executed first, then the onNext signal is propagated downstream.

doOnNext takes a Consumer that returns void. And if we look at doOnNext we see that the function description looks as follows:
public final Mono&lt;T&gt; doOnNext(Consumer&lt;? super T&gt; onNext)`

THis means that it takes in a consumer that is a T or extends a T and it returns a Mono&lt;T&gt;. So to keep a long explanation short, you can see that it consumes something but also returns the same something.
What this means is that this usually used for what is called side effects basically for something that is done on the side that does not hinder the current flow. One of those things could for instance logging. Logging is one of those things that would consume for instance a string and log it, while we want to keep the string flowing down our program. Or maybe we we want to increment a number on the side. Or modify some state somewhere. You can read all about side effects here.
you can of think of it visually this way:
     _____ side effect (for instance logging)
    /
___/______ main reactive flow

That's why your first doOnNext setter works, because you are modifying a state on the side, you are setting the value on your class hence modifying the state of your class to have a value.
The second statement on the other hand, the save, does not get executed. You see that function is actually returning something we need to take care of.
This is what it looks like:
      save
     _____ 
    /     \   &lt; Broken return
___/             ____ no main reactive flow

all we have to do is actually change one single line:
// From
.doOnNext(this::save)

// To
.flatMap(this::save)

flatMap takes whatever is in the Mono, and then we can use that to execute something and then return a &quot;new&quot; something.
So our flow (with flatMap) now looks like this:
   setValue()    save()
    ______       _____ 
   /            /     \   
__/____________/       \______ return to client

So with the use of flatMap we are now saving and returning whatever was returned from that function triggering the rest of the chain.
If you then choose to ignore whatever is returned from the flatMap its completely correct to do as you have done to call then which will

Return a Mono which only replays complete and error signals from this

The general rule is, in a fully reactive application, you should never block.
And you generally don't subscribe unless your application is the final consumer. Which means if your application started the request, then you are the consumerof something else so you subscribe. If a webpage starts off the request, then they are the final consumer and they are subscribing.
If you are subscribing in your application that is producing data its like you are running a bakery and eating your baked breads at the same time.
don't do that, its bad for business :D
"
"I'm developing a Java application where several JPanels (not JFrames) have complex animations that necessitate drawing to an off-screen buffer before blitting to the display surface. A problem I'm having is that Swing is performing UI scaling for high-DPI screens, and the off-screen buffer (a raster) isn't &quot;aware&quot; of the scaling. Consequently, when text or graphics are rendered to the buffer, and the buffer is blitted to the JPanel, Swing scales the graphic as a raster and the result looks like garbage.
A simple example is:
import java.awt.*;
import java.awt.geom.Line2D;

import javax.swing.JComponent;
import javax.swing.JFrame;

public class Main {
    public static void main(String[] args) {
        JFrame jf = new JFrame(&quot;Demo&quot;);
        Container cp = jf.getContentPane();
        MyCanvas tl = new MyCanvas();
        cp.add(tl);
        jf.setSize(500, 250);
        jf.setVisible(true);
        jf.setDefaultCloseOperation( JFrame.EXIT_ON_CLOSE );
    }
}

class MyCanvas extends JComponent {

    @Override
    public void paintComponent(Graphics g) {
        if( g instanceof Graphics2D g2 ) {
            g2.setRenderingHint(RenderingHints.KEY_ANTIALIASING,
                RenderingHints.VALUE_ANTIALIAS_ON);

            g2.setFont( Font.decode( &quot;Times New Roman-26&quot; ) );
            g2.drawString(&quot;The poorly-scaled cake is a lie.&quot;,70,40);
            g2.setStroke( new BasicStroke( 2.3f ) );
            g2.draw( new Line2D.Double( 420, 10, 425, 70 ) );

            Image I = createImage( 500, 150 );
            Graphics2D g2_ = (Graphics2D)I.getGraphics();
            g2_.setRenderingHint(RenderingHints.KEY_ANTIALIASING,
                RenderingHints.VALUE_ANTIALIAS_ON);
            g2_.setColor( Color.BLACK );
            g2_.setFont( Font.decode( &quot;Times New Roman-26&quot; ) );
            g2_.drawString( &quot;The poorly-scaled cake is a lie.&quot;,70,40 );
            g2_.setStroke( new BasicStroke( 2.3f ) );
            g2_.draw( new Line2D.Double( 420, 10, 425, 70 ) );
            g2_.dispose();
            g2.drawImage( I, 0, 130, null );
        }
    }
}

From this, compiling with JDK 20 on my Windows 11 machine, I get:

On the top is text and graphics rendered directly to the JPanel. On the bottom is the same content rendered via an intermediary image.
Ideally, I'm looking for a method, e.g., Image createScalingAwareBuffer( JPanel jp, int width, int height ) that returns an image I, in the same vein as JPanel.createImage( ... ) but where the returned Image is vector scaling aware, such that jp.drawImage( I ) or equivalent displays the lower graphic content identically to the upper content.
I suspect that rendering to the back buffer in a double-buffered Swing component has this kind of &quot;awareness&quot;, but this isn't an option in my case since I need to precisely control when buffer flips occur on a panel-by-panel basis, which (insofar as I know) is impossible in Swing.
Is there any solution for this without a radical rewrite (i.e., migrating away from Swing, etc.)?
I should also note that I don't want to disable the UI scaling (e.g., using -Dsun.java2d.uiScale=1 in VM options), hence &quot;just disable UI scaling&quot; isn't really a solution.
","There is something like scalingAwareBuffer.  It’s the RenderableImage interface.
I’m not certain this will help, but in theory it should.  RenderableImage looks like it has a lot of methods, but most of them are very simple.  The important ones are the three create… methods.
Even this won’t produce an identical copy from the image (due to the use of the GPU when drawing directly to the screen?), but the image should at least scale properly.
import java.util.Vector;
import java.util.Map;

import java.awt.Container;
import java.awt.Color;
import java.awt.Font;
import java.awt.Image;
import java.awt.Shape;
import java.awt.Graphics;
import java.awt.Graphics2D;
import java.awt.BasicStroke;
import java.awt.RenderingHints;
import java.awt.EventQueue;

import java.awt.geom.Line2D;
import java.awt.geom.Point2D;
import java.awt.geom.Rectangle2D;
import java.awt.geom.AffineTransform;

import java.awt.image.BufferedImage;
import java.awt.image.RenderedImage;
import java.awt.image.renderable.RenderableImage;
import java.awt.image.renderable.RenderContext;

import javax.swing.JComponent;
import javax.swing.JFrame;

public class ScaledImageRenderExample1 {
    public static void main(String[] args) {
        EventQueue.invokeLater(() -&gt; {
            JFrame jf = new JFrame(&quot;Demo&quot;);
            Container cp = jf.getContentPane();
            MyCanvas tl = new MyCanvas();
            cp.add(tl);
            jf.setSize(500, 250);
            jf.setLocationByPlatform( true );
            jf.setVisible(true);
            jf.setDefaultCloseOperation( JFrame.EXIT_ON_CLOSE );
        });
    }

    static class MyCanvas extends JComponent {
        private static final long serialVersionUID = 1;

        @Override
        public void paintComponent(Graphics g) {
            super.paintComponent(g);
            if( g instanceof Graphics2D g2 ) {
                g2.setRenderingHint(RenderingHints.KEY_ANTIALIASING,
                    RenderingHints.VALUE_ANTIALIAS_ON);

                g2.setFont( Font.decode( &quot;Times New Roman-26&quot; ) );
                g2.drawString(&quot;The poorly-scaled cake is a lie.&quot;,70,40);
                g2.setStroke( new BasicStroke( 2.3f ) );
                g2.draw( new Line2D.Double( 420, 10, 425, 70 ) );

                g2.drawRenderableImage(new CakeImage(),
                    AffineTransform.getTranslateInstance(0, 130));
            }
        }
    }

    static class CakeImage implements RenderableImage {
        private static final int DEFAULT_WIDTH = 500;
        private static final int DEFAULT_HEIGHT = 150;

        private static final RenderingHints DEFAULT_HINTS =
            new RenderingHints(Map.of(
                RenderingHints.KEY_ANTIALIASING,
                RenderingHints.VALUE_ANTIALIAS_ON,

                RenderingHints.KEY_TEXT_ANTIALIASING,
                RenderingHints.VALUE_TEXT_ANTIALIAS_ON,

                RenderingHints.KEY_FRACTIONALMETRICS,
                RenderingHints.VALUE_FRACTIONALMETRICS_ON,

                RenderingHints.KEY_RESOLUTION_VARIANT,
                RenderingHints.VALUE_RESOLUTION_VARIANT_SIZE_FIT,

                RenderingHints.KEY_RENDERING,
                RenderingHints.VALUE_RENDER_QUALITY,

                RenderingHints.KEY_INTERPOLATION,
                RenderingHints.VALUE_INTERPOLATION_BICUBIC,

                RenderingHints.KEY_STROKE_CONTROL,
                RenderingHints.VALUE_STROKE_PURE,

                RenderingHints.KEY_COLOR_RENDERING,
                RenderingHints.VALUE_COLOR_RENDER_QUALITY
        ));


        private final float x;
        private final float y;

        CakeImage() {
            this(0, 0);
        }

        CakeImage(float x,
                  float y) {

            this.x = x;
            this.y = y;
        }

        @Override
        public float getMinX() {
            return x;
        }

        @Override
        public float getMinY() {
            return y;
        }

        @Override
        public float getWidth() {
            return DEFAULT_WIDTH;
        }

        @Override
        public float getHeight() {
            return DEFAULT_HEIGHT;
        }

        @Override
        public boolean isDynamic() {
            return false;
        }

        @Override
        public Object getProperty(String name) {
            return Image.UndefinedProperty;
        }

        @Override
        public String[] getPropertyNames() {
            return new String[0];
        }

        @Override
        public Vector&lt;RenderableImage&gt; getSources() {
            return null;
        }

        private void drawIn(Graphics2D g2) {
            g2.setColor( Color.BLACK );
            g2.setFont( Font.decode( &quot;Times New Roman-26&quot; ) );
            g2.drawString( &quot;The poorly-scaled cake is a lie.&quot;,70,40 );
            g2.setStroke( new BasicStroke( 2.3f ) );
            g2.draw( new Line2D.Double( 420, 10, 425, 70 ) );
        }

        @Override
        public RenderedImage createDefaultRendering() {
            BufferedImage image = new BufferedImage(
                DEFAULT_WIDTH, DEFAULT_HEIGHT, BufferedImage.TYPE_INT_ARGB);

            Graphics2D g2 = (Graphics2D) image.getGraphics();
            RenderingHints hints = g2.getRenderingHints();
            hints.putAll(DEFAULT_HINTS);
            g2.setRenderingHints(hints);
            drawIn(g2);
            g2.dispose();

            return image;
        }

        @Override
        public RenderedImage createScaledRendering(int width,
                                                   int height,
                                                   RenderingHints hints) {

            BufferedImage image = new BufferedImage(width, height,
                BufferedImage.TYPE_INT_ARGB);

            Graphics2D g2 = (Graphics2D) image.getGraphics();
            g2.setRenderingHints(hints);
            g2.scale((double) width / DEFAULT_WIDTH,
                     (double) height / DEFAULT_HEIGHT);
            drawIn(g2);
            g2.dispose();

            return image;
        }

        @Override
        public RenderedImage createRendering(RenderContext context) {
            Point2D size = new Point2D.Float(getWidth(), getHeight());

            Shape shape = context.getAreaOfInterest();
            if (shape != null) {
                Rectangle2D bounds = shape.getBounds2D();
                size = new Point2D.Double(
                    bounds.getWidth(), bounds.getHeight());
            }

            context.getTransform().transform(size, size);

            BufferedImage image = new BufferedImage(
                (int) Math.ceil(size.getX()),
                (int) Math.ceil(size.getY()),
                BufferedImage.TYPE_INT_ARGB);

            Graphics2D g2 = (Graphics2D) image.getGraphics();
            RenderingHints hints = context.getRenderingHints();
            if (hints != null) {
                g2.setRenderingHints(hints);
            } else {
                hints = g2.getRenderingHints();
                hints.putAll(DEFAULT_HINTS);
                g2.setRenderingHints(hints);
            }
            g2.setTransform(context.getTransform());
            drawIn(g2);
            g2.dispose();

            return image;
        }
    }
}

"
"I have the following issue: I need to talk to an old SOAP service, and that one requires me to send a request object where a large amount of data is directly in the SOAP message body, like so:
&lt;SOAP-ENV:Envelope xmlns:SOAP-ENV=&quot;http://schemas.xmlsoap.org/soap/envelope/&quot; xmlns:xsd=&quot;http://www.w3.org/2001/XMLSchema&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;&gt;
    &lt;SOAP-ENV:Body&gt;
        &lt;MyRequest xmlns=&quot;http://my.company.com/xsd/portals/v4_0&quot;&gt;
            &lt;documentList xmlns=&quot;&quot;&gt;
                &lt;binaryData&gt;
                    &lt;blob&gt;
                        VeryLongDataBlobInHere
                    &lt;/blob&gt;
                    &lt;extension&gt;pdf&lt;/extension&gt;
                &lt;/binaryData&gt;
            &lt;/documentList&gt;
        &lt;/MyRequest&gt;
    &lt;/SOAP-ENV:Body&gt;
&lt;/SOAP-ENV:Envelope&gt;

The problem is, Spring automatically turns that into an attachment like this if MTOM is enabled:
&lt;SOAP-ENV:Envelope xmlns:SOAP-ENV=&quot;http://schemas.xmlsoap.org/soap/envelope/&quot; xmlns:xsd=&quot;http://www.w3.org/2001/XMLSchema&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;&gt;
    &lt;SOAP-ENV:Body&gt;
        &lt;MyRequest xmlns=&quot;http://my.company.com/xsd/portals/v4_0&quot;&gt;
            &lt;documentList xmlns=&quot;&quot;&gt;
                &lt;binaryData&gt;
                    &lt;blob&gt;
                        &lt;xop:Include xmlns:xop=&quot;http://www.w3.org/2004/08/xop/include&quot; href=&quot;cid:3be5f4d8-50ed-4f88-8e50-778f6cc70c74%40null&quot;/&gt;
                    &lt;/blob&gt;
                    &lt;extension&gt;pdf&lt;/extension&gt;
                &lt;/binaryData&gt;
            &lt;/documentList&gt;
        &lt;/MyRequest&gt;
    &lt;/SOAP-ENV:Body&gt;
&lt;/SOAP-ENV:Envelope&gt;

By contrast, if MTOM is disabled, the blob is empty like this:
&lt;SOAP-ENV:Envelope xmlns:SOAP-ENV=&quot;http://schemas.xmlsoap.org/soap/envelope/&quot; xmlns:xsd=&quot;http://www.w3.org/2001/XMLSchema&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;&gt;
    &lt;SOAP-ENV:Body&gt;
        &lt;MyRequest xmlns=&quot;http://my.company.com/xsd/portals/v4_0&quot;&gt;
            &lt;documentList xmlns=&quot;&quot;&gt;
                &lt;binaryData&gt;
                    &lt;blob/&gt;
                    &lt;extension&gt;pdf&lt;/extension&gt;
                &lt;/binaryData&gt;
            &lt;/documentList&gt;
        &lt;/MyRequest&gt;
    &lt;/SOAP-ENV:Body&gt;
&lt;/SOAP-ENV:Envelope&gt;

I have tried various approaches to solve this, including messing with the data types, and trying to adjust the properties of the marshaller in order to increase the MTOM threshold, but nothing I tried worked. Here's my marshaller configuration:
@Configuration
public class Jaxb2MarshallerConfig {

    @Bean
    public Jaxb2Marshaller myMarshaller() {
        Jaxb2Marshaller marshaller = new Jaxb2Marshaller();
        marshaller.setContextPath(&quot;com.company.project.xsd.some_portal.v4_0&quot;);
        marshaller.setMtomEnabled(true); 
        return marshaller;
    }
}

And here's where the binary data is built and assigned:
    private BinaryData buildBinaryData(byte[] documentData) {
        BinaryData binaryData = new BinaryData();
        byte[] encodedData = Base64.getEncoder().encode(documentData);
        DataHandler dataHandler = new DataHandler(encodedData, &quot;application/pdf&quot;);
        binaryData.setBlob(dataHandler);
        binaryData.setExtension(&quot;pdf&quot;);
        return binaryData;
    }

BinaryData meanwhile is a generated class built from an WSDL, so I can't change anything in there. But here's how it looks:
@XmlAccessorType(XmlAccessType.FIELD)
@XmlType(name = &quot;BinaryData&quot;, propOrder = {
    &quot;blob&quot;,
    &quot;extension&quot;
})
public class BinaryData {

    @XmlElement(required = true)
    @XmlMimeType(&quot;application/octet-stream&quot;)
    protected DataHandler blob;
    @XmlElement(required = true)
    protected String extension;

    [...]
}

Finally, here's how I sent this whole mess:
@Component
@Log4j2
public class MySoapClient extends WebServiceGatewaySupport {
    private final WebServiceTemplate template;

    public MySoapClient (
        MyServiceProperties properties,
        Jaxb2Marshaller marshaller
    ) {
        setMarshaller(marshaller);
        setUnmarshaller(marshaller);
        setDefaultUri(properties.getTargetUrl());
        template = getWebServiceTemplate();
    }

    @Override
    public void sendDocuments(MyRequest request) {
        try {
            template.marshalSendAndReceive(request);
        } catch (Exception e) {
            log.error(e, e.getCause());
            throw new RuntimeException(e);
        }
    }
}

My best guess is that I somehow need to increase the MTOM threshold, but I have no idea how. I tried messing around with marshaller.setMarshallerProperties(), but nothing there worked.
Does anyone have any idea of how I can get the marshaller to write the blob inline? Or is the problem somewhere else?

Update
I now created a github repository with the minimum required code, as well as a test to reproduce the issue and check for the desired behavior:
https://github.com/KiraResari/jaxb2-marshalling
If you like, you can check it out and try to get the test to pass somehow.
","afflato provided a working solution for this on the linked GitHub repository which I hereby share:
The key is the DataHandler which is assigned in buildBinaryData. That one determines how the request is marshalled. By writing a custom DataSource and using that here, the marshaller will write the blob into the body of the request.
First, a custom DataSource has to be implemented like this:
import jakarta.activation.DataSource;
import java.io.ByteArrayInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;

public class ByteArrayDataSource implements DataSource {
  private byte[] data;
  private String type;

  public ByteArrayDataSource(byte[] data, String type) {
    this.data = data; this.type = type;
  }

  @Override
  public InputStream getInputStream() {
    return new ByteArrayInputStream(data);
  }

  @Override
  public OutputStream getOutputStream() {
    return new ByteArrayOutputStream(data.length);
  }

  @Override
  public String getContentType() {
    return type;
  }

  @Override
  public String getName() {
    return &quot;ByteArrayDataSource&quot;;
  }
}

After that, this DataSource has to be used in the DataHandler in the buildBinaryData function like this:
    private BinaryData buildBinaryData(byte[] documentData) {
        BinaryData binaryData = new BinaryData();
        DataSource ds = new ByteArrayDataSource(documentData, &quot;application/pdf&quot;);
        DataHandler dataHandler = new DataHandler(ds);
        binaryData.setBlob(dataHandler);
        binaryData.setExtension(&quot;pdf&quot;);
        return binaryData;
    }

That is all that is needed. The end result will be that the blob is always written into the body of the SOAP request and never attached.
"
"I am using the given code to generate a serial number into Listview up to a specific range of 11 digits. Up to ten digits, the serial is generated like 0333624140 to 0333624160 (such as 0333624140,0333624141,0333624142,0333624143 etc.), but if we change the required value to 11 digits like 03336241441 to 03336241450 then the app crashes.
btngenerate.setOnClickListener(new View.OnClickListener() {
    @Override
    public void onClick(View view) {
        int value1;
        int value2;
        value1 = Integer.parseInt(txtfirst.getText().toString());
        value2 = Integer.parseInt(txtsecond.getText().toString());
        for(int i=value1;i&lt;=value2;i++){
            list.add(&quot;&quot;+i);
            li.setAdapter(arrayAdapter);
        }

    }
});

","I have solved the solution that I needed by myself and with the help of all of you. I am presenting it here so that someone else can get information about it. Anyway, thank you all from the bottom of my heart۔
btngenerate.setOnClickListener(new View.OnClickListener() {
    @SuppressLint(&quot;DefaultLocale&quot;)
    @Override
    public void onClick(View view) {
        try {
            long value1;
            long value2;
            value1 = Long.parseLong(txtfirst.getText().toString());
            value2 = Long.parseLong(txtsecond.getText().toString());

            for(Long i = (Long) value1; i&lt;=value2; i++) {
                list.add(String.format(&quot;%011d&quot;, i));
                li.setAdapter(arrayAdapter);
            }

        } catch (NumberFormatException e) {
            e.printStackTrace();
        }
    }
});

"
"I started coding with JavaFX a couple of days ago and this is one exercise who's been bothering me for the past five hours or so.
I want to add circles to the scene by first clicking where I want the center to be and then moving the cursor to get the radius; also I'm forcing myself not to use Canvas for the time being.
The code below was slightly modified from the one available here:
Draw circle with mouse click points (JavaFX)
in order to leave every drawn circle on the screen.
import javafx.application.Application;
import javafx.scene.Group;
import javafx.scene.Scene;
import javafx.scene.paint.Color;
import javafx.scene.shape.Circle;
import javafx.stage.Stage;

public class TestJavaFX extends Application {

    private double cX, cY;
    public boolean firstClick = true;

    @Override

    public void start(Stage primaryStage) {
        Group root = new Group();
        Scene scene = new Scene(root, 960, 540);

        scene.setOnMouseClicked(evt -&gt; {

            if (firstClick) {
                cX = evt.getX();
                cY = evt.getY();
                firstClick = false;
            } else {
                double r = Math.sqrt(Math.pow(cX - evt.getX(), 2) + Math.pow(cY - evt.getY(), 2));
                Circle circle = new Circle(cX, cY, r, Color.BLUE);
                root.getChildren().add(circle);
                firstClick = true;
            }
        });

        primaryStage.setTitle(&quot;TestJavaFX&quot;);
        primaryStage.setScene(scene);
        primaryStage.show();
    }
}

I've come up with the code above to add circles to the scene by clicking twice but I was not able to replicate the same result using setOnMouseMoved. Putting Circle circle = new Circle() inside a setOnMouseMoved event creates a new circle at every movement of the cursor effectively making impossible to interact with the screen.
---------- Update based on @James_D's suggestion ----------
Despite being a wonderful suggestion and the sequence feeling way more natural, a new circle is being added to root.getChildren() even if a single click is performed without actually dragging the mouse.
In other words root is being populated also by circles having radius equal to zero, created from a user's erroneous click.
You can see what I mean in the image below where I simply added a System.out.println(root.getChildren().size()) to the first event.

","Other answers have provided implementations of the functionality as described. For completeness, here is an implementation of the alternative I suggested using press-drag-release as the mouse gesture:
import javafx.application.Application;
import javafx.scene.Group;
import javafx.scene.Scene;
import javafx.scene.paint.Color;
import javafx.scene.shape.Circle;
import javafx.stage.Stage;

public class TestJavaFX extends Application {


    private Circle currentCircle;

    @Override

    public void start(Stage primaryStage) {
        Group root = new Group();
        Scene scene = new Scene(root, 960, 540);

        scene.setOnMousePressed(evt -&gt; {
                currentCircle = new Circle(evt.getX(), evt.getY(), 0, Color.BLUE);
                root.getChildren().add(currentCircle);
        });

        scene.setOnMouseDragged(evt -&gt; {
            double cx = currentCircle.getCenterX();
            double cy = currentCircle.getCenterY();
            double edgeX = evt.getX();
            double edgeY = evt.getY();
            double deltaX = cx - edgeX;
            double deltaY = cy - edgeY;
            double r = Math.sqrt(deltaX * deltaX + deltaY * deltaY);
            currentCircle.setRadius(r);
        });

        scene.setOnMouseReleased( _ -&gt; {
            if (currentCircle.getRadius() &lt; 1) {
                root.getChildren().remove(currentCircle);
            }
            currentCircle = null;
        });

        primaryStage.setTitle(&quot;TestJavaFX&quot;);
        primaryStage.setScene(scene);
        primaryStage.show();
    }
}

Note that in this case the code is simpler, in that there is no testing needed to see what state the drawing process is in. That state is inherent in the semantics of the different types of mouse event (press to start a new circle, drag to change the radius, release to stop drawing). There's also less state to track: the only instance variable is the &quot;current circle&quot;.
In the onMouseReleased handler here I removed the current circle if the radius is less than one pixel. This allows the user to &quot;change their mind&quot; and not add the circle after clicking, by releasing the mouse without dragging it. (Thanks to @SedJ601 for the suugestion.) Ultimately, it might be good to implement some &quot;undo&quot; (e.g. a key pressed handler on the scene handling shortcut+z to remove the last circle) or &quot;remove&quot; (e.g. right-click on the circle to remove it) functionality. Neither of these are difficult to implement, but are, as is commonly said in text books, left as exercises for the reader.
"
"Background

I am writing a web application on Windows. This application consists of two or more WARs.
These WARs make temporary files in processing.

Problem

In program testing, I've found a temporary file is still remains and not deleted. I tried to delete this  file from Explorer, but I got the message like The action cannot be completed because the file is open in &quot;java.exe&quot;.
It is obvious that one of the WARs is still opening the file (because the message says java.exe). But there are two or more WARs on Tomcat, so I couldn't find which application caused this problem.
Additionally, these applications are so complecated, it is tough to dig into which class reads/writes (FileInputStream/FileOutputStream, for example) this this file.

Question
Starting with the path of a specific file, is there any way to know which instance of a class has the file descriptor(FileInputStream/FileOutputStream of the file?
A method applicable without shutdown Tomcat (like jcmd) is preferable because other WARs are being tested on the same Tomcat.
","I presume you reproduced locally. Doing anything I suggest below in prod is not a great idea (could be too slow).
I also presume that you can use the location where it was created, even if you cannot find which instance is holding it (because you could figure how it got shared).
If you are capable of a debugging session, you might be able to add a conditional breakpoint on a few jdk classes that can create tmp files. That would be my first shot. If it is rare enough.
Otherwise, in the following shots, you need to edit the webserver launcher script.
My 2nd shot is it's very intrusive and risky; it would be to redefine boot classes, but you need the source and to prepend a bootclasspath to ensure your redefinition class is loaded first. It used to work 15 years ago, dunno today with modules and sealed packages and digital signature and all...I used to log on Thread.start() to see who called it. Would be similar for a File.createTempFile() or similar call sites.
3rd shot works: instrument those call sites; write a pure java agent to load on the java.exe command line. Fairly good tutorials online. Not that hard, less than a few hours to goal like I just did and it's so cool. You intercept the method call's exit and print the filename created +stacktrace.
Making an instrumentation agent goes like this:
a) download javassist.jar (inside the javassist latest zip) and put it in a lib/ folder for example.
b) construct a manifest, ex: myagent.mf
Manifest-Version: 1.0
Main-Class: tests.MyInstrumentationAgent
Agent-Class: tests.MyInstrumentationAgent
Premain-Class: tests.MyInstrumentationAgent
Class-Path: lib/javassist.jar
Can-Redefine-Classes: true
Can-Retransform-Classes: true

c) write the agent class and jar it with the manifest. Ex:
package tests;
import java.io.ByteArrayInputStream;
import java.io.File;
import java.io.IOException;
import java.lang.instrument.ClassFileTransformer;
import java.lang.instrument.IllegalClassFormatException;
import java.lang.instrument.Instrumentation;
import java.security.ProtectionDomain;

import javassist.ClassPool;
import javassist.CtClass;
import javassist.CtMethod;

public class MyInstrumentationAgent {
    public static void premain(String agentArgs, Instrumentation inst) {
        System.out.println(&quot;Executing premain with args = '&quot;+agentArgs+&quot;'&quot;);
        try {
            inst.addTransformer(new MyClassTransformer(), true);
            inst.retransformClasses(File.class);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
    
    public static class MyClassTransformer implements ClassFileTransformer {
        @Override
        public byte[] transform(ClassLoader loader, String className, Class&lt;?&gt; classBeingRedefined, ProtectionDomain protectionDomain, byte[] classfileBuffer) throws IllegalClassFormatException {
            if (!&quot;java/io/File&quot;.equals(className))
                return classfileBuffer;
            
            System.out.println(&quot;Instrumenting &quot; + className + &quot; ...&quot;);
            byte[] byteCode = classfileBuffer;
            try {
                ClassPool classPool = ClassPool.getDefault();
                CtClass ctClass = classPool.makeClass(new ByteArrayInputStream(classfileBuffer));
                CtMethod m = ctClass.getMethod(&quot;createTempFile&quot;, &quot;(Ljava/lang/String;Ljava/lang/String;Ljava/io/File;)Ljava/io/File;&quot;);
                if (m != null)
                    m.insertAfter(&quot;&quot;&quot;
                        Throwable t = new Throwable(
                            &quot;[&quot;+new java.util.Date()
                            +&quot;] [&quot;+Thread.currentThread().getName()
                            +&quot;]: new temp file '&quot;
                            + $_ +&quot;' created&quot;);
                        t.printStackTrace(System.out);
                        &quot;&quot;&quot;
                    );
                else
                    System.out.println(&quot;method not found&quot;);
                
                byteCode = ctClass.toBytecode();
                ctClass.detach();
            } catch (Throwable t) {
                t.printStackTrace(System.out);
            }
            return byteCode;
        }
    }
    
    public static void main(String[] args) throws IOException {
        File tmp = File.createTempFile(&quot;myagent_demo_&quot;, &quot;.tmp&quot;, new File(&quot;.&quot;));
        tmp.deleteOnExit();
        System.out.println(&quot;created &quot;+tmp);
    }
}

d) launch with the agent: (making sure you have the lib/javassist.jar (or whichever class-path you might adjust) relative to the myagent.jar)
java -javaagent:myagent.jar ......

The output of its own main() looks like this:
Executing premain with args = 'null'
Instrumenting java/io/File ...
java.lang.Throwable: [Sat Dec 28 11:56:29 EST 2024] [main]: new temp file '.\myagent_demo_9579502737453321678.tmp' created
    at java.base/java.io.File.createTempFile(File.java:2173)
    at tests.MyInstrumentationAgent.main(MyInstrumentationAgent.java:60)
created .\myagent_demo_9579502737453321678.tmp

Good luck.
Update: just a note on the added code in .insertAfter(): it's tempting to call one of your own delegate class static method to make it lighter to embed, but in the case here, we are instrumenting a boot class, so its classloader will not see your delegate class (unless you put it in the boot classpath I guess). This is why I only print out instead of trying to append some file.
"
"here is my _ignoreText.xsl file
&lt;xsl:output method=&quot;xml&quot; encoding=&quot;utf-8&quot; omit-xml-declaration=&quot;yes&quot; indent=&quot;no&quot; /&gt;
        &lt;xsl:template match=&quot;*|@*|text()|comment()|processing-instruction()&quot; &gt;
    
            &lt;xsl:if test=&quot;normalize-space(.) != '' or ./@* != ''&quot;&gt;
                &lt;xsl:copy&gt;
                    &lt;xsl:apply-templates select=&quot;*|@*|text()|comment()|processing-instruction()&quot;/&gt;
                &lt;/xsl:copy&gt;
            &lt;/xsl:if&gt;
    
            &lt;xsl:variable name=&quot;type&quot;&gt;
                &lt;xsl:choose&gt;
                    &lt;xsl:when test=&quot;. castable as xs:integer&quot;&gt;
                        &lt;xsl:text&gt;Integer&lt;/xsl:text&gt;
                    &lt;/xsl:when&gt;
                    &lt;xsl:when test=&quot;. castable as xs:boolean&quot;&gt;
                        &lt;xsl:text&gt;Boolean&lt;/xsl:text&gt;
                    &lt;/xsl:when&gt;
                    &lt;xsl:otherwise&gt;
                        &lt;xsl:text&gt;String&lt;/xsl:text&gt;
                    &lt;/xsl:otherwise&gt;
                &lt;/xsl:choose&gt;
            &lt;/xsl:variable&gt;
    
    
        &lt;/xsl:template&gt;
    
    &lt;/xsl:stylesheet&gt;

below is a java code in which i am using above _ignoreText.xsl file to transform xml
import org.custommonkey.xmlunit.Transform;
import java.io.File;


public class TransformDemo1 {
    public static void main(String args[]) throws Exception {

        String xsltfilename=&quot;D:\\Demo\\src\\test\\java\\StringXml\\_ignoreText.xsl&quot;;
        File xsltfile=new File(xsltfilename);

        String strSource = &quot;&lt;?xml version=\&quot;1.0\&quot; encoding=\&quot;UTF-8\&quot; standalone=\&quot;no\&quot;?&gt;\n&quot; +
                &quot;&lt;SOAP-ENV:Envelope xmlns:SOAP-ENV=\&quot;http://schemas.xmlsoap.org/soap/envelope/\&quot; xmlns:xsd=\&quot;http://www.w3.org/1999/XMLSchema\&quot; xmlns:xsi=\&quot;http://www.w3.org/1999/XMLSchema-instance\&quot;&gt;\n&quot; +
                &quot;    &lt;SOAP-ENV:Body&gt;\n&quot; +
                &quot;        &lt;return&gt;\n&quot; +
                &quot;            &lt;ICD10Flag&gt;hello&lt;/ICD10Flag&gt;\n&quot; +
                &quot;            &lt;status&gt;success&lt;/status&gt;\n&quot; +
                &quot;        &lt;/return&gt;\n&quot; +
                &quot;    &lt;/SOAP-ENV:Body&gt;\n&quot; +
                &quot;&lt;/SOAP-ENV:Envelope&gt;\n&quot;;
        Transform docSource = new Transform(strSource, xsltfile);

    }
}

following is the Error i am getting.
ERROR:  'Syntax error in '. castable as xs:integer'.'
FATAL ERROR:  'file:/D:/Demo/src/test/java/StringXml/_ignoreText.xsl: line 18: Required attribute 'test' is missing.'
Exception in thread &quot;main&quot; org.custommonkey.xmlunit.exceptions.ConfigurationException: file:/D:/RijvanPactPOC/2/DemoProjectPactConsumer/src/test/java/StringXml/_ignoreText.xsl: line 18: Required attribute 'test' is missing.
    at org.custommonkey.xmlunit.Transform.getTransformer(Transform.java:201)
    at org.custommonkey.xmlunit.Transform.&lt;init&gt;(Transform.java:161)
    at org.custommonkey.xmlunit.Transform.&lt;init&gt;(Transform.java:92)
    at StringXml.TransformDemo1.main(TransformDemo1.java:31)
Caused by: javax.xml.transform.TransformerConfigurationException: file:/D:/Demo/src/test/java/StringXml/_ignoreText.xsl: line 18: Required attribute 'test' is missing.
    at java.xml/com.sun.org.apache.xalan.internal.xsltc.trax.TransformerFactoryImpl.newTemplates(TransformerFactoryImpl.java:1061)
    at java.xml/com.sun.org.apache.xalan.internal.xsltc.trax.TransformerFactoryImpl.newTransformer(TransformerFactoryImpl.java:817)
    at org.custommonkey.xmlunit.Transform.getTransformer(Transform.java:196)
    ... 3 more

Process finished with exit code 1

please suggest any solution or any other Library using that i can Transform the XML based the data type of the value the xml tag is containing
Ex. 1
&lt;status&gt;success&lt;/status&gt;

should be transformed to
&lt;status&gt;String&lt;/status&gt;

Ex. 2
&lt;status&gt;123&lt;/status&gt;

should be transformed to
&lt;status&gt;Integer&lt;/status&gt;

","Looks like you are trying to match the XML with node content's data type.
If that is the case you can use the custom DifferenceEvaluator as below.
public class XMLUnitDiffDemo{

public static void main(String args[]) throws Exception {
  String strSource = &quot;&lt;root&gt;&lt;test&gt;true&lt;/test&gt;&quot; +
                &quot;&lt;test2&gt;2&lt;/test2&gt;&quot; +
                &quot;&lt;test1&gt;1&lt;/test1&gt;&quot; +
                &quot;&lt;/root&gt;&quot;;
  String strTest = &quot;&lt;root&gt;&lt;test&gt;true&lt;/test&gt;&quot; +
                &quot;&lt;test1&gt;1&lt;/test1&gt;&quot; +
                &quot;&lt;test2&gt;2&lt;/test2&gt;&quot; +
                &quot;&lt;/root&gt;&quot;;


 Diff myDiff = DiffBuilder.compare(xmlSource).withTest(xmlCompareWith)
                .ignoreComments()
                .ignoreWhitespace()
                .withNodeMatcher(new DefaultNodeMatcher(ElementSelectors.byName))
                .withDifferenceEvaluator(new DataTypeElementDifferenceEvaluator())
                .checkForSimilar().build();

System.out.println(!myDiff.hasDifferences())
}
}

Custom implementation.

class DataTypeElementDifferenceEvaluator implements DifferenceEvaluator {

    @Override
    public ComparisonResult evaluate(Comparison comparison, ComparisonResult outcome) {
        if (outcome == ComparisonResult.EQUAL) return outcome; // only evaluate differences.
        Node controlNode = comparison.getControlDetails().getTarget();
        Node testNode = comparison.getTestDetails().getTarget();
        String controlnodename = controlNode.getNodeName();
        String testNodename = testNode.getNodeName();
        String conCN = controlNode.getTextContent();
        String conTN = testNode.getTextContent();
        if(controlnodename.equalsIgnoreCase(testNodename)){
            System.out.println(getDataType(conCN) + &quot; ==  &quot; + getDataType(conTN));
            if(getDataType(conCN).equalsIgnoreCase(getDataType(conTN))) {
                return ComparisonResult.SIMILAR;
            }
        }
        return outcome;
    }

public static String getDataType(String input) {

        String dataType = null;
        // checking for Integer
        if (input.matches(&quot;\\d+&quot;)) {
            dataType = &quot;java.lang.Integer&quot;;
        }
        // checking for floating point numbers
        else if (input.matches(&quot;\\d*[.]\\d+&quot;)) {
            dataType = &quot;java.lang.Double&quot;;
        }
        // checking for date format dd/mm/yyyy
        else if (input.matches(
                &quot;\\d{2}[/]\\d{2}[/]\\d{4}&quot;)) {
            dataType = &quot;java.util.Date&quot;;
        }
        // checking for date format mm/dd/yyyy
        else if (input.matches(
                &quot;\\d{2}[/]\\d{2}[/]\\d{4}&quot;)) {
            dataType = &quot;java.util.Date&quot;;
        }
        // checking for date format dd-mon-yy
        else if (input.matches(
                &quot;\\d{2}[-]\\w{3}[-]\\d{2}&quot;)) {
            dataType = &quot;java.util.Date&quot;;
        }
        // checking for date format dd-mon-yyyy
        else if (input.matches(
                &quot;\\d{2}[-]\\w{3}[-]\\d{4}&quot;)) {
            dataType = &quot;java.util.Date&quot;;
        }
        // checking for date format dd-month-yy
        else if (input.matches(&quot;\\d{2}[-]\\w+[-]\\d{2}&quot;)) {
            dataType = &quot;java.util.Date&quot;;
        }
        // checking for date format dd-month-yyyy
        else if (input.matches(&quot;\\d{2}[-]\\w+[-]\\d{4}&quot;)) {
            dataType = &quot;java.util.Date&quot;;
        }
        // checking for date format yyyy-mm-dd
        else if (input.matches(
                &quot;\\d{4}[-]\\d{2}[-]\\d{2}&quot;)) {
            dataType = &quot;java.util.Date&quot;;
        }
        // checking for String
        else {
            dataType = &quot;java.lang.String&quot;;
        }

        return dataType;

    }

}




"
"I have two security configurations  in two libs
First one is for authentication:
    @Bean
    @Order(10)
    public SecurityFilterChain filterChain(HttpSecurity http) throws Exception {
        http
                .authorizeHttpRequests(authorizeRequests -&gt;
                        authorizeRequests
                                .requestMatchers(createAntRequestMatchers(whitelist))
                                .permitAll().anyRequest()
                                .authenticated()
                )
                .oauth2ResourceServer( ...)
        return http.build();
    }

Second one adds some resource filter:
    @Bean
    @Order(100)
    public SecurityFilterChain filterChain(HttpSecurity http, ResourceFilter resourceFilter) throws Exception {
        return      http
                .authorizeHttpRequests(authorizeRequests -&gt;
                        authorizeRequests
                                .requestMatchers(createAntRequestMatchers(whitelist))
                                .permitAll().anyRequest()
                                .authenticated()
                ).addFilterAfter(resourceFilter, SessionManagementFilter.class).build();
    }   

It worked perfect until spring-boot 3.3.?
After update to spring-boot 3.4.1 spring context don't startet anymore with error message
A filter chain that matches any request [DefaultSecurityFilterChain defined as 'filterChain' in ... has already been configured, which means that this filter chain ... will never get invoked. Please use HttpSecurity#securityMatcher to ensure that there is only one filter chain configured for 'any request' and that the 'any request' filter chain is published last.
After I add in each configuration requestMatcher (all requests)
http.securityMatcher(&quot;/**&quot;).authorizeHttpRequests(...

it works as expected. But if I read spring-security issue comments https://github.com/spring-projects/spring-security/issues/15220
I have a doubts about my solution.
What do you mean?
I adapt my code acording @Roar S. suggestion
    @Bean
    @Order(10)
    public SecurityFilterChain filterChain(HttpSecurity http) throws Exception {
        http.securityMatcher(&quot;/**&quot;)
                .authorizeHttpRequests(authorizeRequests -&gt;
                        authorizeRequests
                                .requestMatchers(createAntRequestMatchers(whitelist))
                                .permitAll().anyRequest()
                                .authenticated()
                )
                .oauth2ResourceServer( ...)
        return http.build();
    }

---------

    @Bean
    @Order(100)
    public SecurityFilterChain filterChain(HttpSecurity http, ResourceFilter resourceFilter) throws Exception {
        return http.securityMatcher(&quot;/**&quot;)
        .addFilterAfter(resourceFilter, SessionManagementFilter.class).build();
    }   



It works, but .securityMatcher(&quot;/**&quot;) looks suspicious. And without .securityMatcher(&quot;/**&quot;) it doesn't start
","Update: OP mentioned in a comment that the first SecurityFilterChain is shared across multiple applications and cannot be modified. Since the issue involves simply adding a filter that needs to execute after the shared SecurityFilterChain, we can address it using FilterRegistrationBean instead of using two security chains. The following code is based on this answer.
LoggingFilter is the same as in my original answer.
import org.springframework.boot.autoconfigure.security.SecurityProperties;
import org.springframework.boot.web.servlet.FilterRegistrationBean;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class FilterConfig {

    @Bean
    public FilterRegistrationBean&lt;LoggingFilter&gt; afterAuthFilterRegistrationBean(
            SecurityProperties securityProperties) {
        
        var filterRegistrationBean = new FilterRegistrationBean&lt;LoggingFilter&gt;();

        // a filter that extends OncePerRequestFilter
        filterRegistrationBean.setFilter(new LoggingFilter());

        // this needs to be a number greater than than spring.security.filter.order
        filterRegistrationBean.setOrder(securityProperties.getFilter().getOrder() + 1);
        return filterRegistrationBean;
    }
}


Original answer
OP has separated security configuration into two chains under the assumption, I believe, that the principal becomes available only after a security chain is fully executed. However, the principal is populated and available after the BearerTokenAuthenticationFilter has completed. Therefore, the two chains in the question can be merged into one.
This behavior can be verified by adding the following logging filter to the chain with:
.addFilterAfter(new LoggingFilter(), BearerTokenAuthenticationFilter.class)

Here is the logging filter implementation:
    private static class LoggingFilter extends OncePerRequestFilter {

        @Override
        protected void doFilterInternal(@NonNull HttpServletRequest request,
                                        @NonNull HttpServletResponse response,
                                        @NonNull FilterChain filterChain) throws ServletException, IOException {

            var authentication = SecurityContextHolder.getContext().getAuthentication();
            if (authentication != null) {
                LOG.info(&quot;Logged in as: {}&quot;, authentication.getName());
                LOG.info(&quot;Authorities: {}&quot;,
                        authentication.getAuthorities().stream()
                                .map(GrantedAuthority::getAuthority)
                                .collect(Collectors.joining(&quot;, &quot;))
                );
            } else {
                LOG.info(&quot;No user&quot;);
            }

            filterChain.doFilter(request, response);
        }
    }

"
"In my project, I am using a barchart and a linechart in the same frame to display the same data. However, due to some reason, I am getting an output where there is no color in either the barchart or the linechart.
For example:

In this image, the linechart has color but the barchart doesn't.
The code that I used:
FXML file:
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;?import javafx.scene.chart.BarChart?&gt;
&lt;?import javafx.scene.chart.CategoryAxis?&gt;
&lt;?import javafx.scene.chart.LineChart?&gt;
&lt;?import javafx.scene.chart.NumberAxis?&gt;
&lt;?import javafx.scene.layout.AnchorPane?&gt;

&lt;AnchorPane id=&quot;AnchorPane&quot; prefHeight=&quot;401.0&quot; prefWidth=&quot;802.0&quot; style=&quot;-fx-background-color: white;&quot; stylesheets=&quot;@stylesheet.css&quot; xmlns=&quot;http://javafx.com/javafx/16&quot; xmlns:fx=&quot;http://javafx.com/fxml/1&quot; fx:controller=&quot;javafxapplication26.FXMLDocumentController&quot;&gt;
   &lt;children&gt;
      &lt;AnchorPane layoutX=&quot;1.0&quot; layoutY=&quot;14.0&quot; prefHeight=&quot;303.0&quot; prefWidth=&quot;801.0&quot; AnchorPane.bottomAnchor=&quot;46.0&quot; AnchorPane.leftAnchor=&quot;1.0&quot; AnchorPane.rightAnchor=&quot;0.0&quot; AnchorPane.topAnchor=&quot;14.0&quot;&gt;
         &lt;children&gt;
            &lt;AnchorPane layoutX=&quot;342.0&quot; layoutY=&quot;-2.0&quot; prefHeight=&quot;244.0&quot; prefWidth=&quot;419.0&quot; style=&quot;-fx-border-color: #4E6172; -fx-background-color: white;&quot; AnchorPane.bottomAnchor=&quot;10.0&quot; AnchorPane.rightAnchor=&quot;10.0&quot; AnchorPane.topAnchor=&quot;-2.0&quot;&gt;
               &lt;children&gt;
                  &lt;LineChart fx:id=&quot;linechart&quot; layoutX=&quot;69.0&quot; layoutY=&quot;11.0&quot; prefHeight=&quot;353.0&quot; prefWidth=&quot;380.0&quot;&gt;
                    &lt;xAxis&gt;
                      &lt;CategoryAxis side=&quot;BOTTOM&quot; /&gt;
                    &lt;/xAxis&gt;
                    &lt;yAxis&gt;
                      &lt;NumberAxis side=&quot;LEFT&quot; /&gt;
                    &lt;/yAxis&gt;
                  &lt;/LineChart&gt;
               &lt;/children&gt;
            &lt;/AnchorPane&gt;
            &lt;AnchorPane layoutX=&quot;8.0&quot; layoutY=&quot;-2.0&quot; prefHeight=&quot;367.0&quot; prefWidth=&quot;392.0&quot; style=&quot;-fx-border-color: #4E6172; -fx-background-color: white;&quot; AnchorPane.bottomAnchor=&quot;10.0&quot; AnchorPane.leftAnchor=&quot;10.0&quot; AnchorPane.rightAnchor=&quot;399.0&quot; AnchorPane.topAnchor=&quot;-2.0&quot;&gt;
               &lt;children&gt;
                  &lt;BarChart fx:id=&quot;barchart&quot; layoutX=&quot;3.0&quot; layoutY=&quot;3.0&quot; prefHeight=&quot;363.0&quot; prefWidth=&quot;391.0&quot; AnchorPane.bottomAnchor=&quot;1.0&quot; AnchorPane.leftAnchor=&quot;1.0&quot; AnchorPane.rightAnchor=&quot;1.0&quot; AnchorPane.topAnchor=&quot;2.0&quot;&gt;
                    &lt;xAxis&gt;
                      &lt;CategoryAxis side=&quot;BOTTOM&quot; /&gt;
                    &lt;/xAxis&gt;
                    &lt;yAxis&gt;
                      &lt;NumberAxis side=&quot;LEFT&quot; /&gt;
                    &lt;/yAxis&gt;
                  &lt;/BarChart&gt;
               &lt;/children&gt;
            &lt;/AnchorPane&gt;
         &lt;/children&gt;
      &lt;/AnchorPane&gt;
   &lt;/children&gt;
&lt;/AnchorPane&gt;

Java Controller:
/*
 * To change this license header, choose License Headers in Project Properties.
 * To change this template file, choose Tools | Templates
 * and open the template in the editor.
 */
package javafxapplication26;

import java.net.URL;
import java.util.ResourceBundle;
import javafx.event.ActionEvent;
import javafx.fxml.FXML;
import javafx.fxml.Initializable;
import javafx.scene.chart.BarChart;
import javafx.scene.chart.LineChart;
import javafx.scene.chart.XYChart;
import javafx.scene.control.Button;
import javafx.scene.control.Label;

/**
 *
 * @author param
 */
public class FXMLDocumentController implements Initializable {
    
  
    
    @FXML
    private LineChart&lt;String, Number&gt; linechart;
    @FXML
    private BarChart&lt;String, Number&gt; barchart;
    
   
    @Override
    public void initialize(URL url, ResourceBundle rb) {
         XYChart.Series&lt;String, Number&gt; series= new  XYChart.Series&lt;String, Number&gt;();
        series.getData().add(new  XYChart.Data&lt;String, Number&gt;(&quot;Jan&quot;,12));
        series.getData().add(new  XYChart.Data&lt;String, Number&gt;(&quot;Feb&quot;,20));
        series.getData().add(new  XYChart.Data&lt;String, Number&gt;(&quot;March&quot;,10));
        series.getData().add(new  XYChart.Data&lt;String, Number&gt;(&quot;April&quot;,14));
    
      
        linechart.getData().add(series);
        barchart.getData().add(series);
        
      
        // TODO
    }      
} 
}

As shown in the image, only one of either the barchart or the linechart is capable of displaying color. I tried using the -fx-bar-fill method, but even that didn't work.
","As @kleopatra and @Slaw mentioned, the problem was that I was using a single Series for both the barchart and the linechart. This was the cause of the error.
The solution was to use two different Series, titled series1 and series2 for the barchart and the linechart respectively.
The corrected code:
package javafxapplication26;

import java.net.URL;
import java.util.ResourceBundle;
import javafx.event.ActionEvent;
import javafx.fxml.FXML;
import javafx.fxml.Initializable;
import javafx.scene.chart.BarChart;
import javafx.scene.chart.LineChart;
import javafx.scene.chart.XYChart;
import javafx.scene.control.Button;
import javafx.scene.control.Label;

/**
 *
 * @author param
 */
public class FXMLDocumentController implements Initializable {
    
  
    
    @FXML
    private LineChart&lt;String, Number&gt; linechart;
    @FXML
    private BarChart&lt;String, Number&gt; barchart;
    
   
    @Override
    public void initialize(URL url, ResourceBundle rb) {
        XYChart.Series&lt;String, Number&gt; series= new  XYChart.Series&lt;String, Number&gt;(); //For the barchart
        series.getData().add(new  XYChart.Data&lt;String, Number&gt;(&quot;Jan&quot;,12));
        series.getData().add(new  XYChart.Data&lt;String, Number&gt;(&quot;Feb&quot;,20));
        series.getData().add(new  XYChart.Data&lt;String, Number&gt;(&quot;March&quot;,10));
        series.getData().add(new  XYChart.Data&lt;String, Number&gt;(&quot;April&quot;,14));
    
        XYChart.Series&lt;String, Number&gt; series2= new  XYChart.Series&lt;String, Number&gt;(); //For the linechart
        series2.getData().add(new  XYChart.Data&lt;String, Number&gt;(&quot;Jan&quot;,12));
        series2.getData().add(new  XYChart.Data&lt;String, Number&gt;(&quot;Feb&quot;,20));
        series2.getData().add(new  XYChart.Data&lt;String, Number&gt;(&quot;March&quot;,10));
        series2.getData().add(new  XYChart.Data&lt;String, Number&gt;(&quot;April&quot;,14));
        
        
        barchart.getData().add(series);   
        linechart.getData().add(series2);

    }     
}

As @Slaw mentioned, the problem was that each barchart or linechart needs to have it's own Series and Data. This is because each XYChart.Series and XYChart.Data supply the Node associated with them, and this is what is displayed in the graph.
Corrected Output:

"
"I created a constraint request validator. How do I set the String message in each if else condition, so the user can see specific details? I am trying to access this.message() and change the default .
@Constraint(validatedBy = ProductExportFiltersValidator.class)
@Target({ TYPE, ANNOTATION_TYPE })
@Retention(RUNTIME)
@Documented
public @interface ProductExportFiltersConstraint {
    String message() default &quot;Invalid product export filters.&quot;;
    Class &lt;?&gt; [] groups() default {};
    Class &lt;? extends Payload&gt; [] payload() default {};
}

public class ProductExportFiltersValidator implements ConstraintValidator&lt;ProductExportFiltersConstraint, ProductExportFilters&gt; {
    @Override
    public void initialize(ProductExportFiltersConstraint constraintAnnotation) {
        ConstraintValidator.super.initialize(constraintAnnotation);
    }
   
    @Override
    public boolean isValid(ProductExportFilters productExportFilters, ConstraintValidatorContext constraintValidatorContext) {
        if (productExportFilters == null) {
            return false;
        }
        try {
            DateTimeFormatter dateTimeFormatter = DateTimeFormatter.ISO_DATE_TIME;
            LocalDateTime startDate = null;
            LocalDateTime endDate = null;
            if (productExportFilters.getStartDate() != null) {
                startDate = LocalDateTime.from(dateTimeFormatter.parse(productExportFilters.getStartDate()));
            }
            if (productExportFilters.getEndDate() != null) {
                endDate = LocalDateTime.from(dateTimeFormatter.parse(productExportFilters.getEndDate()));
            }
            if (startDate == null &amp;&amp; endDate == null) {
                return true;
            }
            if ((startDate != null &amp;&amp; endDate == null) ||
                    (startDate == null &amp;&amp; endDate != null)) {
                return false;
            }
            return startDate.equals(endDate) || startDate.isBefore(endDate);
        } catch (DateTimeException e) {
            return false;
        }
    }

","You can use ConstraintValidatorContext.buildConstraintViolationWithTemplate() to set message different from the one defined in message(). You use it like this:
context.buildConstraintViolationWithTemplate(&quot;some message&quot;).addConstraintViolation();

In your validator:
public class ProductExportFiltersValidator implements ConstraintValidator&lt;ProductExportFiltersConstraint, ProductExportFilters&gt; {

    @Override
    public boolean isValid(ProductExportFilters productExportFilters, ConstraintValidatorContext constraintValidatorContext) {
        if (productExportFilters == null) {
            constraintValidatorContext.buildConstraintViolationWithTemplate(&quot;missing filters&quot;).addConstraintViolation();
            return false;
        }
        try {
            DateTimeFormatter dateTimeFormatter = DateTimeFormatter.ISO_DATE_TIME;
            LocalDateTime startDate = null;
            LocalDateTime endDate = null;
            if (productExportFilters.getStartDate() != null) {
                startDate = LocalDateTime.from(dateTimeFormatter.parse(productExportFilters.getStartDate()));
            }
            if (productExportFilters.getEndDate() != null) {
                endDate = LocalDateTime.from(dateTimeFormatter.parse(productExportFilters.getEndDate()));
            }
            if (startDate == null &amp;&amp; endDate == null) {
                return true;
            }
            if ((startDate != null &amp;&amp; endDate == null) ||
                    (startDate == null &amp;&amp; endDate != null)) {
                constraintValidatorContext.buildConstraintViolationWithTemplate(&quot;your detailed message&quot;).addConstraintViolation();
                return false;
            }
            return startDate.equals(endDate) || startDate.isBefore(endDate);
        } catch (DateTimeException e) {
            constraintValidatorContext.buildConstraintViolationWithTemplate(&quot;invalid dates format&quot;).addConstraintViolation();
            return false;
        }
    }
}

"
"In a Java annotation processor, I use DocTrees#getDocCommentTree(Element) to obtain a DocCommentTree, which I walk over with a visitor. Visitor's visitLink(LinkTree,C) method is invoked for found {@link tokens. For a {@link Foo}, LinkTree#getReference().getSignature() returns Foo, though it doesn't give you the fully-qualified class name. That is, is it java.lang.Foo? Is it Foo in the same package? Is it some Foo class imported? How can I get the fully qualified name of the reference while parsing links in Javadoc?
","While parsing Javadoc in an annotation processor, we can first create an ImportTree and use it to determine the FQCN of a ReferenceTree returned by LinkTree#getReference(), that is, the FQCN in {@link signature label}. I have already implemented this for log4j-docgen in this commit. The pseudo-code is as follows:
public class ExampleProcessor extends AbstractProcessor {

    private DocTrees docTrees;

    private Trees trees;

    @Override
    public synchronized void init(final ProcessingEnvironment processingEnv) {
        super.init(processingEnv);
        docTrees = DocTrees.instance(processingEnv);
        trees = Trees.instance(processingEnv);
    }

    private void scanDocTree(Element element) {
        Map&lt;String, String&gt; imports = collectElementImports(element);
        ExampleContext context = new ExampleContext(imports);
        DocCommentTree tree = docTrees.getDocCommentTree(element);
        ExampleDocTreeVisitor visitor = new ExampleDocTreeVisitor();
        tree.accept(visitor, context);
    }

    private static final class ExampleDocTreeVisitor&lt;Void, ExampleContext&gt;
            extends SimpleDocTreeVisitor&lt;Void, ExampleContext&gt; {

        @Override
        public Void visitLink(LinkTree linkTree, final ExampleContext context) {
            String signature = linkTree.getReference().getSignature();
            String fqcn = context.imports.get(signature);
            // ...
            return super.visitLink(node, data);
        }

    }

    private Map&lt;String, String&gt; collectElementImports(Element element) {
        ImportCollectingTreeScanner scanner = new ImportCollectingTreeScanner();
        TreePath treePath = trees.getPath(element);
        scanner.scan(treePath.getCompilationUnit(), null);
        return scanner.imports;
    }

    private static final class ImportCollectingScanner
            extends TreeScanner&lt;Object, Trees&gt; {

        private final Map&lt;String, String&gt; imports = new HashMap&lt;&gt;();

        @Override
        public Object visitImport(ImportTree importTree, Trees trees) {
            Tree qualifiedIdentifier = importTree.getQualifiedIdentifier();
            String qualifiedClassName = qualifiedIdentifier.toString();
            String simpleClassName = qualifiedClassName.substring(qualifiedClassName.lastIndexOf('.') + 1);
            imports.put(simpleClassName, qualifiedClassName);
            return super.visitImport(importTree, trees);
        }

    }

}

Credits

@piotr-p-karwasz for the ImportTree hint
Andi's blog on how to extract ImportTree in an annotation processor

"
"Imagine that we have two interfaces which both have method display() with identical signatures. So far so good. Now I define a default implementation for one of them. Now when I want to implement both of them in my class, it gives me a syntax error. Can anyone help me understand the reason?
For example in the code below,
interface Show1 {
    default void display() {
        System.out.println(&quot;hello&quot;);
    }
}

interface Show2 {
    void display();
}

public class Person implements Show1, Show2 {

    public static void main(String args[]) {
        Person p = new Person();
        p.display();
    }
}

I thought that it would print hello because we have the default implementation. And since the two methods in the interfaces doesn't have any interference with each other.
But it gives me syntax error. I would be thankful if you help me find the reason behind this behavior.
","That is because the designers of Java decided that if you have two or more interfaces with the same (or override-equivalent) method signature, and one or more have a default implementation, then you must implement it yourself (that is, it behaves as if there is no default method, just an abstract method).
That implementation could simply be the invocation of the default implementation of one of the interfaces.
For example:
interface Show1 {
    default void display() {
        System.out.println(&quot;hello&quot;);
    }
}

interface Show2 {
    void display();
}

public class Person implements Show1, Show2 {

    public static void main(String args[]) {
        Person p = new Person();
        p.display();
    }

    @Override
    public void display() {
        Show1.super.display();
    }
}

This is not something that happened by accident, this is something that was explicitly and intentionally designed and specified, specifically by 8.4.8 Inheritance, Overriding, and Hiding of The Java 17 Language Specification.
"
"I am using Java 21.
I have two classes:
abstract class MySuperClass {
    private final Object mySuperField;
    MySuperClass(Object myField) {
        this.mySuperField = myField;
    }
    public Object getMySuperField() {
        return mySuperField;
    }
}

public class MySubClass extends MySuperClass {
    private final Object mySubField;
    public MySubClass(MySubClass toCopy) {
        super(toCopy.getMySuperField());
        this.mySubField = toCopy.mySubField;
    }
}

The class MySubClass has a copy constructor, as shown above.
I want to avoid throwing a NullPointerException in the MySubClass constructor if the toCopy argument is null, and would rather throw my own exception. But of course, the call to super(...) must be the first line in the constructor.
Is there some Java pattern which I can use to do my own argument validation before calling super(...)?
I have tried a few different things, but they all seem ugly/hacky, like passing nulls into the super class constructor and not doing validation in the super class, and then doing the argument validation after the call to super(...) in the subclass, or making mySuperField in MySuperClass not final, and providing a setter in the super class. There must be something better.
Of everything I have tried, this might be the best option, but it still feels hacky.
public class MySubClass extends MySuperClass {
    private final Object mySubField;
    public MySubClass(MySubClass toCopy) {
        super(toCopy == null ? null : toCopy.getMySuperField());
        if (toCopy == null) {
            // Throw my exception
        }
        this.mySubField = toCopy.mySubField;
    }
}

","Unfortunately, there is no JDK builtin to do this that doesn't use a NullPointerException (though if an NPE would work with a custom message, you can use Objects.requireNonNull).
However, this could be accomplished with a static method in MySubClass, without any modifications to MySuperClass. This works similarly to Objects.requireNonNull:
public class MySubClass extends MySuperClass {

    private static Object notNull(Object obj, Throwable onNull) throws Throwable {
        if (obj == null) throw onNull; // onNull cannot be null
        else return obj;
    }

    private final Object mySubField;
    public MySubClass(MySubClass toCopy) throws Throwable {
        super(
            notNull(
                toCopy,
                new AssertionError(&quot;Replace with something that makes sense&quot;)
           ).getMySuperField()
        );
        this.mySubField = toCopy.mySubField;
    }
}

This could be improved in a couple ways (e.g. generics for object and exception, avoiding throws Throwable) but this implementation should be the lowest common denominator (and should work on all JDKs).
Edit: For convenience, here's a better implementation with generics and better handling:
private static &lt;T, E extends Exception&gt; T notNull(T obj, E onErr) throws E {
    if (obj == null) throw Objects.requireNonNull(onErr, &quot;Error must not be null&quot;);
    else return obj;
}

"
"I've been using the JavaFX ControlsFX TextFields.bindAutoCompletion() with asynchronous javafx tasks in order to populate autocompletion results from my neo4j database after a user enters two characters. The problem is that if the user clears out the text field and types new values to search, there are now two bindings, so two autocompletion popups show.

I need to be able to completely unbind the textfield from the old list and bind it's autocompletion to the new list. It seems the abstract method i'm using, dispose() doesn't do anything in the standard AutoCompletionBinding class?
    AutoCompletionBinding&lt;Client&gt; clientBinding;
    private void getClientAutoComplete(TextField clientNameTextField) {
        String input = clientNameTextField.getText().toUpperCase();
        if (input.length() &lt; 2  &amp;&amp; clientBinding != null) {
            clientBinding.dispose();
        } else if (input.length() == 2) {
            var queryTask = SimpleCypher.getClientAutoComplete(input);

            queryTask.setOnSucceeded(event -&gt; {
                AutoCompletionBinding&lt;Client&gt; clientBinding = TextFields.bindAutoCompletion(clientNameTextField, queryTask.getValue());
                clientBinding.setOnAutoCompleted(e -&gt; getClientData(e.getCompletion().getId()));
            });

            // Start the task asynchronously
            Thread queryThread = new Thread(queryTask);
            queryThread.setDaemon(true); // Set as daemon thread to allow application exit
            queryThread.start();
        }
    }

Here is the Javafx Task:
    public static Task&lt;List&lt;Client&gt;&gt; getClientAutoComplete(String input){
        Task&lt;List&lt;Client&gt;&gt; task = new Task&lt;&gt;() {
                @Override
                protected List&lt;Client&gt; call() throws Exception {
                    List&lt;Client&gt; resultClients = new ArrayList&lt;&gt;();
                    try (Session session = DatabaseConnection.getSession()) {
                        Result result = session.run(
                                &quot;&quot;&quot;
                                MATCH (n:Client)
                                WHERE toUpper(n.name) CONTAINS $textFieldInput
                                RETURN n.id AS id
                                , n.name AS name
                                , n.phone AS num
                                &quot;&quot;&quot;,
                                Values.parameters(&quot;textFieldInput&quot;, input));
                        while (result.hasNext()) {
                            Record record = result.next();
                            resultClients.add(
                                new Client(
                                    record.get(&quot;id&quot;).asInt(),
                                    record.get(&quot;name&quot;).asString(),
                                    record.get(&quot;num&quot;).isNull() ? null : record.get(&quot;num&quot;).asString()
                            ));
                        }
                    }
                    return resultClients;
                }
            };
        task.setOnFailed(event -&gt; SimpleCypher.handleQueryError(event));
        return task;
    }

I feel like the solution is to create my own custom class that overrides some of the abstract methods of AutoCompletionBinding. But what is the best way for me to implement this based on what i need, which is the ability for the user to type a value that is queried against the database and then populates the text field, while also removing any previous bindings from previous input?
Here is what I have so far for my implementation, but I'm not sure what all I have to actually put in the implementation to get it to work?:
import java.util.Collection;

import org.controlsfx.control.textfield.AutoCompletionBinding;

import javafx.scene.Node;
import javafx.util.Callback;
import javafx.util.StringConverter;

public class Neo4jAutoCompletionBinding&lt;T&gt; extends AutoCompletionBinding&lt;T&gt; {

    protected Neo4jAutoCompletionBinding(Node completionTarget,
            Callback&lt;ISuggestionRequest, Collection&lt;T&gt;&gt; suggestionProvider, StringConverter&lt;T&gt; converter) {
        super(completionTarget, suggestionProvider, converter);
        // TODO Auto-generated constructor stub
    }

    @Override
    public void dispose() {
        // TODO Auto-generated method stub
        
    }

    @Override
    protected void completeUserInput(T completion) {
        // TODO Auto-generated method stub
        
    }

}


I tried to dispose previous autocompletion bindings everytime a new query was ran. But it didn't work, all bindings remained.
I tried binding to an ObservableList where the ObservableList was fed by the Javafx Task query results, but the binding never would update to show the newly added values. It would bind to blank list and stay that way despite the fact the ObservableList would add the new values from the database.

I'm expecting to be able to type in a few characters, hit the database asynchronously so it doesn't freeze the UI. And then show valid results, while also eliminating any previous binding so the bindings don't stack on top of each other and cause confusion when the user autocompletes and it autocompletes to the wrong value because the application focus was on another binding popup, as can be seen in this image:

Update: Adding a MCVE for others to troubleshoot and experiment with solutions:
Project Structure:

Code:
package com.autocomplete.example;

import org.controlsfx.control.textfield.AutoCompletionBinding;
import org.controlsfx.control.textfield.TextFields;

import javafx.application.Application;
import javafx.collections.FXCollections;
import javafx.collections.ObservableList;
import javafx.scene.Scene;
import javafx.scene.control.TextField;
import javafx.scene.layout.VBox;
import javafx.stage.Stage;

//Run project using mvn javafx:run
//You can see the bindings coninutally stack on top of eachother by using the ESC key on the keyboard to move the front one out of focus
public class AutocompleteExample extends Application {

private static final ObservableList&lt;String&gt; names1 = FXCollections.observableArrayList(
        &quot;Alice&quot;, &quot;Adam&quot;, &quot;Alfred&quot;, &quot;Amon&quot;, &quot;Alfredo&quot;, &quot;Al&quot;, &quot;Albert&quot;
);

private static final ObservableList&lt;String&gt; names2 = FXCollections.observableArrayList(
        &quot;Bob&quot;, &quot;Conner&quot;, &quot;Robin&quot;, &quot;Fred&quot;, &quot;Freddy&quot;, &quot;Edward&quot;, &quot;Fredward&quot;, &quot;Mariam&quot;
);

@Override
public void start(Stage primaryStage) {
    TextField textField = new TextField();
    
    textField.setOnKeyTyped(event -&gt; {
        AutoCompletionBinding&lt;String&gt; nameBinding = null;
        String input = textField.getText().toUpperCase();
        if (input.length() == 2){
            if (input.startsWith(&quot;A&quot;)) {
                if (nameBinding != null) nameBinding.dispose();
                nameBinding = TextFields.bindAutoCompletion(textField, names1);
                nameBinding.setOnAutoCompleted(val -&gt; System.out.println(&quot;You selected &quot;+ val.getCompletion() +&quot; from list 1.&quot;));
            } else {
                if (nameBinding != null) nameBinding.dispose();
                nameBinding = TextFields.bindAutoCompletion(textField, names2);
                nameBinding.setOnAutoCompleted(val -&gt; System.out.println(&quot;You selected &quot;+ val.getCompletion() +&quot; from list 2.&quot;));
            }
        } else if (nameBinding != null &amp;&amp; input.length() &lt; 2) nameBinding.dispose();
    });

    VBox root = new VBox(10, textField);
    Scene scene = new Scene(root, 300, 200);
    primaryStage.setScene(scene);
    primaryStage.setTitle(&quot;Autocomplete Example&quot;);
    primaryStage.show();
}

public static void main(String[] args) {
    launch(args);
}

}
POM:
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;groupId&gt;com.autocomplete.example&lt;/groupId&gt;
    &lt;artifactId&gt;AutocompleteExample&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;maven.compiler.release&gt;21&lt;/maven.compiler.release&gt;
        &lt;javafx.version&gt;21.0.4&lt;/javafx.version&gt;
        &lt;exec.mainClass&gt;com.autocomplete.example.AutocompleteExample&lt;/exec.mainClass&gt;
    &lt;/properties&gt;
        &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
            &lt;artifactId&gt;javafx-controls&lt;/artifactId&gt;
            &lt;version&gt;${javafx.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
            &lt;artifactId&gt;javafx-fxml&lt;/artifactId&gt;
            &lt;version&gt;${javafx.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
            &lt;artifactId&gt;javafx-base&lt;/artifactId&gt;
            &lt;version&gt;${javafx.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- https://mvnrepository.com/artifact/org.neo4j.driver/neo4j-java-driver --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.neo4j.driver&lt;/groupId&gt;
            &lt;artifactId&gt;neo4j-java-driver&lt;/artifactId&gt;
            &lt;version&gt;5.18.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- https://mvnrepository.com/artifact/org.controlsfx/controlsfx --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.controlsfx&lt;/groupId&gt;
            &lt;artifactId&gt;controlsfx&lt;/artifactId&gt;
            &lt;version&gt;11.2.0&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.8.1&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;release&gt;${maven.compiler.release}&lt;/release&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
                &lt;artifactId&gt;javafx-maven-plugin&lt;/artifactId&gt;
                &lt;version&gt;0.0.8&lt;/version&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;!-- Default configuration for running --&gt;
                        &lt;!-- Usage: mvn clean javafx:run --&gt;
                        &lt;id&gt;default-cli&lt;/id&gt;
                        &lt;configuration&gt;
                            &lt;mainClass&gt;${exec.mainClass}&lt;/mainClass&gt;
                            &lt;options&gt;
                                &lt;option&gt;--add-exports&lt;/option&gt;
                                &lt;option&gt;javafx.base/com.sun.javafx.event=org.controlsfx.controls&lt;/option&gt;
                                &lt;option&gt;--add-modules=javafx.base&lt;/option&gt;
                            &lt;/options&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;

module-info file:
module com.autocomplete.example {
requires javafx.base;
requires javafx.fxml;
requires transitive javafx.controls;
requires transitive javafx.graphics;
requires org.controlsfx.controls;

opens com.autocomplete.example to javafx.fxml;
exports com.autocomplete.example;
}


","Thanks to @SedJ601 for the inspiration that led me to take a new approach which solved all my problems! In order to use results from a database, but also change the values that are bound to the textfield based on what is input. You must bind the textfield using a suggestion provider that is bound to a list. Then based on the On Key Typed, whenever the user input is at the length needed, you query the database and populate the query results in the list. This way, you only bind the textfield once, but constantly manipulate that list itself. I previously attempted to do this without using the suggestion provider using the method:
TextFields.bindAutoCompletion(TextField tf, Collection&lt;E&gt; c);

USING THE ABOVE METHOD will not work with manipulating the list.
You must use the suggestion provider method below in your initialize() method:
List&lt;String&gt; autoCompleteModels = new ArrayList&lt;&gt;();
@FXML private void initialize(){
    TextFields
        .bindAutoCompletion(modelQuickSearchTextField, input -&gt; {
            if (input.getUserText().length() &lt; 2) {
                return Collections.emptyList();
            }
            return autoCompleteModels.stream().filter(s -&gt; s.toLowerCase().contains(input.getUserText().toLowerCase())).collect(Collectors.toList());
        })
        .setOnAutoCompleted( e -&gt; {
            String model = e.getCompletion().split(&quot;\s\\|\s&quot;)[0];
            openExistingInventory(SimpleCypher.getModelData(model));
            modelQuickSearchTextField.clear();
        });
}

And then use this method &quot;On Key Typed&quot; (I have it as a seperate method since I'm using JavaFX FXML Scenebuilder, but you can also do textField.onKeyTyped())
//TextField On Key Typed
@FXML TextField modelQuickSearchTextField;
@FXML private void modelAutoComplete() {
    String input = modelQuickSearchTextField.getText().toUpperCase();
    if (input.length() == 2) {
        Task&lt;List&lt;String&gt;&gt; queryTask = new Task&lt;&gt;() {
            @Override
            protected List&lt;String&gt; call() throws Exception {
                List&lt;String&gt; resultModels = new ArrayList&lt;&gt;();
                try (Session session = DatabaseConnection.getSession()) {
                    Result result = session.run(&quot;&quot;&quot;
                            MATCH (mo:InventoryModel)
                            WHERE mo.id CONTAINS $textFieldInput 
                            CALL{
                                WITH mo
                                OPTIONAL MATCH (mo)-[:EXISTS_AS]-&gt;(:InventoryItem)-[hcs:HAS_CURRENT_STATUS]-&gt;(:Status{id:'AVAILABLE'})
                                RETURN sum(hcs.qty) AS available
                            }
                            RETURN mo.id + ' | ' + toString(available)
                            &quot;&quot;&quot;,
                            Values.parameters(&quot;textFieldInput&quot;, input));
                    while (result.hasNext()) {
                        Record record = result.next();
                        resultModels.add(record.get(0).asString());
                    }
                }
                return resultModels;
            }
        };

        queryTask.setOnSucceeded(event -&gt; autoCompleteModels = queryTask.getValue());

        // Start the task asynchronously
        Thread queryThread = new Thread(queryTask);
        queryThread.setDaemon(true); // Set as daemon thread to allow application exit
        queryThread.start();
    }
}

"
"I am using the Apache POI library to export data to Excel. I have tried all the latest versions (3.17, 4.1.2, and 5.2.1).
I have a problem with Excel 97 (.xls) format in relation to cell styles. The cell style somehow is lost (or not displayed) after a certain number of columns.
Here is my sample code:
private void exportXls() {
  try (
      OutputStream os = new FileOutputStream(&quot;test.xls&quot;);
      Workbook wb = new HSSFWorkbook();) {
    Sheet sh = wb.createSheet(&quot;test&quot;);
    Row r = sh.createRow(0);
    for (int i = 0; i &lt; 50; i++) {
      Cell c = r.createCell(i);
      c.setCellValue(i + 1);
      
      CellStyle cs = wb.createCellStyle();
      cs.setFillBackgroundColor(IndexedColors.WHITE.index);
      cs.setFillPattern(FillPatternType.SOLID_FOREGROUND);
      cs.setFillForegroundColor(IndexedColors.LIGHT_BLUE.getIndex());
      c.setCellStyle(cs);
    }
    wb.write(os);
    os.flush();
  } catch (Exception e) {
    e.printStackTrace();
  }
}

And the result as viewed by MS Excel 2019
Viewed by MS Excel
As you can see, the style/format is lost after cell 43rd.
But, when I open the same file by other applications like XLS Viewer Free (from Microsoft Store) or Google Sheets (online), the style/format still exists and is displayed well.
Viewed by XLS Viewer Free
Viewed by Google Sheets
Could anyone please tell me what is going on here?
Did I miss something in my code?
Is there any hidden setting in MS Excel that causes this problem?
Thank you.
","Creating cell styles for each single cell is not a good idea using apache poi. Cell styles are stored on workbook level in Excel. The sheets and cells share the cell styles if possible.
And there are limits for maximum count of different cell styles in all Excel versions. The limit for the binary *.xls is less than the one for the OOXML *.xlsx.
The limit alone cannot be the only reason for the result you have. But it seems as if Excel is not very happy with the 50 exactly same cell styles in workbook. Those are memory waste as only one shared style would be necessary as all the 50 cells share the same style.
Solutions are:
Do creating the cell styles on workbook level outside cell creating loops and only set the styles to the cells in the loop.
Example:
 private static void exportXlsCorrect() {
  try (
   OutputStream os = new FileOutputStream(&quot;testCorrect.xls&quot;);
   Workbook wb = new HSSFWorkbook();) {
       
   CellStyle cs = wb.createCellStyle();
   cs.setFillBackgroundColor(IndexedColors.WHITE.index);
   cs.setFillPattern(FillPatternType.SOLID_FOREGROUND);
   cs.setFillForegroundColor(IndexedColors.LIGHT_BLUE.getIndex());
    
   Sheet sh = wb.createSheet(&quot;test&quot;);
   Row r = sh.createRow(0);
   for (int i = 0; i &lt; 50; i++) {
    Cell c = r.createCell(i);
    c.setCellValue(i + 1);
      
    c.setCellStyle(cs);
   }
   wb.write(os);
   os.flush();
  } catch (Exception e) {
   e.printStackTrace();
  }
 }

Sometimes it is not really possible to know all possible needed cell styles before creating the cells. Then CellUtil can be used. This has a method CellUtil.setCellStyleProperties which is able to set specific style properties to cells. Doing that new cell styles are created on workbook level only if needed. If already present, the present cell styles are used.
Example:
 private static void exportXlsUsingCellUtil() {
  try (
   OutputStream os = new FileOutputStream(&quot;testUsingCellUtil.xls&quot;);
   Workbook wb = new HSSFWorkbook();) {
            
   Sheet sh = wb.createSheet(&quot;test&quot;);
   Row r = sh.createRow(0);
   for (int i = 0; i &lt; 50; i++) {
    Cell c = r.createCell(i);
    c.setCellValue(i + 1);
    
    java.util.Map&lt;java.lang.String,java.lang.Object&gt; properties = new java.util.HashMap&lt;java.lang.String,java.lang.Object&gt;();
    properties.put(org.apache.poi.ss.util.CellUtil.FILL_BACKGROUND_COLOR, IndexedColors.WHITE.index);
    properties.put(org.apache.poi.ss.util.CellUtil.FILL_FOREGROUND_COLOR, IndexedColors.LIGHT_BLUE.getIndex());
    properties.put(org.apache.poi.ss.util.CellUtil.FILL_PATTERN, FillPatternType.SOLID_FOREGROUND);
    org.apache.poi.ss.util.CellUtil.setCellStyleProperties(c, properties);

   }
   wb.write(os);
   os.flush();
  } catch (Exception e) {
   e.printStackTrace();
  }
 }

"
"I have a collection as follows
Map&lt;String, Set&lt;Long&gt;&gt; myMap = new HashMap&lt;&gt;();

I want to find out if any entry in this map has set which is contained in another entry of same map.
For example, lets say map has the following 5 entries
a - {1, 2, 3}
b - {4, 5}
c - {1}
d - {2, 3}
e - {5}
f - {6}

So, it has the following overlapping entries as set maybe
a - {1, 2, 3}  and c - {1} 
b - {4, 5}     and e - {5}
a - {1, 2, 3}  and d - {2, 3}

Or just list of Set for keys like
a and c
b and e
a and d

I could iterate each of the keyset and then use disjoint or anyMatch for each set, but I was wondering if there is an optimized way (Java 8, 9, 10, 11).
","Compare the solution as nested loops or stream.
Edit: Reduced the code to what is relevant
import java.util.*;
import java.util.function.BiPredicate;
import java.util.stream.Collectors;

class Main {
    public static void main(String[] args) {
        Map&lt;String, Set&lt;Long&gt;&gt; myMap = new HashMap&lt;&gt;();

        myMap.put(&quot;a&quot;, Set.of( 1l, 2l, 3l ));
        myMap.put(&quot;b&quot;, Set.of( 4l, 5l ));
        myMap.put(&quot;c&quot;, Set.of( 1l ));
        myMap.put(&quot;d&quot;, Set.of( 2l, 3l ));
        myMap.put(&quot;e&quot;, Set.of( 5l ));
        myMap.put(&quot;f&quot;, Set.of( 6l ));

        Set&lt;String&gt; keys = myMap.keySet();
        BiPredicate&lt;String, String&gt; condition = (a, b) -&gt; !a.equals(b) &amp;&amp; 
                                                          myMap.get(a).size() &gt;= myMap.get(b).size() &amp;&amp; 
                                                          myMap.get(a).containsAll(myMap.get(b));

        // nested Loop
        Set&lt;Map.Entry&lt;String, String&gt;&gt; nested = new HashSet&lt;&gt;();
        for (String a : keys)
            for (String b : keys)
                if (condition.test(a, b)) nested.add(Map.entry(a, b));

        System.out.println(nested);

        // stream
        Set&lt;Map.Entry&lt;String, String&gt;&gt; collect = keys.stream()
            .flatMap(a -&gt; keys.stream()
            .filter(b -&gt; condition.test(a, b))
            .map(b -&gt; Map.entry(a, b)))
            .collect(Collectors.toSet());

        System.out.println(collect);

    }
}

"
"Let's assume that we have a fullstack application that has a page in the frontend side where we can select the constraints that we should apply to a specific problem. The list of those constraints will be sent to the backend side when we run the Timefold Solver for that specific  problem.
How can I make sure that the Timefold Solver will apply just the constraints I chose from the frontend side? How can I modify the TimetableConstraintProvider (for example) to achieve the mentioned functionality.
public class TimetableConstraintProvider implements ConstraintProvider {

    @Override
    public Constraint[] defineConstraints(ConstraintFactory constraintFactory) {
        return new Constraint[] {
                // Hard constraints
                roomConflict(constraintFactory),
                teacherConflict(constraintFactory),
                studentGroupConflict(constraintFactory),
                // Soft constraints
                teacherRoomStability(constraintFactory),
                teacherTimeEfficiency(constraintFactory),
                studentGroupSubjectVariety(constraintFactory)
        };
    }
                //implementation of the constraints
}

I assume that, first, we should have an POST/GET endpoints for the selected constraints. After that what are the next steps? Any help is welcomed. Thank you!
","Based on the documentation about Constraint Configuration (https://timefold.ai/docs/timefold-solver/latest/constraints-and-score/constraint-configuration#constraintWeight) I managed to build dynamic constraints by extending the timetable project (https://github.com/TimefoldAI/timefold-quickstarts/tree/stable/technology/java-spring-boot). Please note that the solution is not using Spring Data JPA for the simplicity:
So, first, you need a Constraint Configuration Bean where you input all the constraints' weights (ZERO means that the certain constraint will not be taken into account by the Solver):
import ai.timefold.solver.core.api.domain.constraintweight.ConstraintConfiguration;
import ai.timefold.solver.core.api.domain.constraintweight.ConstraintWeight;
import ai.timefold.solver.core.api.score.buildin.hardsoft.HardSoftScore;
import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.util.List;

//constraintPackage = &quot;the package where you can find your ConstraintProvider&quot;
@ConstraintConfiguration(constraintPackage = &quot;com.timetablealgo.testingtimetablealgo.solver&quot;)
@Data
@NoArgsConstructor
@AllArgsConstructor
public class TimetableConstraintConfiguration {

    @ConstraintWeight(&quot;roomConflict&quot;)
    private HardSoftScore roomConflict = HardSoftScore.ZERO; //hard
    @ConstraintWeight(&quot;teacherConflict&quot;)
    private HardSoftScore teacherConflict = HardSoftScore.ZERO; //hard
    @ConstraintWeight(&quot;studentGroupConflict&quot;)
    private HardSoftScore studentGroupConflict= HardSoftScore.ZERO; //hard
    @ConstraintWeight(&quot;teacherRoomStability&quot;)
    private HardSoftScore teacherRoomStability= HardSoftScore.ZERO; //soft
    @ConstraintWeight(&quot;teacherTimeEfficiency&quot;)
    private HardSoftScore teacherTimeEfficiency = HardSoftScore.ZERO; //soft
    @ConstraintWeight(&quot;studentGroupVariety&quot;)
    private HardSoftScore studentGroupVariety = HardSoftScore.ZERO; //soft

    public TimetableConstraintConfiguration(List&lt;String&gt; constraints){
       constraints.forEach(element -&gt; {
            switch (element) {
                case &quot;roomConflict&quot; -&gt; roomConflict = HardSoftScore.ONE_HARD;
                case &quot;teacherConflict&quot; -&gt; teacherConflict = HardSoftScore.ONE_HARD;
                case &quot;studentGroupConflict&quot; -&gt; studentGroupConflict = HardSoftScore.ONE_HARD;
                case &quot;teacherRoomStability&quot; -&gt; teacherRoomStability = HardSoftScore.ONE_SOFT;
                case &quot;teacherTimeEfficiency&quot; -&gt; teacherTimeEfficiency = HardSoftScore.ONE_SOFT;
                case &quot;studentGroupVariety&quot; -&gt; studentGroupVariety = HardSoftScore.ONE_SOFT;
                // Add more cases for other constraints if needed
            }
        });
    }
}

Add the Constraint Configuration into your @PlanningSolution Bean:
@PlanningSolution
public class Timetable {

    @ConstraintConfigurationProvider
    private TimetableConstraintConfiguration timetableConstraintConfiguration;
...

public Timetable(String name, List&lt;Timeslot&gt; timeslots, List&lt;Room&gt; rooms, List&lt;Lesson&gt; lessons, TimetableConstraintConfiguration timetableConstraintConfiguration) {
        this.name = name;
        this.timeslots = timeslots;
        this.rooms = rooms;
        this.lessons = lessons;
        this.timetableConstraintConfiguration = timetableConstraintConfiguration;
    }
...
    }

Rewrite every constraint in the ConstraintProvider Bean using penalizeConfigurable() or rewardConfigurable() methods:
Constraint roomConflict(ConstraintFactory constraintFactory) {
        // A room can accommodate at most one lesson at the same time.
        return constraintFactory
                // Select each pair of 2 different lessons ...
                .forEachUniquePair(Lesson.class,
                        // ... in the same timeslot ...
                        Joiners.equal(Lesson::getTimeslot),
                        // ... in the same room ...
                        Joiners.equal(Lesson::getRoom))
                //.penalize(HardSoftScore.ONE_HARD)
               // you can also add a lambda function to multiply
               // the score penalty: (l1,l2) -&gt; return a number 
                .penalizeConfigurable()
                .justifyWith((lesson1, lesson2, score) -&gt; new RoomConflictJustification(lesson1.getRoom(), lesson1, lesson2))
                .asConstraint(&quot;roomConflict&quot;);
    }

In this case, this constraint will search in the ConstraintConfiguration Bean for a constraint with the value &quot;roomConflict&quot; to apply the score penalty.
Then, in the DemoDataController Bean add a list of constraints to be taken into account by the Timefold Solver:
//constraints
// if you want to use Spring Jpa, it would be 
// List&lt;String&gt; constraints = constraintsRepo.findAll();
        List&lt;String&gt; constraints = List.of(&quot;roomConflict&quot;, &quot;teacherConflict&quot;, &quot;studentGroupConflict&quot;,
                &quot;teacherRoomStability&quot;, &quot;teacherTimeEfficiency&quot;, &quot;studentGroupVariety&quot;);
        TimetableConstraintConfiguration timetableConstraintConfiguration = new TimetableConstraintConfiguration(constraints);

and return the Timetable with the timetableConstraintConfiguration included:
return ResponseEntity.ok(new Timetable(demoData.name(), timeslots, rooms, lessons, timetableConstraintConfiguration));

And that's it. It should work. Please let me know if it worked.
PS: if you want to use Spring Data JPA, you would want to create a CRUD application where you can post and get all the data needed for the Timetable (there is in the Timefold documentation how you use Spring JPA with Timefold Solver). So, before you initialize the Timetable object that you want to be solved, make sure that you use the Repository to get all the data from your domain (Room, Timeslot, Lesson, Constraints) and let Timefold do the dirty work for you. In short, to make an application that also uses JPA practically you just need to adapt the code from GitHub to a CRUD environment.
PPS: Thank you Timefold team for your support!
"
"Does WebFlux Spring Boot @Transactional annotation work with reactive MongoDB?
I use WebFlux Spring Boot with reactive MongoDB like:
    id 'org.springframework.boot' version '2.6.7'
    ...
    implementation 'org.springframework.boot:spring-boot-starter-webflux'
    implementation 'org.springframework.boot:spring-boot-starter-data-mongodb-reactive'
    ...

I marked one of my method @Transactional to test. But it seems the annotation does not work for me. If an error occurs inside this method, then it still adds a row to my mongoDB database.
      import org.springframework.transaction.annotation.Transactional;

      ...

      @Transactional
      public Mono&lt;Chat&gt; createChat(Chat chat) {
        return chatRepository
            .save(chat)
            .map(
                c-&gt; {
                  if (true) {
                    throw new RuntimeException();
                  }
                  return c;
                });
      }

Do I miss something or Spring Boot @Transactional annotation does not work with reactive MongoDB?
I use MongoDB v5.0.8
","It seems like that Spring Data for reactive MongoDB requires to set explicitly a special bean transactionManager. As soon as I have added this bean to my  configuration for the reactive MongoDB, the @Transactional annotation started working. So the example method posted in my question does not add a new row to the database anymore if an error occurs inside the method.
Here is my configuration with transactionManager bean:
@Configuration
@EnableReactiveMongoRepositories
@AllArgsConstructor
public class ReactiveMongoConfiguration extends AbstractReactiveMongoConfiguration {

  private final MongoProperties mongoProperties;

  @Override
  public MongoClient reactiveMongoClient() {
    return MongoClients.create();
  }

  @Override
  protected String getDatabaseName() {
    return mongoProperties.getDatabase();
  }

  @Bean
  ReactiveMongoTransactionManager transactionManager(ReactiveMongoDatabaseFactory reactiveMongoDatabaseFactory) {
    return new ReactiveMongoTransactionManager(reactiveMongoDatabaseFactory);
  }

P.S.
It turns out the defining of transactionManager bean is not enough to enable transactions in reactive MongoDB. The very server of MongoDB should be also configured with replication. I followed these steps and it worked for me.
"
"I have an authentication provider, that throwing my custom exception.
This provider validating token on every request to controllers. Exceptions in controllers handling by controller advice, but provider works before controller, so controller advice cant handle exceptions that provider throws.
How can i handle exception from provider?
Provider
@Component
@RequiredArgsConstructor
public class BearerTokenAuthenticationProvider implements AuthenticationProvider {

private final Wso2TokenVerificationClient client;

@Override
public Authentication authenticate( Authentication authentication ) {
    BearerTokenAuthenticationToken token = (BearerTokenAuthenticationToken) authentication;
    Map&lt;String, String&gt; requestBody = new HashMap&lt;&gt;();
    requestBody.put( &quot;token&quot;, token.getToken() );
    Wso2TokenValidationResponse tokenValidationResponse = client.introspectToken( requestBody );
    if( !Boolean.parseBoolean( tokenValidationResponse.getActive() ) ) {
        throw new AuthenticationException(
            &quot;Token not valid&quot;, HttpStatus.UNAUTHORIZED
        );
    }
    DecodedJWT jwt = JWT.decode(token.getToken());
    UserDetails details = new UserDetails();
    details.setId( Long.parseLong(jwt.getClaim( OidcUserClaims.USER_ID ).asString()) );
    details.setEmail( jwt.getClaim( OidcUserClaims.EMAIL ).asString() );
    token.setDetails( details );
    return token;
}

@Override
public boolean supports( Class&lt;?&gt; aClass ) {
    return BearerTokenAuthenticationToken.class.equals( aClass );
}

Security Config
@Configuration
@RequiredArgsConstructor
public class CommonWebSecurityConfigurationAdapter extends WebSecurityConfigurerAdapter {

private final BearerTokenAuthenticationProvider bearerTokenProvider;

@Override
protected void configure(HttpSecurity http) throws Exception {
    http.headers().contentSecurityPolicy(&quot;script-src 'self'&quot;);
    http
            .csrf().disable()
            .authorizeRequests(auth -&gt; auth
                    .antMatchers(&quot;/public/**&quot;).not().hasAuthority(&quot;ROLE_ANONYMOUS&quot;)
            )
        .and()
        .oauth2ResourceServer(OAuth2ResourceServerConfigurer::jwt);
}

@Override
protected void configure( AuthenticationManagerBuilder auth ) throws Exception {
    auth.authenticationProvider( bearerTokenProvider );
}

}
","You can add an authenticationEntryPoint to handle custom exception.
@Configuration
@RequiredArgsConstructor
static class CommonWebSecurityConfigurationAdapter extends WebSecurityConfigurerAdapter {

    private final BearerTokenAuthenticationProvider bearerTokenProvider;

    @Override
    protected void configure(HttpSecurity http) throws Exception {
        http
            .headers()
                .contentSecurityPolicy(&quot;script-src 'self'&quot;);
        http
            .csrf().disable()
            .authorizeRequests(auth -&gt; auth
                 .antMatchers(&quot;/public/**&quot;).not().hasAuthority(&quot;ROLE_ANONYMOUS&quot;)
            )
            .oauth2ResourceServer(c -&gt; c.jwt()
                .and()
                .authenticationEntryPoint((request, response, authException) -&gt; {
                    //handle CustomAuthenticationException
                    
                    }
                )
            );
    }

    @Override
    protected void configure(AuthenticationManagerBuilder auth) throws Exception {
        auth.authenticationProvider(bearerTokenProvider);
    }
}

public class CustomAuthenticationException extends AuthenticationException {
    HttpStatus status;
    
    public CustomAuthenticationException(String message, HttpStatus status) {
        super(message);
        this.status = status;
    }
}

"
"I was interviewing for one of the big techs where I was asked a programming question in the problem solving round. The question is very similar to the Two Sum problem in Leet Code except for one tricky constraint. The question goes like this :
Given an array of integers nums, an integer target and an integer limit, return exactly one set of elements that counts up to the given limit and adds up to the given target.
 Input: nums = [2,7,11,15], target = 20, limit = 3
 
 Output: [2, 7, 11]

Explanation : The target is 20 and the limit is 3, so, we will have to find 3 numbers from the array that add up to 20.
I wasn't able to solve this during the interview and have been searching for a solution ever since.
The brute force approach is to run as many loops as the limit, which is not viable, considering the fact that the limit may be &lt;= 10,000
And another is to extract sub-arrays of length = limit, run through each and every one, add their elements and return a sub-array that adds up to Target.
But, I am sure there must be a more efficient approach to solve this.
Any ideas?
Edit :
The output that we return may be random and not necessarily contiguous.
The limit has to be met and the number of elements that we return must be equal to the limit.
There is no limit on the size of the array
","Use Stack (recursively) to find the array elements which will sum to the desired target within the required array elements limit. Doing it this way will actually find all combinations but only those which use fall on the elements limit are placed into a List.
Please read the comments in code. Delete them later if you like. Here is a runnable to demonstrate this process:
package sumtargetlimit_demo;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Stack;


public class SumTargetLimit_Demo {

    // The desired Target Sum
    private int targetSum = 20;
    
    /* The max allowable array elements to use in order to acquire 
       the desired Target Sum.    */
    private int numbersLimit = 3;
    
    // A Stack to hold the Array elements which sum to the desired Target Sum.
    private Stack&lt;Integer&gt; stack = new Stack&lt;&gt;();

    // Store the summation of current elements held in stack.
    private int sumInStack = 0;
    
    /* A List Interface of Integer[] array to hold all the 
       combinations of array elements which sum to target. */
    private List&lt;Integer[]&gt; combinationsList = new ArrayList&lt;&gt;();
    
    
    public static void main(String[] args) {
        // Demo started this way to avoid the need for statics.
        new SumTargetLimit_Demo().startDemo(args);
    }
    
    private void startDemo(String[] args) {
        // The int array to work against.
        int[] intData = {2, 7, 11, 15};
        
        /* See which array elements can acquire the desired 
           Target Sum with the maximum number of array elements 
           specified in the numbersLimit member variable.     */
        getSummations(intData, 0, intData.length);
        
        // Display the found results to Console window...
        if (combinationsList.isEmpty()) {
            System.err.println(&quot;No integer elements within the supplied Array will&quot;);
            System.err.println(&quot;provide a Taget Sum of &quot; + targetSum + &quot; with a maximum number&quot;);
            System.err.println(&quot;limit of &quot; + numbersLimit + &quot;.&quot;);
        }
        else {
            for (Integer[] intArray : combinationsList) {
                System.out.println(Arrays.toString(intArray).replaceAll(&quot;[\\[\\]]&quot;, &quot;&quot;));
            }
        }
    }
    
    // Note: This method is recursive...
    public void getSummations(int[] data, int startIndex, int endIndex) {
        /* Check to see if the sum of array elements stored in the
           Stack is equal to the desired Target Sum. If it is then 
           convert the array elements in the Stack to an Integer[] 
           Array and add it to the conmbinationsList List.       */
        if (sumInStack == targetSum) {
            if (stack.size() &lt;= numbersLimit) {
                combinationsList.add(stack.toArray(new Integer[stack.size()]));
            }
        }

        for (int currIndex = startIndex; currIndex &lt; endIndex; currIndex++) {
            if (sumInStack + data[currIndex] &lt;= targetSum) {
                stack.push(data[currIndex]);
                sumInStack += data[currIndex];

                // Make the currentIndex +1, and then use recursion to carry on.
                getSummations(data, currIndex + 1, endIndex);
                sumInStack -= stack.pop();
            }
        }
    }
    
}

Try a much larger int[] array and play with the Target Sum and Number Limit to see how things work.
"
"In my Java application I am using Azure Data Lake Storage Gen2 for storage (ABFS).
In the class that handles the requests to the filesystem, I get a file path as an input and then use some regex to extract Azure connection info from it.
The Azure Data Lake Storage Gen2 URI is in the following format:
abfs[s]://&lt;file_system&gt;@&lt;account_name&gt;.dfs.core.windows.net/&lt;path&gt;/&lt;file_name&gt;

I use the following regex abfss?://([^/]+)@([^\\.]+)(\\.[^/]+)/?((.+)?) to parse a given file path to extract:

fileSystem
accountName
accountSuffix
relativePath (path + file_name)

Below is just a test Java code with comments stating result/value in each variable after matching.
private void parsePath(String path) {
    //path = abfs://storage@myaccount.dfs.core.windows.net/selim/test.csv
    Pattern azurePathPattern = Pattern.compile(&quot;abfss?://([^/]+)@([^\\.]+)(\\.[^/]+)/?((.+)?)&quot;);
    Matcher matcher = azurePathPattern.matcher(path);
    if (matcher.find()) {
        String fileSystem = matcher.group(1); //storage
        String accountName = matcher.group(2); //myaccount
        String accountSuffix = matcher.group(3); //.dfs.core.windows.net
        //relativePath is &lt;path&gt;/&lt;file_name&gt;
        String relativePath = matcher.group(4); //selim/test.csv
    }
}

The problem is when I decided to use Azurite which is an Azure Storage API compatible server (emulator) that allow me to run unit tests against this emulator instead of against an actual Azure Server as recommended in the Microsoft documentation.
Azurite uses a different file URI than Azure so this makes my above Regex invalid for testing purposes. Azurite file URI is in the following format:
abfs[s]://&lt;file_system&gt;@&lt;local_ip&gt;:&lt;local_port&gt;/&lt;account_name&gt;/&lt;path&gt;/&lt;file_name&gt;

Azurite default account_name is devstoreaccount1 so here is an example path for a file on Azurite:
abfs://storage@127.0.0.1:10000/devstoreaccount1/selim/test.csv

If parsed by above regex this will be the output, causing incorrect api calls to Azurite server:

fileSystem: storage (correct)
accountName: 127 (incorrect, should be: devstoreaccount1)
accountSuffix: .0.0.1:10000 (incorrect, should be empty string)
relativePath: devstoreaccount1/selim/test.csv (incorrect, should be selim/test.csv)

Is it possible to have a 1 regex that can handle both URIs or 2 regexes to solve this issue
","Solution 1
You can use a single pattern for this, but you will need to check which group matched in the code to determine where the necessary details are captured.
The regex will look like
abfss?://(?:([^@/]*)@(\d{1,3}(?:\.\d{1,3}){3}:\d+)/([^/]+)|([^/]+)@([^.]+)(\.[^/]+))(?:/(.+))?

See the regex demo. The ([^@/]*)@(\d{1,3}(?:\.\d{1,3}){3}:\d+)/([^/]+) alternative allows capturing the file system, the IP address like part (with port number) after @, and the account name after a slash.
The Java code will look like
import java.util.*;
import java.util.regex.*;

class Test
{
    public static void main (String[] args) throws java.lang.Exception
    {
        Pattern pattern = Pattern.compile(&quot;abfss?://(?:([^@/]*)@(\\d{1,3}(?:\\.\\d{1,3}){3}:\\d+)/([^/]+)|([^/]+)@([^.]+)(\\.[^/]+))(?:/(.+))?&quot;);
        String[] inputs = {
            &quot;abfs://storage@myaccount.dfs.core.windows.net/selim/test.csv&quot;,
            &quot;abfs://storage@127.0.0.1:10000/devstoreaccount1/selim/test.csv&quot;
        };
        for (String s: inputs) {
            Matcher matcher = pattern.matcher(s);
            if (matcher.find()){
                if (matcher.group(5) != null) { // If original URL is found
                    String fileSystem = matcher.group(4); //storage
                    String accountName = matcher.group(5); //myaccount
                    String accountSuffix = matcher.group(6); //.dfs.core.windows.net
                    String relativePath = matcher.group(7); //selim/test.csv
                    System.out.println(s + &quot;:\nfileSystem: &quot; + fileSystem + &quot;\naccountName: &quot; + accountName + &quot;\naccountSuffix: '&quot; + accountSuffix + &quot;'\nrelativePath:&quot; + relativePath + &quot;\n-----&quot;);
                } else { // we have an Azurite URL
                    String fileSystem = matcher.group(1); //storage
                    String accountName = matcher.group(3); //devstoreaccount1
                    String accountSuffix = &quot;&quot;; // empty (or do you need matcher.group(2) to get &quot;127.0.0.1:10000&quot;?)
                    String relativePath = matcher.group(7); //selim/test.csv
                    System.out.println(s + &quot;:\nfileSystem: &quot; + fileSystem + &quot;\naccountName: &quot; + accountName + &quot;\naccountSuffix: '&quot; + accountSuffix + &quot;'\nrelativePath:&quot; + relativePath + &quot;\n-----&quot;);
                }
            }
        }
    }
}

Output:
abfs://storage@myaccount.dfs.core.windows.net/selim/test.csv:
fileSystem: storage
accountName: myaccount
accountSuffix: '.dfs.core.windows.net'
relativePath:selim/test.csv
-----
abfs://storage@127.0.0.1:10000/devstoreaccount1/selim/test.csv:
fileSystem: storage
accountName: devstoreaccount1
accountSuffix: ''
relativePath:selim/test.csv

Solution 2
You can use two different regular expressions and if the first one does not find a match, the second will be tried. The first one:
abfss?://([^@/]*)@(\d{1,3}(?:\.\d{1,3}){3}:\d+)/([^/]+)(?:/(.+))?

See this regex demo. The second one:
abfss?://([^/]+)@([^.]+)(\.[^/]+)(?:/(.+))?

See this regex demo. As this one also matches URLs of the first type, you need to make sure you run them in the fixed order.
See the Java demo:
import java.util.*;
import java.util.regex.*;

class Test
{
    public static void main (String[] args) throws java.lang.Exception
    {
        Pattern pattern_azurite = Pattern.compile(&quot;abfss?://([^@/]*)@(\\d{1,3}(?:\\.\\d{1,3}){3}:\\d+)/([^/]+)(?:/(.+))?&quot;);
        Pattern pattern_original = Pattern.compile(&quot;abfss?://([^/]+)@([^.]+)(\\.[^/]+)(?:/(.+))?&quot;);
        String[] inputs = {
            &quot;abfs://storage@myaccount.dfs.core.windows.net/selim/test.csv&quot;,
            &quot;abfs://storage@127.0.0.1:10000/devstoreaccount1/selim/test.csv&quot;,
            &quot;http://www.blahblah.blah&quot;
        };
        for (String s: inputs) {
            Map&lt;String, String&gt; result = null;
            Matcher matcher_azurite = pattern_azurite.matcher(s);
            if (matcher_azurite.find()){
                result = parseMatchResult(matcher_azurite, new int[] {1, 3, -1, 4});
            } else {
                Matcher matcher_original = pattern_original.matcher(s);
                if (matcher_original.find()){
                    result = parseMatchResult(matcher_original, new int[] {1, 2, 3, 4});
                }
            }
            if (result != null) {                        // Now print
                for (String key : result.keySet()) {
                    System.out.println(&quot;'&quot; + key + &quot;': '&quot; + result.get(key) + &quot;'&quot;);
                }
                System.out.println(&quot;----------------&quot;);
            } else {
                System.out.println(&quot;No match!&quot;);
            }
            
        }
    }
    public static Map&lt;String, String&gt; parseMatchResult(Matcher m, int[] indices) {
        Map&lt;String, String&gt; res = new HashMap&lt;String, String&gt;();
        res.put(&quot;fileSystem&quot;, m.group(indices[0]));
        res.put(&quot;accountName&quot;, m.group(indices[1]));
        res.put(&quot;accountSuffix&quot;, indices[2] &gt; -1 ? m.group(indices[2]) : &quot;&quot;);
        res.put(&quot;relativePath&quot;, m.group(indices[3]));
        return res;
    }
}

Output:
'fileSystem': 'storage'
'accountSuffix': '.dfs.core.windows.net'
'accountName': 'myaccount'
'relativePath': 'selim/test.csv'
----------------
'fileSystem': 'storage'
'accountSuffix': ''
'accountName': 'devstoreaccount1'
'relativePath': 'selim/test.csv'
----------------
No match!

"
"I am trying to connect my Metamask wallet to my Java Spring-Boot backend. I was trying to follow the example here. I am able to autogenerate the nonce and receive the wallet ID without a problem. I am trying to verify the signed nonce from the Wallet on the server to make sure that the sender is indeed who they say they are. However, I am unable to find any documentation on Web3J to do this.
Is web3j not the right package to use for this? The example shows how to do the verification on NodeJS based on javascript but I don't find any example on how to do this on Java.
My understanding is that the public key is the wallet ID itself and that the message is the nonce signed by the private key of the wallet which is not shared for obvious reasons. According to this, I would need to &quot;decrypt&quot; the message using the public key and see if the decrypted message is same as the nonce that the backend sent to Metamask to sign. Is this correct?
Here is my code to create and send the nonce to UI:
public User findUserByPublicAddress(String publicWalletId) {
    User u = userRepository.findByPublicWalletId(publicWalletId);
    if(u == null) {
        u = new User(&quot;&quot;, &quot;&quot;, &quot;&quot;, null, publicWalletId, &quot;&quot;);
        String nonce = StringUtil.generateRandomAlphaNumericString();
        u.setNonce(nonce);
        userRepository.saveAndFlush(u);
    }
    return u;
}

Here, I see if the user is already in my system and if they are not, then I just create a temporary user with a random nonce generated and saved in the DB. This nonce is sent to the UI for Metamask to sign. However, I am not sure how to do the verification part of it.
","I was able to figure this out finally. My initial understanding was incorrect. I was not supposed to attempt to decrypt the message to retrieve the nonce. Rather I needed to use the nonce to see if I can retrieve the public key of the private key used to sign the message and see if that public key retrieved matches the wallet ID.
The algorithm:

Receive the signed message and the wallet ID from the client
Retrieve the nonce sent to the client with the same wallet ID
Generate the hash of the nonce
Generate the signature data from the message. This basically retrieves the V, R and S and. R and S are the outputs of the ECDSA Signature and V is the Recovery ID.
Using the ECDSA Signature and Hash of the Nonce, generate the possible public Key that was used to sign the message. At max, one will be able to generate 4 possible public keys for this message.
Check if any of the generated keys match public wallet ID that the client sent. If it matches, then we have a positive match. Generate the JWT and respond to the client. If not, we know that the nonce was not signed by the Metamask wallet we expected.

The Code:
Here is a sample code for UI (JavaScript and HTML):
web3.eth.sign(
    web3.utils.sha3(nonce),
    window.userWalletAddress)
.then((message) =&gt; {
    console.log(message)
    data['message'] = message // BODY
    var xmlReq = new XMLHttpRequest();
    xmlReq.onreadystatechange = function() {
        if(this.readyState == 4 &amp;&amp; this.status == 200) {
            response = this.responseText
            console.log(response)
        }
    };
    xmlReq.open(&quot;POST&quot;, &quot;/api/users/login&quot;, true)
    xmlReq.setRequestHeader('Content-Type', 'application/json')
    xmlReq.send(JSON.stringify(data))
})

The web3.eth.sign() takes the message to be signed and takes the wallet ID that is signing it. This is then sent to the backend. In the backend:
public User signin(UserLoginDTO loginDetails, HttpServletResponse response) {
    try {
        // Get the wallet ID and signed message from the body stored in the DTO
        String publicWalletId = loginDetails.getPublicWalletId();
        String message = loginDetails.getMessage();

        // Find the nonce from the DB that was used to sign this message
        User user = userRepository.findByPublicWalletId(publicWalletId);
        String nonce = user.getNonce();

        // Generate the HASH of the Nonce
        byte[] nonceHash = Hash.sha3(nonce.getBytes()) // org.web3j.crypto.Hash

        // Generate the Signature Data
        byte[] signatureBytes = Numeric.hexStringToByteArray(message); // org.web3j.utils.Numeric
        
        byte v = (byte) ((signatureBytes[64] &lt; 27) ? (signatureBytes[64] + 27) : signatureBytes[64]);
        byte[] r = Arrays.copyOfRange(signatureBytes, 0, 32);
        byte[] s = Arrays.copyOfRange(signatureBytes, 32, 64);
        
        SignatureData signatureData = new SignatureData(v, r, s); // org.web3j.crypto.Sign.SignatureData

        // Generate the 4 possible Public Keys
        List&lt;String&gt; recoveredKeys = new ArrayList&lt;&gt;();
        for(int i = 0; i &lt; 4; i++) {
            BigInteger r = new BigInteger(1, signatureData.getR());
            BigInteger s = new BigInteger(1, signatureData.getS());
            ECDSASignature ecdsaSignature = new ECDSASignature(r, s);
            BigInteger recoveredKey = Sign.recoverFromSignature((byte)i, ecdsaSignature, nonceHash);
            if(recoveredKey != null) {
                recoveredKeys.add(&quot;0x&quot; + Keys.getAddressFromKey(recoveredKey)); // org.web3j.crypto.Keys
            }
        }

        // Check if one of the generated Keys match the public wallet ID.
        for(String recoveredKey : recoveredKeys) {
            if(recoveredKey.equalsIgnoreCase(publicWalletId)) { 
                // Add Code here to create the JWT and add that to your HttpServletResponse. Not shown here.
                return user;
            }
        }
        throw new CustomException(&quot;Message Sign Invalid&quot;, HttpStatus.UNAUTHORIZED);
    }
    catch (Exception ex) {
         // Custom Error Handling.
    }
}

"
"I have the following enum.
public enum AggregationType {
    MIN,
    MAX,
    AVERAGE
}

Let's assume that I have a function where I pass the enum value like:
public Float someFunction(AggregationType e) {
    return (float) provides.stream()
        .mapToDouble(this::someFunc)
        .average()
        .orElse(-1);
}

I want to apply this .average() .min() .max() methods on a stream based on enum value.
How I can achieve this? I don't want to use simply switch function inside someFunction but rather in this return statement.
So I want to have something like:
public Float someFunction(final AggregationType e) {
    return (float) provides.stream()
        .mapToDouble(this::someFunc)
        .decideWhichMethodShouldBeUsed()
        .orElse(-1);
}

where decideWhichMethodShouldBeUsed() decides which function to use based on the enum.
","When you can change the enum type to
public enum AggregationType {
    MIN(DoubleStream::min),
    MAX(DoubleStream::max),
    AVERAGE(DoubleStream::average);

    public final Function&lt;DoubleStream, OptionalDouble&gt; operation;

    AggregationType(Function&lt;DoubleStream, OptionalDouble&gt; f) {
        operation = f;
    }
}

you can implement the method like
public Float someFunction(final AggregationType aType) {
    return (float)aType.operation.apply(provides.stream().mapToDouble(this::someFunc))
        .orElse(-1);
}

If changing the enum type is not an option, you have to handle the mapping to the actual operation at the place where you want to implement someFunction, e.g.
private static final Map&lt;AggregationType,Function&lt;DoubleStream, OptionalDouble&gt;&gt; OPS;
static {
    EnumMap&lt;AggregationType,Function&lt;DoubleStream, OptionalDouble&gt;&gt;
        m = new EnumMap&lt;&gt;(AggregationType.class);
    m.put(AggregationType.MIN, DoubleStream::min);
    m.put(AggregationType.MAX, DoubleStream::max);
    m.put(AggregationType.AVERAGE, DoubleStream::average);
    OPS = Collections.unmodifiableMap(m);
}

public Float someFunction(final AggregationType aType) {
    return (float)OPS.get(aType).apply(provides.stream().mapToDouble(this::someFunc))
        .orElse(-1);
}


You can also use a switch statement, but it’s rather clunky
public Float someFunction(final AggregationType aType) {
    DoubleStream ds = provides.stream().mapToDouble(this::someFunc);
    OptionalDouble d;
    switch(aType) {
        case MAX: d = ds.max(); break;
        case MIN: d = ds.min(); break;
        case AVERAGE: d = ds.average(); break;
        default: throw new AssertionError();
    }
    return (float)d.orElse(-1);
}

Things get better when you’re using a recent Java version, as then, you can use a switch expression:
public Float someFunction(final AggregationType aType) {
    DoubleStream ds = provides.stream().mapToDouble(this::someFunc);
    return (float)(switch(aType) {
        case MAX -&gt; ds.max();
        case MIN -&gt; ds.min();
        case AVERAGE -&gt; ds.average();
    }).orElse(-1);
}

This is only accepted by the compiler when all enum constants are handled. Then, it will generate an equivalent to default -&gt; throw new AssertionError(); behind the scenes which will never be taken at runtime, as long as no-one changes the enum type after this code has been compiled.
Generally, only the first variant forces developers who consider adding new constants to AggregationType to also also consider handling the associated operation.
"
"Scenario:
I have 30 testcases for end-to-end process flow that includes (scheduler, producer and consumer). So, I'm automating the 30 testcase in java springmvc web application. I have created an endpoint which is to start testing, then it will run 30 testcase one after other in order, I have created 30 methods for each testcase, each test case approx takes 5 min to complete because (have to execute scheduler, producer and consumer). so, after one test case is complete, I want to show in UI the status and message, I don't want to wait till all the 30 testcase completion then show status of all testcase. How to achieve this using rest endpoint?
","Summary
send multiple via same end point , it is not REST API  scenario. In general , we often use Long connection to handle this scenario , there many tech to choose  like websocket , tcp protocol , socket , but all of them are too heavy , just send a mejssage to specified host may  configure so much .
Solution
In a nutshell ,  there is  Server-Sent Events could easy solve your problem ,  i would demonstrae bacic demo for this tech  in spring ， also you can read offical spring sse doc
1. set up endpoint in spring
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;
import org.springframework.web.servlet.mvc.method.annotation.SseEmitter;

import java.io.IOException;

@RestController
@RequestMapping(&quot;/sse&quot;)
public class SseController {

    @GetMapping(&quot;/events&quot;)
    public SseEmitter handleSse() {
        SseEmitter emitter = new SseEmitter();

        // Asynchronous processing to send events
        new Thread(() -&gt; {
            try {
                for (int i = 0; i &lt; 10; i++) {
                    // Send events every 1 second
                    emitter.send(SseEmitter.event().name(&quot;message&quot;).data(&quot;Event &quot; + i));

                    Thread.sleep(1000);
                }
                // Signal the end of the event stream
                emitter.complete();
            } catch (IOException | InterruptedException e) {
                emitter.completeWithError(e);
            }
        }).start();

        return emitter;
    }
}



connect endpoint by client side

&lt;!DOCTYPE html&gt;
&lt;html lang=&quot;en&quot;&gt;
&lt;head&gt;
    &lt;meta charset=&quot;UTF-8&quot;&gt;
    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
    &lt;title&gt;SSE Example&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;h1&gt;SSE Example&lt;/h1&gt;
    &lt;div id=&quot;sse-events&quot;&gt;&lt;/div&gt;

    &lt;script&gt;
        const eventSource = new EventSource('/sse/events');

        eventSource.onmessage = function (event) {
            const eventsDiv = document.getElementById('sse-events');
            eventsDiv.innerHTML += `&lt;p&gt;${event.data}&lt;/p&gt;`;
        };

        eventSource.onerror = function (error) {
            console.error('EventSource failed:', error);
            eventSource.close();
        };
    &lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;


In this example, the /sse/events endpoint in the SseController returns an SseEmitter, and a separate thread is used to send events asynchronously. The JavaScript code on the client side uses the EventSource API to listen for events and update the HTML content.
Remember to configure your Spring Boot application with appropriate dependencies, and you may need to handle exceptions and cleanup appropriately in a production scenario.
Appendix
  &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;version&gt;2.4.2&lt;/version&gt; // replace appropriate  version , 3.0.0 or else later is ok
      &lt;/dependency&gt;

you can find major class at following package path (may different from other spring boot version )
org.springframework.web.servlet.mvc.method.annotation.SseEmitter

"
"I have a complex generic type implementation in Java but I could not complete it. I am getting error on AddItemEvent class. The return type in the getHandler does not match with it's parent class requires.
AddItemEventHandler is actually a BaseEventHandler&lt;BaseEvent&lt;AddItemCommand, AddItemPayload&gt;, AddItemCommand, AddItemPayload&gt; but I get an error on that line.

public abstract class BasePayload {
}

public class AddItemPayload extends BasePayload {
    private int id;
    private String name;
    public AddItemPayload(int id, String name) {
        this.id = id;
        this.name = name;
    }
    public int getId() {
        return id;
    }
    public String getName() {
        return name;
    }
}

public class BaseCommand&lt;T extends BasePayload&gt; {
    String command;
    T payload;

    public BaseCommand(String command, T payload) {
        this.command = command;
        this.payload = payload;
    }
}

public class AddItemCommand extends BaseCommand&lt;AddItemPayload&gt; {
    public AddItemCommand(AddItemPayload payload) {
        super(&quot;AddItem&quot;, payload);
    }
}


public abstract class BaseEventHandler&lt;E extends BaseEvent&lt;C, P&gt;, C extends BaseCommand&lt;P&gt;, P extends BasePayload&gt; {
    abstract public void onTry(E event, ArrayList&lt;BaseEvent&lt;?, ?&gt;&gt; actualEvents);
    abstract public void onCommit(String uuid, E event);
}

public class AddItemEventHandler extends BaseEventHandler&lt;AddItemEvent, AddItemCommand, AddItemPayload&gt; {
    @Override
    public void onTry(AddItemEvent event, ArrayList&lt;BaseEvent&lt;?, ?&gt;&gt; actualEvents) {
    }
    @Override
    public void onCommit(String uuid, AddItemEvent event) {
    }
}


public abstract class BaseEvent&lt;C extends BaseCommand&lt;P&gt;, P extends BasePayload&gt; {
    abstract protected BaseEventHandler&lt;BaseEvent&lt;C, P&gt;, C, P&gt; getHandler();
}

public class AddItemEvent extends BaseEvent&lt;AddItemCommand, AddItemPayload&gt; {
    @Override
    protected BaseEventHandler&lt;BaseEvent&lt;AddItemCommand, AddItemPayload&gt;, AddItemCommand, AddItemPayload&gt; getHandler() {
        return new AddItemEventHandler(); // I have an error on this line
        /*
        Incompatible types. Found: 'org.example.handlers.AddItemEventHandler',
        required: 'org.example.handlers.BaseEventHandler&lt;org.example.events.BaseEvent&lt;org.example.commands.AddItemCommand,org.example.payload.AddItemPayload&gt;,
        org.example.commands.AddItemCommand,org.example.payload.AddItemPayload&gt;'
        */
    }
}










","You cannot assign a variable of type List&lt;AddItemEvent&gt; to a reference of type List&lt;BaseEvent&lt;AddItemCommand, AddItemPayload&gt;&gt;. That's because the latter allows more - it allows not just AddItemEvent instances, but any instance that extends BaseEvent&lt;AddItemCommand, AddItemPayload&gt;.
Your compiler error has the same cause. The handler would allow more than only AddItemEvent instances.
One solution can be to add an extra generic type to BaseEvent for the event itself:
public abstract class BaseEvent&lt;E extends BaseEvent&lt;E, C, P&gt;, C extends BaseCommand&lt;P&gt;, P extends BasePayload&gt; {
    abstract protected BaseEventHandler&lt;E, C, P&gt; getHandler();
}

That has to be propagated to some of the other types:
public abstract class BaseEventHandler&lt;E extends BaseEvent&lt;E, C, P&gt;, C extends BaseCommand&lt;P&gt;, P extends BasePayload&gt; {
    // additional ? in BaseEvent
    abstract public void onTry(E event, ArrayList&lt;BaseEvent&lt;?, ?, ?&gt;&gt; actualEvents);
    abstract public void onCommit(String uuid, E event);
}

public class AddItemEventHandler extends BaseEventHandler&lt;AddItemEvent, AddItemCommand, AddItemPayload&gt; {
    @Override
    // additional ? in BaseEvent
    public void onTry(AddItemEvent event, ArrayList&lt;BaseEvent&lt;?, ?, ?&gt;&gt; actualEvents) {
    }
    @Override
    public void onCommit(String uuid, AddItemEvent event) {
    }
}

// additional generic type: AddItemEvent
public class AddItemEvent extends BaseEvent&lt;AddItemEvent, AddItemCommand, AddItemPayload&gt; {
    @Override
    // replace BaseEvent&lt;AddItemCommand, AddItemPayload&gt; with AddItemEvent
    protected BaseEventHandler&lt;AddItemEvent, AddItemCommand, AddItemPayload&gt; getHandler() {
        return new AddItemEventHandler(); // no more error
    }
}

"
"My aim - create spring boot application, collect metrics using DropWizard and expose endpoint for Prometheus to consume application metrics:
My code:
@SpringBootApplication
@EnableMetrics(proxyTargetClass = true)
public class DemoApplication {

    public static void main(String[] args) {

        SpringApplication.run(DemoApplication.class, args);
    }

}


package com.example.demo;

import com.codahale.metrics.Counter;
import com.codahale.metrics.MetricRegistry;
import com.codahale.metrics.annotation.Timed;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;

import javax.annotation.PostConstruct;
import java.util.concurrent.atomic.AtomicLong;

@RestController
public class HelloController {
    private AtomicLong atomicLong = new AtomicLong();
    private Counter counter;

    @Autowired
    private MetricRegistry metricRegistry;

    @PostConstruct
    public void init() {
        counter = metricRegistry.counter(&quot;counter&quot;);
    }

    @GetMapping(&quot;/hello&quot;)
    @Timed(name = &quot;my-index&quot;)
    public String index() {
        counter.inc();

        return &quot;Greetings from Spring Boot!. count=&quot; + atomicLong.incrementAndGet();
    }

}


package com.example.demo;

import com.codahale.metrics.ConsoleReporter;
import com.codahale.metrics.MetricRegistry;
import com.codahale.metrics.jvm.FileDescriptorRatioGauge;
import com.codahale.metrics.jvm.GarbageCollectorMetricSet;
import com.codahale.metrics.jvm.MemoryUsageGaugeSet;
import com.codahale.metrics.jvm.ThreadStatesGaugeSet;
import com.codahale.metrics.servlets.AdminServlet;
import com.codahale.metrics.servlets.CpuProfileServlet;
import com.codahale.metrics.servlets.MetricsServlet;
import com.ryantenney.metrics.spring.config.annotation.EnableMetrics;
import com.ryantenney.metrics.spring.config.annotation.MetricsConfigurerAdapter;
import io.prometheus.client.dropwizard.DropwizardExports;
import org.springframework.boot.web.servlet.ServletRegistrationBean;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

import java.util.concurrent.TimeUnit;

@Configuration
public class Config /*extends MetricsConfigurerAdapter*/ {
    //@Override
            //public void configureReporters(MetricRegistry metricRegistry) {
        //    // registerReporter allows the MetricsConfigurerAdapter to
        //    // shut down the reporter when the Spring context is closed
        //   // registerReporter(ConsoleReporter
        //   //         .forRegistry(metricRegistry)
        //   //         .build())
        //   //         .start(1, TimeUnit.MINUTES);


        //    new DropwizardExports(metricRegistry).register();
        //}

   @Bean
   public DropwizardExports dropwizardExports(MetricRegistry metricRegistry){
       DropwizardExports dropwizardExports = new DropwizardExports(metricRegistry);
       dropwizardExports.register();
       return dropwizardExports;
   }

    @Bean
    public MetricRegistry metricRegistry() {
        MetricRegistry metricRegistry = new MetricRegistry();
        metricRegistry.registerAll(new GarbageCollectorMetricSet());
        metricRegistry.registerAll(new MemoryUsageGaugeSet());
        metricRegistry.registerAll(new ThreadStatesGaugeSet());
        return metricRegistry;
    }

    @Bean
    public ConsoleReporter consoleReporter(MetricRegistry metricRegistry) {
        ConsoleReporter reporter = ConsoleReporter.forRegistry(metricRegistry).build();
        reporter.start(5, TimeUnit.SECONDS);
        reporter.report();
        return reporter;
    }

    @Bean
    public ServletRegistrationBean&lt;MetricsServlet&gt; registerMetricsServlet(MetricRegistry metricRegistry) {
        return new ServletRegistrationBean&lt;&gt;(new MetricsServlet(metricRegistry), &quot;/metrics/*&quot;);
    }

    @Bean
    public ServletRegistrationBean&lt;CpuProfileServlet&gt; registerCpuServlet() {
        return new ServletRegistrationBean&lt;&gt;(new CpuProfileServlet(), &quot;/cpu/*&quot;);
    }
}

build.gradle:
plugins {
    id 'org.springframework.boot' version '2.7.1'
    id 'io.spring.dependency-management' version '1.0.11.RELEASE'
    id 'java'
}

group = 'com.example'
version = '0.0.1-SNAPSHOT'
sourceCompatibility = '17'

repositories {
    mavenCentral()
}

dependencies {
    implementation 'org.springframework.boot:spring-boot-starter-web'
    implementation &quot;org.springframework.boot:spring-boot-starter-actuator&quot;
    // Minimum required for metrics.
    implementation ('com.ryantenney.metrics:metrics-spring:3.1.3') {
        exclude group: 'com.codahale.metrics'
        exclude group: 'org.springframework'
    }
    implementation 'io.dropwizard.metrics:metrics-core:4.2.9'
    implementation 'io.dropwizard.metrics:metrics-annotation:4.2.9'
    implementation 'io.dropwizard.metrics:metrics-servlets:4.2.9'

    implementation 'io.prometheus:simpleclient_dropwizard:0.15.0'
    implementation 'io.prometheus:simpleclient_servlet:0.15.0'
    implementation 'io.dropwizard:dropwizard-core:2.1.0'

    implementation 'com.ryantenney.metrics:metrics-spring:3.1.3'
    implementation 'io.prometheus:simpleclient_common:0.16.0'

    testImplementation 'org.springframework.boot:spring-boot-starter-test'
}

tasks.named('test') {
    useJUnitPlatform()
}

I access localhost:8080/metrics and receive following response:
{
  &quot;version&quot;: &quot;4.0.0&quot;,
  &quot;gauges&quot;: {
    &quot;G1-Old-Generation.count&quot;: {
      &quot;value&quot;: 0
    },
    &quot;G1-Old-Generation.time&quot;: {
      &quot;value&quot;: 0
    },
    &quot;G1-Young-Generation.count&quot;: {
      &quot;value&quot;: 7
    },
    &quot;G1-Young-Generation.time&quot;: {
      &quot;value&quot;: 31
    },
    &quot;blocked.count&quot;: {
      &quot;value&quot;: 0
    },
    &quot;count&quot;: {
      &quot;value&quot;: 26
    },
    &quot;daemon.count&quot;: {
      &quot;value&quot;: 22
    },
    &quot;deadlock.count&quot;: {
      &quot;value&quot;: 0
    },
    &quot;deadlocks&quot;: {
      &quot;value&quot;: []
    },
    &quot;heap.committed&quot;: {
      &quot;value&quot;: 301989888
    },
    &quot;heap.init&quot;: {
      &quot;value&quot;: 532676608
    },
    &quot;heap.max&quot;: {
      &quot;value&quot;: 8518631424
    },
    &quot;heap.usage&quot;: {
      &quot;value&quot;: 0.008041180864688155
    },
    &quot;heap.used&quot;: {
      &quot;value&quot;: 68499856
    },
    &quot;new.count&quot;: {
      &quot;value&quot;: 0
    },
    &quot;non-heap.committed&quot;: {
      &quot;value&quot;: 51707904
    },
    &quot;non-heap.init&quot;: {
      &quot;value&quot;: 2555904
    },
    &quot;non-heap.max&quot;: {
      &quot;value&quot;: -1
    },
    &quot;non-heap.usage&quot;: {
      &quot;value&quot;: -5.0738536E7
    },
    &quot;non-heap.used&quot;: {
      &quot;value&quot;: 50738536
    },
    &quot;peak.count&quot;: {
      &quot;value&quot;: 32
    },
    &quot;pools.CodeCache.committed&quot;: {
      &quot;value&quot;: 10551296
    },
    &quot;pools.CodeCache.init&quot;: {
      &quot;value&quot;: 2555904
    },
    &quot;pools.CodeCache.max&quot;: {
      &quot;value&quot;: 50331648
    },
    &quot;pools.CodeCache.usage&quot;: {
      &quot;value&quot;: 0.2039642333984375
    },
    &quot;pools.CodeCache.used&quot;: {
      &quot;value&quot;: 10265856
    },
    &quot;pools.Compressed-Class-Space.committed&quot;: {
      &quot;value&quot;: 5177344
    },
    &quot;pools.Compressed-Class-Space.init&quot;: {
      &quot;value&quot;: 0
    },
    &quot;pools.Compressed-Class-Space.max&quot;: {
      &quot;value&quot;: 1073741824
    },
    &quot;pools.Compressed-Class-Space.usage&quot;: {
      &quot;value&quot;: 0.004625104367733002
    },
    &quot;pools.Compressed-Class-Space.used&quot;: {
      &quot;value&quot;: 4966168
    },
    &quot;pools.G1-Eden-Space.committed&quot;: {
      &quot;value&quot;: 188743680
    },
    &quot;pools.G1-Eden-Space.init&quot;: {
      &quot;value&quot;: 29360128
    },
    &quot;pools.G1-Eden-Space.max&quot;: {
      &quot;value&quot;: -1
    },
    &quot;pools.G1-Eden-Space.usage&quot;: {
      &quot;value&quot;: 0.26666666666666666
    },
    &quot;pools.G1-Eden-Space.used&quot;: {
      &quot;value&quot;: 50331648
    },
    &quot;pools.G1-Eden-Space.used-after-gc&quot;: {
      &quot;value&quot;: 0
    },
    &quot;pools.G1-Old-Gen.committed&quot;: {
      &quot;value&quot;: 109051904
    },
    &quot;pools.G1-Old-Gen.init&quot;: {
      &quot;value&quot;: 503316480
    },
    &quot;pools.G1-Old-Gen.max&quot;: {
      &quot;value&quot;: 8518631424
    },
    &quot;pools.G1-Old-Gen.usage&quot;: {
      &quot;value&quot;: 0.0017806278080379123
    },
    &quot;pools.G1-Old-Gen.used&quot;: {
      &quot;value&quot;: 15168512
    },
    &quot;pools.G1-Old-Gen.used-after-gc&quot;: {
      &quot;value&quot;: 15168512
    },
    &quot;pools.G1-Survivor-Space.committed&quot;: {
      &quot;value&quot;: 4194304
    },
    &quot;pools.G1-Survivor-Space.init&quot;: {
      &quot;value&quot;: 0
    },
    &quot;pools.G1-Survivor-Space.max&quot;: {
      &quot;value&quot;: -1
    },
    &quot;pools.G1-Survivor-Space.usage&quot;: {
      &quot;value&quot;: 0.7151832580566406
    },
    &quot;pools.G1-Survivor-Space.used&quot;: {
      &quot;value&quot;: 2999696
    },
    &quot;pools.G1-Survivor-Space.used-after-gc&quot;: {
      &quot;value&quot;: 2999696
    },
    &quot;pools.Metaspace.committed&quot;: {
      &quot;value&quot;: 35979264
    },
    &quot;pools.Metaspace.init&quot;: {
      &quot;value&quot;: 0
    },
    &quot;pools.Metaspace.max&quot;: {
      &quot;value&quot;: -1
    },
    &quot;pools.Metaspace.usage&quot;: {
      &quot;value&quot;: 0.9868604316086066
    },
    &quot;pools.Metaspace.used&quot;: {
      &quot;value&quot;: 35506512
    },
    &quot;runnable.count&quot;: {
      &quot;value&quot;: 10
    },
    &quot;terminated.count&quot;: {
      &quot;value&quot;: 0
    },
    &quot;timed_waiting.count&quot;: {
      &quot;value&quot;: 5
    },
    &quot;total.committed&quot;: {
      &quot;value&quot;: 353697792
    },
    &quot;total.init&quot;: {
      &quot;value&quot;: 535232512
    },
    &quot;total.max&quot;: {
      &quot;value&quot;: 8518631423
    },
    &quot;total.used&quot;: {
      &quot;value&quot;: 119238392
    },
    &quot;total_started.count&quot;: {
      &quot;value&quot;: 47
    },
    &quot;waiting.count&quot;: {
      &quot;value&quot;: 11
    }
  },
  &quot;counters&quot;: {
    &quot;counter&quot;: {
      &quot;count&quot;: 9
    }
  },
  &quot;histograms&quot;: {},
  &quot;meters&quot;: {},
  &quot;timers&quot;: {}
}

Obviously this output is not applicable for Prometheus (all dots should be replaced with &quot;_&quot; at least)
How can I make output in format ready for prometheus ?
P.S.
Based on documentation I've understand that class io.prometheus.client.dropwizardDropwizardExports is responsible for generating metric in format ready for Prometheus but I can't understand how.
","I don't see significant difference but it is working example:
public class JavaDropwizard {
  // Create registry for Dropwizard metrics.
  static final MetricRegistry metrics = new MetricRegistry();
  // Create a Dropwizard counter.
  static final Counter counter = metrics.counter(&quot;my.example.counter.total&quot;);

  public static void main( String[] args ) throws Exception {
      // Increment the counter.
      counter.inc();

      // Hook the Dropwizard registry into the Prometheus registry
      // via the DropwizardExports collector.
      CollectorRegistry.defaultRegistry.register(new DropwizardExports(metrics));


      // Expose Prometheus metrics.
      Server server = new Server(1234);
      ServletContextHandler context = new ServletContextHandler();
      context.setContextPath(&quot;/&quot;);
      server.setHandler(context);
      context.addServlet(new ServletHolder(new MetricsServlet()), &quot;/metrics&quot;);
      // Add metrics about CPU, JVM memory etc.
      DefaultExports.initialize();
      // Start the webserver.
      server.start();
      server.join();
  }
}

pom.xml
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
  &lt;groupId&gt;io.robustperception.java_examples&lt;/groupId&gt;
  &lt;artifactId&gt;java_dropwizard&lt;/artifactId&gt;
  &lt;packaging&gt;jar&lt;/packaging&gt;
  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
  &lt;properties&gt;
    &lt;maven.compiler.source&gt;17&lt;/maven.compiler.source&gt;
    &lt;maven.compiler.target&gt;17&lt;/maven.compiler.target&gt;
  &lt;/properties&gt;
  &lt;name&gt;java_dropwizard&lt;/name&gt;
  &lt;url&gt;http://maven.apache.org&lt;/url&gt;
  &lt;dependencies&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;io.prometheus&lt;/groupId&gt;
      &lt;artifactId&gt;simpleclient&lt;/artifactId&gt;
      &lt;version&gt;0.15.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;io.prometheus&lt;/groupId&gt;
      &lt;artifactId&gt;simpleclient_hotspot&lt;/artifactId&gt;
      &lt;version&gt;0.15.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;io.prometheus&lt;/groupId&gt;
      &lt;artifactId&gt;simpleclient_servlet&lt;/artifactId&gt;
      &lt;version&gt;0.15.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;io.prometheus&lt;/groupId&gt;
      &lt;artifactId&gt;simpleclient_dropwizard&lt;/artifactId&gt;
      &lt;version&gt;0.15.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.dropwizard.metrics&lt;/groupId&gt;
        &lt;artifactId&gt;metrics-core&lt;/artifactId&gt;
        &lt;version&gt;4.2.9&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt;
      &lt;artifactId&gt;jetty-servlet&lt;/artifactId&gt;
      &lt;version&gt;8.1.7.v20120910&lt;/version&gt;
    &lt;/dependency&gt;
  &lt;/dependencies&gt;

  &lt;build&gt;
    &lt;plugins&gt;
      &lt;plugin&gt;
        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
        &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
        &lt;version&gt;3.1&lt;/version&gt;
      &lt;/plugin&gt;
      &lt;!-- Build a full jar with dependencies --&gt; 
      &lt;plugin&gt;
        &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
        &lt;configuration&gt;
          &lt;archive&gt;
        &lt;manifest&gt;
          &lt;mainClass&gt;io.robustperception.java_examples.JavaDropwizard&lt;/mainClass&gt;
        &lt;/manifest&gt;
      &lt;/archive&gt;
      &lt;descriptorRefs&gt;
        &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
      &lt;/descriptorRefs&gt;
    &lt;/configuration&gt;
    &lt;executions&gt;
      &lt;execution&gt;
        &lt;id&gt;make-assembly&lt;/id&gt;
        &lt;phase&gt;package&lt;/phase&gt;
        &lt;goals&gt;
          &lt;goal&gt;single&lt;/goal&gt;
        &lt;/goals&gt;
      &lt;/execution&gt;
    &lt;/executions&gt;
  &lt;/plugin&gt;
&lt;/plugins&gt;

  

"
"I'm reading through JDK9 release notes and found a small lack of information.
In one of the paragraphs, there is written:

The classes in many non-core modules are now defined to the platform class loader rather than the boot class loader. This may impact code that creates class loaders with null as the parent class loader and assumes that all platform classes are visible to the parent class loader.

Based on this paragraph I tried to find out what are core and non-core modules in Java, but can not find any answer.
I also tried to run a simple program to check classloaders of classes from different modules, but so far sometimes it returned null (which means boot classloader) and sometimes it returned PlatformClassLoader.
I do not want to check all modules manualy. Is there available some list of modules which are considered as core and non-core?
Thank you.
","It is easy to check all modules manually, using ModuleLayer.boot().modules().
For completeness, you should also add the --add-modules=ALL-MODULE-PATH VM option on the command line:
public class ModuleCL {
    public static void main(String[] args) {
        Map&lt;ClassLoader, Set&lt;Module&gt;&gt; moduleMap = new HashMap&lt;&gt;();
        for (var m : ModuleLayer.boot().modules()) {
            ClassLoader cl = m.getClassLoader();
            Set&lt;Module&gt; modules = moduleMap.computeIfAbsent(cl, __ -&gt; new HashSet&lt;&gt;());
            modules.add(m);
        }
        for (var e : moduleMap.entrySet()) {
            System.out.println(e.getKey());
            for (var m : e.getValue()) {
                System.out.println(&quot;    &quot; + m);
            }
        }
    }
}

Which outputs on (my machine) with Java 18:

null
    module java.rmi
    module java.xml
    module java.datatransfer
    module jdk.nio.mapmode
    module jdk.jfr
    module jdk.naming.rmi
    module java.naming
    module java.management.rmi
    module jdk.net
    module jdk.management.jfr
    module java.management
    module java.logging
    module jdk.sctp
    module java.security.sasl
    module jdk.management.agent
    module java.instrument
    module jdk.unsupported
    module java.base
    module java.desktop
    module java.prefs
    module jdk.management
jdk.internal.loader.ClassLoaders$PlatformClassLoader@2f2c9b19
    module java.net.http
    module java.transaction.xa
    module jdk.dynalink
    module java.scripting
    module jdk.crypto.mscapi
    module jdk.crypto.ec
    module jdk.localedata
    module jdk.security.jgss
    module jdk.jsobject
    module java.security.jgss
    module java.sql.rowset
    module jdk.accessibility
    module jdk.zipfs
    module java.xml.crypto
    module java.sql
    module jdk.naming.dns
    module jdk.charsets
    module java.smartcardio
    module java.compiler
    module jdk.security.auth
    module jdk.xml.dom
    module jdk.httpserver
    module jdk.crypto.cryptoki
jdk.internal.loader.ClassLoaders$AppClassLoader@1d44bcfa
    module jdk.attach
    module jdk.internal.le
    module jdk.jpackage
    module jdk.internal.opt
    module jdk.jdeps
    module jdk.compiler
    module jdk.jartool
    module jdk.javadoc
    module jdk.internal.ed
    module jdk.jlink
    module jdk.internal.jvmstat
    module jdk.editpad
    module jdk.random
    module jdk.jdwp.agent
    module jdk.jshell
    module jdk.unsupported.desktop
    module jdk.jconsole
    module jdk.jstatd
    module jdk.jdi

This file defines the mapping of the different modules to the different loaders
"
"Hello when I run the following code using java 8 all works fine
public class Main {
    public static void main(String[] args) {
       LocalDate date =  LocalDate.parse(&quot;24ENE1982&quot;, new DateTimeFormatterBuilder().parseCaseInsensitive()
                .appendPattern(&quot;ddMMMyyyy&quot;)
                .toFormatter(new Locale(&quot;es&quot;, &quot;ES&quot;)));
        System.out.println(&quot;Hello world! &quot; + date);
    }

but fail with java 11
More specific

java 11.0.19 2023-04-18 LTS Java(TM) SE Runtime Environment 18.9
(build 11.0.19+9-LTS-224) Java HotSpot(TM) 64-Bit Server VM 18.9
(build 11.0.19+9-LTS-224, mixed mode)

If I use java 18 works too.
Any idea to solve this issue without upgrade or downgrade the java version
I have tried to set the Locale using
Locale.forLanguageTag(&quot;es-ES&quot;)

and
new Locale(&quot;es&quot;, &quot;ES&quot;)

But with no changes
Expected value
Hello world! 1982-01-24

but an exception sin thrown
Exception in thread &quot;main&quot; java.time.format.DateTimeParseException: Text '24ENE1982' could not be parsed at index 2
    at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2046)
    at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1948)
    at java.base/java.time.LocalDate.parse(LocalDate.java:428)
    at Main.main(Main.java:7)

","As noted in the comments, in Java 11 the short month names have a trailing dot in Spanish.  This is the cause of the problem, as a solution you could use a DateTimeFormatter with custom month names derived from the &quot;official&quot; ones but without any dot.  The following example works with Java 8, Java 11 and Java 17.
import java.time.LocalDate;
import java.time.Month;
import java.time.format.DateTimeFormatter;
import java.time.format.DateTimeFormatterBuilder;
import java.time.format.TextStyle;
import java.time.temporal.ChronoField;
import java.util.HashMap;
import java.util.Locale;
import java.util.Map;

public class Main {

    public static void main(String[] args) {
        Locale locale = new Locale(&quot;es&quot;, &quot;ES&quot;);

        Map&lt;Long, String&gt; months = new HashMap&lt;&gt;();
        for (Month m : Month.values()) {
            months.put(Long.valueOf(m.getValue()),
                       m.getDisplayName(TextStyle.SHORT, locale).replace(&quot;.&quot;, &quot;&quot;));
        }                         

        DateTimeFormatter formatter = new DateTimeFormatterBuilder()
                .parseCaseInsensitive()
                .appendPattern(&quot;dd&quot;)
                .appendText(ChronoField.MONTH_OF_YEAR, months)
                .appendPattern(&quot;yyyy&quot;)
                .toFormatter(locale);
        
        LocalDate date = LocalDate.parse(&quot;24ENE1982&quot;, formatter);
        System.out.println(&quot;Hello world! &quot; + date);
    }

}

Read also about Locale Data Providers at oracle.com.
"
"I want to change the TextArea cursor, but nothing happens when I use this:
numTextArea.setCursor(Cursor.DISAPPEAR);

","The default skin of TextArea includes a &quot;content area&quot;, which is documented as a Region. It's this content area that has its cursor set to Cursor.TEXT (by the default user agent stylesheet). And since the mouse is hovering over the content area, not the text area directly, you see the cursor of the content area instead of the text area.
The likely easiest way to change the cursor is to use CSS.
.text-area .content {
    -fx-cursor: disappear;
}

 
Then add the stylesheet to the scene (or the text area, or an ancestor layout). For example:
import javafx.application.Application;
import javafx.scene.Scene;
import javafx.scene.control.TextArea;
import javafx.stage.Stage;

public class Main extends Application {

    private static final String CSS = &quot;&quot;&quot;
            .text-area .content {
                -fx-cursor: hand;
            }
            &quot;&quot;&quot;.stripIndent();

    @Override
    public void start(Stage primaryStage) {
        Scene scene = new Scene(new TextArea(), 500, 300);
        scene.getStylesheets().add(&quot;data:text/css,&quot; + CSS);
        primaryStage.setScene(scene);
        primaryStage.show();
    }
}

Note: The above adds the stylehseet to the scene via a data URL. This requires JavaFX 17+.
The example uses Cursor.HAND because Cursor.DISAPPEAR just gave me the default cursor on Windows 10 with Java/JavaFX 20.0.1. Not sure if that's a bug.
Note you can use null or inherit for -fx-cursor in the CSS, and then set the text area's cursor in code (like you're currently trying to do).
"
"I'm running the below Java code for fetching option-chain data from the NSE stock exchange's REST api. At first I'm making a GET to the home page and using the cookie from the response in the subsequent request to actually fetch the option-chain data. I repeat both these steps continuously with a scheduled task. It works one or two times but after that it starts giving 401 unauthorized error in the HTTP response. I'm setting a browser name in both request headers. Any help is much appreciated.
import java.net.HttpURLConnection;
import java.net.URL;
import java.util.List;
import java.io.InputStream;

public class PollNSEIndia {
    public static void main(String args[]) throws Exception {
        while (true) {
            HttpURLConnection baseUrlConnection = (HttpURLConnection) new URL(&quot;https://www.nseindia.com/&quot;).openConnection();
            baseUrlConnection.setRequestProperty(&quot;Connection&quot;, &quot;keep-alive&quot;);
            baseUrlConnection.setRequestProperty(&quot;Cache-Control&quot;, &quot;max-age=0&quot;);
            baseUrlConnection.setRequestProperty(&quot;Upgrade-Insecure-Requests&quot;, &quot;1&quot;);
            baseUrlConnection.setRequestProperty(
                    &quot;User-Agent&quot;,
                    &quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko)&quot;
                            + &quot; Chrome/89.0.4389.114 Safari/537.36&quot;);
            baseUrlConnection.setRequestProperty(
                    &quot;Accept&quot;,
                    &quot;text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9&quot;);
            baseUrlConnection.setRequestProperty(&quot;Accept-Language&quot;, &quot;en-US,en;q=0.9&quot;);
            List&lt;String&gt; cookies = baseUrlConnection.getHeaderFields().get(&quot;Set-Cookie&quot;);

            URL url = new URL(&quot;https://www.nseindia.com/api/option-chain-indices?symbol=MIDCPNIFTY&quot;);
            HttpURLConnection httpURLConnection = (HttpURLConnection) url.openConnection();
            httpURLConnection.setRequestMethod(&quot;GET&quot;);
            for (String cookie : cookies) {
                httpURLConnection.addRequestProperty(&quot;Cookie&quot;, cookie.split(&quot;;&quot;, 2)[0]);
            }
            httpURLConnection.setRequestProperty(&quot;Connection&quot;, &quot;keep-alive&quot;);
            httpURLConnection.setRequestProperty(&quot;Cache-Control&quot;, &quot;max-age=0&quot;);
            httpURLConnection.setRequestProperty(&quot;Upgrade-Insecure-Requests&quot;, &quot;1&quot;);
            httpURLConnection.setRequestProperty(
                    &quot;User-Agent&quot;,
                    &quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko)&quot;
                            + &quot; Chrome/89.0.4389.114 Safari/537.36&quot;);
            httpURLConnection.setRequestProperty(
                    &quot;Accept&quot;,
                    &quot;text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9&quot;);
            httpURLConnection.setRequestProperty(&quot;Accept-Language&quot;, &quot;en-US,en;q=0.9&quot;);
            InputStream inputStream = httpURLConnection.getInputStream();
            System.out.println(&quot;Got inputstream.&quot;);
            Thread.sleep(1000);
        }
    }
}

","You were adding multiple Cookie request property while calling &quot;https://www.nseindia.com/api/option-chain-indices?symbol=MIDCPNIFTY&quot; which was causing 401.
Also, I updated your code to fix the way you were adding Cookie in the headers while doing the request.
Now this will work :)
Try this:
import com.fasterxml.jackson.databind.ObjectMapper;
import java.net.*;
import java.util.List;
import java.io.InputStream;
import java.util.Map;
import java.util.Objects;
import java.util.stream.Collectors;

public class PollNSEIndia {
    public static void main(String args[]) throws Exception {
        while (true) {
            CookieManager cookieManager = new CookieManager();
            CookieHandler.setDefault(cookieManager);
            HttpURLConnection baseUrlConnection = (HttpURLConnection) new URL(&quot;https://www.nseindia.com/&quot;).openConnection();
            baseUrlConnection.setRequestProperty(&quot;Connection&quot;, &quot;keep-alive&quot;);
            baseUrlConnection.setRequestProperty(&quot;Cache-Control&quot;, &quot;max-age=0&quot;);
            baseUrlConnection.setRequestProperty(&quot;Upgrade-Insecure-Requests&quot;, &quot;1&quot;);
            baseUrlConnection.setRequestProperty(
                    &quot;User-Agent&quot;,
                    &quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko)&quot;
                            + &quot; Chrome/89.0.4389.114 Safari/537.36&quot;);
            baseUrlConnection.setRequestProperty(
                    &quot;Accept&quot;,
                    &quot;text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9&quot;);
            baseUrlConnection.setRequestProperty(&quot;Accept-Language&quot;, &quot;en-US,en;q=0.9&quot;);
            baseUrlConnection.getContent();
            List&lt;HttpCookie&gt; cookieList = cookieManager.getCookieStore().getCookies();

            URL url = new URL(&quot;https://www.nseindia.com/api/option-chain-indices?symbol=MIDCPNIFTY&quot;);
            HttpURLConnection httpURLConnection = (HttpURLConnection) url.openConnection();
            httpURLConnection.setRequestMethod(&quot;GET&quot;);
            httpURLConnection.setRequestProperty(&quot;Cookie&quot;, cookieList.stream().filter(Objects::nonNull)
                    .map(cookie -&gt; cookie.getName() + &quot;=&quot; + cookie.getValue()).collect(Collectors.joining(&quot;;&quot;)));
            httpURLConnection.setRequestProperty(&quot;Connection&quot;, &quot;keep-alive&quot;);
            httpURLConnection.setRequestProperty(&quot;Cache-Control&quot;, &quot;max-age=0&quot;);
            httpURLConnection.setRequestProperty(&quot;Upgrade-Insecure-Requests&quot;, &quot;1&quot;);
            httpURLConnection.setRequestProperty(
                    &quot;User-Agent&quot;,
                    &quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko)&quot;
                            + &quot; Chrome/89.0.4389.114 Safari/537.36&quot;);
            httpURLConnection.setRequestProperty(
                    &quot;Accept&quot;,
                    &quot;text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9&quot;);
            httpURLConnection.setRequestProperty(&quot;Accept-Language&quot;, &quot;en-US,en;q=0.9&quot;);
            InputStream inputStream = httpURLConnection.getInputStream();
            System.out.println(httpURLConnection.getResponseCode());
            System.out.println(&quot;Got inputstream.&quot;);

            // checking output via Jackson for testing (ignore this)
            ObjectMapper mapper = new ObjectMapper();
            Map data = mapper.readValue(inputStream, Map.class);
            System.out.println(mapper.writerWithDefaultPrettyPrinter().writeValueAsString(data));

            Thread.sleep(1000);
        }
    }
}

Output:
200
Got inputstream.
{
  &quot;records&quot; : {
    &quot;expiryDates&quot; : [ &quot;09-Oct-2023&quot;, &quot;16-Oct-2023&quot;, &quot;23-Oct-2023&quot;, &quot;30-Oct-2023&quot;, &quot;06-Nov-2023&quot;, &quot;22-Dec-2023&quot; ],
    &quot;data&quot; : [ {
      &quot;strikePrice&quot; : 7800,
      &quot;expiryDate&quot; : &quot;09-Oct-2023&quot;,
      &quot;PE&quot; : {
        &quot;strikePrice&quot; : 7800,
        &quot;expiryDate&quot; : &quot;09-Oct-2023&quot;,
        &quot;underlying&quot; : &quot;MIDCPNIFTY&quot;,
        &quot;identifier&quot; : &quot;OPTIDXMIDCPNIFTY09-10-2023PE7800.00&quot;,
        &quot;openInterest&quot; : 386,
        &quot;changeinOpenInterest&quot; : -33,
        &quot;pchangeinOpenInterest&quot; : -7.875894988066825,
        &quot;totalTradedVolume&quot; : 985,
        &quot;impliedVolatility&quot; : 52.73,
        &quot;lastPrice&quot; : 0.05,
        &quot;change&quot; : -0.4,
....

"
"I am trying to parse formula in excel with 3 params: Formula, original cell address, destination cell address.
Example, with some rules:



Formula
Original  Address
Destination  Address
Result  Expected
Note




=(A2+B2)
C2
C3
=(A3+B3)



=(A2+B2)
C2
D2
=(B2+C2)
Increase column by 1 (C-&gt;D)


=(A2+$B$2)
C2
D10
=(B10+$B$2)
The $ expression


=(SheetA2!A2+B2)
C2
C3
=(SheetA2!A3+B3)
The Sheet name(SheetA2) is invariant


=IF(A2=A3,A4,A5)
A6
C6
=IF(C2=C3,C4,C5)
Replace All variant



I know some library in C#, python ... did it, but now I want to do it in java. I need algorithm to parse formula to token and replace it, or a library do it fast.
","There is a Java library Apache POI, which is able to do this.
This library provides formula evaluation and shifting cells. So it needs parsing formulas and adjusting formulas to new destination cells. To do this, it provides FormulaParser and FormulaRenderer.
Of course those classes are programmed to do their work in an existing workbook. But if only the adjustment of formula strings is needed, then an empty temporary dummy workbook can be used. I use XSSFWorkbook in my example as this is the representation of current *.xlsx, which provides greater row count and columns count than the older *.xls.
Complete example testing your given examples and one more:
import org.apache.poi.ss.formula.*;
import org.apache.poi.ss.formula.ptg.*;
import org.apache.poi.xssf.usermodel.XSSFEvaluationWorkbook;
import org.apache.poi.xssf.usermodel.XSSFWorkbook;

import org.apache.poi.ss.util.CellAddress;

public class ExcelGetAdjustedFormula {

 private static String getAdjustedFormula(String formula, String originalAddress, String destinationAddress) {
     
  CellAddress originalCellAddress = new CellAddress(originalAddress);
  CellAddress destinationCellAddress = new CellAddress(destinationAddress);
  int coldiff = destinationCellAddress.getColumn() - originalCellAddress.getColumn();
  int rowdiff = destinationCellAddress.getRow() - originalCellAddress.getRow();

  XSSFEvaluationWorkbook workbookWrapper = 
   XSSFEvaluationWorkbook.create(new XSSFWorkbook());
  Ptg[] ptgs = FormulaParser.parse(formula, workbookWrapper, FormulaType.CELL, 0);

  for (int i = 0; i &lt; ptgs.length; i++) {
   if (ptgs[i] instanceof RefPtgBase) { // base class for cell references
    RefPtgBase ref = (RefPtgBase) ptgs[i];
    if (ref.isColRelative())
     ref.setColumn(ref.getColumn() + coldiff);
    if (ref.isRowRelative())
     ref.setRow(ref.getRow() + rowdiff);
   }
   else if (ptgs[i] instanceof AreaPtgBase) { // base class for range references
    AreaPtgBase ref = (AreaPtgBase) ptgs[i];
    if (ref.isFirstColRelative())
     ref.setFirstColumn(ref.getFirstColumn() + coldiff);
    if (ref.isLastColRelative())
     ref.setLastColumn(ref.getLastColumn() + coldiff);
    if (ref.isFirstRowRelative())
     ref.setFirstRow(ref.getFirstRow() + rowdiff);
    if (ref.isLastRowRelative())
     ref.setLastRow(ref.getLastRow() + rowdiff);
   }
  }

  formula = FormulaRenderer.toFormulaString(workbookWrapper, ptgs);
  return formula;
 }

 public static void main(String[] args) {
     
  String[][] examples = new String[][]{
   new String[]{&quot;(A2+B2)&quot;, &quot;C2&quot;, &quot;C3&quot;},
   new String[]{&quot;(A2+B2)&quot;, &quot;C2&quot;, &quot;D2&quot;},
   new String[]{&quot;(A2+$B$2)&quot;, &quot;C2&quot;, &quot;D10&quot;},
   new String[]{&quot;(SheetA2!A2+B2)&quot;, &quot;C2&quot;, &quot;C3&quot;},
   new String[]{&quot;IF(A2=A3,A4,A5)&quot;, &quot;A6&quot;, &quot;C6&quot;},
   new String[]{&quot;VLOOKUP(A2,'Sheet Name'!$A$1:$G$1000,4,FALSE)&quot;, &quot;F2&quot;, &quot;F10&quot;}
  };
     
  for (String[] example : examples) {
   String formula = example[0];
   String originalAddress = example[1];
   String destinationAddress = example[2];
  
   String newformula = getAdjustedFormula(formula, originalAddress, destinationAddress);
   System.out.println(newformula);
  }
  
 }
}

"
"I don't want to append and I don't want to truncate existing data. I want to overwrite existing data. For example, the following code leaves the test.txt file containing &quot;hello&quot; but I want the file to contain &quot;hello6789&quot;.
try(
   FileWriter fw = new FileWriter(&quot;test.txt&quot;); ){
   fw.write(&quot;123456789&quot;);
}    
try(
   FileWriter fw = new FileWriter(&quot;test.txt&quot;); ){
   fw.write(&quot;hello&quot;);
}

Is it possible?
","I suggest using NIO.2 which was added in Java 7.
(Note that below code also uses try-with-resources.)
import java.io.BufferedWriter;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardOpenOption;

public class Proj2 {

    public static void main(String[] args) {
        Path path = Paths.get(&quot;test.txt&quot;);
        try (BufferedWriter bw = Files.newBufferedWriter(path,
                                                         StandardOpenOption.CREATE,
                                                         StandardOpenOption.WRITE)) {
            bw.write(&quot;123456789&quot;);
        }
        catch (IOException xIo) {
            xIo.printStackTrace();
        }
        try (BufferedWriter bw = Files.newBufferedWriter(path,
                                                         StandardOpenOption.CREATE,
                                                         StandardOpenOption.WRITE)) {
            bw.write(&quot;hello&quot;);
        }
        catch (IOException xIo) {
            xIo.printStackTrace();
        }
    }
}

Contents of file test.txt after running above code:
hello6789

Also refer to javadoc for [enum] StandardOpenOption.
"
"I have 3 classes the first one is Library Item this is the super class. The other two classes are Book and Movie. When I want to fill my table view I want to make sure the correct property is called when populating the table view. I know it is easier to just call the director and author the same for ease of use, but I want to get it working for learning purposes. I have left out packages and imports for relevance.
LibraryItem class
public abstract class LibraryItem {
    private int itemCode;
    private String title;
    private boolean availability;
    private int memberIdentifier;
    private LocalDate dateLent;

    protected LibraryItem(int itemCode, String title, boolean availability, int memberIdentifier, LocalDate dateLent) {
        this.itemCode = itemCode;
        this.title = title;
        this.availability = availability;
        this.memberIdentifier = memberIdentifier;
        this.dateLent = dateLent;
    }

    public int getItemCode() {
        return itemCode;
    }

    public String getTitle() {
        return title;
    }

    public boolean isAvailability() {
        return availability;
    }

    public void setAvailability(boolean availability) {
        this.availability = availability;
    }

    public int getMemberIdentifier() {
        return memberIdentifier;
    }

    public void setMemberIdentifier(int memberIdentifier) {
        this.memberIdentifier = memberIdentifier;
    }

    public LocalDate getDateLent() {
        return dateLent;
    }

    public void setDateLent(LocalDate dateLent) {
        this.dateLent = dateLent;
    }
}

Book class
public class Book extends LibraryItem {
    private String author;

    protected Book(int itemCode, String title, boolean isLent, int memberIdentifier, LocalDate dateLent, String author) {
        super(itemCode, title, isLent, memberIdentifier, dateLent);
        this.author = author;
    }
}

Movie class
public class Movie extends LibraryItem {
    private String director;

    protected Movie(int itemCode, String title, boolean isLent, int memberIdentifier, LocalDate dateLent, String director) {
        super(itemCode, title, isLent, memberIdentifier, dateLent);
        this.director = director;
    }
}

I was thinking maybe there is some kind of check I can do for each row implemented so the correct value will be given,
This was my attempt:
public class CollectionController implements Initializable {
    @FXML
    private TableView&lt;LibraryItem&gt; libraryItemsTable;
    @FXML
    private TableColumn&lt;LibraryItem, String&gt; itemCodeColumn;
    @FXML
    private TableColumn&lt;LibraryItem, String&gt;  availableColumn;
    @FXML
    private TableColumn&lt;LibraryItem, String&gt;  titleColumn;
    @FXML
    private TableColumn&lt;LibraryItem, String&gt;  authorDirectorColumn;
    private LibraryService libraryService = new LibraryService();

    @Override
    public void initialize(URL location, ResourceBundle resources) {
        initializeTableView();
    }

    private void initializeTableView() {
        List&lt;LibraryItem&gt; libraryItems = libraryService.getLibraryItems();

        itemCodeColumn.setCellValueFactory(new PropertyValueFactory&lt;&gt;(&quot;itemCode&quot;));
        availableColumn.setCellValueFactory(new PropertyValueFactory&lt;&gt;(&quot;availability&quot;));
        titleColumn.setCellValueFactory(new PropertyValueFactory&lt;&gt;(&quot;title&quot;));
        
        // implement here check for each new row
        if (checkIfBook(row))
            authorDirectorColumn.setCellValueFactory(new PropertyValueFactory&lt;&gt;(&quot;author&quot;));
        else
            authorDirectorColumn.setCellValueFactory(new PropertyValueFactory&lt;&gt;(&quot;director&quot;));
        //

        libraryItemsTable.getItems().addAll(libraryItems);
    }

","If you follow the advice here and avoid the use of PropertyValueFactory, the solution becomes reasonably clear:
titleColumn.setCellValueFactory(data -&gt; 
    new SimpleStringProperty(data.getValue().getTitle()));

authorDirectorColumn.setCellValueFactory(data -&gt; {
    LibraryItem item = data.getValue();
    if (item instanceof Book book) {
        return new SimpleStringProperty(book.getAuthor());
    } else if (item instanceof Movie movie) {
        return new SimpleStringProperty(movie.getProducer());
    } else {
        return null ;
    }
});

Here's a complete example (I simplified the model classes for brevity, but retained enough to demonstrate the point):
import javafx.application.Application;
import javafx.beans.property.SimpleStringProperty;
import javafx.scene.Scene;
import javafx.scene.control.TableColumn;
import javafx.scene.control.TableView;
import javafx.scene.layout.BorderPane;
import javafx.stage.Stage;

import java.io.IOException;

public class HelloApplication extends Application {
    @Override
    public void start(Stage stage) throws IOException {
        TableView&lt;LibraryItem&gt; table = new TableView&lt;&gt;();
        TableColumn&lt;LibraryItem, String&gt; titleColumn = new TableColumn&lt;&gt;(&quot;Title&quot;);
        TableColumn&lt;LibraryItem, String&gt; authorProducerColumn = new TableColumn&lt;&gt;(&quot;Author/Producer&quot;);
        table.getColumns().add(titleColumn);
        table.getColumns().add(authorProducerColumn);

        titleColumn.setCellValueFactory(data -&gt; new SimpleStringProperty(data.getValue().getTitle()));

        authorProducerColumn.setCellValueFactory(data -&gt; {
            LibraryItem item = data.getValue();
            if (item instanceof Book book) {
                return new SimpleStringProperty(book.getAuthor());
            } else if (item instanceof Movie movie) {
                return new SimpleStringProperty(movie.getProducer());
            } else return null ;
        });

        for (int i = 1 ; i &lt;= 10 ; i++) {
            Book book = new Book(&quot;Book &quot;+i, &quot;Author &quot;+i);
            Movie movie = new Movie(&quot;Movie &quot;+i, &quot;Producer &quot;+i);
            table.getItems().addAll(book, movie);
        }

        BorderPane root = new BorderPane(table);
        Scene scene = new Scene(root);
        stage.setScene(scene);
        stage.show();
    }

    public class LibraryItem {
        private String title ;
        public LibraryItem(String title) {
            this.title = title ;
        }

        public String getTitle() {
            return title;
        }

        public void setTitle(String title) {
            this.title = title;
        }
    }

    public class Movie extends LibraryItem {
        private String producer ;
        public Movie(String title, String producer) {
            super(title);
            this.producer = producer ;
        }

        public String getProducer() {
            return producer;
        }

        public void setProducer(String producer) {
            this.producer = producer;
        }
    }

    public class Book extends LibraryItem {
        private String author ;
        public Book(String title, String author) {
            super(title);
            this.author = author ;
        }

        public String getAuthor() {
            return author;
        }

        public void setAuthor(String author) {
            this.author = author;
        }
    }

    public static void main(String[] args) {
        launch();
    }
}


"
"There is a SpringBoot-based application running on a server which regularly inserts/updates records in a relational database.
The database connection is set up like this:
import org.springframework.context.annotation.Bean;
import org.springframework.boot.context.properties.ConfigurationProperties;
import javax.sql.DataSource;

[...]

@Configuration
@EnableConfigurationProperties
public class DbConfigClass {

    [...]

    @Bean(name = &quot;myDataSource&quot;)
    @ConfigurationProperties(prefix = &quot;com.mycompany.somedatabase&quot;)
    public DataSource dsSomeDataSource() {
        return DataSourceBuilder.create().build();
    }
    
    [...]
}

Sometimes the connection is interrupted in irregular, unpredictable intervals. Then, I get errors like this:
java.sql.SQLTransientConnectionException: HikariPool-1 - Connection is not available,
request timed out after 30001ms.

at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:694)

I need to make sure that my application can deal with such interruptions. To do so, I need to be able to reproduce this behavior locally.
How can I do it?
I looked at ToxyProxy which seems to do what I want.
However, I am wondering whether or not I can simulate such conditions with less effort, e. g. by using Mockito.spy to modify the data source so that it sometimes throws the exception above.
","Found an easy and seemingly good-enough solution:
First I created a class whose getConnection method throws an exception in every second call:
public class ShakyDataSource implements DataSource {
    private final DataSource ds;
    private AtomicLong getConnectionCallsCount = new AtomicLong(1);
    
    public ShakyDataSource(final DataSource ds) {
        this.ds = ds;
    }
    
    @Override
    public Connection getConnection() throws SQLException {
        final long newGetConnectionCallCount = getConnectionCallsCount.incrementAndGet();
        
        if ((newGetConnectionCallCount % 2) == 0) {
            throw new SQLException(&quot;Simulated connection failure&quot;);
        }
        else {
            return ds.getConnection();
        }
    }
    
    // All other methods of DataSource call corresponding methods of ds
}

Then I modified the configuration like this:
import org.springframework.context.annotation.Bean;
import org.springframework.boot.context.properties.ConfigurationProperties;
import javax.sql.DataSource;

[...]

@Configuration
@EnableConfigurationProperties
public class DbConfigClass {

    [...]

    @Bean(name = &quot;myDataSource&quot;)
    @ConfigurationProperties(prefix = &quot;com.mycompany.somedatabase&quot;)
    public DataSource dsSomeDataSource() {
        if (SIMULATE_SHAKY_CONNECTION) {
            LOGGER.error(&quot;Don't do this in production!&quot;);
            return new ShakyDataSource(DataSourceBuilder.create().build());
        }
        return DataSourceBuilder.create().build();
    }
    
    [...]
}

"
"I am new in unit testing and use JUnit in my Java (Spring Boot) app. I sometimes need to test update methods, but when I search on the web, there is not a proper example or suggestion. So, could you please clarify me how to test the following update method? I think this may require a different approach than testing void. I also thought that while testing first mocking the record and then update its field and then update. Finally retrieve the record again and compare the updated properties. But I think there may be more proper approach than this inexperienced one.
public PriceDTO update(UUID priceUuid, PriceRequest request) {
    Price price = priceRepository
                    .findByUuid(priceUuid)
                    .orElseThrow(() -&gt; new EntityNotFoundException(PRICE));

    mapRequestToEntity(request, price);
    Price updated = priceRepository.saveAndFlush(price);
    
    return new PriceDTO(updated);
}

private void mapRequestToEntity(PriceRequest request, Price entity) {
    entity.setPriceAmount(request.getPriceAmount());
    // set other props
}

","You would need to do something along the following lines:
public class ServiceTest {

    @Mock
    private PriceRepository priceRepository;

    (...)

    @Test
    public void shouldUpdatePrice() throws Exception {
        // Arrange
        UUID priceUuid = // build the Price UUID
        PriceRequest priceUpdateRequest = // build the Price update request
        Price originalPrice = // build the original Price  
        doReturn(originalPrice).when(this.priceRepository).findByUuid(isA(UUID.class));
        doAnswer(AdditionalAnswers.returnsFirstArg()).when(this.priceRepository).saveAndFlush(isA(Price.class));

        // Act
        PriceDTO updatedPrice = this.service.update(priceUuid, priceUpdateRequest);

        // Assert
        // here you need to assert that updatedPrice is as you expect according to originalPrice and priceUpdateRequest
    }
}

"
"Our JavaFX application is built with mvn clean javafx:jlink to create a standalone package for distribution. Now I need to include external resources (by that I mean config/content files in JSON that are not packaged into the application but reside outside in a freely accessible folder structure) into that bundle, preferably within the build process with maven.
So I would like to achieve the following:
Copy MyProject/res/* to MyProject/target/MyProject/res
Many solutions I've found use the maven resources plugin and I tried the following to no avail:
&lt;plugin&gt;
            &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt;
            &lt;version&gt;3.3.0&lt;/version&gt;
            &lt;executions&gt;
                &lt;execution&gt;
                    &lt;id&gt;copy-external-resources&lt;/id&gt;
                    &lt;phase&gt;generate-sources&lt;/phase&gt;
                    &lt;goals&gt;
                        &lt;goal&gt;copy-resources&lt;/goal&gt;
                    &lt;/goals&gt;
                    &lt;configuration&gt;
                        &lt;outputDirectory&gt;${basedir}/target/res&lt;/outputDirectory&gt;
                        &lt;resources&gt;
                            &lt;resource&gt;
                                &lt;directory&gt;res&lt;/directory&gt;
                            &lt;/resource&gt;
                        &lt;/resources&gt;
                    &lt;/configuration&gt;
                &lt;/execution&gt;
            &lt;/executions&gt;
        &lt;/plugin&gt;

I know the path itself (/target/res) isn't necessarily right since I want it in the MyProject folder, but either way, no folder is copied at all. What am I doing wrong here?
Please note that I'm not too familiar with Maven and it's phases and different stages.
This is how it's supposed to look like:

The red path is what's supposed to be copied to the target folder after build.
","As suggested in the comments, one strategy for this is to use a &quot;known location&quot; (typically somewhere in the hierarchy under the user's home directory) for the file. Keep a default version of the file as a resource in the application bundle. At startup, check if the file exists in the expected location, and if not, copy the contents from the resource.
Here is a complete example. Note this will generate a folder (.configApp) in your home directory, and a file (config.properties) inside that folder. For convenience, pressing &quot;OK&quot; in the dialog on exit will remove these artifacts. You probably don't want this in production, so press &quot;Cancel&quot; to see how it works, and run it again and press &quot;OK&quot; to keep your filesystem clean.
package org.jamesd.examples.config;

import javafx.application.Application;
import javafx.application.Platform;
import javafx.geometry.Insets;
import javafx.geometry.Pos;
import javafx.scene.Scene;
import javafx.scene.control.Alert;
import javafx.scene.control.Button;
import javafx.scene.control.ButtonType;
import javafx.scene.control.Label;
import javafx.scene.layout.VBox;
import javafx.stage.Stage;

import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.Properties;

public class ConfigExample extends Application {

    private static final Path CONFIG_LOCATION
            = Paths.get(System.getProperty(&quot;user.home&quot;), &quot;.configApp&quot;, &quot;config.properties&quot;);

    private Properties readConfig() throws IOException {
        if (!Files.exists(CONFIG_LOCATION)) {
            Files.createDirectories(CONFIG_LOCATION.getParent());
            Files.copy(getClass().getResourceAsStream(&quot;config.properties&quot;), CONFIG_LOCATION);
        }
        Properties config = new Properties();
        config.load(Files.newBufferedReader(CONFIG_LOCATION));
        return config ;
    }
    
    @Override
    public void start(Stage stage) throws IOException {
        Properties config = readConfig();
        Label greeting = new Label(config.getProperty(&quot;greeting&quot;));
        Button exit = new Button(&quot;Exit&quot;);
        exit.setOnAction(e -&gt; Platform.exit());
        VBox root = new VBox(10, greeting, exit);
        root.setAlignment(Pos.CENTER);
        root.setPadding(new Insets(20));
        Scene scene = new Scene(root);
        stage.setScene(scene);
        stage.show();
    }

    @Override
    // Provide option for cleanup. Probably don't want this for production.
    public void stop() {
        Alert confirmation = new Alert(Alert.AlertType.CONFIRMATION, &quot;Delete configuration after exit?&quot;);
        confirmation.showAndWait().ifPresent(response -&gt; {
            if (response == ButtonType.OK) {
                try {
                    Files.delete(CONFIG_LOCATION);
                    Files.delete(CONFIG_LOCATION.getParent());
                } catch (IOException exc) {
                    exc.printStackTrace();
                }
            }
        });
    }


    public static void main(String[] args) {
        launch();
    }
}

with config.properties under src/main/resources and in the same package as the application class:
greeting=Hello

"
"I want to use both Redis and Mongo with repository manner (I do not want to use spring cache annotations but repository methods).
I annotate the main class with the following annotations.
@EnableMongoRepositories(basePackageClass = PersistencyRepository.class)
@EnableRedisRepositories(basePackageClass = CacheRepository.class)
@SpringBootApplication

Repos
public interface PersistencyRepository extends CrudRepository&lt;Store, String&gt; {}

public interface CacheRepository extends MongoRepository&lt;Store, String&gt; {}

Now, I am getting the following error.
The bean &quot;cacheRepository&quot; defined in com.repository.CacheRepository defined in @EnableMongoRepositories declared on StoreApplication, could not be registered. A bean with that name has already been defined in com.repository.CacheRepository defined in @EnableRedisRepositories declared on StoreApplication and overriding is disabled. 

How can I use repos of differenet databases (mongo, redis)?
","You extended the wrong repository interface (MongoRepository) on CacheRepository try extending CrudRepository instead.
Also, your mongo and redis entities should be separated to different packages, usually I just went with com.my.company.entity.mongo and com.my.company.entity.redis for each.
After that, you need to update those Configuration annotations. A better package design, instead of putting all annotations on Main is putting them on a separate package, then putting those annotations there. This has an added benefit of clearly splitting each configurations for what they actually do
for example:
package com.your.company.configuration;

import com.your.company.configuration.properties.ApplicationProperties;
import com.your.company.entity.mongo.BaseDocument;
import com.your.company.entity.postgres.BaseEntity;
import com.your.company.entity.redis.BaseHash;
import com.your.company.repository.mongo.BaseMongoRepository;
import com.your.company.repository.postgres.BaseJpaRepository;
import com.your.company.repository.redis.BaseRedisRepository;
import org.springframework.boot.context.properties.EnableConfigurationProperties;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.data.jpa.repository.config.EnableJpaRepositories;
import org.springframework.data.mongodb.repository.config.EnableMongoRepositories;
import org.springframework.data.redis.repository.configuration.EnableRedisRepositories;
import org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder;
import org.springframework.security.crypto.password.PasswordEncoder;

@Configuration
@EnableConfigurationProperties(ApplicationProperties.class)
@EnableJpaRepositories(basePackageClasses = {BaseEntity.class, BaseJpaRepository.class})
@EnableMongoRepositories(basePackageClasses = {BaseDocument.class,
        BaseMongoRepository.class}, repositoryFactoryBeanClass = EnhancedMongoRepositoryFactoryBean.class)
@EnableRedisRepositories(basePackageClasses = {BaseHash.class, BaseRedisRepository.class})
public class BasicConfiguration {

    @Bean
    public PasswordEncoder passwordEncoder() {
        return new BCryptPasswordEncoder();
    }
}

The above is only an example, usually I would split them further into one class each within the same package with names that describes what they are actually configuring, for example: MongoConfiguration.java, JpaConfiguration.java, etc. Note if you decide to go with that design, you need the @Configuration annotation in each of the separate classes
"
"In the database, the column &quot;status&quot; is integer.
xml mybatis
&lt;resultMap id=&quot;TaskStatus&quot; type=&quot;ru....domain.Task$Status&quot;&gt;
            &lt;result typeHandler=&quot;org.apache.ibatis.type.EnumTypeHandler&quot;
                    property=&quot;id&quot; column=&quot;status&quot;/&gt;
&lt;/resultMap&gt;
    
&lt;select id=&quot;selectStatus&quot; resultMap=&quot;TaskStatus&quot;&gt;
            select id, status
            from task
            where id = #{id}
&lt;/select&gt;

my enum class
public class Task{
    
        @Getter
        @AllArgsConstructor
        public enum Status {
            CREATED(1),
            RUNNING(2),
            PAUSED(3),
            FINISHED(4),
            ARCHIVED(5),
            MODERATION_READY(6),
            MODERATING(7),
            REJECTED(8);
    
    
            private final Integer id;
        }
    ....
    }

I want to put a column in enum class.
Error
Error querying database.  Cause: org.apache.ibatis.executor.result.ResultMapException: Error attempting to get column 'status' from result set.  Cause: java.lang.IllegalArgumentException: No enum constant ru...domain.Task.Status.2
","The default EnumTypeHandler maps enum's name (e.g. &quot;CREATED&quot;, &quot;RUNNING&quot;), so the column type must be one of text types like VARCHAR [1].
As MyBatis knows nothing about the id property, you have to write a custom type handler.
Here is an example implementation.
import java.sql.CallableStatement;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Types;

import org.apache.ibatis.type.JdbcType;
import org.apache.ibatis.type.MappedTypes;
import org.apache.ibatis.type.TypeHandler;

@MappedTypes(Status.class)
public class StatusTypeHandler implements TypeHandler&lt;Status&gt; {
  @Override
  public void setParameter(PreparedStatement ps, 
       int i, Status parameter, JdbcType jdbcType) throws SQLException {
    if (parameter == null) {
      ps.setNull(i, Types.INTEGER);
    } else {
      ps.setInt(i, parameter.getId());
    }
  }

  @Override
  public Status getResult(ResultSet rs, String columnName) throws SQLException {
    return getStatus(rs.getInt(columnName));
  }

  @Override
  public Status getResult(ResultSet rs, int columnIndex) throws SQLException {
    return getStatus(rs.getInt(columnIndex));
  }

  @Override
  public Status getResult(CallableStatement cs, int columnIndex) throws SQLException {
    return getStatus(cs.getInt(columnIndex));
  }

  private static Status getStatus(int id) {
    if (id == 0) {
      return null;
    }
    for (Status status : Status.values()) {
      if (id == status.getId()) {
        return status;
      }
    }
    throw new IllegalArgumentException(&quot;Cannot convert &quot; + id + &quot; to Status&quot;);
  }
}

You should register this type handler globally.
Then it's unnecessary to specify typeHandler explicitly in most cases.

If you use mybatis-spring-boot, specifying mybatis.type-handlers-package in the config may be the easiest way to register type handlers globally.

If you use XML config, add the following.
&lt;typeHandlers&gt;
  &lt;typeHandler
    handler=&quot;xxx.yyy.StatusTypeHandler&quot; /&gt;
&lt;/typeHandlers&gt;




If the Status is the only enum in your project, you can stop reading.
But, what if there are many enums that have id property and you don't want to write a similar custom type handler for each of them?
If your enums implement a common interface like below, you can write a type handler that can map all of them.
public interface HasId {
  Integer getId();
}

Here is an example type handler implementation.
Note that it has a constructor that takes java.lang.Class as its argument.
import java.sql.CallableStatement;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Types;

import org.apache.ibatis.type.JdbcType;
import org.apache.ibatis.type.MappedTypes;
import org.apache.ibatis.type.TypeHandler;

@MappedTypes(HasId.class)
public class HasIdTypeHandler&lt;E extends Enum&lt;E&gt; &amp; HasId&gt; implements TypeHandler&lt;E&gt; {
  private Class&lt;E&gt; type;
  private final E[] enums;

  public HasIdTypeHandler(Class&lt;E&gt; type) {
    if (type == null)
      throw new IllegalArgumentException(&quot;Type argument cannot be null&quot;);
    this.type = type;
    this.enums = type.getEnumConstants();
    if (!type.isInterface() &amp;&amp; this.enums == null)
      throw new IllegalArgumentException(type.getSimpleName()
          + &quot; does not represent an enum type.&quot;);
  }

  @Override
  public void setParameter(PreparedStatement ps, 
      int i, E parameter, JdbcType jdbcType) throws SQLException {
    if (parameter == null) {
      ps.setNull(i, Types.INTEGER);
    } else {
      ps.setInt(i, parameter.getId());
    }
  }

  @Override
  public E getResult(ResultSet rs, String columnName) throws SQLException {
    return getEnum(rs.getInt(columnName));
  }

  @Override
  public E getResult(ResultSet rs, int columnIndex) throws SQLException {
    return getEnum(rs.getInt(columnIndex));
  }

  @Override
  public E getResult(CallableStatement cs, int columnIndex) throws SQLException {
    return getEnum(cs.getInt(columnIndex));
  }

  private E getEnum(int id) {
    if (id == 0) {
      return null;
    }
    for (E e : enums) {
      if (id == e.getId()) {
        return e;
      }
    }
    throw new IllegalArgumentException(&quot;Cannot convert &quot; +
      id + &quot; to &quot; + type.getSimpleName());
  }
}

Note that if you try to specify this type handler in a mapper, it might not work properly. There is a known issue.
[1] FYI, there is another built-in type handler for enums :EnumOrdinalTypeHandler  maps enum's ordinal.
"
"I'm trying to control a taskbar so I can show a progress of some long running task in the JavaFX application. For communicating with winapi I want to use the new Java FFM API, which should replace the JNI one day.
So far I was able successfully create instance of ITaskbarList3 instance, but I'm not able to call any method on it.
I'm using jextract to extract functions from winapi to make sure they are correctly mapped to API:
jextract --output target/generated-sources/jextract -t &quot;taskbar_test.gen&quot; -l :shell32 -l :Explorerframe -l :ole32 -I &quot;C:\Program Files (x86)\Windows Kits\10\Include\10.0.26100.0\shared&quot; -I &quot;C:\Program Files (x86)\Windows Kits\10\Include\10.0.26100.0\um&quot; -I &quot;C:\Program Files (x86)\Windows Kits\10\Include\10.0.26100.0\km&quot; -I &quot;C:\Program Files (x86)\Windows Kits\10\Include\10.0.26100.0\km\crt&quot; &quot;C:\Program Files (x86)\Windows Kits\10\Include\10.0.26100.0\um\ShObjIdl_core.h&quot;

In the code below, you can find complete application with my attempt to in the end call function SetProgressValue. My issue is that I'm not able to successfully call function HrInit which should be called to initialize the ITaskbarList.
package taskbar_test;
import com.sun.glass.ui.Window;
import javafx.application.Application;
import javafx.stage.Stage;
import taskbar_test.gen.CLSID;
import taskbar_test.gen.IID;
import taskbar_test.gen.ITaskbarList;
import taskbar_test.gen.ITaskbarList3;
import taskbar_test.gen.ITaskbarList3Vtbl;
import taskbar_test.gen.ITaskbarListVtbl;
import taskbar_test.gen.ShObjIdl_core_h;
import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.nio.charset.StandardCharsets;
import java.util.concurrent.Executors;
public class FxWinTaskbar extends Application {
     public static final String GUID_FORMAT = &quot;{%s}&quot;;
     // CLSID of ITaskbarList3
     public static final String CLSID_CONST = &quot;56FDF344-FD6D-11d0-958A-006097C9A090&quot;;
     // IID of ITaskbarList3
     public static final String IID_ITASKBAR_LIST = &quot;56FDF342-FD6D-11d0-958A-006097C9A090&quot;;
     public static final String IID_ITASKBAR_LIST_3 = &quot;EA1AFB91-9E28-4B86-90E9-9E9F8A5EEFAF&quot;;
     @Override
     public void start(Stage stage) throws Exception {
         var button = new javafx.scene.control.Button(&quot;Click Me&quot;);
         button.setOnAction(e -&gt; handleClick());
         var root = new javafx.scene.layout.StackPane(button);
         var scene = new javafx.scene.Scene(root, 300, 200);
         stage.setTitle(&quot;JavaFX Stage with Button&quot;);
         stage.setScene(scene);
         stage.show();
     }
    void handleClick() {
        long rawHandle = Window.getWindows().getFirst().getRawHandle();
        Executors.newSingleThreadExecutor().submit(() -&gt; {
            try (var arena = Arena.ofConfined()) {
                // 1. Initialize variables

                // https://learn.microsoft.com/en-us/windows/win32/api/combaseapi/nf-combaseapi-clsidfromstring#remarks
                // The CLSID format is {xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx}.
                var clsidString = arena.allocateFrom(GUID_FORMAT.formatted(CLSID_CONST), StandardCharsets.UTF_16LE);
                var iidITaskbarList = arena.allocateFrom(GUID_FORMAT.formatted(IID_ITASKBAR_LIST), StandardCharsets.UTF_16LE);
                var iidITaskbarList3 = arena.allocateFrom(GUID_FORMAT.formatted(IID_ITASKBAR_LIST_3), StandardCharsets.UTF_16LE);
                var clsid = CLSID.allocate(arena);
                var iidTaskbarList = IID.allocate(arena);
                var iidTaskbarList3 = IID.allocate(arena);
                var taskbarPtrToPtr = arena.allocate(ShObjIdl_core_h.C_POINTER);
                var taskbar3PtrToPtr = arena.allocate(ShObjIdl_core_h.C_POINTER);
                MemorySegment windowHandle = arena.allocate(ValueLayout.ADDRESS, rawHandle);

                // 2. Initialize COM
                int hr = ShObjIdl_core_h.CoInitializeEx(MemorySegment.NULL, ShObjIdl_core_h.COINIT_MULTITHREADED());
                if (hr != ShObjIdl_core_h.S_OK()) {
                    throw new RuntimeException(&quot;CoInitialize failed with error code: &quot; + hr);
                }

                // 3. Create CLSID and IIDs
                hr = ShObjIdl_core_h.CLSIDFromString(clsidString, clsid);
                if (hr != ShObjIdl_core_h.S_OK()) {
                    throw new RuntimeException(&quot;CLSIDFromString failed with error code: &quot; + hr);
                }

                hr = ShObjIdl_core_h.IIDFromString(iidITaskbarList, iidTaskbarList);
                if (hr != ShObjIdl_core_h.S_OK()) {
                    throw new RuntimeException(&quot;IIDFromString failed with error code: &quot; + hr);
                }

                hr = ShObjIdl_core_h.IIDFromString(iidITaskbarList3, iidTaskbarList3);
                if (hr != ShObjIdl_core_h.S_OK()) {
                    throw new RuntimeException(&quot;IIDFromString failed with error code: &quot; + hr);
                }

                // 4. Create instance of ITaskbarList
                hr = ShObjIdl_core_h.CoCreateInstance(clsid, MemorySegment.NULL, ShObjIdl_core_h.CLSCTX_ALL(), iidTaskbarList, taskbarPtrToPtr);
                if (hr != ShObjIdl_core_h.S_OK()) {
                    if (hr == ShObjIdl_core_h.REGDB_E_CLASSNOTREG()) {
                        System.out.println(&quot;COM class is not registered!&quot;);
                    }
                    throw new RuntimeException(&quot;CoCreateInstance failed with error code: &quot; + hr);
                }
                // CoCreateInstance returns pointer to pointer to ITaskbarList so here we obtain the &quot;inner&quot; pointer
                var taskbarPtr = taskbarPtrToPtr.get(ValueLayout.ADDRESS, 0);
                // Use reinterpret method to have access to the actual ITaskbarList instance
                var taskbarListInstance = ITaskbarList.reinterpret(taskbarPtr, arena, _ -&gt; {
                    System.out.println(&quot;Some cleanup...&quot;);
                });

                // 5. Obtain lpVtbl pointer from ITaskbarList
                MemorySegment taskbarListVtblPtr = ITaskbarList.lpVtbl(taskbarListInstance);
                // Use reinterpret method to have access to the actual ITaskbarListVtbl instance
                MemorySegment taskbarListVtbl = ITaskbarListVtbl.reinterpret(taskbarListVtblPtr, arena, _ -&gt; {
                    System.out.println(&quot;Some cleanup...&quot;);
                });

                // 6. Get pointer to function HrInit to initialize ITaskbarList
                // https://learn.microsoft.com/en-us/windows/win32/api/shobjidl_core/nf-shobjidl_core-itaskbarlist-hrinit
                // Initializes the taskbar list object. This method must be called before any other ITaskbarList methods can be called.
                MemorySegment functionHrInitPtr = ITaskbarListVtbl.HrInit(taskbarListVtbl);
                hr = ITaskbarListVtbl.HrInit.invoke(functionHrInitPtr, taskbarListVtbl);
                if (hr != ShObjIdl_core_h.S_OK()) {
                    throw new RuntimeException(&quot;HrInit failed with error code: &quot; + hr);
                }

                // 7. Create instance of ITaskbarList3
                hr = ShObjIdl_core_h.CoCreateInstance(clsid, MemorySegment.NULL, ShObjIdl_core_h.CLSCTX_ALL(), iidTaskbarList3, taskbar3PtrToPtr);
                if (hr != ShObjIdl_core_h.S_OK()) {
                    if (hr == ShObjIdl_core_h.REGDB_E_CLASSNOTREG()) {
                        System.out.println(&quot;COM class is not registered!&quot;);
                    }
                    throw new RuntimeException(&quot;CoCreateInstance failed with error code: &quot; + hr);
                }
                // 8. Obtain a pointer to the instance
                var taskbar3Ptr = taskbar3PtrToPtr.get(ValueLayout.ADDRESS, 0);
                // Use reinterpret method to have access to the actual ITaskbarList3 instance
                var taskbarList3Instance = ITaskbarList3.reinterpret(taskbar3Ptr, arena, _ -&gt; {
                    System.out.println(&quot;Some cleanup...&quot;);
                });

                // 9. Obtain lpVtbl pointer from ITaskbarList3
                MemorySegment taskbarList3VtblPtr = ITaskbarList3.lpVtbl(taskbarList3Instance);
                // Use reinterpret method to have access to the actual ITaskbarList3Vtbl instance
                MemorySegment taskbarList3Vtbl = ITaskbarList3Vtbl.reinterpret(taskbarList3VtblPtr, arena, _ -&gt; {
                    System.out.println(&quot;Some cleanup...&quot;);
                });

                // 10. Set progress state to indeterminate
                MemorySegment functionSetProgressStatePtr = ITaskbarList3Vtbl.SetProgressState(taskbarList3Vtbl);
                hr = ITaskbarList3Vtbl.SetProgressState.invoke(functionSetProgressStatePtr, taskbarList3Vtbl, windowHandle, ShObjIdl_core_h.TBPF_INDETERMINATE());
                if (hr != ShObjIdl_core_h.S_OK()) {
                    throw new RuntimeException(&quot;SetProgressState failed with error code: &quot; + hr);
                }

            } catch (Throwable ex) {
                ex.printStackTrace();

            } finally {
                ShObjIdl_core_h.CoUninitialize();
            }
        });
    }

    public static void main(String[] args) {
         launch(args);
     }
 }

I'm not able to call the function SetProgressState directly on interface ITaskbarList3 because generated sources does not have the ability to do so. Instead I have to manually obtain vtbl structure and call the function on this structure.
As you can see on the picture below, the address of vtblPtr and function for HrInit are completely off. Calling function HrInit will fail, because it is accesssing wrong memory.
Does anyone have idea what am I doing wrong?
Thank you.
Petr

Edit: I have applied suggestions from comments. Now, there is only one instance ITaskbarList3 created and all functions are called on it. I have also extended the code to simulate some progress to see if it can set the progress. The code seems to be running, but unfortunately the taskbar is still without any changes.
package taskbar_test;

import com.sun.glass.ui.Window;
import javafx.application.Application;
import javafx.stage.Stage;
import taskbar_test.gen.CLSID;
import taskbar_test.gen.IID;
import taskbar_test.gen.ITaskbarList3;
import taskbar_test.gen.ITaskbarList3Vtbl;
import taskbar_test.gen.ShObjIdl_core_h;

import java.lang.foreign.Arena;
import java.lang.foreign.MemorySegment;
import java.lang.foreign.ValueLayout;
import java.nio.charset.StandardCharsets;
import java.util.concurrent.Executors;

public class FxWinTaskbar extends Application {

    public static final String GUID_FORMAT = &quot;{%s}&quot;;

    // CLSID of ITaskbarList3
    public static final String CLSID_CONST = &quot;56FDF344-FD6D-11d0-958A-006097C9A090&quot;;
    // IID of ITaskbarList3
    public static final String IID_ITASKBAR_LIST_3 = &quot;EA1AFB91-9E28-4B86-90E9-9E9F8A5EEFAF&quot;;

    @Override
    public void start(Stage stage) throws Exception {
        var button = new javafx.scene.control.Button(&quot;Click Me&quot;);
        button.setOnAction(e -&gt; handleClick());

        var root = new javafx.scene.layout.StackPane(button);
        var scene = new javafx.scene.Scene(root, 300, 200);

        stage.setTitle(&quot;JavaFX Stage with Button&quot;);
        stage.setScene(scene);
        stage.show();
    }

    void handleClick() {
        long rawHandle = Window.getWindows().getFirst().getRawHandle();
        Executors.newSingleThreadExecutor().submit(() -&gt; {
            try (var arena = Arena.ofConfined()) {
                // 1. Initialize variables

                // https://learn.microsoft.com/en-us/windows/win32/api/combaseapi/nf-combaseapi-clsidfromstring#remarks
                // The CLSID format is {xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx}.
                var clsidString = arena.allocateFrom(GUID_FORMAT.formatted(CLSID_CONST), StandardCharsets.UTF_16LE);
                var iidITaskbarList3 = arena.allocateFrom(GUID_FORMAT.formatted(IID_ITASKBAR_LIST_3), StandardCharsets.UTF_16LE);
                var clsid = CLSID.allocate(arena);
                var iidTaskbarList3 = IID.allocate(arena);
                var taskbar3PtrToPtr = arena.allocate(ShObjIdl_core_h.C_POINTER);
                MemorySegment windowHandle = arena.allocate(ValueLayout.ADDRESS, rawHandle);

                // 2. Initialize COM
                int hr = ShObjIdl_core_h.CoInitializeEx(MemorySegment.NULL, ShObjIdl_core_h.COINIT_MULTITHREADED());
                if (hr != ShObjIdl_core_h.S_OK()) {
                    throw new RuntimeException(&quot;CoInitialize failed with error code: &quot; + hr);
                }

                // 3. Create CLSID and IIDs
                hr = ShObjIdl_core_h.CLSIDFromString(clsidString, clsid);
                if (hr != ShObjIdl_core_h.S_OK()) {
                    throw new RuntimeException(&quot;CLSIDFromString failed with error code: &quot; + hr);
                }

                hr = ShObjIdl_core_h.IIDFromString(iidITaskbarList3, iidTaskbarList3);
                if (hr != ShObjIdl_core_h.S_OK()) {
                    throw new RuntimeException(&quot;IIDFromString failed with error code: &quot; + hr);
                }

                // 4. Create instance of ITaskbarList3
                hr = ShObjIdl_core_h.CoCreateInstance(clsid, MemorySegment.NULL, ShObjIdl_core_h.CLSCTX_ALL(), iidTaskbarList3, taskbar3PtrToPtr);
                if (hr != ShObjIdl_core_h.S_OK()) {
                    if (hr == ShObjIdl_core_h.REGDB_E_CLASSNOTREG()) {
                        System.out.println(&quot;COM class is not registered!&quot;);
                    }
                    throw new RuntimeException(&quot;CoCreateInstance failed with error code: &quot; + hr);
                }
                // 5. Obtain a pointer to the instance
                var taskbar3Ptr = taskbar3PtrToPtr.get(ValueLayout.ADDRESS, 0);
                // Use reinterpret method to have access to the actual ITaskbarList3 instance
                var taskbarList3Instance = taskbar3Ptr.reinterpret(ITaskbarList3.sizeof());

                // 6. Obtain lpVtbl pointer from ITaskbarList3
                MemorySegment taskbarList3VtblPtr = ITaskbarList3.lpVtbl(taskbarList3Instance);
                // Use reinterpret method to have access to the actual ITaskbarList3Vtbl instance
                MemorySegment taskbarList3Vtbl = taskbarList3VtblPtr.reinterpret(ITaskbarList3Vtbl.sizeof());

                // https://learn.microsoft.com/en-us/windows/win32/api/shobjidl_core/nf-shobjidl_core-itaskbarlist-hrinit
                // Initializes the taskbar list object. This method must be called before any other ITaskbarList methods can be called.
                MemorySegment functionHrInitPtr = ITaskbarList3Vtbl.HrInit(taskbarList3Vtbl);
                hr = ITaskbarList3Vtbl.HrInit.invoke(functionHrInitPtr, taskbarList3Instance);
                if (hr != ShObjIdl_core_h.S_OK()) {
                    throw new RuntimeException(&quot;HrInit failed with error code: &quot; + hr);
                }

                // 7. Set progress state to indeterminate
                MemorySegment functionSetProgressStatePtr = ITaskbarList3Vtbl.SetProgressState(taskbarList3Vtbl);
                hr = ITaskbarList3Vtbl.SetProgressState.invoke(functionSetProgressStatePtr, taskbarList3Instance, windowHandle, ShObjIdl_core_h.TBPF_INDETERMINATE());
                if (hr != ShObjIdl_core_h.S_OK()) {
                    throw new RuntimeException(&quot;SetProgressState failed with error code: &quot; + hr);
                }

                // 8. Simulate some progress
                for (int i = 0; i &lt; 100; i+=20) {
                    System.out.println(&quot;Progress is: &quot; + i);
                    MemorySegment functionSetProgressValuePtr = ITaskbarList3Vtbl.SetProgressValue(taskbarList3Vtbl);
                    hr = ITaskbarList3Vtbl.SetProgressValue.invoke(functionSetProgressValuePtr, taskbarList3Instance, windowHandle, i, 100);
                    if (hr != ShObjIdl_core_h.S_OK()) {
                        throw new RuntimeException(&quot;SetProgressValue failed with error code: &quot; + hr);
                    }
                    Thread.sleep(500);

                }

                // 9. Reset progress state
                hr = ITaskbarList3Vtbl.SetProgressState.invoke(functionSetProgressStatePtr, taskbarList3Instance, windowHandle, ShObjIdl_core_h.TBPF_INDETERMINATE());
                if (hr != ShObjIdl_core_h.S_OK()) {
                    throw new RuntimeException(&quot;SetProgressState failed with error code: &quot; + hr);
                }

            } catch (Throwable ex) {
                ex.printStackTrace();

            } finally {
                ShObjIdl_core_h.CoUninitialize();
            }
        });
    }

    public static void main(String[] args) {
        launch(args);
    }
}


","The code changes you have edited get you much closer, firstly by calling each callback with vtable+instance.  The jextract generated code gives calls to lookup each Method in the vtable of each OLE interface IXXX. The MemorySegment must be reinterpreted to match the exact memory sizes or the bounds checking of MemorySegment will fail:
// instance is CoCreateInstance return value
MemorySegment instance = pointer.get(ValueLayout.ADDRESS, 0); 

// order of Method invoke() is vTable, instance, ... params
// IXXXVtbl.Method.invoke(IXXXVtbl.Method(IXXX.lpVtbl(instance)), instance, ...)
// Interpret correct memory bounds:
MemorySegment iUnknown = instance.reinterpret(IUnknown.sizeof());
MemorySegment vtabXXX = IUnknown.lpVtbl(iUnknown ).reinterpret(IXXXVtbl.sizeof());
IXXXVtbl.Method.invoke(vtabXXX, instance, ...)

Remy Lebeau's comment eliminates the unnecessary CoCreateInstance.
The last issue is that you have not assigned hWnd correctly, it can be resolved as an address using:
// Wrong: allocates new address containing hWnd
// MemorySegment windowHandle = arena.allocate(ValueLayout.ADDRESS, rawHandle);
// hWnd is address:
MemorySegment windowHandle = MemorySegment.ofAddress(rawHandle);

I could not run your JavaFX example, but packaged the foreign memory calls to a new method and tested with a hWnd of a JFrame - deliberately not tidying your code here:
static void updateTaskBar(long rawHandle) throws InterruptedException {
    try (var arena = Arena.ofConfined()) {
        // 1. Initialize variables

        // https://learn.microsoft.com/en-us/windows/win32/api/combaseapi/nf-combaseapi-clsidfromstring#remarks
        // The CLSID format is {xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx}.
        var clsidString = arena.allocateFrom(GUID_FORMAT.formatted(CLSID_CONST), StandardCharsets.UTF_16LE);
        var iidITaskbarList3 = arena.allocateFrom(GUID_FORMAT.formatted(IID_ITASKBAR_LIST_3), StandardCharsets.UTF_16LE);
        var clsid = CLSID.allocate(arena);
        var iidTaskbarList3 = IID.allocate(arena);
        var taskbar3PtrToPtr = arena.allocate(Win_h.C_POINTER);
        // FiXED:
        // MemorySegment windowHandle = arena.allocate(ValueLayout.ADDRESS, rawHandle);
        MemorySegment windowHandle = MemorySegment.ofAddress(rawHandle);

        // 2. Initialize COM
        int hr = Win_h.CoInitializeEx(MemorySegment.NULL, Win_h.COINIT_MULTITHREADED());
        if (hr != Win_h.S_OK()) {
            throw new RuntimeException(&quot;CoInitialize failed with error code: &quot; + hr);
        }

        // 3. Create CLSID and IIDs
        hr = Win_h.CLSIDFromString(clsidString, clsid);
        if (hr != Win_h.S_OK()) {
            throw new RuntimeException(&quot;CLSIDFromString failed with error code: &quot; + hr);
        }

        hr = Win_h.IIDFromString(iidITaskbarList3, iidTaskbarList3);
        if (hr != Win_h.S_OK()) {
            throw new RuntimeException(&quot;IIDFromString failed with error code: &quot; + hr);
        }

        // 4. Create instance of ITaskbarList3
        hr = Win_h.CoCreateInstance(clsid, MemorySegment.NULL, Win_h.CLSCTX_ALL(), iidTaskbarList3, taskbar3PtrToPtr);
        if (hr != Win_h.S_OK()) {
            if (hr == Win_h.REGDB_E_CLASSNOTREG()) {
                System.out.println(&quot;COM class is not registered!&quot;);
            }
            throw new RuntimeException(&quot;CoCreateInstance failed with error code: &quot; + hr);
        }
        // 5. Obtain a pointer to the instance
        var taskbar3Ptr = taskbar3PtrToPtr.get(ValueLayout.ADDRESS, 0);
        // Use reinterpret method to have access to the actual ITaskbarList3 instance
        var taskbarList3Instance = taskbar3Ptr.reinterpret(ITaskbarList3.sizeof());

        // 6. Obtain lpVtbl pointer from ITaskbarList3
        MemorySegment taskbarList3VtblPtr = ITaskbarList3.lpVtbl(taskbarList3Instance);
        // Use reinterpret method to have access to the actual ITaskbarList3Vtbl instance
        MemorySegment taskbarList3Vtbl = taskbarList3VtblPtr.reinterpret(ITaskbarList3Vtbl.sizeof());

        // https://learn.microsoft.com/en-us/windows/win32/api/shobjidl_core/nf-shobjidl_core-itaskbarlist-hrinit
        // Initializes the taskbar list object. This method must be called before any other ITaskbarList methods can be called.
        MemorySegment functionHrInitPtr = ITaskbarList3Vtbl.HrInit(taskbarList3Vtbl);
        hr = ITaskbarList3Vtbl.HrInit.invoke(functionHrInitPtr, taskbarList3Instance);
        if (hr != Win_h.S_OK()) {
            throw new RuntimeException(&quot;HrInit failed with error code: &quot; + hr);
        }

        // 7. Set progress state to indeterminate
        MemorySegment functionSetProgressStatePtr = ITaskbarList3Vtbl.SetProgressState(taskbarList3Vtbl);
        hr = ITaskbarList3Vtbl.SetProgressState.invoke(functionSetProgressStatePtr, taskbarList3Instance, windowHandle, Win_h.TBPF_INDETERMINATE());
        if (hr != Win_h.S_OK()) {
            throw new RuntimeException(&quot;SetProgressState failed with error code: &quot; + hr);
        }

        // 8. Simulate some progress
        int max = 100;
        for (int i = 0; i &lt; max; i++) {
            System.out.println(windowHandle+ &quot; SetProgressValue &quot;+i);
            MemorySegment functionSetProgressValuePtr = ITaskbarList3Vtbl.SetProgressValue(taskbarList3Vtbl);
            hr = ITaskbarList3Vtbl.SetProgressValue.invoke(functionSetProgressValuePtr, taskbarList3Instance, windowHandle, i, max);
            if (hr != Win_h.S_OK()) {
                throw new RuntimeException(&quot;SetProgressValue failed with error code: &quot; + hr);
            }
            Thread.sleep(500);
        }
        // 9. Reset progress state
        hr = ITaskbarList3Vtbl.SetProgressState.invoke(functionSetProgressStatePtr, taskbarList3Instance, windowHandle, Win_h.TBPF_INDETERMINATE());
        if (hr != Win_h.S_OK()) {
            throw new RuntimeException(&quot;SetProgressState failed with error code: &quot; + hr);
        }
    } finally {
        Win_h.CoUninitialize();
    }
}

"
"I have already succeeded with this operation with images, but I cannot do it with other type of file, in my case I try to insert a database.
Here is an example of the code for the images:
 if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.Q){
        try {
            try {
                pictures = assetManager.list(&quot;photos/dataset1&quot;);
            } catch (IOException e) {
                Log.e(&quot;tag&quot;, &quot;Failed to get asset file list.&quot;, e);
            }
            if (pictures != null) {
                for (String filename : pictures) {
                    InputStream in;
                    OutputStream out;
                    InputStream inputStream = assetManager.open(&quot;photos/dataset1/&quot;+filename);
                    Bitmap bitmap = BitmapFactory.decodeStream(inputStream);
                    saveImageToGallery(bitmap);
                }
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

This method below works for the images :
public void saveImageToGallery(Bitmap bitmap) {
    OutputStream outputStream;
    Context myContext = requireContext();
    try {
        if(Build.VERSION.SDK_INT &gt;=Build.VERSION_CODES.Q){
            ContentResolver contentResolver = requireContext().getContentResolver();
            ContentValues contentValues = new ContentValues();
            contentValues.put(MediaStore.MediaColumns.DISPLAY_NAME,&quot;Image_&quot;+&quot;.jpg&quot;);
            contentValues.put(MediaStore.MediaColumns.RELATIVE_PATH, Environment.DIRECTORY_PICTURES);
            Uri imageUri = contentResolver.insert(MediaStore.Images.Media.EXTERNAL_CONTENT_URI, contentValues);
            outputStream = contentResolver.openOutputStream(Objects.requireNonNull(imageUri));
            bitmap.compress(Bitmap.CompressFormat.JPEG,100, outputStream);
            Objects.requireNonNull(outputStream);

        }
    }catch (FileNotFoundException e) {

        e.printStackTrace();
    }
}

and there my try for the other type of file :
        AssetManager assetManager = Objects.requireNonNull(requireContext()).getAssets();
    Context myContext = requireContext();
    //Essential for creating the external storage directory for the first launch
    myContext.getExternalFilesDir(null);
    File databasesFolder = new File(myContext.getExternalFilesDir(null).getParent(), &quot;com.mydb.orca/databases&quot;);
    databasesFolder.mkdirs();

 if (files!= null) {
        for (String filename : files) {
            InputStream in;
            OutputStream out;
            try {
                in = assetManager.open(&quot;database/test/&quot; + filename);
                File outFile = new File(databasesFolder, filename);
                out = new FileOutputStream(outFile);
                copyFile(in, out);
                in.close();
                out.flush();
                out.close();
            } catch (IOException e) {
                Log.e(&quot;tag&quot;, &quot;Failed to copy asset file: &quot; + filename, e);
            }
        }
    } else {
        Log.e(&quot;Error NPE&quot;, &quot;files is null&quot;);
    }



    private void copyFile(InputStream in, OutputStream out) throws IOException {
    byte[] buffer = new byte[1024];
    int read;
    while ((read = in.read(buffer)) != -1) {
        out.write(buffer, 0, read);
    }
}

This code above is not working, I mean, I don't get any errors or the desired result. I want something like this or a function similary as the function for my images but for any type of file.
When I run my application I have no error however nothing happens
","I finally find a solution, I pretty sure it's not the best way but it work.
I give me access to all files acccess by this way :
if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.R){
  try {
        Intent intentFiles = new Intent();
        intentFiles.setAction(Settings.ACTION_MANAGE_ALL_FILES_ACCESS_PERMISSION);
        Uri uriFiles = Uri.fromParts(&quot;package&quot;, myContext.getPackageName(), null);
        intentFiles.setData(uriFiles);
        myContext.startActivity(intentFiles);
       } catch (Exception e)
       {
        Intent intentFiles = new Intent();
        intentFiles.setAction(Settings.ACTION_MANAGE_ALL_FILES_ACCESS_PERMISSION);
        myContext.startActivity(intentFiles);
       }

add this line to your manifest:
    &lt;uses-permission android:name=&quot;android.permission.MANAGE_EXTERNAL_STORAGE&quot; /&gt;

after that, this code below work :
AssetManager assetManager = Objects.requireNonNull(requireContext()).getAssets();
Context myContext = requireContext();
//Essential for creating the external storage directory for the first launch
myContext.getExternalFilesDir(null);
File databasesFolder = new File(myContext.getExternalFilesDir(null).getParent(), &quot;com.mydb.orca/databases&quot;);
databasesFolder.mkdirs();

 if (files!= null) {
    for (String filename : files) {
        InputStream in;
        OutputStream out;
        try {
            in = assetManager.open(&quot;database/test/&quot; + filename);
            File outFile = new File(databasesFolder, filename);
            out = new FileOutputStream(outFile);
            copyFile(in, out);
            in.close();
            out.flush();
            out.close();
        } catch (IOException e) {
            Log.e(&quot;tag&quot;, &quot;Failed to copy asset file: &quot; + filename, e);
        }
    }
} else {
    Log.e(&quot;Error NPE&quot;, &quot;files is null&quot;);
}



private void copyFile(InputStream in, OutputStream out) throws IOException {
byte[] buffer = new byte[1024];
int read;
while ((read = in.read(buffer)) != -1) {
    out.write(buffer, 0, read);
}

}
if someone have a best solution it should be nice
I tested on android 11 and it work
"
"I came across huge performance difference between adding 1st and 2nd item into a collection(tried ArrayList and HashSet), but I cannot explain why. Have searched but didn't find any answer.
public class Main {

    public static void main(String[] args) {
        // also tried HashSet
        // also tried new ArrayList&lt;&gt;(2)
        ArrayList&lt;String&gt; collection = new ArrayList&lt;&gt;();
        long t1 = System.nanoTime();
        collection.add(&quot;a&quot;);
        long t2 = System.nanoTime();
        collection.add(&quot;b&quot;);
        long t3 = System.nanoTime();
        System.out.println(String.valueOf(t2 - t1) + &quot;\n&quot;
                + String.valueOf(t3 - t2));
        //typical output:
        //4399
        //1201
    }
}

Some guess:

because collection is lazily initialzed when adding 1st item?
or I used the wrong way to measure performance?
or related to how jvm works(which is beyond my knowledge)?

Environment: jdk11, win10, intellij.
","This is because Of lazily initialized. when you are running this line
ArrayList&lt;String&gt; collection = new ArrayList&lt;&gt;();

it is only holding a reference to the list but the actual memory allocation does not happen for that list. But when you are adding the very first element to the collection then it is first allocating memory for the next 10 elements of the  List (10 is the default size of array list) after that adding the first value.
Results of the next 9 elements will take less time to insert but again for the 11th element, it will take more time than previous.
    public static void main(String[] args) {

    ArrayList&lt;String&gt; collection = new ArrayList&lt;&gt;();
    
    for (int i = 0; i &lt; 12; i++) {
          long t1 = System.nanoTime();
          collection.add(&quot;a&quot;);
          long t2 = System.nanoTime();
          System.out.println(&quot;Index : &quot;+ (i+1) +&quot;: Time: &quot;+ String.valueOf(t2 - t1));
    }
    /** Output:
     *  Index : 1: Time: 6800
        Index : 2: Time: 800
        Index : 3: Time: 500
        Index : 4: Time: 700
        Index : 5: Time: 600
        Index : 6: Time: 500
        Index : 7: Time: 600
        Index : 8: Time: 600
        Index : 9: Time: 500
        Index : 10: Time: 500
        Index : 11: Time: 2800
        Index : 12: Time: 500
     */

}

"
"There are a lot of old questions regarding the Java Windows/Linux scaling topic with no clear answer. Does some Swing expert know of any updates, for example on Nimbus?
I have coded a really nice Swing application on a computer with 1920x1080 pixel screen. Today I saw the application on a high resolution screen and the app was tiny.
I do not know how to fix this problem. I googled a lot, but could not find a good answer. What I found is JEP 263 https://bugs.openjdk.org/browse/JDK-8055212
It says the issue is resolved. But not how to fix the code?
This is the Nimbus I use:
NimbusLookAndFeel nimbus = new NimbusLookAndFeel();
     UIManager.setLookAndFeel(nimbus);
     UIManager
           .put(&quot;control&quot;, Color.WHITE);
     UIManager.put(&quot;nimbusBlueGrey&quot;, ApplicationColors.getLightGrayGold());
     UIManager.put(&quot;nimbusBase&quot;, ApplicationColors.getDarkGold());
     UIManager.put(&quot;textForeground&quot;, Color.BLACK);
     UIManager.put(&quot;nimbusFocus&quot;, ApplicationColors.getSunflowerYellow());
     UIManager
           .put(&quot;ToolBar:Button.contentMargins&quot;, new Insets(5, 15, 5, 15));
     UIManager
           .put(&quot;TextField.background&quot;, ApplicationColors.getLightYellow());
     UIManager.put(&quot;ComboBox.forceOpaque&quot;, false);
     UIManager.put(&quot;TitledBorder.border&quot;, new Insets(10, 10, 10, 10));
     UIManager.put(&quot;TitledBorder.position&quot;, TitledBorder.ABOVE_BOTTOM);
     UIManager.put(&quot;TitledBorder.font&quot;, ApplicationFonts.getGermanFont(16F));
     UIManager.put(&quot;TitledBorder.titleColor&quot;, Color.GRAY);
     UIManager.put(&quot;Table.opaque&quot;, false);
     UIManager.put(&quot;List.opaque&quot;, false);
     UIManager.put(&quot;Table.cellRenderer&quot;, false);
     UIManager.put(&quot;OptionPane.buttonFont&quot;, ApplicationFonts.getGermanFont(16F));

     UIManager.put(&quot;OptionPane.cancelButtonText&quot;, translator.realisticTranslate(Translation.ABBRECHEN));
     UIManager.put(&quot;OptionPane.yesButtonText&quot;, translator.realisticTranslate(Translation.JA));
     UIManager.put(&quot;OptionPane.noButtonText&quot;, translator.realisticTranslate(Translation.NEIN));
     UIManager.put(&quot;OptionPane.titleText&quot;, translator.realisticTranslate(Translation.BILD_LOESCHEN));
     
     UIManager.put(&quot;FileChooser.openButtonText&quot;, translator.realisticTranslate(Translation.OEFFNEN));
     UIManager.put(&quot;FileChooser.cancelButtonText&quot;, translator.realisticTranslate(Translation.ABBRECHEN));
     UIManager.put(&quot;FileChooser.saveButtonText&quot;, translator.realisticTranslate(Translation.SPEICHERN));
     UIManager.put(&quot;FileChooser.cancelButtonToolTipText&quot;, translator.realisticTranslate(Translation.ABBRECHEN_DER_AUSWAHL));
     UIManager
           .put(&quot;FileChooser.saveButtonToolTipText&quot;,
                 translator.realisticTranslate(Translation.AUSGEWAEHLTE_DATEI_SPEICHERN));
     UIManager
           .put(&quot;FileChooser.openButtonToolTipText&quot;,
                 &quot;AusgewÃ¤hlte Datei Ã¶ffnen&quot;);
     UIManager.put(&quot;FileChooser.upFolderToolTipText&quot;, &quot;Eine Ebene hÃ¶her&quot;);
     UIManager.put(&quot;FileChooser.homeFolderToolTipText&quot;, &quot;Home&quot;);
     UIManager
           .put(&quot;FileChooser.newFolderToolTipText&quot;,
                 &quot;Neuen Ordner erstellen&quot;);
     UIManager.put(&quot;FileChooser.listViewButtonToolTipText&quot;, &quot;Liste&quot;);
     UIManager.put(&quot;FileChooser.detailsViewButtonToolTipText&quot;, &quot;Details&quot;);
     UIManager.put(&quot;FileChooser.lookInLabelText&quot;, &quot;Suchen in:&quot;);
     UIManager.put(&quot;FileChooser.fileNameLabelText&quot;, &quot;Dateiname:&quot;);
     UIManager.put(&quot;FileChooser.filesOfTypeLabelText&quot;, &quot;Dateityp:&quot;);
     UIManager
           .put(&quot;FileChooser.acceptAllFileFilterText&quot;,
                 &quot;Alle Dateien (*.*)&quot;);
     UIManager.put(&quot;FileChooser.folderNameLabelText&quot;, &quot;Ordnername:&quot;);
     UIManager.put(&quot;FileChooser.openDialogTitleText&quot;, translator.realisticTranslate(Translation.OEFFNEN));
     UIManager.put(&quot;FileChooser.saveDialogTitleText&quot;, translator.realisticTranslate(Translation.SPEICHERN));
     UIManager.put(&quot;OptionPane.background&quot;, ApplicationColors.getWhite());

How to go about scaling on high DPI Windows/Linux screens?
UPDATE
I found this on the internet:
The Per-monitor DPI-aware value means the following:
true - JRE-managed HiDPI
false - IDE-managed HiDPI

If you need to test IDE with scale 1.0 there're two options:
In JRE-managed HiDPI mode:
-Dsun.java2d.uiScale.enabled=true
-Dsun.java2d.uiScale=1.0

In IDE-managed HiDPI mode:
-Dsun.java2d.uiScale.enabled=false
-Dide.ui.scale=1.0

I will test this and report as soon as possible.
UPDATE
The minimal app example would be to much code, since I use my own Layout Managers extended from LayoutManager2. Of course I set sizes on the UI. But it still should scale.
UPDATE
If you want to see the scaling problem, you can download the software Cerebrummi for free from heise.de/download
software download
The software Cerebrummi needs Java 21 jdk to run.
The software Cerebrummi can be set to display in English if you click on the flag in the top row and choose English.
UPDATE
I followed Holgers advice and tested a small example with some features from my large software and IT DID SCALE on high resolution screen. So it is something in my Software that I have to find.
UPDATE
I found the offending code:
public void paintComponent(Graphics g)
   {
      super.paintComponent(g);
      if (ApplicationImages.getImage() != null)
      {
         float factorWidth = getParent().getWidth() / 1280F;
         float factorHeight = getParent().getHeight() / 859F;
         if (factorWidth &lt; factorHeight)
         {
            int width = (int) (1280F * factorHeight);
            int x = getParent().getWidth() / 2 - width / 2;
            g.drawImage(
              
ApplicationImages.getImage().getScaledInstance(width,
                    getParent().getHeight(), 
BufferedImage.SCALE_SMOOTH),
              x, 0, this);
         }
         else
         {
            int height = (int) (859F * factorWidth);
            int y = getParent().getHeight() / 2 - height / 2;
        
g.drawImage(ApplicationImages.getImage().getScaledInstance(
              getParent().getWidth(), height, 
BufferedImage.SCALE_SMOOTH),
              0, y, this);
        }
     }
  }

It is the large image in the background of the first screenshot!!! How to fix the code?



","Thank you very, very much to @Holger.
Now I learned:
Do NOT set the System scaling options!!!
The offending code could be fixed with:
public void paintComponent(Graphics g)
   {
      super.paintComponent(g);
      if (ApplicationImages.getImage() != null)
      {
         float factorWidth = 1536 / 1280F; // here is the fix
         float factorHeight = 960 / 859F; // here is the fix
         if (factorWidth &lt; factorHeight)
         {
            int width = (int) (1280F * factorHeight);
            int x = getParent().getWidth() / 2 - width / 2;
            g.drawImage(
                  ApplicationImages.getImage().getScaledInstance(width,
                    getParent().getHeight(), BufferedImage.SCALE_SMOOTH),
              x, 0, this);
         }
         else
         {
            int height = (int) (859F * factorWidth);
            int y = getParent().getHeight() / 2 - height / 2;
            g.drawImage(ApplicationImages.getImage().getScaledInstance(
              getParent().getWidth(), height, BufferedImage.SCALE_SMOOTH),
              0, y, this);
         }
      }
   }

"
"I wanted to try power of virtual threads in Java in a simple application which consists of many tasks. Each task executes a query agains a database which takes around 10 seconds.
My expectation was that the queries are executed almost at the same time because the significant part of task is basically waiting for the response.
But it doesn't work like that. Unfortunately, I am missing probably something.
In order to execute the tasks, I am using:
ExecutorService executorService = Executors.newVirtualThreadPerTaskExecutor()

the tasks are executed in a the following way:
StopWatch stopWatch = StopWatch.createStarted();
int numberOfTasks = 10;
List&lt;? extends Future&lt;String&gt;&gt; futures;
try(ExecutorService executorService = Executors.newVirtualThreadPerTaskExecutor()) {
     futures = IntStream.range(1, numberOfTasks + 1).mapToObj(i -&gt; new Task(i)).map(executorService::submit).toList();
}
        
for(Future&lt;String&gt; future: futures) {
            future.get();
}
stopWatch.stop();
System.out.println(format(&quot;The total time of execution was: {0} ms&quot;, stopWatch.getTime(TimeUnit.MILLISECONDS)));

The Task.call() method looks like this:
    @Override
    public String call() {
        System.out.println(format(&quot;Task: {0} started&quot;, taskId));
        StopWatch stopWatch = StopWatch.createStarted();
        Connection connection = null;
        String result = null;
        try {
            connection = DriverManager.getConnection(&quot;jdbc:mysql://localhost/sakila?user=sakila&amp;password=sakila&quot;);
            System.out.println(format(&quot;Task: {0} connection established&quot;, taskId));
            var statement = connection.createStatement();
            System.out.println(format(&quot;Task: {0} executes SQL statement&quot;, taskId));
            ResultSet resultSet = statement.executeQuery(&quot;SELECT hello_world() AS output&quot;);
            while (resultSet.next()) {
                result = resultSet.getString(&quot;output&quot;);
            }
            statement.close();
        } catch (SQLException e) {
            e.printStackTrace();
        } finally {
            try {
                if (connection != null &amp;&amp; !connection.isClosed()) {
                    connection.close();
                }
            } catch (SQLException e) {
                e.printStackTrace();
            }
            System.out.println(format(&quot;Task: {0} connection closed&quot;, taskId));
        }
        stopWatch.stop();
        System.out.println(format(&quot;Task: {0} completed in {1} ms&quot;, taskId, stopWatch.getTime(TimeUnit.MILLISECONDS)));
        return result;
    }


The output is as follows:
Task: 1 started
Task: 5 started
Task: 9 started
Task: 7 started
Task: 3 started
Task: 6 started
Task: 8 started
Task: 2 started
Task: 4 started
Task: 10 started
Task: 1 connection established
Task: 6 connection established
Task: 7 connection established
Task: 9 connection established
Task: 8 connection established
Task: 5 connection established
Task: 7 executes SQL statement
Task: 2 connection established
Task: 1 executes SQL statement
Task: 6 executes SQL statement
Task: 3 connection established
Task: 8 executes SQL statement
Task: 2 executes SQL statement
Task: 5 executes SQL statement
Task: 4 connection established
Task: 4 executes SQL statement
Task: 10 connection established
Task: 10 executes SQL statement
Task: 10 connection closed
Task: 6 connection closed
Task: 10 completed in 10Â 319 ms
Task: 8 connection closed
Task: 2 connection closed
Task: 2 completed in 10Â 335 ms
Task: 1 connection closed
Task: 9 executes SQL statement
Task: 3 executes SQL statement
Task: 1 completed in 10Â 337 ms
Task: 4 connection closed
Task: 4 completed in 10Â 320 ms
Task: 5 connection closed
Task: 8 completed in 10Â 336 ms
Task: 6 completed in 10Â 336 ms
Task: 7 connection closed
Task: 5 completed in 10Â 338 ms
Task: 7 completed in 10Â 338 ms
Task: 9 connection closed
Task: 3 connection closed
Task: 9 completed in 20Â 345 ms
Task: 3 completed in 20Â 345 ms
The total time of execution was: 20Â 363 ms

Summary:

In the beginning all Tasks were started.
Secondly, all tasks established a jdbc connection with a database
Only 8 out of 10 tasks started to execute a SELECT statement
The last 2 tasks started to execute the SELECT statement when two task completed their job

Long story short: Since communication with a database is an I/O operation then the virtual threads should execute the SELECTs almost at the same time.
P.S. I have 8 cores CPU.
Thank you a lot for explanations.
","
Can VT improve performance?

Yes and no, of course.
VTs share a native thread, so they can't run instructions in parallel beyond the OS capability. But as you aim to have them yield (park) when waiting, it should have helped, IF they were not hooked on mysql internal synchronized blocks.
Mysql driver 9.0.0+ (currently 9.1.0) has ReentrantLock(s) now and it should work as this enabled the virtual thread to go do something else.
(I'll presume you don't have a limit of 8 connections to the DB. It's usually more like 100. You can assert that by temporarily adding a pause to your task after connecting. You can also use a 'sleep(delay)' in mysql, to make statements last longer to prove your tasks can make parallel statements, but you've got a nasty 10 seconds hard statement already. Perhaps there is a difference in the eye of mysqld, but with a 60 seconds pause, you'll have time to show processlist by hand now).
If you are on linux, you can write a small java method to read the &quot;/proc/thread-self/status&quot; on each task to get some outsight from the OS (the 'Pid' row in particular). See http://man.he.net/man5/proc . This would prove on which distinct native OS threads your VTs are running.
For Windoze I used JNA:
    &lt;dependency&gt;
        &lt;groupId&gt;net.java.dev.jna&lt;/groupId&gt;
        &lt;artifactId&gt;jna&lt;/artifactId&gt;
        &lt;version&gt;5.15.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;net.java.dev.jna&lt;/groupId&gt;
        &lt;artifactId&gt;jna-platform&lt;/artifactId&gt;
        &lt;version&gt;5.15.0&lt;/version&gt;
    &lt;/dependency&gt;

Here is a sample test:
package vttests;

import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.util.List;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.Future;
import java.util.concurrent.TimeUnit;
import java.util.stream.IntStream;

import com.mysql.cj.jdbc.MysqlDataSource;
import com.sun.jna.Library;
import com.sun.jna.Native;

public class TestMysqlWithVirtualThreads {
    record TD(Integer pid, Integer tid) {}
    
    public interface MyKernel32 extends Library  {
        int GetCurrentProcessId();
        int GetCurrentThreadId();
    }
    
    static MyKernel32 K32;
    static {
        try {
            K32 = Native.load(&quot;kernel32&quot;, MyKernel32.class);
        } catch (Throwable t) {
            t.printStackTrace(System.out);
        }
    }
    
    static ThreadLocal&lt;TD&gt; thrDetailsTL = ThreadLocal.withInitial(() -&gt; new TD(pid(), tid()));
    
    static Integer pid() {
        return K32 != null ? K32.GetCurrentProcessId() : null;
    }
    
    static Integer tid() {
        return K32 != null ? K32.GetCurrentThreadId() : null;
    }
    
    static void p(Object msg) {
        TD td = thrDetailsTL.get();
        System.out.println(&quot;[&quot;+ (Thread.currentThread().isVirtual() ? &quot;V&quot; : &quot;P&quot;)+ &quot;/&quot;+ Thread.currentThread().getName()+ &quot;(&quot; + td.pid + &quot;.&quot; + td.tid + &quot;)]: &quot;+ msg);
    }
    
    public static void main(String[] args) throws Exception {
        testVirtualThread(false);
        testVirtualThread(true);
    }
    
    static void testVirtualThread(boolean useVT) throws InterruptedException, ExecutionException {
        int cores = Runtime.getRuntime().availableProcessors();
        p(&quot;\n\n==========================\nprocessors count = &quot; + cores);
        
        int N = 3 * cores;
        int delay = 2000;
        long t0;
        long d;
        
        ExecutorService es = useVT ? Executors.newVirtualThreadPerTaskExecutor() : Executors.newThreadPerTaskExecutor(r -&gt; new Thread(r));
        try (es) {
            t0 = System.nanoTime();
            List&lt;? extends Future&lt;?&gt;&gt; futures = IntStream
                .range(1, N)
                .mapToObj(i -&gt; new T(i, delay))
                .map(es::submit)
                .toList();
            
            //--------
            
            p(&quot;waiting for all futures...&quot;);
            for(Future&lt;?&gt; future: futures) {
                        future.get();
            }
            d = System.nanoTime()-t0;
            p(&quot;waiting futures took &quot;+1e-9*d+&quot; sec&quot;);
            
            //--------
            
            p(&quot;shutting down es and await termination...&quot;);
            t0 = System.nanoTime();
            es.shutdown();
            es.awaitTermination(1, TimeUnit.SECONDS);
            d = System.nanoTime()-t0;
            p(&quot;shut down took &quot;+1e-9*d+&quot; sec&quot;);
            
            //--------
            
            p(&quot;closing VirtualThreadPerTaskExecutor...&quot;);
            t0 = System.nanoTime();
        }
        
        d = System.nanoTime() - t0;
        p(&quot;closing executor took &quot; + 1e-9 * d + &quot; sec&quot;);
        
        p(&quot;The end.&quot;);
    }
    
    static class T implements Runnable {
        int id;
        int delayms;
        String indent;
        
        T(int id, int delayms) {
            this.id = id;
            this.delayms = delayms;
            indent = &quot;\t&quot;.repeat(id) +&quot;\\_&gt;&quot;;
        }
        
        @Override
        public void run() {
            p(indent + &quot;task &quot; + id + &quot; started, pid=&quot; + pid() + &quot;, tid=&quot; + tid());
            work();
            p(indent + &quot;tast &quot; + id + &quot; ended.&quot;);
        }
        
        void work() {
            try {
                MysqlDataSource ds = new MysqlDataSource();
                ds.setServerName(&quot;localhost&quot;);
                ds.setPort(3306);
                ds.setUser(&quot;some-dbUser&quot;);
                ds.setPassword(&quot;some-dbPass&quot;);
                try (Connection c = ds.getConnection();
                    PreparedStatement ps = c.prepareStatement(&quot;select sleep(?), connection_id()&quot;)
                    ) {
                    ps.setDouble(1, 1e-3*delayms);
                    
                    for(int i=1; i&lt;=3; i++) {
                        p(indent + &quot;call &quot;+i+&quot;starting ... &quot;);
                        ResultSet rs = ps.executeQuery();
                        p(indent + &quot;call &quot;+i+&quot; finished on connection id &quot;+(rs.next() ? rs.getString(2) : null));
                    }
                }
                
            } catch (SQLException e) {
                e.printStackTrace();
            }
        }
    }
}

What I see here (with 4 cores) is 4 native threads despite creating 12 virtual threads. With driver 8.4, it's slow like yours, with 9.1.0 it behaves as expected.
And now that you restored sanity, the questions remaind: did it help? The answer has more to do with how many threads can you afford.
Would you rather use 20 native threads (more memory) with only 20 connections (less memory) guaranteed to work in parallel, or would you rather have 100 connections (more memory) which are sadly never working on more than the 4 native threads shared by the virtual threads (less memory)?.
Understand, you really only have N cores at work but my point is that one's optimization is often another one's problem... In particular, allowing 20000 virtual threads to share 4 cores will be awesome on the client side, but you'll never get 20000 jdbc connections without severe performance issues.
As always, bring the back pressure at the entry of the system, don't overcommit to work upsteam only to have to deal with the problem downstream.
As a client in most protocols, the number of outgoing connections is likely lower than the number of threads you can tolerate being blocked for a responses. For a server, it's another story. Virtual threads help the servicing side just like async NIO did.
"
"This related with my previous question after I changed readFile and make it read from URI for devices running in android 11 and above I got ANR error while I tried to read file
gif showing the error

this my full code
public class MainActivity extends AppCompatActivity {

    private static final int REQUEST_CODE_DOC = 1;

    private static final String TAG = &quot;MainActivity&quot;;

    private ActivityMainBinding activityMainBinding = null;

    private File file;
    private Uri selectedFileURI;
    BufferedReader bufferedReader;
    InputStream inputStream;
    FileReader fileReader;

    @Override
    protected void onDestroy() {
        super.onDestroy();
        activityMainBinding = null;
    }

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

        activityMainBinding = ActivityMainBinding.inflate(getLayoutInflater());

        setContentView(activityMainBinding.getRoot());


    }

    @Override
    protected void onStart() {
        super.onStart();
        activityMainBinding.textView.setMovementMethod(new ScrollingMovementMethod());
        activityMainBinding.browseButton.setOnClickListener(view -&gt; {


            browseDocuments();
        });

        activityMainBinding.read.setOnClickListener(view -&gt; {
            if (TextUtils.isEmpty(activityMainBinding.editTextPath.getText())) {
                activityMainBinding.editTextPath.setError(&quot;The file path cannot be empty&quot;);
            } else {
                readFile();

            }
        });

        activityMainBinding.clear.setOnClickListener(view -&gt; activityMainBinding.textView.setText(null));
    }


    @Override
    protected void onActivityResult(int requestCode, int resultCode, Intent data) {
        super.onActivityResult(requestCode, resultCode, data);
        if (requestCode == REQUEST_CODE_DOC &amp;&amp; resultCode == Activity.RESULT_OK) {

            try {

                if (data != null) {

                    selectedFileURI = data.getData();
                    file = new File(selectedFileURI.getPath());
                    activityMainBinding.editTextPath.setText(file.getAbsolutePath());
                    Log.d(TAG, &quot;onActivityResult: &quot; + file.getAbsolutePath());

                } else {
                    Toast.makeText(this, &quot;Allow permission for storage access!&quot;, Toast.LENGTH_SHORT).show();
                }

                String mimeType = getContentResolver().getType(selectedFileURI);
                Log.i(&quot;Type of file&quot;, mimeType + &quot;&quot;);
            } catch (Exception exception) {

                if (exception.getMessage() != null) {

                    Log.e(&quot;test Exception&quot;, exception.getMessage());

                } else if (exception.getCause() != null) {
                    Log.e(&quot;test Exception&quot;, Objects.requireNonNull(exception.getCause()).toString());
                }


            }
        }

    }

    public String getPath(Uri uri) {
        String[] projection = {MediaStore.Images.Media.DATA};
        Cursor cursor = getContentResolver().query(uri, projection, null, null, null);
        if (cursor == null) return null;
        int column_index = cursor.getColumnIndexOrThrow(MediaStore.Images.Media.DATA);
        cursor.moveToFirst();
        String s = cursor.getString(column_index);
        cursor.close();
        return s;
    }


    private void readFile() {
        try {

            StringBuilder sb = new StringBuilder();
            String line;

            if (SDK_INT &gt;= Build.VERSION_CODES.R) {

                inputStream = getContentResolver().openInputStream(selectedFileURI);
                bufferedReader = new BufferedReader(new InputStreamReader(inputStream));

            } else {
                fileReader = new FileReader(file);
                bufferedReader = new BufferedReader(fileReader);
            }
            while ((line = bufferedReader.readLine()) != null) {
                sb.append(line).append(&quot;\n&quot;);
            }

            activityMainBinding.textView.setText(sb.toString());

            if(inputStream != null) {
                inputStream.close();
            }else if(bufferedReader != null) {
                bufferedReader.close();
            }else if(fileReader != null) {
            fileReader.close();
            }

        } catch (IOException e) {
            Log.e(&quot;IOException&quot;, e.getMessage());
            Log.e(&quot;IOException2&quot;, e.getCause() + &quot;&quot;);
            Log.e(&quot;IOException3&quot;, &quot;exception&quot;, e);
            Toast.makeText(MainActivity.this, &quot;Cannot read this file&quot;, Toast.LENGTH_LONG).show();

        }

    }


    private boolean checkPermission() {
        if (SDK_INT &gt;= Build.VERSION_CODES.R) {
            return Environment.isExternalStorageManager();
        } else {
            int result = ContextCompat.checkSelfPermission(this, READ_EXTERNAL_STORAGE);
            int result1 = ContextCompat.checkSelfPermission(this, WRITE_EXTERNAL_STORAGE);
            return result == PackageManager.PERMISSION_GRANTED &amp;&amp; result1 == PackageManager.PERMISSION_GRANTED;
        }
    }

    private void requestPermission() {
        if (SDK_INT &gt;= Build.VERSION_CODES.R) {
            try {
                Intent intent = new Intent(Settings.ACTION_MANAGE_APP_ALL_FILES_ACCESS_PERMISSION);
                intent.addCategory(&quot;android.intent.category.DEFAULT&quot;);
                intent.setData(Uri.parse(String.format(&quot;package:%s&quot;, getApplicationContext().getPackageName())));
                startActivityForResult(intent, 1);
            } catch (Exception e) {
                Intent intent = new Intent();
                intent.setAction(Settings.ACTION_MANAGE_ALL_FILES_ACCESS_PERMISSION);
                startActivityForResult(intent, 1);
            }
        } else {

            ActivityCompat.requestPermissions(this, new String[]{READ_EXTERNAL_STORAGE,
                    WRITE_EXTERNAL_STORAGE}, 1);
        }
    }


    @Override
    public void onRequestPermissionsResult(int requestCode, @NonNull String[] permissions, @NonNull int[] grantResults) {
        super.onRequestPermissionsResult(requestCode, permissions, grantResults);
        switch (requestCode) {
            case REQUEST_CODE_DOC:
                if (grantResults.length &gt; 0) {
                    boolean READ_EXTERNAL_STORAGE = grantResults[0] == PackageManager.PERMISSION_GRANTED;
                    boolean WRITE_EXTERNAL_STORAGE = grantResults[1] == PackageManager.PERMISSION_GRANTED;

                    if (READ_EXTERNAL_STORAGE &amp;&amp; WRITE_EXTERNAL_STORAGE) {
                        readFile();


                    } else {
                        Toast.makeText(this, &quot;Allow permission for storage access!&quot;, Toast.LENGTH_SHORT).show();
                    }
                }
                break;
        }
    }

    private void browseDocuments() {

        if (!checkPermission()) {
            requestPermission();
        } else {


            String[] mimeTypes =
                    {&quot;text/plain&quot;, &quot;application/msword&quot;, &quot;application/vnd.openxmlformats-officedocument.wordprocessingml.document&quot;,
                            &quot;application/vnd.ms-powerpoint&quot;, &quot;application/vnd.openxmlformats-officedocument.presentationml.presentation&quot;,
                            &quot;application/vnd.ms-excel&quot;, &quot;application/vnd.openxmlformats-officedocument.spreadsheetml.sheet&quot;,
                            &quot;textView/plain&quot;,
                            &quot;application/pdf&quot;};

            Intent intent = new Intent(Intent.ACTION_GET_CONTENT);
            intent.addCategory(Intent.CATEGORY_OPENABLE);
            setResult(Activity.RESULT_OK);

            intent.setType(&quot;*/*&quot;);
            intent.putExtra(Intent.EXTRA_MIME_TYPES, mimeTypes);


            startActivityForResult(Intent.createChooser(intent, &quot;ChooseFile&quot;), REQUEST_CODE_DOC);
        }
    }

}

","That's because you reading files on the Main UI thread which blocks it and causes ANR until is finished, you need to do this work on a background thread, I suggest to take look at the answers on this question even it's old &quot;since 10 years approximately&quot; but it's a good place to start trying, you will find some methods to do the process in background threads like AsyncTask, Callbacks and Executors all this ways can help you to do the fix the issue, but I'll focus in my answer on the newest and recommended way is using &quot;RX Java&quot; I suggest to take look at this RxJava Tutorial you will learn more about it.
let's start to fix this

Add Rx Java dependencies to your project
in build.gradle(app)

  implementation 'io.reactivex.rxjava3:rxandroid:3.0.0'
    // Because RxAndroid releases are few and far between, it is recommended you also
    // explicitly depend on RxJava's latest version for bug fixes and new features.
    // (see https://github.com/ReactiveX/RxJava/releases for latest 3.x.x version)
    implementation 'io.reactivex.rxjava3:rxjava:3.0.3'


in read button onClick create a new Observable and call method readFile on it,
the subscribeOn it's defines the thread that the observable will work on it I choosed Schedulers.computation() because you will not be able to determine the size of the doc/text file or How long this process will take, but you can choose other threads like Schedulers.io(), in observeOn you added the thread that observer will work on it, in your case, it's the main thread, and finally call subscribe to connect observable with the observer, I also suggest to add progressBar on your layout to show it while reading the file and hide it when finished the process

activityMainBinding.read.setOnClickListener(view -&gt; {
            if (TextUtils.isEmpty(activityMainBinding.editTextPath.getText())) {
                activityMainBinding.editTextPath.setError(&quot;The file path cannot be empty&quot;);
            } else {
                

                Observable.fromCallable(this::readFile)
                        .subscribeOn(Schedulers.computation())
                        .observeOn(AndroidSchedulers.mainThread())
                        .subscribe(new Observer&lt;Object&gt;() {
                            @Override public void onSubscribe(Disposable d) {
                                activityMainBinding.progressBar.setVisibility(View.VISIBLE);
                            }

                            @Override public void onNext(Object o) {
                                if(o instanceof StringBuilder){
                                    activityMainBinding.textView.setText((StringBuilder) o);
                                }
                            }

                            @Override public void onError(Throwable e) {

                            }

                            @Override public void onComplete() {
                                activityMainBinding.progressBar.setVisibility(View.INVISIBLE);
                            }
                        });

            }
        });

also, I would suggest you if the file is PDF use AndroidPdfViewer library it makes a lot easier for you and you will not need all these permissions to read PDF files, you can check these this article to learn more about it.
"
"I have tried a number of different variations of the code below and cannot find any solution which doesn't rely on an unsafe cast or cause another other compiler warning. I am confident the goal is possible, but maybe not?
To put it simply, the goal is that I have derived types which are related to each other and have invariant relationship, which can be enforced by a generic method.
AlphaTask always returns AlphaTaskResult.
AlphaTask is a concrete implementation of ITask&lt;T&gt;, where T is String.
AlphaTaskResult extends the base class of TaskResult&lt;T&gt;, where again T is String.
Everything checks out until it comes to writing a generic method which take any Task and get back the corresponding TaskResult type.
The error is:
Required type: List&lt;U&gt;
Provided:      List&lt;TaskResult&lt;T&gt;&gt;
no instance(s) of type variable(s) exist so that TaskResult&lt;T&gt; conforms to U inference variable T has incompatible bounds: equality constraints: U lower bounds: TaskResult&lt;T&gt;

package com.adobe.panpipe;

import java.util.List;
import java.util.Arrays;
import java.util.stream.Collectors;


interface ITask&lt;T&gt;{
    TaskResult&lt;T&gt; make();
}

class TaskResult&lt;T&gt;{
    T value;
}

class AlphaTaskResult extends TaskResult&lt;String&gt; {
    AlphaTaskResult(String value){
        this.value = value;
    }
}

class BetaTaskResult extends TaskResult&lt;Integer&gt; {
    BetaTaskResult(Integer value){
        this.value = value;
    }
}

class AlphaTask implements ITask&lt;String&gt; {
    public AlphaTaskResult make(){
        return new AlphaTaskResult(&quot;alphaTask&quot;);
    }
}

class BetaTask implements ITask&lt;Integer&gt; {
    public BetaTaskResult make(){
        return new BetaTaskResult(9001);
    }
}

public class Main &lt;T&gt;{

    public static &lt;T, U extends TaskResult&lt;T&gt;, V extends ITask&lt;T&gt;&gt; List&lt;U&gt; run(List&lt;V&gt; tasks){

        List&lt;U&gt; results =  tasks
                .stream()
                .map(ITask::make)
                .collect(Collectors.toList());

        return results;
    }

    public static void main(String[] args) {

        List&lt;AlphaTaskResult&gt; alphaResults = run(Arrays.asList(new AlphaTask(), new AlphaTask()));
        List&lt;BetaTaskResult&gt; betaResults = run(Arrays.asList(new BetaTask(), new BetaTask()));

    }
}

","Just add the type R extends TaskResult&lt;T&gt; of the result to the type ITask of the task. The following compiles just fine:
import java.util.List;
import java.util.Arrays;
import java.util.stream.Collectors;


interface ITask&lt;T, R extends TaskResult&lt;T&gt;&gt;{
    R make();
}

class TaskResult&lt;T&gt;{
    T value;
}

class AlphaTaskResult extends TaskResult&lt;String&gt; {
    AlphaTaskResult(String value){
        this.value = value;
    }
}

class BetaTaskResult extends TaskResult&lt;Integer&gt; {
    BetaTaskResult(Integer value){
        this.value = value;
    }
}

class AlphaTask implements ITask&lt;String, AlphaTaskResult&gt; {
    public AlphaTaskResult make(){
        return new AlphaTaskResult(&quot;alphaTask&quot;);
    }
}

class BetaTask implements ITask&lt;Integer, BetaTaskResult&gt; {
    public BetaTaskResult make(){
        return new BetaTaskResult(9001);
    }
}

public class Main &lt;T&gt;{

    public static &lt;T, R extends TaskResult&lt;T&gt;&gt; List&lt;R&gt; run(List&lt;ITask&lt;T, R&gt;&gt; tasks){

        List&lt;R&gt; results =  tasks
                .stream()
                .map(ITask::make)
                .collect(Collectors.toList());

        return results;
    }

    public static void main(String[] args) {
        List&lt;AlphaTaskResult&gt; alphaResults = run(Arrays.asList(new AlphaTask(), new AlphaTask()));
        List&lt;BetaTaskResult&gt; betaResults = run(Arrays.asList(new BetaTask(), new BetaTask()));

    }
}

The reason why your version does not compile is that the exact type of the result is not included in the type of the task, and since there are no type refinements in Java, there is nothing from which the type U could be inferred.
"
"I'm trying to run windows CLI command from java.
I got an issue when parsing results but only when running the code as a runnable jar from cli, from within eclipse it runs fine
private static List&lt;String&gt; runWindowsCommandAsRuntime(String command) {
        
        List&lt;String&gt; out = new ArrayList&lt;String&gt;();
        
        String[] comm = {
                &quot;C:\\Windows\\System32\\cmd.exe&quot;,
                &quot;/S&quot;,
                &quot;/K&quot;,
                &quot;\&quot;&quot;+command+&quot;\&quot;&quot;,
                &quot;&amp;&quot;,
                &quot;exit&quot; //devo uscire o il processo CMD resta appeso e non esce l'output
                };
        
    
        String dbg = &quot;&quot;;
        for(String s : comm)
            dbg += s + &quot; &quot;;
        System.out.println(&quot;COMMAND: &quot;+dbg);
        
        try {
            Runtime rt = Runtime.getRuntime();
            Process p = rt.exec(comm);
            
            //get the output
            
            out.addAll(
                    new BufferedReader(new InputStreamReader(p.getInputStream()))
                   .lines().toList() //the exception is thrown here
               );
            
            
            int exitVal = p.exitValue();
            System.out.println(&quot;Exited with error code &quot; + exitVal);

            p.destroy();
      
        } catch (Exception ex) {
            Utility.logException(&quot;Utility(SystemWindows)&quot;, ex);
            return null;
        }
        
        return out;
        
    }

// sample call: runWindowsCommandAsRuntime(&quot;WMIC OS Get Caption,Version&quot;);


When I run the program trough eclipse it works fine,
when I call it from cli (java -jar my_program.jar) it starts then throws this
I checked the java version and is both on eclipse and from cli java 11
Exception in thread &quot;main&quot; java.lang.reflect.InvocationTargetException
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.eclipse.jdt.internal.jarinjarloader.JarRsrcLoader.main(JarRsrcLoader.java:61)
Caused by: java.lang.NoSuchMethodError: java.util.stream.Stream.toList()Ljava/util/List;

","Explanation:
You are trying to call .toList() on a stream, and streams don't have the .toList() method (in Java &lt; 16), therefore you have to use a Collector.
Short answer:
you could either use .collect(Collectors.toList()) instead of .toList() if you want to run your program with Java &lt; 16, or you could use .toList() on the stream (as you are doing now) but run it with at least Java 16.
And your entire code should look something like this, if you would like to run it with Java older than 16:
private static List&lt;String&gt; runWindowsCommandAsRuntime(String command) {

    List&lt;String&gt; out = new ArrayList&lt;String&gt;();

    String[] comm = {
            &quot;C:\\Windows\\System32\\cmd.exe&quot;,
            &quot;/S&quot;,
            &quot;/K&quot;,
            &quot;\&quot;&quot; + command + &quot;\&quot;&quot;,
            &quot;&amp;&quot;,
            &quot;exit&quot; //devo uscire o il processo CMD resta appeso e non esce l'output
    };


    String dbg = &quot;&quot;;
    for (String s : comm)
        dbg += s + &quot; &quot;;
    System.out.println(&quot;COMMAND: &quot; + dbg);

    try {
        Runtime rt = Runtime.getRuntime();
        Process p = rt.exec(comm);

        //get the output

        out.addAll(
                new BufferedReader(new InputStreamReader(p.getInputStream()))
                        .lines().collect(Collectors.toList()) //the exception is thrown here
        );


        int exitVal = p.exitValue();
        System.out.println(&quot;Exited with error code &quot; + exitVal);

        p.destroy();

    } catch (Exception ex) {
        return null;
    }

    return out;

}

"
"I have a stream of data from database using Spring Data Jpa that needs to be Json serialized and write to a Http response, without storing in memory. This is the sample code.
try (Stream&lt;Employee&gt; dataStream = empRepo.findAllStream()) {
        response.setHeader(&quot;content-type&quot;, &quot;application/json&quot;);
        PrintWriter respWriter = response.getWriter();
        respWriter.write(&quot;[&quot;);     // array begin
        dataStream.forEach(data -&gt; {
            try {
                respWriter.write(jsonSerialize(data));
                respWriter.write(&quot;,&quot;);
            } catch (JsonProcessingException e) {
                log(e);
            }
            entityManager.detach(data);
        });
        respWriter.write(&quot;]&quot;);    // array end
        respWriter.flush();
    } catch (IOException e) {
        log(e);
    }
}

But this logic will write an extra comma after the last element. How can I not to do respWriter.write(&quot;,&quot;);, if it is the last element?
There are solutions with stream operators - peek, reduce etc, but what's the most optimized solution? Is there something like Stream.hasNext() so that I can use an if condition inside forEach?
","First I'd like to say that I don't think that your problem is a good fit for a single pipeline stream. You are performing side effects both with the write call and the detach call. Maybe you are better of with a normal for-loop? Or using multiple streams instead?
That being said, you can use the technique that Eran describes in an answer to this question: Interleave elements in a stream with separator
try (Stream&lt;Employee&gt; dataStream = empRepo.findAllStream()) {
    response.setHeader(&quot;content-type&quot;, &quot;application/json&quot;);
    PrintWriter respWriter = response.getWriter();
    respWriter.write(&quot;[&quot;);     // array begin
    dataStream.map(data -&gt; {
        try {
            String json = jsonSerialize(data);

            // NOTE! It is confusing to have side effects like this in a stream!
            entityManager.detach(data);
            return json;
        } catch (JsonProcessingException e) {
            throw new RuntimeException(e);
        }
    })
    .flatMap(json -&gt; Stream.of(&quot;,&quot;, json))
    .skip(1)
    .forEach(respWriter::write);

    respWriter.write(&quot;]&quot;);    // array end
    respWriter.flush();
} catch (IOException e) {
    log(e);
}

"
"I'm an Android Developer who has to use KeyChain not KeyStore. The KeyStore variant of our code works. I need to add KeyChain equivalent.
this works
  final char[] PASSWORD = &quot;***SOMEPASSWORD****&quot;.toCharArray();
  TrustManager[] trustManager;
  SSLSocketFactory sslSocketFactory;
  KeyStore keyStore;

  InputStream inputStream = context.getResources().getAssets().open(&quot;xxxx-xxxxx-xxxxx-xxxx.pfx&quot;);
  keyStore = KeyStore.getInstance(&quot;PKCS12&quot;);
  keyStore.load(inputStream,PASSWORD);
  TrustManagerFactory trustManagerFactory = TrustManagerFactory.getInstance (TrustManagerFactory.getDefaultAlgorithm());
  trustManagerFactory.init(keyStore);
  TrustManager[] trustManagers = trustManagerFactory.getTrustManagers();
  if (trustManagers.length != 1 || !(trustManagers[0] instanceof X509TrustManager))
  {
    throw new IllegalStateException(&quot;Unexpected default trust managers:&quot;
      + Arrays.toString(trustManagers));
  }
  trustManager = trustManagers;

  KeyManagerFactory keyManagerFactory = KeyManagerFactory.getInstance(&quot;X509&quot;);
  keyManagerFactory.init(keyStore,PASSWORD);
  SSLContext sslContext = SSLContext.getInstance(&quot;TLS&quot;);
  sslContext.init(keyManagerFactory.getKeyManagers(),null,null);
  sslSocketFactory = sslContext.getSocketFactory();


  OkHttpClient.Builder builder = new OkHttpClient.Builder()
    .connectTimeout(15000, TimeUnit.MILLISECONDS).readTimeout(0, TimeUnit.MILLISECONDS)
    .writeTimeout(15000, TimeUnit.MILLISECONDS).cookieJar(new ReactCookieJarContainer());
  builder.sslSocketFactory(sslSocketFactory, (X509TrustManager) trustManager[0]);

  OkHttpClient okHttpClient = builder.build();

The problem is this line InputStream inputStream = context.getResources().getAssets().open(&quot;xxxx-xxxxx-xxxxx-xxxx.pfx&quot;); we're not allowed to use the assets folder (for reasons outside the scope of this conversation) but we are allowed to put the self same file in the KeyChain so I did, and I can retrieve it using the following. X509Certificate[] chain = KeyChain.getCertificateChain(context, &quot;xxxx-xxxxx-xxxxx-xxxx&quot;); 
so since
   X509Certificate[] chain = KeyChain.getCertificateChain(context, &quot;xxxx-xxxxx-xxxxx-xxxx&quot;); //this gets the correct X509Certificate

Gets the certificate via KeyChain my instinct was to swap it out with this:
   X509TrustManager customTm = new X509TrustManager() {
    @Override
    public void checkClientTrusted(java.security.cert.X509Certificate[] chain, String authType) throws CertificateException {

    }

    @Override
    public void checkServerTrusted(java.security.cert.X509Certificate[] chain, String authType) throws CertificateException {

    }

    @Override
    public java.security.cert.X509Certificate[] getAcceptedIssuers() {
      try {
        return X509Certificate[] chain = KeyChain.getCertificateChain(context, &quot;xxxx-xxxxx-xxxxx-xxxx&quot;);
      } catch (InterruptedException e) {
        e.printStackTrace();
      } catch (KeyChainException e) {
        e.printStackTrace();
      }
      return null;
    }
  };
  TrustManager[] trustManager = new TrustManager[] { customTm };
  sslContext.init(null, trustManager, null);
 

but it doesn't work, so my question is: How do I use the X509Certificate I have from the KeyChain as a drop in replacement to the asset I pulled into the KeyStore?
","I figured it out, the code needs to become slightly different, but this wasn't documented anywhere I could find: I had to literally face roll the keyboard to get it done
so. the OP example was me using the pfx file in the assets folder.

In order to not use a from an asset file in the build environment (aka KeyStore), in order to load the cert direcly from a part of android OS at run time and get at it in code (aka KeyChain)(i.e. in order to load the cert you can see in the screen below on the device) ....
THIS IS AN illustration of where it is in android, BUT NOT MY SCREEN SHOT 
...use this code
final char[] PASSWORD = &quot;***SOMEPASSWORD****&quot;.toCharArray();
TrustManager[] trustManager;
SSLSocketFactory sslSocketFactory;
KeyStore keyStore;

X509Certificate[] keychain;  
PrivateKey privateKey;   
keychain = KeyChain.getCertificateChain(context, &quot;xkeyx-xaliasxx&quot;);
privateKey = KeyChain.getPrivateKey(context, &quot;xkeyx-xaliasxx&quot;); 
keyStore = KeyStore.getInstance(&quot;PKCS12&quot;);
keyStore.load(null, null);
keyStore.setKeyEntry(&quot;xkeyx-xaliasxx&quot;, privateKey, PASSWORD, keychain);  
TrustManagerFactory trustManagerFactory = TrustManagerFactory.getInstance (TrustManagerFactory.getDefaultAlgorithm());
trustManagerFactory.init(keyStore);
TrustManager[] trustManagers = trustManagerFactory.getTrustManagers();
if (trustManagers.length != 1 || !(trustManagers[0] instanceof X509TrustManager))
  {
    throw new IllegalStateException(&quot;Unexpected default trust managers:&quot;
      + Arrays.toString(trustManagers));
  }
trustManager = trustManagers;

KeyManagerFactory keyManagerFactory = KeyManagerFactory.getInstance(&quot;X509&quot;);
keyManagerFactory.init(keyStore,PASSWORD);
SSLContext sslContext = SSLContext.getInstance(&quot;TLS&quot;);
sslContext.init(keyManagerFactory.getKeyManagers(),null,null);
sslSocketFactory = sslContext.getSocketFactory();


OkHttpClient.Builder builder = new OkHttpClient.Builder()
    .connectTimeout(15000, TimeUnit.MILLISECONDS).readTimeout(0, TimeUnit.MILLISECONDS)
    .writeTimeout(15000, TimeUnit.MILLISECONDS).cookieJar(new ReactCookieJarContainer());
builder.sslSocketFactory(sslSocketFactory, (X509TrustManager) trustManager[0]);

OkHttpClient okHttpClient = builder.build();

NOTE this will only work AFTER the user in app has confirmed the certificate: so this code needs to have been run at least ONCE

KeyChain.choosePrivateKeyAlias(RN.getReactActivity(), (KeyChainAliasCallback) alias -&gt; {
Log.d(&quot;cert&quot;, String.format(&quot;User has selected client certificate alias: %s should be xkeyx-xaliasxx&quot;, alias));
 X509Certificate[] truechain;
 try {
    truechain = KeyChain.getCertificateChain(Context, alias);
    if (truechain != null) {
        Log.d(&quot;cert&quot;, &quot;truechain count&quot; + truechain.length);
    }
    else{
        Log.d(&quot;cert&quot;, &quot;truechain count is null&quot;);
    }
 } catch (InterruptedException e) {
     Log.e(&quot;cert&quot;, &quot;InterruptedException&quot;, e);
 } catch (KeyChainException e) {
     Log.e(&quot;cert&quot;, &quot;KeyChainException&quot;, e);
 }
}, null, null, null, -1, null);

"
"I have the following piece of code which reads a CSV file.
public class TestMain {
    public static void parseTsv(String filePath) throws Exception {
        try (CSVReader reader = new CSVReader(new InputStreamReader(Objects.requireNonNull(TestMain.class.getResourceAsStream(filePath))))) {
            String[] line;
            while ((line = reader.readNext()) != null) {
                System.out.println(line[0] + &quot; &quot; + line[1]);
            }
        }
    }

    public static void main(String[] args) {
        try {
            parseTsv(&quot;path-to-tsv-file&quot;);
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }
}

And I want to modify the delimiter so that it can read tsv files (tab-separated). Any help would be greatly appreciated!
","With g00se's help, please see below the correct code:
public class TestMain {
    public static void parseTsv(String filePath) throws Exception {
        try (CSVReader reader = new CSVReaderBuilder(new InputStreamReader(Objects.requireNonNull(TestMain.class.getResourceAsStream(filePath))))
                .withCSVParser(new CSVParserBuilder().withSeparator('\t').build())
                .build()) {
            String[] line;
            while ((line = reader.readNext()) != null) {
                System.out.println(line[0] + &quot; &quot; + line[1]);
            }
        }
    }

    public static void main(String[] args) {
        try {
            parseTsv(&quot;path-to-tsv-file&quot;);
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }
}

"
"I have an Interface and multiple implementation. I'm auto wiring the interface in classes for usage. I need to choose different implementation at runtime.
public class Util {
  public void getClient();
}


Implementations
public class UtilOne implements Util {
  public void getClient() {...}
}


public class UtilTwo implements Util {
  public void getClient() {...}
}


@Configuration
public class AppConfig {
  
  @Autowired
  @Bean
  @Primary
  public Util utilOne() {
    return new UtilOne();
  }

  @Autowired
  @Bean
  public Util utilTwo() {
    return new UtilTwo();
  }

}

@Component
public class DemoService {

  @Autowired
  private Util util;
}

For some reason if we are unable to get client in UtilOne, I want to switch to UtilTwo without restarting the app. I want to change the Util object in DemoService to UtilTwo object.
Property active.util will come from DB and can we updated from UI.
","It doesn't work this way - if you have a certain implementation of Util wired to, say, class SampleClass (which is a singleton) you can't really change the implementation of the Util to something different without restarting the application context.
So instead of going this way, I suggest an alternative. You say that under certain conditions that evaluate in runtime you want to switch implementations. What kind of condition it is? Is it possible to extract this condition decision logic?
If so, you can autowire a special DynamicUtil that will hold the reference to all the utils and will call the required util depending on the condition:
// represents all possible business 'runtime' outcomes
enum ConditionOutcome {
   A, B, C 
}
interface ConditionEvaluator {
   ConditionOutcome evaluate(); // when called in runtime will evaluate a condition that currently exists in the system
}

interface Util {
   void foo();
   ConditionOutcome relevantOfOutcome();
}

class Utill1Impl implements Util {
   public void foo() {...}
   public ConditionOutcome relevantOfOutcome() {return ConditionOutcome.A;}
}
class Utill2Impl implements Util {
   public void foo() {...}
   public ConditionOutcome relevantOfOutcome() {return ConditionOutcome.B;}
}
class Utill3Impl implements Util {
   public void foo() {...}
   public ConditionOutcome relevantOfOutcome() {return ConditionOutcome.C;}
}

class DynamicUtil {
   private final Map&lt;ConditionOutcome, Util&gt; possibleImpls;
   private final ConditionEvaluator evaluator;

   public class DynamicUtil(List&lt;Util&gt; allImplementations, ConditionEvaluator evaluator) {
      // create a map by calling the 'relevantOfOutcome' per util impl in a loop
    this.evaluator = evaluator;
   }
   
   public void foo() {
      ConditionOutcome key = evaluator.evaluate();
      // pick the relevant implementation based on evaluated key
      possibleImpls.get(key).foo();
   }
}


Now with such a design you can dynamically add new possible outcomes (along with utils that should implement them. You classes in the system will have to autowire DynamicUtil though, so effectively you'll introduce one additional level of indirection but will gain flexibility
class SampleClass { // a business class that will need to work with util capable of being changed during the runtime 
   @Autowired
   private DynamicUtil util;
   ... 
}

"
"i really got stuck on this and i'd love your help.
I'm trying to write a method with the signature:
public static boolean search (int [][] mat, int num)

The method gets as parameters two-dimensional array that is circularly-sorted, and a value to search for num. If the value num is in the mat array, the method returns true. If the num value is not in the mat array, the method returns false.

The array is circular if all the values in Quarter 1 are really smaller than all those in Quarter 2, those in Quarter 2 are really smaller than all those in Quarter 3, and those in Quarter 3 are really smaller than all those in Quarter 4.
For example, the following array is circularly-sorted:

If the array mat is the array drawn above, and the number num is 22, the method returns the value true. 
If the array mat is the array drawn above, and the number num is 23, the method will return the value false
The conditions:

The array is quadratic two-dimensional, meaning that the number of rows and columns is equal
The mat array is not null and is circularly-sorted. You do not need to check this.
The method should be as effective as possible, both in terms of time complexity and
In terms of memory complexity.

","The construction of this sort is such that the smallest element of each quarter is in the top  left and the largest is in bottom left. You can check in which quarter the searched for element should be located and repeat the search in sub-quarters of that quarter.
An example implementation:
private static boolean search(int[][] mat, int num) {
    if (!square(mat)) {
        throw new IllegalArgumentException(&quot;matrix is not square&quot;);
    }
    
    int dim = mat.length;
    int topRow = 0;
    int leftColumn = 0;
    
    while (dim &gt; 1) {
        if (dim % 2 != 0) {
            throw new IllegalArgumentException(&quot;matrix dimension is not a power of two&quot;);
        }
        
        int qdim = dim / 2; // quarter dimensions
        
        // Order of checks is important.
        if (num &gt;= mat[topRow + qdim][leftColumn]) {
            // has to be in the bottom left quarter
            topRow += qdim;
        } else if (num &gt;= mat[topRow + qdim][leftColumn + qdim]) {
            // has to be in the bottom right quarter
            topRow += qdim;
            leftColumn += qdim;
        } else if (num &gt;= mat[topRow][leftColumn + qdim]) {
            // has to be in the top right quarter
            leftColumn += qdim;
        }

        // If all the conditions above give false, it's in the top left quarter
        // and topRow and leftColumn don't change.
        
        dim = qdim;
    }
    
    return mat[topRow][leftColumn] == num;
}

Full code
"
"I have several documents and I want to combine them all into one docx file.
My code :
import java.io.InputStream;
import java.io.OutputStream;
import java.util.ArrayList;
import java.util.List;
import org.apache.poi.openxml4j.opc.OPCPackage;
import org.apache.poi.xwpf.usermodel.XWPFDocument;
import org.openxmlformats.schemas.wordprocessingml.x2006.main.CTBody;

public class WordMerge {

private final OutputStream result;
private final List&lt;InputStream&gt; inputs;
private XWPFDocument first;

public WordMerge(OutputStream result) {
    this.result = result;
    inputs = new ArrayList&lt;&gt;();
}

public void add(InputStream stream) throws Exception{            
    inputs.add(stream);
    OPCPackage srcPackage = OPCPackage.open(stream);
    XWPFDocument src1Document = new XWPFDocument(srcPackage);         
    if(inputs.size() == 1){
        first = src1Document;
    } else {            
        CTBody srcBody = src1Document.getDocument().getBody();
        first.getDocument().addNewBody().set(srcBody);            
    }        
}

public void doMerge() throws Exception{
    first.write(result);                
}

public void close() throws Exception{
    result.flush();
    result.close();
    for (InputStream input : inputs) {
        input.close();
    }
}   
}

And it use :
public static void main(String[] args) throws Exception {

FileOutputStream faos = new FileOutputStream(&quot;/home/victor/result.docx&quot;);

WordMerge wm = new WordMerge(faos);

wm.add( new FileInputStream(&quot;/home/victor/001.docx&quot;) );
wm.add( new FileInputStream(&quot;/home/victor/002.docx&quot;) );

wm.doMerge();
wm.close();
}

It works, unfortunatly it becomes a bit messy if you have listings in any of the non-first document. Listing symbols change to numbers and worse sometimes a listing from the previous document will be continued in the attached document. Say doc1 has a.b.c listing , second has non ordered listing then this latter one becomes d.e.f. (It followed the previous document formatting.)
How to make each document that is merged on the next page and not follow the formatting of the previous document?
","Your code only appends multiple CTBody elements into the document. But that is not how a Word document is structured. &quot;It works&quot; because Microsoft Word is tolerant enough to interpret it. But it fails when it comes to references within the Word document structure.
For example to numbering definitions are referenced by IDs. And that can be the same ID for different definitions in different documents. So ID 1 in first document might be pointing to a decimal numbering while ID 1 in second document might be pointing to a bullet numbering. So the numIDs needs to be merged and not only copied.
Embedded  media (images for ex.) are referenced by rIDs. So the CTBody only contains the IDs. The media itself is stored outside the document body. So if the document body refers to a picture having rID12 and this picture is not stored, then the document gets corrupted.
Same is with many other document elements.
So that approach is not usable at all.
The need is traversing all body elements of the document, which shall be appended. Then append each found body element to the first document and update it's references.
The following is a working draft to show the principle. It is not ready yet. For example it does not considering hyperlinks, footnotes, comments, structured document tags and much more things. And as you see from the sheer amount of code needed, considering all possible things will be a very laborious task to do. To avoid even more code I simply copy the underlying XML bean if possible. This also should be better formed out for productive usage. But the principle should be clear from this code.
The code is commented when names of methods and variables are not self explained.
The code is tested and works using current apache poi 5.1.0. Former versions are not tested and also should  not be used since they offer even less support for XWPF.
The code needs the full jar of all of the schemas as mentioned in Apache POI FAQ.
import java.io.FileInputStream;
import java.io.FileOutputStream;
import org.apache.poi.xwpf.usermodel.*;
import org.apache.poi.util.Units;

import java.util.List;
import java.util.Map;
import java.util.HashMap;
import java.math.BigInteger;

public class WordMerger {
    
 private Map&lt;BigInteger, BigInteger&gt; numIDs = null; // to handle merging numID 
 
 public WordMerger() {
  this.numIDs= new HashMap&lt;BigInteger, BigInteger&gt;();
 }
                
 private void traverseBodyElements(List&lt;IBodyElement&gt; bodyElements, IBody resultBody) throws Exception {
  for (IBodyElement bodyElement : bodyElements) {
   if (bodyElement instanceof XWPFParagraph) {
    XWPFParagraph paragraph = (XWPFParagraph)bodyElement;
    XWPFParagraph resultParagraph = createParagraphWithPPr(paragraph, resultBody);
    traverseRunElements(paragraph.getIRuns(), resultParagraph);
   } else if (bodyElement instanceof XWPFSDT) {
    XWPFSDT sDT = (XWPFSDT)bodyElement;
    XWPFSDT resultSDT = createSDT(sDT, resultBody);
    //ToDo: handle further issues ...
   } else if (bodyElement instanceof XWPFTable) {
    XWPFTable table = (XWPFTable)bodyElement;
    XWPFTable resultTable = createTableWithTblPrAndTblGrid(table, resultBody);
    traverseTableRows(table.getRows(), resultTable);
   }
  }
 }

 private XWPFSDT createSDT(XWPFSDT sDT, IBody resultBody) {
  //not ready yet
  //we simply add paragraphs to avoid corruped documents
  if (resultBody instanceof XWPFDocument) {
   XWPFDocument resultDocument = (XWPFDocument)resultBody;
   XWPFParagraph resultParagraph = resultDocument.createParagraph();
   //ToDo: handle further issues ...
  } else if (resultBody instanceof XWPFTableCell) {
   XWPFTableCell resultTableCell = (XWPFTableCell)resultBody;
   XWPFParagraph resultParagraph = resultTableCell.addParagraph();
   //ToDo: handle further issues ...
  } //ToDo: else others ...
  //ToDo: handle SDT properly
  return null;
 }

 private XWPFParagraph createParagraphWithPPr(XWPFParagraph paragraph, IBody resultBody) {
  if (resultBody instanceof XWPFDocument) {
   XWPFDocument resultDocument = (XWPFDocument)resultBody;
   XWPFParagraph resultParagraph = resultDocument.createParagraph();
   resultParagraph.getCTP().setPPr(paragraph.getCTP().getPPr());//simply copy the underlying XML bean to avoid more code
   handleStyles(resultDocument, paragraph);
   handleNumberings(paragraph, resultParagraph);
   handleOMathParaArray(paragraph, resultParagraph);
   handleOMathIfFirstChild(paragraph, resultParagraph);
   //ToDo: handle further issues ...
   return resultParagraph;
  } else if (resultBody instanceof XWPFTableCell) {
   XWPFTableCell resultTableCell = (XWPFTableCell)resultBody;
   XWPFParagraph resultParagraph = resultTableCell.addParagraph();
   resultParagraph.getCTP().setPPr(paragraph.getCTP().getPPr());//simply copy the underlying XML bean to avoid more code
   handleStyles(resultTableCell, paragraph);
   handleOMathParaArray(paragraph, resultParagraph);
   handleOMathIfFirstChild(paragraph, resultParagraph);
   //ToDo: handle further issues ...  
   return resultParagraph;
  } //ToDo: else others ...
  return null;
 }

 private void handleOMathParaArray(XWPFParagraph paragraph, XWPFParagraph resultParagraph) {
  resultParagraph.getCTP().setOMathParaArray​(paragraph.getCTP().getOMathParaArray());//simply copy the underlying XML bean to avoid more code
 }
 
 private void handleOMathIfFirstChild(XWPFParagraph paragraph, XWPFParagraph resultParagraph) {
  org.apache.xmlbeans.XmlCursor cursor = paragraph.getCTP().newCursor();
  cursor.toChild(0);
  org.apache.xmlbeans.XmlObject xmlObject = cursor.getObject();
  if (xmlObject instanceof org.openxmlformats.schemas.officeDocument.x2006.math.CTOMath) {
   org.openxmlformats.schemas.officeDocument.x2006.math.CTOMath ctOMath = (org.openxmlformats.schemas.officeDocument.x2006.math.CTOMath)xmlObject;
   resultParagraph.getCTP().addNewOMath();
   resultParagraph.getCTP().setOMathArray(resultParagraph.getCTP().getOMathArray().length-1, ctOMath);
  }  
 }
 
 private void handleNumberings(XWPFParagraph paragraph, XWPFParagraph resultParagraph) {
  //if we have numberings, we need merging the numIDs and abstract numberings of the two different documents
  BigInteger numID = paragraph.getNumID();
  if (numID == null) return;
  BigInteger resultNumID = this.numIDs.get(numID);
  if (resultNumID == null) {
   XWPFDocument document = paragraph.getDocument(); 
   XWPFNumbering numbering = document.createNumbering();
   XWPFNum num = numbering.getNum(numID);
   BigInteger abstractNumID = numbering.getAbstractNumID(numID);
   XWPFAbstractNum abstractNum = numbering.getAbstractNum(abstractNumID);
   XWPFAbstractNum resultAbstractNum = new XWPFAbstractNum((org.openxmlformats.schemas.wordprocessingml.x2006.main.CTAbstractNum)abstractNum.getCTAbstractNum().copy());
   XWPFDocument resultDocument = resultParagraph.getDocument(); 
   XWPFNumbering resultNumbering = resultDocument.createNumbering();
   int pos = resultNumbering.getAbstractNums().size();
   resultAbstractNum.getCTAbstractNum().setAbstractNumId(BigInteger.valueOf(pos));
   BigInteger resultAbstractNumID = resultNumbering.addAbstractNum(resultAbstractNum);
   resultNumID = resultNumbering.addNum(resultAbstractNumID);
   XWPFNum resultNum = resultNumbering.getNum(resultNumID);
   resultNum.getCTNum().setLvlOverrideArray(num.getCTNum().getLvlOverrideArray());
   this.numIDs.put(numID, resultNumID);
  }
  resultParagraph.setNumID(resultNumID);  
 }

 private void handleStyles(IBody resultBody, IBodyElement bodyElement) {
  //if we have bodyElement styles we need merging those styles of the two different documents
  XWPFDocument document = null;
  String styleID = null;
  if (bodyElement instanceof XWPFParagraph) {
   XWPFParagraph paragraph = (XWPFParagraph)bodyElement;  
   document = paragraph.getDocument(); 
   styleID = paragraph.getStyleID();   
  } else if (bodyElement instanceof XWPFTable) {
   XWPFTable table = (XWPFTable)bodyElement;
   if (table.getPart() instanceof XWPFDocument) {
    document = (XWPFDocument)table.getPart();
    styleID = table.getStyleID();
   }
  } //ToDo: else others ...
  if (document == null || styleID == null || &quot;&quot;.equals(styleID)) return;
  XWPFDocument resultDocument = null;
  if (resultBody instanceof XWPFDocument) {
   resultDocument = (XWPFDocument)resultBody;
  } else if (resultBody instanceof XWPFTableCell) {
   XWPFTableCell resultTableCell = (XWPFTableCell)resultBody;
   resultDocument = resultTableCell.getXWPFDocument();
  } //ToDo: else others ...
  if (resultDocument != null) {
   XWPFStyles styles = document.getStyles();  
   XWPFStyles resultStyles = resultDocument.getStyles(); 
   XWPFStyle style = styles.getStyle(styleID);
   //merge each used styles, also the related ones
   for (XWPFStyle relatedStyle : styles.getUsedStyleList(style)) {
    if (resultStyles.getStyle(relatedStyle.getStyleId()) == null) {
     resultStyles.addStyle(relatedStyle);
    }
   }
  }
 }

 private XWPFTable createTableWithTblPrAndTblGrid(XWPFTable table, IBody resultBody) {
  if (resultBody instanceof XWPFDocument) {
   XWPFDocument resultDocument = (XWPFDocument)resultBody;
   XWPFTable resultTable = resultDocument.createTable();
   resultTable.removeRow(0);   
   resultTable.getCTTbl().setTblPr(table.getCTTbl().getTblPr());//simply copy the underlying XML bean to avoid more code
   resultTable.getCTTbl().setTblGrid(table.getCTTbl().getTblGrid());//simply copy the underlying XML bean to avoid more code
   handleStyles(resultDocument, table);
   //ToDo: handle further issues ...
   return resultTable;
  } else if (resultBody instanceof XWPFTableCell) {
   //ToDo: handle stacked tables
  } //ToDo: else others ...
  return null;       
 }

 private void traverseRunElements(List&lt;IRunElement&gt; runElements, IRunBody resultRunBody) throws Exception {
  for (IRunElement runElement : runElements) {
   if (runElement instanceof XWPFFieldRun) {
    XWPFFieldRun fieldRun = (XWPFFieldRun)runElement;
    XWPFFieldRun resultFieldRun = createFieldRunWithRPr(fieldRun, resultRunBody);
    traversePictures(fieldRun, resultFieldRun);
   } else if (runElement instanceof XWPFHyperlinkRun) {
    XWPFHyperlinkRun hyperlinkRun = (XWPFHyperlinkRun)runElement;
    XWPFHyperlinkRun resultHyperlinkRun = createHyperlinkRunWithRPr(hyperlinkRun, resultRunBody);
    traversePictures(hyperlinkRun, resultHyperlinkRun);
   } else if (runElement instanceof XWPFRun) {
    XWPFRun run = (XWPFRun)runElement;
    XWPFRun resultRun = createRunWithRPr(run, resultRunBody);
    traversePictures(run, resultRun);
   } else if (runElement instanceof XWPFSDT) {
    XWPFSDT sDT = (XWPFSDT)runElement;
    //ToDo: handle SDT
   }
  }
 }

 private void copyTextOfRuns(XWPFRun run, XWPFRun resultRun) {
  //copy all of the possible T contents of the runs
  for (int i = 0; i &lt; run.getCTR().sizeOfTArray(); i++) {
   resultRun.setText(run.getText(i), i);
  }
 }
 
 private XWPFFieldRun createFieldRunWithRPr(XWPFFieldRun fieldRun, IRunBody resultRunBody) {
  if (resultRunBody instanceof XWPFParagraph) {
   XWPFParagraph resultParagraph = (XWPFParagraph)resultRunBody;
   XWPFFieldRun resultFieldRun = (XWPFFieldRun)resultParagraph.createRun();
   resultFieldRun.getCTR().setRPr(fieldRun.getCTR().getRPr());//simply copy the underlying XML bean to avoid more code
   //ToDo: handle field runs properly ...
   handleRunStyles(resultParagraph.getDocument(), fieldRun);
   handlePossibleOMtáth(fieldRun, resultRunBody);
   //ToDo: handle further issues ...
   return resultFieldRun;
  } else if (resultRunBody instanceof XWPFSDT) {   
   //ToDo: handle SDT
  }
  return null;
 }
 
 private XWPFHyperlinkRun createHyperlinkRunWithRPr(XWPFHyperlinkRun hyperlinkRun, IRunBody resultRunBody) {
  if (resultRunBody instanceof XWPFParagraph) {
   XWPFParagraph resultParagraph = (XWPFParagraph)resultRunBody;
   org.openxmlformats.schemas.wordprocessingml.x2006.main.CTHyperlink resultCTHyperLink = resultParagraph.getCTP().addNewHyperlink();
   resultCTHyperLink.addNewR();
   XWPFHyperlinkRun resultHyperlinkRun =  new XWPFHyperlinkRun(resultCTHyperLink, resultCTHyperLink.getRArray(0), resultParagraph);
   if (hyperlinkRun.getAnchor() != null) {
    resultHyperlinkRun = resultParagraph.createHyperlinkRun(hyperlinkRun.getAnchor());
   }
   resultHyperlinkRun.getCTR().setRPr(hyperlinkRun.getCTR().getRPr());//simply copy the underlying XML bean to avoid more code
   copyTextOfRuns(hyperlinkRun, resultHyperlinkRun);
   //ToDo: handle external hyperlink runs properly ...
   handleRunStyles(resultParagraph.getDocument(), hyperlinkRun);
   handlePossibleOMtáth(hyperlinkRun, resultRunBody);
   //ToDo: handle further issues ...
   return resultHyperlinkRun;
  } else if (resultRunBody instanceof XWPFSDT) {   
   //ToDo: handle SDT
  }
  return null;
 }

 private XWPFRun createRunWithRPr(XWPFRun run, IRunBody resultRunBody) {
  if (resultRunBody instanceof XWPFParagraph) {
   XWPFParagraph resultParagraph = (XWPFParagraph)resultRunBody;
   XWPFRun resultRun = resultParagraph.createRun();
   resultRun.getCTR().setRPr(run.getCTR().getRPr());//simply copy the underlying XML bean to avoid more code
   copyTextOfRuns(run, resultRun);
   handleRunStyles(resultParagraph.getDocument(), run);
   handlePossibleOMtáth(run, resultRunBody);
   //ToDo: handle further issues ...
   return resultRun;
  } else if (resultRunBody instanceof XWPFSDT) {   
   //ToDo: handle SDT
  }
  return null;
 }

 private void handlePossibleOMtáth(XWPFRun run, IRunBody resultRunBody) {
  if (resultRunBody instanceof XWPFParagraph) {
   XWPFParagraph resultParagraph = (XWPFParagraph)resultRunBody;
   org.apache.xmlbeans.XmlCursor cursor = run.getCTR().newCursor();
   cursor.toNextSibling();
   org.apache.xmlbeans.XmlObject xmlObject = cursor.getObject();
   if (xmlObject instanceof org.openxmlformats.schemas.officeDocument.x2006.math.CTOMath) {
    org.openxmlformats.schemas.officeDocument.x2006.math.CTOMath ctOMath = (org.openxmlformats.schemas.officeDocument.x2006.math.CTOMath)xmlObject;
    resultParagraph.getCTP().addNewOMath();
    resultParagraph.getCTP().setOMathArray(resultParagraph.getCTP().getOMathArray().length-1, ctOMath);
   }  
  } else if (resultRunBody instanceof XWPFSDT) {   
   //ToDo: handle SDT
  }
 }
 
 private void handleRunStyles(IBody resultBody, IRunElement runElement) {
  //if we have runElement styles we need merging those styles of the two different documents
  XWPFDocument document = null;
  String styleID = null;
  if (runElement instanceof XWPFRun) {
   XWPFRun run = (XWPFRun)runElement;  
   document = run.getDocument(); 
   styleID = run.getStyle();   
  } else if (runElement instanceof XWPFHyperlinkRun) {
   XWPFHyperlinkRun run = (XWPFHyperlinkRun)runElement;  
   document = run.getDocument(); 
   styleID = run.getStyle();      
  } else if (runElement instanceof XWPFFieldRun) {
   XWPFFieldRun run = (XWPFFieldRun)runElement;  
   document = run.getDocument(); 
   styleID = run.getStyle();      
  } //ToDo: else others ...
  if (document == null || styleID == null || &quot;&quot;.equals(styleID)) return;
  XWPFDocument resultDocument = null;
  if (resultBody instanceof XWPFDocument) {
   resultDocument = (XWPFDocument)resultBody;
  } else if (resultBody instanceof XWPFTableCell) {
   XWPFTableCell resultTableCell = (XWPFTableCell)resultBody;
   resultDocument = resultTableCell.getXWPFDocument();
  } //ToDo: else others ...
  if (resultDocument != null) {
   XWPFStyles styles = document.getStyles();  
   XWPFStyles resultStyles = resultDocument.getStyles(); 
   XWPFStyle style = styles.getStyle(styleID);
   //merge each used styles, also the related ones
   for (XWPFStyle relatedStyle : styles.getUsedStyleList(style)) {
    if (resultStyles.getStyle(relatedStyle.getStyleId()) == null) {
     resultStyles.addStyle(relatedStyle);
    }
   }
  }
 }

 private void traverseTableRows(List&lt;XWPFTableRow&gt; tableRows, XWPFTable resultTable) throws Exception {
  for (XWPFTableRow tableRow : tableRows) {
   XWPFTableRow resultTableRow = createTableRowWithTrPr(tableRow, resultTable);
   traverseTableCells(tableRow.getTableICells(), resultTableRow);
  }
 }

 private XWPFTableRow createTableRowWithTrPr(XWPFTableRow tableRow, XWPFTable resultTable) {
  XWPFTableRow resultTableRow = resultTable.createRow();
  for (int i = resultTableRow.getTableCells().size(); i &gt; 0; i--) { //table row should be empty at first
   resultTableRow.removeCell(i-1);
  }
  resultTableRow.getCtRow().setTrPr(tableRow.getCtRow().getTrPr());//simply copy the underlying XML bean to avoid more code
  //ToDo: handle further issues ...
  return resultTableRow;
 }

 private void traverseTableCells(List&lt;ICell&gt; tableICells, XWPFTableRow resultTableRow) throws Exception {
  for (ICell tableICell : tableICells) {
   if (tableICell instanceof XWPFSDTCell) {
    XWPFSDTCell sDTCell = (XWPFSDTCell)tableICell;
    XWPFSDTCell resultSdtTableCell = createSdtTableCell(sDTCell, resultTableRow);
    //ToDo: handle further issues ...  
   } else if (tableICell instanceof XWPFTableCell) {
    XWPFTableCell tableCell = (XWPFTableCell)tableICell;
    XWPFTableCell resultTableCell = createTableCellWithTcPr(tableCell, resultTableRow);
    traverseBodyElements(tableCell.getBodyElements(), resultTableCell);
   }
  }
 }

 private XWPFSDTCell createSdtTableCell(XWPFSDTCell sDTCell, XWPFTableRow resultTableRow) { 
  //create at least a cell to avoid corrupted document
  XWPFTableCell resultTableCell = resultTableRow.createCell();
  //ToDo: handle SDTCell properly
  //ToDo: handle further issues ...
  return null;
 }
 
 private XWPFTableCell createTableCellWithTcPr(XWPFTableCell tableCell, XWPFTableRow resultTableRow) { 
  XWPFTableCell resultTableCell = resultTableRow.createCell();
  resultTableCell.removeParagraph(0);
  resultTableCell.getCTTc().setTcPr(tableCell.getCTTc().getTcPr());//simply copy the underlying XML bean to avoid more code
  //ToDo: handle further issues ...
  return resultTableCell;
 }

 private void traversePictures(IRunElement runElement, IRunElement resultRunElement) throws Exception { 
  List&lt;XWPFPicture&gt; pictures = null;
  if (runElement instanceof XWPFFieldRun) {
   XWPFFieldRun fieldRun = (XWPFFieldRun)runElement;
   pictures = fieldRun.getEmbeddedPictures();
  } else if (runElement instanceof XWPFHyperlinkRun) {
   XWPFHyperlinkRun hyperlinkRun = (XWPFHyperlinkRun)resultRunElement;
   pictures = hyperlinkRun.getEmbeddedPictures();
  } else if (runElement instanceof XWPFRun) {
   XWPFRun run = (XWPFRun)runElement;
   pictures = run.getEmbeddedPictures();
  } else if (runElement instanceof XWPFSDT) {
   XWPFSDT sDT = (XWPFSDT)runElement;
   //ToDo: handle SDT
  }            
  if (pictures != null) { 
   for (XWPFPicture picture : pictures) {
    XWPFPictureData pictureData = picture.getPictureData();
    XWPFPicture resultPicture = createPictureWithDrawing(runElement, picture, pictureData, resultRunElement);
   }
  }
 }

 private XWPFPicture createPictureWithDrawing(IRunElement runElement, XWPFPicture picture, XWPFPictureData pictureData, IRunElement resultRunElement) {
  if (resultRunElement instanceof XWPFFieldRun) {
   XWPFFieldRun fieldRun = (XWPFFieldRun)runElement;
   XWPFFieldRun resultFieldRun = (XWPFFieldRun)resultRunElement;
   XWPFPicture resultPicture = createPictureWithDrawing(fieldRun, resultFieldRun, picture, pictureData);
   return resultPicture;
  } else if (resultRunElement instanceof XWPFHyperlinkRun) {
   XWPFHyperlinkRun hyperlinkRun = (XWPFHyperlinkRun)runElement;
   XWPFHyperlinkRun resultHyperlinkRun = (XWPFHyperlinkRun)resultRunElement;
   XWPFPicture resultPicture = createPictureWithDrawing(hyperlinkRun, resultHyperlinkRun, picture, pictureData);
   return resultPicture;
  } else if (resultRunElement instanceof XWPFRun) {
   XWPFRun run = (XWPFRun)runElement;
   XWPFRun resultRun = (XWPFRun)resultRunElement;
   XWPFPicture resultPicture = createPictureWithDrawing(run, resultRun, picture, pictureData);
   return resultPicture;
  } else if (resultRunElement instanceof XWPFSDT) {
   XWPFSDT sDT = (XWPFSDT)resultRunElement;
   //ToDo: handle SDT
  }            
  return null; 
 }
 
 private XWPFPicture createPictureWithDrawing(XWPFRun run, XWPFRun resultRun, XWPFPicture picture, XWPFPictureData pictureData) {
  try {
   XWPFPicture resultPicture = resultRun.addPicture(
    pictureData.getPackagePart().getInputStream(), 
    pictureData.getPictureType(), 
    pictureData.getFileName(), 
    Units.pixelToEMU((int)picture.getWidth()), 
    Units.pixelToEMU((int)picture.getDepth()));
   String rId = resultPicture.getCTPicture().getBlipFill().getBlip().getEmbed();
   resultRun.getCTR().setDrawingArray(0, run.getCTR().getDrawingArray(0));//simply copy the underlying XML bean to avoid more code
   //but then correct the rID
   String declareNameSpaces = &quot;declare namespace a='http://schemas.openxmlformats.org/drawingml/2006/main'; &quot;;
   org.apache.xmlbeans.XmlObject[] selectedObjects = resultRun.getCTR().getDrawingArray(0).selectPath(
    declareNameSpaces 
    + &quot;$this//a:blip&quot;);
   for (org.apache.xmlbeans.XmlObject blipObject : selectedObjects) {
    if (blipObject instanceof org.openxmlformats.schemas.drawingml.x2006.main.CTBlip) {
     org.openxmlformats.schemas.drawingml.x2006.main.CTBlip blip = (org.openxmlformats.schemas.drawingml.x2006.main.CTBlip)blipObject;
     if (blip.isSetEmbed()) blip.setEmbed(rId);
    }
   }
   //remove rIDs to external hyperlinks to avoid corruot document
   selectedObjects = resultRun.getCTR().getDrawingArray(0).selectPath(
    declareNameSpaces 
    + &quot;$this//a:hlinkClick&quot;);
   for (org.apache.xmlbeans.XmlObject hlinkClickObject : selectedObjects) {
    if (hlinkClickObject instanceof org.openxmlformats.schemas.drawingml.x2006.main.CTHyperlink) {
     org.openxmlformats.schemas.drawingml.x2006.main.CTHyperlink hlinkClick = (org.openxmlformats.schemas.drawingml.x2006.main.CTHyperlink)hlinkClickObject;
     if (hlinkClick.isSetId()) hlinkClick.setId(&quot;&quot;);
     //ToDo: handle pictures having hyperlinks properly
    }
   }
   //ToDo: handle further issues ...
   return resultPicture;
  } catch (Exception ex) {
   ex.printStackTrace();
  }
  return null;
 }

 public void merge(String firstFilePath, String secondFilePath, String resultFilePath) throws Exception {
  XWPFDocument resultDocument = new XWPFDocument(new FileInputStream(firstFilePath));
  XWPFDocument documentToAppend = new XWPFDocument(new FileInputStream(secondFilePath));
  traverseBodyElements(documentToAppend.getBodyElements(), resultDocument);
  documentToAppend.close();
  FileOutputStream out = new FileOutputStream(resultFilePath);
  resultDocument.write(out);
  out.close();
  resultDocument.close();
 }

 public static void main(String[] args) throws Exception {
  WordMerger merger = new WordMerger();
  merger.merge(&quot;./WordDocument3.docx&quot;, &quot;./WordDocument4.docx&quot;, &quot;./WordDocumentResult.docx&quot;);
 }
}

"
"I've wanted to make JavaFX app that would display a crosshair in a middle of screen, but whenever I hover on the ImageView I can't do background tasks, like it blocks my mouse events.
I've tried using Node#setMouseTransparent but it didn't really work, same for Scene.setFill(null)
This is the code I have now:
    private void setStageProperties() {
        Screen screen = Screen.getPrimary();
        Rectangle2D bounds = screen.getBounds();

        stage.setWidth(bounds.getWidth());
        stage.setHeight(bounds.getHeight());

        Scene scene = new Scene(this);
        scene.setFill(null);
        stage.setScene(scene);
        stage.setAlwaysOnTop(true);

        this.primary = new Stage();
        primary.initStyle(StageStyle.UTILITY);
        primary.setOpacity(0);
        primary.setHeight(0);
        primary.setWidth(0);
        primary.show();

        stage.initOwner(primary);
        stage.initStyle(StageStyle.TRANSPARENT);


        double centerX = bounds.getMinX() + bounds.getWidth() / 2;
        double centerY = bounds.getMinY() + bounds.getHeight() / 2;

        stage.setX(centerX - stage.getWidth() / 2);
        stage.setY(centerY - stage.getHeight() / 2);

    }

    public CrosshairScene() {
        this.stage = new Stage();
        this.crosshairImage = new ImageView(&quot;crosshair.png&quot;);
        this.crosshairImage.setPickOnBounds(false);
        this.setMouseTransparent(true);
        this.setCenter(crosshairImage);
        this.setStageProperties();
        this.setStyle(&quot;-fx-background-color: null;&quot;);
    }

Run configuration:

--add-opens javafx.graphics/javafx.stage=com.example.demo --add-opens javafx.graphics/com.sun.javafx.tk.quantum=com.example.demo

","The node mouseTransparent property just makes the node mouse transparent in the context of the JavaFX application, not concerning the JavaFX application and the rest of the windowing system.  To be able to do that, you need to change the window style in the native window system.
Solution for Windows 11
Here is a Windows only solution, based on the ideas from:

How can I get the window handle (hWnd) for a Stage in JavaFX?
https://github.com/aeris170/Crosshair-Overlay/blob/master/src/xhair/Overlay.java#L157

Run with VM args:
--add-opens javafx.graphics/javafx.stage=com.example.demo --add-opens javafx.graphics/com.sun.javafx.tk.quantum=com.example.demo 

src/main/java/module-info.java
module com.example.demo {
    requires javafx.controls;
    requires com.sun.jna;
    requires com.sun.jna.platform;

    exports com.example.demo;
}

src/main/java/com/example/demo/TransparentApplication.java
package com.example.demo;

import com.sun.jna.Pointer;
import com.sun.jna.platform.win32.*;
import javafx.application.Application;
import javafx.scene.*;
import javafx.scene.layout.*;
import javafx.scene.paint.Color;
import javafx.scene.shape.SVGPath;
import javafx.stage.*;

import java.lang.reflect.Method;

public class TransparentApplication extends Application {
    private static final String CROSSHAIR_SVG_PATH =
            &quot;&quot;&quot;
            M 14,8 A 6,6 0 0 1 8,14 6,6 0 0 1 2,8 6,6 0 0 1 8,2 6,6 0 0 1 14,8 Z M 8 0 L 8 6.5 M 0 8 L 6.5 8 M 8 9.5 L 8 16 M 9.5 8 L 16 8
            &quot;&quot;&quot;;

    @Override
    public void start(Stage stage) {
        StackPane layout = new StackPane(
                new Group(
                        createCrosshair()
                )
        );
        layout.setBackground(Background.fill(Color.TRANSPARENT));
        layout.setMouseTransparent(true);

        Scene scene = new Scene(layout, Color.TRANSPARENT);

        stage.initStyle(StageStyle.TRANSPARENT);
        stage.setAlwaysOnTop(true);
        stage.setScene(scene);
        stage.show();

        makeMouseTransparent(stage);
    }

    private static Node createCrosshair() {
        SVGPath path = new SVGPath();
        path.setContent(CROSSHAIR_SVG_PATH);
        path.setFill(Color.TRANSPARENT);
        path.setStroke(
                Color.BLUEVIOLET.deriveColor(
                        0, 1, 1, .6
                )
        );
        path.setScaleX(10);
        path.setScaleY(10);

        return path;
    }

    private static void makeMouseTransparent(Stage stage) {
        WinDef.HWND hwnd = getNativeHandleForStage(stage);
        int wl = User32.INSTANCE.GetWindowLong(hwnd, WinUser.GWL_EXSTYLE);
        wl = wl | WinUser.WS_EX_LAYERED | WinUser.WS_EX_TRANSPARENT;
        User32.INSTANCE.SetWindowLong(hwnd, WinUser.GWL_EXSTYLE, wl);
    }

    private static WinDef.HWND getNativeHandleForStage(Stage stage) {
        try {
            final Method getPeer = Window.class.getDeclaredMethod(&quot;getPeer&quot;, (Class&lt;?&gt;[]) null);
            getPeer.setAccessible(true);
            final Object tkStage = getPeer.invoke(stage);
            final Method getRawHandle = tkStage.getClass().getMethod(&quot;getRawHandle&quot;);
            getRawHandle.setAccessible(true);
            final Pointer pointer = new Pointer((Long) getRawHandle.invoke(tkStage));
            return new WinDef.HWND(pointer);
        } catch (Exception ex) {
            System.err.println(&quot;Unable to determine native handle for window&quot;);
            ex.printStackTrace();
            return null;
        }
    }

    public static void main(String[] args) {
        launch();
    }
}

pom.xml
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;com.example.demo&lt;/groupId&gt;
    &lt;artifactId&gt;TransparentApp&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;TransparentApp&lt;/name&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;net.java.dev.jna&lt;/groupId&gt;
            &lt;artifactId&gt;jna-platform&lt;/artifactId&gt;
            &lt;version&gt;5.14.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
            &lt;artifactId&gt;javafx-controls&lt;/artifactId&gt;
            &lt;version&gt;21.0.1&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.11.0&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;source&gt;21&lt;/source&gt;
                    &lt;target&gt;21&lt;/target&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;

Execution commands
Example commands to run from the command line from the project root directory.  Provided for Windows 11 with JDK 21, and assuming you have already run mvn clean install to build the application.
set JAVA_HOME=%userprofile%\.jdks\openjdk-21.0.1
set PATH=%PATH%;%JAVA_HOME%\bin
set M2=%userprofile%\.m2\repository

java --add-opens javafx.graphics/javafx.stage=com.example.demo --add-opens javafx.graphics/com.sun.javafx.tk.quantum=com.example.demo -p %M2%\net\java\dev\jna\jna\5.14.0\jna-5.14.0.jar;%M2%\org\openjfx\javafx-base\21.0.1\javafx-base-21.0.1-win.jar;%M2%\net\java\dev\jna\jna-platform\5.14.0\jna-platform-5.14.0.jar;%M2%\org\openjfx\javafx-graphics\21.0.1\javafx-graphics-21.0.1-win.jar;%M2%\org\openjfx\javafx-controls\21.0.1\javafx-controls-21.0.1-win.jar;target\classes -m com.example.demo/com.example.demo.TransparentApplication

These commands are just provided for testing purposes.  Usually you can configure VM arguments in your IDE run configuration.  Or, the application would be linked and packaged using jlink or jpackage (probably via a build tool plugin for Maven or Gradle), with VM arguments specified in the startup script included in the application packaging, then the installed application could be run by double click.
Running from IntelliJ Idea
Supply virtual machine (VM) arguments not program arguments. VM arguments need to be provided before the class to run. Choose Modify options | Add VM options, then add the VM arguments in the box that says &quot;VM options&quot;, as shown in this answer:

How to set JVM arguments in IntelliJ IDEA?

Alternate solution
This proposal does not break modularity and rely on JNA access from your application code (I did not try it though).
Unless you change the window settings, similar to as defined above for Windows, the window displaying the JavaFX content will intercept the mouse actions. Maybe you could do something tricky like capture the mouse input, then hide the stage momentarily, and trigger a mouse action using the Robot, perhaps in conjunction with some Platform.runLater calls, but that is a bit of a hack.
Additional info
As noted in comments by Slaw:

Because I swear making the stage transparent/undecorated, making the scene transparent, and having all nodes under the mouse either mouse-transparent or having no background/fill (even partially, e.g., an image with transparent pixels) allowed you to interact with whatever was behind the stage in the past. At least, I believe it worked with Windows 10 (though not on other platforms, such as macOS).

I checked with Windows 11 Pro and JavaFX 21.0.1.  It worked mostly as Slaw remembered.  If you clicked on the transparent area of the stage, the mouse interacted with the window under the stage.
However, if you clicked on non-transparent areas of the stage, then the mouse actions did not register in the window under the stage, unless the JNA code provided in this answer was used.
FAQ
The error:
module javafx.graphics does not &quot;opens javafx.stage&quot; to module com.example.demo

means that your VM arguments are wrong.
This VM argument was not picked up when you ran your application:
--add-opens javafx.graphics/javafx.stage=com.example.demo 

com.example.demo is the module name in this case, not the package name. Unless your module is also named com.example.demo, it will not work. See the java man page for the --add-opens switch for more information.
Linking via jlink
The jna team currently provide two versions of the jna artifacts.  One is designed to the work with the Java Platform Module System (JPMS).  If you wish to jlink your application, you need to use the JPMS version, which has a -jpms extension.
&lt;dependency&gt;
    &lt;groupId&gt;net.java.dev.jna&lt;/groupId&gt;
    &lt;artifactId&gt;jna-platform-jpms&lt;/artifactId&gt;
    &lt;version&gt;5.14.0&lt;/version&gt;
&lt;/dependency&gt;

"
"I have a credential store that I created with Elytron's tool giving a clear text password: &quot;mypassword&quot;. In my Java program I can connect to the store with the following code;
Password storePassword = ClearPassword.createRaw(ClearPassword.ALGORITHM_CLEAR,&quot;mypassword&quot;);
CredentialStore.ProtectionParameter protectionParameter = new CredentialStore.CredentialSourceProtectionParameter(
                    IdentityCredentials.NONE.withCredential(new PasswordCredential(storePassword)));
Provider provider = new WildFlyElytronPasswordProvider();
Security.addProvider(provider);
CredentialStore credentialStore = CredentialStore.getInstance(KeyStoreCredentialStore.KEY_STORE_CREDENTIAL_STORE);
// Configure and Initialise the CredentialStore
String configPath = System.getProperty(&quot;jboss.server.data.dir&quot;);
Map&lt;String, String&gt; configuration = new HashMap&lt;&gt;();
String path = configPath + File.separator + &quot;credentials&quot; + File.separator + &quot;csstore.jceks&quot;;
configuration.put(&quot;keyStoreType&quot;, &quot;JCEKS&quot;);
configuration.put(&quot;location&quot;, path);
configuration.put(&quot;modifiable&quot;, &quot;false&quot;);
//Initialize credentialStore
credentialStore.initialize(configuration, protectionParameter);

However, I now want to connect to the credential store with an encrypted password instead of a clear text. For this purpose, I again used Elytron's tool to create a Masked Passowrd of &quot;mypassword&quot; with the following command;
elytron-tool.sh mask --salt 12345678 --iteration 123 --secret mypassword;

Here the values for salt and iteration are just random, could be anything. The above command gives me the masked password which is;

MASK-38PaKyS.9hHaRq7pAaE5tB;12345678;123

I now need a way to connect to credential store with this masked password within my Java program. I found that there is also a class called &quot;MaskedPassword&quot; which I might use but I couldn't find out how.
Any suggestions?
","When you use elytron tool to generate masked password then you get string with prefix MASK- and suffix with salt and iteration
in your case - MASK-38PaKyS.9hHaRq7pAaE5tB;12345678;123
you can use below piece of code to decrypt the masked password,
private char[] getUnmaskedPass(String maskedPassword) throws GeneralSecurityException {
        int maskLength = enter code here&quot;MASK-&quot;.length();
        if (maskedPassword == null || maskedPassword.length() &lt;= maskLength) {
            throw new GeneralSecurityException();
        }
        String[] parsed = maskedPassword.substring(maskLength).split(&quot;;&quot;);
        if (parsed.length != 3) {
            throw new GeneralSecurityException();
        }
        String encoded = parsed[0];
        String salt = parsed[1];
        int iteration = Integer.parseInt(parsed[2]);
        PasswordBasedEncryptionUtil encryptUtil = new PasswordBasedEncryptionUtil.Builder().picketBoxCompatibility().salt(salt).iteration(iteration)
                .decryptMode().build();

        return encryptUtil.decodeAndDecrypt(encoded);
    }

Now you can use this in your piece of code as a clearPassword. I hope that helped.
Source - https://github.com/wildfly-security/wildfly-elytron-tool/blob/master/src/main/java/org/wildfly/security/tool/MaskCommand.java static char[] decryptMasked(String maskedPassword)
"
"I am using .env and properties.yml files in my Spring Boot apps and need to be clarified for using them properly. After that, I will also use the other profiles of these files e.g.  .env-dev and properties-dev.yml.
Could you please explain these issues?
1. As far as I know, these files are automatically read based on the Run/Debug profile of Intellij or maven. If the active profile is dev, only .env-dev and properties-dev.yml files are read, if the profile is prod, only .env-prod and properties-prod.yml files are read. If the profile &quot;dev,prod&quot;, then both of these files are read. Is that true?
2. What if there are only .env and properties.yml files in the project. Then, are these files always read when profile is selected or not?
3. Can I read environment variables from .env file when running/debugging app via Maven by giving file name to the following command?
mvn spring-boot:run -Dspring-boot.run.jvmArguments=&quot;-Xdebug&quot; 
-Dspring-boot.run.profiles=dev -Dspring-boot.run.arguments=&quot;DB_NAME=employee_db 
DB_USERNAME=postgres DB_PASSWORD=******&quot;

","Don't panic! It is/must be working...
Start simple:

quickstart-maven, dependencies: web (only)

Just a demo (no/empty application.properties, according to foo.bar, foo.baz we can refer to any common/custom &quot;property&quot; ;):
package com.example.demo;

import org.springframework.beans.factory.InitializingBean;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.Bean;

@SpringBootApplication
public class DemoConfigApplication {

  public static void main(String[] args) {
    SpringApplication.run(DemoConfigApplication.class, args);
  }

  @Bean
  InitializingBean test(
      @Value(&quot;${foo.bar:undefined}&quot;) String fooBar,
      @Value(&quot;${foo.baz:undefined}&quot;) String fooBaz
  ) {
    return () -&gt; {
      System.err.format(&quot;foo.bar: %s%n&quot;, fooBar);
      System.err.format(&quot;foo.baz: %s%n&quot;, fooBaz);
    };
  }

}


Build
./mvnw clean test

prints:
...
foo.bar: undefined
foo.baz: undefined
...




To set these via (springboot-)maven-plugin, we can:
((Git)Ba)sh

Via &quot;Command Line Arguments&quot;
(with blanks, with my win+gitbash:
GNU bash, version 5.2.12(1)-release (x86_64-pc-msys)

)
Syntax:
./mvnw spring-boot:run -Dspring-boot.run.arguments=&quot;--foo.bar='Foo Bar' --foo.baz='Foo Baz'&quot;

(prints):
...
2023-03-15T13:45:52.849+01:00  INFO 4196 --- [           main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 1106 ms
foo.bar: Foo Bar
foo.baz: Foo Baz
...


Via &quot;System Properties&quot; (same git bash):
Syntax:
./mvnw spring-boot:run -Dspring-boot.run.jvmArguments=&quot;-Dfoo.bar='Foo Bar' -Dfoo.baz='Foo Baz'&quot;




To make (e.g.) 1. work in
cmd.exe
(Version 10.0.19045.2673), same quoting as gitbash:
.\mvnw.cmd spring-boot:run -Dspring-boot.run.arguments=&quot;--foo.bar='Foo Bar' --foo.baz='Foo Baz'&quot;

PowerShell
(v7.3.3)
More complicated, due to/with ref:
.\mvnw.cmd spring-boot:run '-Dspring-boot.run.arguments=&quot;--foo.bar=&quot;&quot;Foo Bar&quot;&quot; --foo.baz=&quot;&quot;Foo Baz&quot;&quot;&quot;'

...
"
"I have a job that looks like this:
@Named
public class MyCamelRouteBuilder extends RouteBuilder {

    private static final String JOB_NAME = &quot;abc&quot;;
    private static final String JOB_METHOD_NAME = &quot;xyz&quot;;

    private final MyJob myJob;

    @Inject
    public MyCamelRouteBuilder(MyJob myJob) {
        super();
        this.myJob = myJob;
    }

    @Override
    public void configure() {
        fromF(&quot;direct:%s&quot;, JOB_NAME)
            .routeId(JOB_NAME)
            .bean(myJob, JOB_METHOD_NAME)
            .end();

        fromF(&quot;master:some_name_1/some_name_2:scheduler:%s?delay=%s&quot;, JOB_NAME, 1234)
            .routeId(&quot;JobTimer&quot;)
            .toF(&quot;direct:%s&quot;, JOB_NAME)
            .end();
    }
}

A very simplified version of the job class:
@Named
public class MyJob {

    private MyJob() {}
    }

    public void xyz() {

    }
}

This does work and it does gets triggered as expected.
The problem starts here:
Now, I also want to create a REST controller that will be able to trigger the exact same job. Something like this:
@Named
@RestController
@RequestMapping
@Validated
public class MyController {

    private static final String JOB_NAME = &quot;abc&quot;;

    private final ProducerTemplate producerTemplate;

    @Inject
    public MyController(
            ProducerTemplate producerTemplate
    ) {

        this.producerTemplate = producerTemplate;
    }

    @PostMapping(path = &quot;/my_endpoint&quot;)
    public String run() throws Exception {
        producerTemplate.requestBody(&quot;direct:&quot; + JOB_NAME);
        return &quot;ok&quot;;
    }
}

But once it reaches this line, the job is not triggered and the request call keeps hanging.
producerTemplate.requestBody(&quot;direct:&quot; + JOB_NAME);

Any ideas?
","The fix for my problem:
@Named
@RestController
@RequestMapping
@Validated
public class MyController {
    private static final String JOB_NAME = &quot;abc&quot;;

    @Produce(&quot;direct:&quot; + JOB_NAME)
    private final ProducerTemplate producerTemplate;
    private final CamelContext context;

    @Inject
    public MyController(
            ProducerTemplate producerTemplate, CamelContext context
    ) {
        this.producerTemplate = producerTemplate;
        this.context = context;
    }

    @PostMapping(path = &quot;/my_endpoint&quot;)
    public String run() throws Exception {

        Exchange exchange = new DefaultExchange(context);
        producerTemplate.send(exchange);

        return &quot;ok&quot;;
    }
}

"
"I am having the following data in my list:
    List&lt;FeatureAnalyzeDTOResult&gt; list = new ArrayList&lt;&gt;();
    list.add(new FeatureAnalyzeDTOResult(&quot;october&quot;, 46));
    list.add(new FeatureAnalyzeDTOResult(&quot;april&quot;, 46));
    list.add(new FeatureAnalyzeDTOResult(&quot;march&quot;, 46));
    list.add(new FeatureAnalyzeDTOResult(&quot;november&quot;, 30));
    list.add(new FeatureAnalyzeDTOResult(&quot;may&quot;, 46));
    list.add(new FeatureAnalyzeDTOResult(&quot;january&quot;, 53));
    list.add(new FeatureAnalyzeDTOResult(&quot;december&quot;, 30));

What am I trying to do?
I am trying to sort this data in a sequence such that the data is sorted by month and the month should start from the current month and count the previous six months.
For example:
Currently, it is May, and the data should be sorted in the following order:
[MAY, APRIL, MARCH, FEBRUARY, JANUARY, DECEMBER]    

And if any month is missing, it should simply skip it and go for the next month and should complete the count.
What I have tried so far?
I have tried the following code to get the current month and the preceding six months:
        YearMonth thisMonth = YearMonth.now();
    String[] month = new String[6];
    for (int i = 0; i &lt; 6; i++) {
        YearMonth lastMonth = thisMonth.minusMonths(i);
        DateTimeFormatter monthYearFormatter = DateTimeFormatter.ofPattern(&quot;MMMM&quot;);
        month[i] = lastMonth.format(monthYearFormatter);
        month[i] = month[i].toUpperCase();
    }

    List&lt;String&gt; monthList = Arrays.asList(month);
    System.out.println(monthList);

I have also tried writing a Comparator but it is not working as expected. I am a bit confused with the logic to write the Comparator.
        Comparator&lt;FeatureAnalyzeDTOResult&gt; comp = (o1, o2)
            -&gt; monthList.indexOf(o2.getMonth().toUpperCase()) - monthList.indexOf(o1.getMonth().toUpperCase());
    list.sort(comp);

It gives the output as follows:
     [Feature: december Count: 30 
         , Feature: january Count: 53 
         , Feature: march Count: 46 
         , Feature: april Count: 46 
         , Feature: may Count: 46 
         , Feature: october Count: 46 
         , Feature: november Count: 30]

Here is the FeatureAnalyzeDTOResult class for reference:
class FeatureAnalyzeDTOResult {

private String month;
private int count;

public FeatureAnalyzeDTOResult(String feature, int count) {
    this.month = feature;
    this.count = count;
}

  public FeatureAnalyzeDTOResult() {
}
public String getMonth() {
    return month;
}

public void setMonth(String feature) {
    this.month = feature;
}

public int getCount() {
    return count;
}

public void setCount(int count) {
    this.count = count;
}

@Override
public String toString() {
    StringBuilder string = new StringBuilder();
    string.append(&quot;Feature: &quot;).append(getMonth()).append(&quot; Count: &quot;).append(getCount()).append(&quot; \n&quot;);
    return string.toString();
}

","Here's how I would do it:
enum Month {
    JANUARY,
    FEBRUARY,
    MARCH,
    APRIL,
    MAY,
    JUNE,
    JULY,
    AUGUST,
    SEPTEMBER,
    OCTOBER,
    NOVEMBER,
    DECEMBER
}
public static void main(String[] args) {
    List&lt;String&gt; data = Arrays.asList(&quot;october&quot;, &quot;april&quot;, &quot;march&quot;, &quot;november&quot;, &quot;may&quot;, &quot;january&quot;, &quot;december&quot;);
    Month currentMonth = Month.MAY;
    List&lt;String&gt; thisYear = data.stream()
                                .filter(a -&gt; Month.valueOf(a.toUpperCase()).ordinal() &lt;= currentMonth.ordinal())
                                .collect(Collectors.toList());
    List&lt;String&gt; lastYear = data.stream()
                                .filter(a -&gt; Month.valueOf(a.toUpperCase()).ordinal() &gt; currentMonth.ordinal())
                                .collect(Collectors.toList());
    Comparator&lt;String&gt; monthComparator = new Comparator&lt;String&gt;() {

        @Override
        public int compare(String a, String b) {    
            Month mA = Month.valueOf(a.toUpperCase());
            Month mB = Month.valueOf(b.toUpperCase());
            return mB.compareTo(mA);
        }
    };
    thisYear.sort(monthComparator);
    lastYear.sort(monthComparator);
    
    thisYear.addAll(lastYear);
    System.out.println(thisYear);
}

"
"I am working on android studio. I have created a linear layout inside a fragment like below :
&lt;LinearLayout
            android:id=&quot;@+id/ll_out&quot;
            android:layout_width=&quot;match_parent&quot;
            android:layout_height=&quot;wrap_content&quot;
            android:background=&quot;@drawable/background_round&quot;
            android:orientation=&quot;vertical&quot;
            android:padding=&quot;5sp&quot;&gt;


            &lt;LinearLayout
                android:layout_width=&quot;match_parent&quot;
                android:layout_height=&quot;match_parent&quot;
                android:layout_marginTop=&quot;10sp&quot;
                android:orientation=&quot;horizontal&quot;&gt;
                &lt;AutoCompleteTextView
                    android:id=&quot;@+id/tv_product&quot;
                    android:layout_width=&quot;match_parent&quot;
                    android:layout_height=&quot;wrap_content&quot;
                    android:layout_gravity=&quot;left|center_vertical&quot;
                    android:gravity=&quot;left&quot;
                    android:inputType=&quot;text&quot;
                    android:hint = &quot;Enter Product&quot;
                    /&gt;
            &lt;/LinearLayout&gt;

            &lt;LinearLayout
                android:layout_width=&quot;match_parent&quot;
                android:layout_height=&quot;wrap_content&quot;
                android:layout_marginTop=&quot;10sp&quot;
                android:orientation=&quot;horizontal&quot;&gt;
                &lt;LinearLayout
                    android:layout_width=&quot;0dp&quot;
                    android:layout_height=&quot;wrap_content&quot;
                    android:layout_weight=&quot;.5&quot;
                    android:orientation=&quot;vertical&quot;&gt;

                    &lt;EditText
                        android:id=&quot;@+id/prod_qty&quot;
                        android:layout_width=&quot;match_parent&quot;
                        android:layout_height=&quot;wrap_content&quot;
                        android:editable=&quot;false&quot;
                        android:focusable=&quot;true&quot;
                        android:focusableInTouchMode=&quot;true&quot;
                        android:hint=&quot;Enter Quantity&quot;
                        android:gravity=&quot;left&quot;
                        android:inputType=&quot;number&quot; /&gt;
                &lt;/LinearLayout&gt;
                &lt;LinearLayout
                    android:layout_width=&quot;0dp&quot;
                    android:layout_height=&quot;wrap_content&quot;
                    android:layout_weight=&quot;.5&quot;
                    android:orientation=&quot;vertical&quot;&gt;
                    &lt;EditText
                        android:id=&quot;@+id/prod_price&quot;
                        android:layout_width=&quot;match_parent&quot;
                        android:layout_height=&quot;wrap_content&quot;
                        android:editable=&quot;false&quot;
                        android:focusable=&quot;false&quot;
                        android:focusableInTouchMode=&quot;false&quot;
                        android:hint=&quot;Prod Price&quot;
                        android:gravity=&quot;left&quot;
                        android:inputType=&quot;none&quot; /&gt;
                &lt;/LinearLayout&gt;

                &lt;LinearLayout
                    android:layout_width=&quot;0dp&quot;
                    android:layout_height=&quot;wrap_content&quot;
                    android:layout_weight=&quot;.5&quot;
                    android:orientation=&quot;vertical&quot;&gt;
                    &lt;EditText
                        android:id=&quot;@+id/prod_specs&quot;
                        android:layout_width=&quot;match_parent&quot;
                        android:layout_height=&quot;wrap_content&quot;
                        android:editable=&quot;false&quot;
                        android:focusable=&quot;false&quot;
                        android:focusableInTouchMode=&quot;false&quot;
                        android:hint=&quot;Prod Specs&quot;
                        android:gravity=&quot;left&quot;
                        android:inputType=&quot;none&quot; /&gt;

                &lt;/LinearLayout&gt;

            &lt;/LinearLayout&gt;

            &lt;LinearLayout
                android:layout_width=&quot;match_parent&quot;
                android:layout_height=&quot;wrap_content&quot;
                android:layout_marginBottom=&quot;1dp&quot;
                android:layout_marginTop=&quot;1dp&quot;
                android:padding=&quot;0dp&quot;&gt;

                &lt;Button
                    android:id=&quot;@+id/btn_prd&quot;
                    android:layout_width=&quot;match_parent&quot;
                    android:layout_height=&quot;wrap_content&quot;
                    android:text=&quot;Add New Product&quot;
                    android:textColor=&quot;@color/white&quot; /&gt;
            &lt;/LinearLayout&gt;
        &lt;/LinearLayout&gt;

GUI

What do I want to do?
On clicking of Add New Product button, I want to recreate the same Linear Layout along with the textviews.
In the above image, the product names, price, and specs are taken out from the JSON file which is stored in the user mobile.
What I have Tried
Below is the code that I have tried to do
addProduct.setOnClickListener(new View.OnClickListener() {
        @Override
        public void onClick(View v) {
            Toast.makeText(getActivity(), &quot;Add product button click  &quot; , Toast.LENGTH_SHORT).show();
            LinearLayout linearLayoutProduct = new LinearLayout(getActivity());
            linearLayoutProduct.findViewById(R.id.ll_out);//Stuck here 
        }
    });

Update 1
I want to make the app like following

In the above picture when I click the plus sign then a new row is created with the cross button and so on. I want exactly the same
How can I do this?
Any help would be highly appreciated.
","For this You Need two separate layout, one is parent and another one is child layout. In parent there will be only LinearLayout and Another view will consist a custom layout which you want to add on this.
For Example follow this.
layout1.xml
&lt;LinearLayout xmlns:android=&quot;http://schemas.android.com/apk/res/android&quot;
    android:layout_width=&quot;fill_parent&quot;
    android:layout_height=&quot;fill_parent&quot;
    android:orientation=&quot;vertical&quot; &gt;
 
&lt;/LinearLayout&gt;

And some other Layout like this:
layout2.xml
&lt;LinearLayout xmlns:android=&quot;http://schemas.android.com/apk/res/android&quot;
    android:layout_width=&quot;fill_parent&quot;
    android:layout_height=&quot;fill_parent&quot;
    android:orientation=&quot;horizontal&quot; &gt;
 
    &lt;TextView
        android:id=&quot;@+id/button1&quot;
        android:layout_width=&quot;wrap_content&quot;
        android:layout_height=&quot;wrap_content&quot;
        android:text=&quot;TextView 1&quot; /&gt;
 
    &lt;Button
        android:id=&quot;@+id/button2&quot;
        android:layout_width=&quot;wrap_content&quot;
        android:layout_height=&quot;wrap_content&quot;
        android:text=&quot;Button 2&quot; /&gt;
 
    &lt;Button
        android:id=&quot;@+id/button3&quot;
        android:layout_width=&quot;wrap_content&quot;
        android:layout_height=&quot;wrap_content&quot;
        android:text=&quot;Button 3&quot; 
        android:layout_weight=&quot;1&quot;/&gt;
 
&lt;/LinearLayout&gt;

You can inflate the layout2.xml file, edit the texts, and add it to the first layout:
public class MyFragment extends Fragment {

        private LinearLayout mLinearLayout;

        @Override
        public View onCreateView(LayoutInflater inflater, ViewGroup container,  Bundle savedInstanceState) {
            View view = inflater.inflate(R.layout.yourMainLayout, container, false);
            .
            .
            .
            mLinearLayout = (LinearLayout) view.findViewById(R.id.linear_layout);
            Button fab = (Button) view.findViewById(R.id.fab);
            fab.setOnClickListener(new View.OnClickListener() {
              @Override
                public void onClick(View view) {
                  // new elements on click
                  addLayout(&quot;This is text 1&quot;, &quot;This is first button&quot;, &quot;This is second Button&quot;);
            }
        });
            addLayout(&quot;This is text 1&quot;, &quot;This is first button&quot;, &quot;This is second Button&quot;);
        }

        private void addLayout(String textViewText, String buttonText1, String buttonText2) {
            View layout2 = LayoutInflater.from(getActivity()).inflate(R.layout.layout2, mLinearLayout, false);

            TextView textView = (TextView) layout2.findViewById(R.id.button1);
            Button button1 = (Button) layout2.findViewById(R.id.button2);
            Button button2 = (Button) layout2.findViewById(R.id.button3);

            textView1.setText(textViewText);
            button1.setText(buttonText1);
            button2.setText(buttonText2);

            mLinearLayout.addView(layout2);
        }

You may want to change android:layout_height of the layout2.xml root view to wrap_content.
Here I haven't taken the Button in XML LAYOUT, you can put the Button according to your need.
If you are using ViewBinding, here is how it would look like for the addLayout function :
MyLayoutBinding binding = MyLayoutBinding.inflate(getLayoutInflater(), mLinearLayout, false);
binding.getTextView1().setText(textViewText);
binding.getButton1().setText(buttonText1);
binding.getButton2().setText(buttonText2);

mLinearLayout.addView(binding.getRoot());

In your Case you can call this method from onClick() of Add Button
addLayout(&quot;This is text 1&quot;, &quot;This is first button&quot;, &quot;This is second Button&quot;);

"
"I'm writing a code to consume a private key to encrypt and decrypt a message.
The problem is that the key i use is protected by a passphrase. So i have to decrypt the key itself before use it to encrypt and decrypt.
This is the header of the key content:
-----BEGIN RSA PRIVATE KEY-----
Proc-Type: 4,ENCRYPTED
DEK-Info: DES-EDE3-CBC,E51B4CCF38530A32

b9gvBvJNyUxA/2AH5mb+7dNcIns05EIXlbiM47xSUiQZgOdbP5ZHy5WL6S+uxU7s
.
.
.
-----END RSA PRIVATE KEY-----

How can I achieve that in Java?
","This is an encrypted private RSA key in PKCS#1 format, PEM encoded, which is most convenient to import using BouncyCastle:
import java.io.FileReader;
import java.security.PrivateKey;
import org.bouncycastle.asn1.pkcs.PrivateKeyInfo;
import org.bouncycastle.openssl.PEMEncryptedKeyPair;
import org.bouncycastle.openssl.PEMKeyPair;
import org.bouncycastle.openssl.PEMParser;
import org.bouncycastle.openssl.bc.BcPEMDecryptorProvider;
import org.bouncycastle.openssl.jcajce.JcaPEMKeyConverter;

...

String path = &quot;...&quot;;
String password = &quot;...&quot;;
try (PEMParser pemParser = new PEMParser(new FileReader(path))){

    PEMEncryptedKeyPair encKeyPair = (PEMEncryptedKeyPair)pemParser.readObject();
    PEMKeyPair keyPair = encKeyPair.decryptKeyPair(new BcPEMDecryptorProvider(password.toCharArray()));

    JcaPEMKeyConverter converter = new JcaPEMKeyConverter();
    PrivateKeyInfo privKeyInfo = keyPair.getPrivateKeyInfo();
    PrivateKey privKey = converter.getPrivateKey(privKeyInfo);
}

"
"We are researching the possibility to migrate some web JSF applications from Payara 5 to Tomcat 9 (TomEE 8). We are using Java 11 and Java EE 7/8. Our applications connect to a backend server using RMI. At the moment, with Payara 5, when the code that connects us to the backend server fails (exception is thrown because server is unavailable or credentials defined in web.xml are invalid), the deployment fails.
See this piece of code:
public class MainServlet extends HttpServlet {
  //constructor, variables etc.
  @Override
  public void init(ServletConfig config) throws ServletException {
    super.init(config);
    //read configurations from web.xml
    try {
      connectToBackendServer();
    catch (Exception e) {
      throw new UnavailableException(&quot;Cannot connect to Backend Server&quot;);
    }
  }
  //other methods
}

The above piece of code makes the deployment fail on Payara 5, but Tomcat 9 allows the deployment. With Tomcat we notice that the backend is not ok by checking the logs or by trying the front-end and getting the errors. See the below picture where the NullPointerException is thrown by our connectToBackendServer() method.

We are fully aware that this is not the best approach as the backend may fail later, after the successful deployment, but at least we are covering the cases when the configuration from web.xml is wrong.
Can we achieve a similar functionality with Tomcat 9(TomEE 8)?
Thank you all in advance!
..
","Move your logic to a ServletContextListener and throw a runtime exception from contextInitialized(). On many servers this will fail the deployment and any requests to the application will return error 500. The spec does not require this exact behaviour though, so the outcome is slightly different between servers.
This is an example implementation using a ServletContextListener that fails the deployment:
package com.example;
import jakarta.servlet.ServletContextEvent;
import jakarta.servlet.ServletContextListener;
import jakarta.servlet.annotation.WebListener;

@WebListener
public class ExampleServletContextListener implements ServletContextListener{
    @Override
    public void contextInitialized(ServletContextEvent e) {
        try {
            callThatFailsAndThrowsAnException();

            catch (Exception e) {
                throw new UnavailableException(&quot;Something went very wrong - I'm bailing out.&quot;);
            }
    }

    @Override
    public void contextDestroyed(ServletContextEvent e) {
        /* Application shutdown */
    }
}

@WebListener registers the context listener with the container. If you are using an older version of JakartaEE/JavaEE and the annotation is unavailable, you can register the context listener in web.xml instead.
"
"I need to compile a JavaFX 20 application into an executable Jar file that includes all the dependencies listed in the pom.xml (including javaFX itself and all the other JAR files). My project is based on Maven. I tried many different maven plugins but none of them were able to make the final jar executable by double clicking, although I was able to run in in the command line with the java -jar command. The intention is to distribute this app with next/next/finish installers on Linux, Windows and MacOS. The end user profile is a lab researcher with low IT knowledge (I work for a NPO that helps protecting the Amazon forest). Is there an objective way to do this?
I already tried many maven plugins with different goals (resources, dependencies, shade, compiler, etc) but no success at all.
","Self-Contained Application
Your desire to create an installer lends itself to creating an entirely self-contained application. That is an application that bundles not only your code and its third-party dependencies, but also the Java Run-Time Environment itself. Your end user won't need to install Java separately.
GraalVM Native Image
One option for creating a self-contained application is GraalVM's native image. Note that this will also ahead-of-time compile your Java code to native code.
I have very little experience with GraalVM, so I won't give an example, but here are some links that may help:

Create native JavaFX applications using GraalVM 22 builds from Gluon

How to create JavaFX native images | BellSoft Java


jpackage
A relatively accessible tool for creating a self-contained applications and installers is jpackage. It comes with the JDK since version 16. See the Packaging Tool User Guide for more information. It would also help to understand jlink, as jpackage will either use jlink &quot;behind the scenes&quot; to generate a custom run-time image, or you'll use jlink explicitly and then provide jpackage with the custom run-time image.
One significant downside to using jpackage, however, is that it can only create application images and installers for the operating system it is running on. If you want a Windows application, then you'll have to build your project on Windows, with the same being true for Linux and macOS. And as jewelsea explains in a comment, there are also platform-specific issues you'll have to resolve:

Creating installers for multiple platforms is tricky and each platform has its own quirks you need to work with. For example, Windows requires increasing versions for updates, OS X requires (paid I believe) developer signing certs. So be prepared to spend some time and (probably) a bit of money to create the installers. Though there are existing guides, such as those I referenced, there will probably still be some platform and app-specific quirks you will need to work through on your own with limited outside help.

If you're going to create a self-contained application, then I would suggest you forgo creating a fat/uber/shadowed JAR file. A self-contained application already contains all dependencies, so a shadowed JAR file is unlikely to provide any benefits. Plus, if you're going to be using jpackage to create the installer, you might as well design your entire deployment around jpackage from the start.
Using Maven profiles can solve a lot of the platform-specific configuration issues. While any remaining platform-specific configuration, as well as building the application on multiple platforms and publishing it automatically, can likely be accomplished with some kind of CI/CD pipeline. Though remember to never commit any secrets to your VCS (e.g., Git), which by extension means do not put any secrets in the POM file(s).
Example
Here is a POM that is configured to create an msi installer file when the windows profile is activated. Note there is only configurations for Windows in this example. The org.panteleyev:jpackage-maven-plugin plugin is used to execute jpackage.
This POM is designed for a non-modular application. As such, it's configured so that the JavaFX modules are added to the custom run-time image, while the project code and any other dependencies are configured to be placed on the class-path via --input and --main-jar. By having JavaFX in the custom run-time image, it will implicitly be on the module-path.
Disclaimer: I'm more familiar with Gradle than Maven, so there may be ways to simplify the following POM.
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
  xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

  &lt;groupId&gt;com.example&lt;/groupId&gt;
  &lt;artifactId&gt;app&lt;/artifactId&gt;
  &lt;version&gt;1.0&lt;/version&gt;

  &lt;name&gt;app&lt;/name&gt;

  &lt;properties&gt;
    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
    &lt;maven.compiler.source&gt;21&lt;/maven.compiler.source&gt;
    &lt;maven.compiler.target&gt;21&lt;/maven.compiler.target&gt;
    &lt;javafxVersion&gt;21.0.1&lt;/javafxVersion&gt;
    &lt;libsDir&gt;${project.build.directory}/artifacts/libs&lt;/libsDir&gt;
    &lt;javafxModsDir&gt;${project.build.directory}/artifacts/javafx&lt;/javafxModsDir&gt;
  &lt;/properties&gt;

  &lt;dependencies&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.openjfx&lt;/groupId&gt;
      &lt;artifactId&gt;javafx-controls&lt;/artifactId&gt;
      &lt;version&gt;${javafxVersion}&lt;/version&gt;
    &lt;/dependency&gt;
  &lt;/dependencies&gt;

  &lt;build&gt;
    &lt;pluginManagement&gt;
      &lt;!-- Lock in versions. --&gt;
      &lt;plugins&gt;
        &lt;plugin&gt;
          &lt;artifactId&gt;maven-clean-plugin&lt;/artifactId&gt;
          &lt;version&gt;3.3.2&lt;/version&gt;
        &lt;/plugin&gt;
        &lt;plugin&gt;
          &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt;
          &lt;version&gt;3.3.1&lt;/version&gt;
        &lt;/plugin&gt;
        &lt;plugin&gt;
          &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
          &lt;version&gt;3.11.0&lt;/version&gt;
        &lt;/plugin&gt;
        &lt;plugin&gt;
          &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;
          &lt;version&gt;3.3.0&lt;/version&gt;
        &lt;/plugin&gt;
        &lt;plugin&gt;
          &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;
          &lt;version&gt;3.6.1&lt;/version&gt;
        &lt;/plugin&gt;
        &lt;plugin&gt;
          &lt;groupId&gt;org.panteleyev&lt;/groupId&gt;
          &lt;artifactId&gt;jpackage-maven-plugin&lt;/artifactId&gt;
          &lt;version&gt;1.6.0&lt;/version&gt;
        &lt;/plugin&gt;
      &lt;/plugins&gt;
    &lt;/pluginManagement&gt;

    &lt;plugins&gt;
      &lt;!-- 
        Configure JAR plugin to set Main-Class entry in the JAR's manifest. Also, put the JAR
        file in the same directory as the non-JavaFX dependencies to make using jpackage easier.
      --&gt;
      &lt;plugin&gt;
        &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;
        &lt;configuration&gt;
          &lt;outputDirectory&gt;${libsDir}&lt;/outputDirectory&gt;
          &lt;archive&gt;
            &lt;manifest&gt;
              &lt;mainClass&gt;com.example.app.Main&lt;/mainClass&gt;
            &lt;/manifest&gt;
          &lt;/archive&gt;
        &lt;/configuration&gt;
      &lt;/plugin&gt;

      &lt;!-- Copy dependencies into project-local directories to make using jpackage easier. --&gt;
      &lt;plugin&gt;
        &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;
        &lt;executions&gt;
          &lt;!-- 
            Put JavaFX JARs in their own directory so they are not included in the input directory
            and can easily be placed on the module-path of jpackage.
          --&gt;
          &lt;execution&gt;
            &lt;id&gt;copy-javafx-deps&lt;/id&gt;
            &lt;phase&gt;package&lt;/phase&gt;
            &lt;goals&gt;
              &lt;goal&gt;copy-dependencies&lt;/goal&gt;
            &lt;/goals&gt;
            &lt;configuration&gt;
              &lt;outputDirectory&gt;${javafxModsDir}&lt;/outputDirectory&gt;
              &lt;includeGroupIds&gt;org.openjfx&lt;/includeGroupIds&gt;
            &lt;/configuration&gt;
          &lt;/execution&gt;
          &lt;!-- 
            Put all non-JavaFX JARs in a separate directory to use with the input argument
            of jpackage. The project JAR will be placed here as well.
          --&gt;
          &lt;execution&gt;
            &lt;id&gt;copy-nonjavafx-deps&lt;/id&gt;
            &lt;phase&gt;package&lt;/phase&gt;
            &lt;goals&gt;
              &lt;goal&gt;copy-dependencies&lt;/goal&gt;
            &lt;/goals&gt;
            &lt;configuration&gt;
              &lt;outputDirectory&gt;${libsDir}&lt;/outputDirectory&gt;
              &lt;excludeGroupIds&gt;org.openjfx&lt;/excludeGroupIds&gt;
            &lt;/configuration&gt;
          &lt;/execution&gt;
        &lt;/executions&gt;
      &lt;/plugin&gt;

      &lt;!-- Common jpackage configurations. --&gt;
      &lt;plugin&gt;
        &lt;groupId&gt;org.panteleyev&lt;/groupId&gt;
        &lt;artifactId&gt;jpackage-maven-plugin&lt;/artifactId&gt;
        &lt;configuration&gt;
          &lt;modulePaths&gt;
            &lt;modulePath&gt;${javafxModsDir}&lt;/modulePath&gt;
          &lt;/modulePaths&gt;
          &lt;!-- 
            Must explicitly add modules since project code is not modular, meaning there's
            no module-info.java file with requires directives.

            Include the jdk.localedata module to ensure internationalization works. You can filter
            which locales are included via jlink options.
          --&gt;
          &lt;addModules&gt;javafx.controls,jdk.localedata&lt;/addModules&gt;
          &lt;input&gt;${libsDir}&lt;/input&gt;
          &lt;mainJar&gt;${project.name}-${project.version}.jar&lt;/mainJar&gt;
          &lt;temp&gt;${project.build.directory}/jpackage/temp&lt;/temp&gt;
        &lt;/configuration&gt;
      &lt;/plugin&gt;
    &lt;/plugins&gt;

  &lt;/build&gt;

  &lt;profiles&gt;

    &lt;profile&gt;
      &lt;id&gt;windows&lt;/id&gt;
      &lt;build&gt;
        &lt;plugins&gt;
          &lt;!-- Windows-specific jpackage configurations. --&gt;
          &lt;plugin&gt;
            &lt;groupId&gt;org.panteleyev&lt;/groupId&gt;
            &lt;artifactId&gt;jpackage-maven-plugin&lt;/artifactId&gt;
            &lt;executions&gt;
              &lt;execution&gt;
                &lt;id&gt;windows-msi&lt;/id&gt;
                &lt;phase&gt;package&lt;/phase&gt;
                &lt;goals&gt;
                  &lt;goal&gt;jpackage&lt;/goal&gt;
                &lt;/goals&gt;
                &lt;configuration&gt;
                  &lt;type&gt;MSI&lt;/type&gt;
                  &lt;destination&gt;${project.build.directory}/jpackage/windows-msi&lt;/destination&gt;
                  &lt;winPerUserInstall&gt;true&lt;/winPerUserInstall&gt;
                  &lt;winDirChooser&gt;true&lt;/winDirChooser&gt;
                  &lt;winUpgradeUuid&gt;7c59c875-1ad3-4042-9c9f-fed5fc3f8ab9&lt;/winUpgradeUuid&gt;
                &lt;/configuration&gt;
              &lt;/execution&gt;
            &lt;/executions&gt;
          &lt;/plugin&gt;
        &lt;/plugins&gt;
      &lt;/build&gt;
    &lt;/profile&gt;

  &lt;/profiles&gt;

&lt;/project&gt;

And here's an example com.example.app.Main class to go along with the POM:
package com.example.app;

import javafx.application.Application;
import javafx.scene.Scene;
import javafx.scene.control.ListView;
import javafx.stage.Stage;

public class Main extends Application {

    @Override
    public void start(Stage primaryStage) {
        // List view will display the modules included in the custom run-time image.
        var listView = new ListView&lt;String&gt;();
        ModuleLayer.boot()
            .modules()
            .stream()
            .map(Module::getName)
            .sorted()
            .forEach(listView.getItems()::add);
        primaryStage.setTitle(&quot;JavaFX &quot; + System.getProperty(&quot;javafx.version&quot;) + &quot; Application&quot;);
        primaryStage.setScene(new Scene(listView, 600, 400));
        primaryStage.show();        
    }
}

To create the msi file, execute:
mvn -P windows package

And you should get a target/jpackage/windows-msi/app-1.0.msi installer file.
Note you must execute that on Windows, and you must have WiX Toolset 3.x.x installed (version 4.x.x doesn't seem to work with jpackage, or at least I couldn't get it to work). If you don't want to install WiX 3.x.x just for running the example, then change the type to &lt;type&gt;APP_IMAGE&lt;/type&gt;. You won't get an installer file, but you'll end up with an application image that you can run as is.

Executable JAR
You can create an executable fat/uber/shadowed JAR file with JavaFX via Maven using the Maven Shade Plugin. Note this will inherently put everything on the class-path. Any code that relies explicitly on modules may break with this setup, though such code is rare. Additionally, JavaFX does not technically support being loaded from the class-path. Doing so will not break anything as of JavaFX 21, as far as I know, but any issues caused by this configuration are unlikely to be fixed by the JavaFX developers.
See David Weber's answer for an example using the Maven Shade Plugin.
If you want to go with a shadowed executable JAR file as your deployment strategy, then I suggest you forgo creating an installer. Just have your end user install the appropriate version of Java (you can send them the installer for that), and then they can just double-click the JAR file to make it run. Make sure your end user enables associating *.jar files with Java if the Java installer gives that option.
If you want a cross-platform executable JAR file, you'll have to declare a JavaFX dependency for each platform manually. JavaFX relies on platform-specific native code, and the Maven artifacts embed that native code in the JAR file, but only for a particular platform. For instance, javafx-graphics-21.0.1-win.jar contains native code for Windows, but not Linux or macOS.
Note that when JavaFX is on the class-path, as is the case here, then your main class cannot be a subclass of javafx.application.Application. You must have a separate &quot;launcher class&quot; as the main class. Something like:
package sample;

import javafx.application.Application;

public class Launcher {

    public static void main(String[] args) {
        Application.launch(YourAppClass.class, args);
    }
}

"
"I have the below program and looks like ZonedDateTime is not able to parse the date string. Should I use a different date format or different library to parse?
import java.time.ZonedDateTime;
import java.time.format.DateTimeFormatter;

class Scratch {
    public static void main(String[] args) {
        final String inputDate = &quot;2022-03-12T03:59:59+0000Z&quot;;
        ZonedDateTime.parse(inputDate, DateTimeFormatter.ISO_DATE_TIME).toEpochSecond();
    }
}


Exception in thread &quot;main&quot; java.time.format.DateTimeParseException: Text '2022-03-12T03:59:59+0000Z' could not be parsed, unparsed text found at index 19
    at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2053)
    at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1952)
    at java.base/java.time.ZonedDateTime.parse(ZonedDateTime.java:599)
    at Scratch.main(scratch_29.java:7)

Process finished with exit code 1

","That isn't a ISO_DATE_TIME format.  That would require something like 2022-03-12T03:59:59+0000 (no 'Z').  A formatter that works looks something like:
import java.time.ZonedDateTime;
import java.time.format.DateTimeFormatter;
import java.time.format.DateTimeFormatterBuilder;


class Scratch {
    public static void main(String[] args) {
        final String inputDate = &quot;2022-03-12T03:59:59+0000Z&quot;;

        DateTimeFormatter formatter = new DateTimeFormatterBuilder()
                .parseCaseInsensitive()
                .append(DateTimeFormatter.ISO_LOCAL_DATE_TIME)
                .optionalStart()
                .appendPattern(&quot;.SSS&quot;)
                .optionalEnd()
                .optionalStart()
                .appendZoneOrOffsetId()
                .optionalEnd()
                .optionalStart()
                .appendOffset(&quot;+HHMM&quot;, &quot;0000&quot;)
                .optionalEnd()
                .optionalStart()
                .appendLiteral('Z')
                .optionalEnd()
                .toFormatter();

        long epochSecond = ZonedDateTime.parse(inputDate, formatter).toEpochSecond();

        System.out.println(&quot;epochSecond is &quot; + epochSecond);
    }
}

as derived from this post.  You can create that formatter in one place and use it over again.
"
"I met the following problem in a Java exam, why following recursively calling a function can run forever even though StackOverflowError?
public class Solution {
    static int i = 0;

    public static void f(){
        System.out.println(i++);
        try {
            f();
        } catch (StackOverflowError e) {
            System.out.println(e);
            f();
        }
    }

    public static void main(String[] args) {
        f();
    }
}

I cannot understand why JVM can still run when I've exhausted all call stack memory? Is there any reference, like JVM specification or JVM source code, can explain above phenomenon?
","I'm neither java nor jvm expert, but I think adding a extra recursive depth output can show exactly what is going on with the call stack.
public class Solution {
  static int i = 0;

  public static void f(int depth) {
    System.out.println(i++);
    System.out.println(String.format(&quot;depth=%d&quot;, ++depth));
    try {
      f(depth);
    } catch (StackOverflowError e) {
      System.out.println(e);
      f(depth);
    }
  }

  public static void main(String[] args) {
    f(1);
  }
}

In my test, typical output is
...more
java.lang.StackOverflowError
161735
depth=3745
161736
depth=3746
161737
java.lang.StackOverflowError
161738
java.lang.StackOverflowError
161739
java.lang.StackOverflowError
161740
depth=3744
161741
depth=3745
...more

StackOverflowError is thrown because jvm is considering a size limit(1M by default?) of stack memory has reached, but the exception is caught, so the recursion is not stopped, it still tries to call, jvm throw away the recent 1 or 2 problematic calls, and then try continue, so the depth value goes around the same maximum value again and again.
I think this way the OP can understand why i is increasing without any problem. I've exhaust the call stack memory -- you haven't, you nearly exhaust, but jvm always cleared latest few calls to avoid exhaust.
"
"I need an emoji along with text editing support, but when I insert any emoji in the text area (no matter if via code or via clipboard) it doesn't display at all. I tried downloading NotoColorEmoji font and setting it for the text area, but again nothing is displayed. Moreover, this font only contains emoji, so regular characters are not displayed as well.
var url = getClass().getResource(&quot;NotoColorEmoji-Regular.ttf&quot;).toExternalForm();
var ta = new TextArea(&quot;ðŸ˜€ðŸ˜ƒðŸ˜„&quot;);
ta.setFont(Font.loadFont(url, 12));

I know about 3rd party libs, but they can't all be used for various reasons. So, please, don't recommend any of them.
Is there a way to get emoji support in standard text input control? Any workarounds?
UPDATE:
TL;DR: I think it's a JavaFX bug. Some emoji fonts fully/partially cannot be rendered. Those that do only rendered as greyscale.
Environment: Ubuntu 22.04 (KDE 5.24) or Fedora 39 (KDE 5.27), JDK/JFX 21 or JDK/JFX 22 (latest)
Out-of-the-box, both Ubuntu and Fedora use the 'Noto Color Emoji' font, which cannot be rendered by JavaFX. Most symbols are missing.
JavaFX has own font renderer engine which reads the Linux font config settings. It's easy to check by not closing the XML tag. You'll get a JavaFX warning on startup.
So I downloaded all the available emoji TTF fonts and tried them out. To change the font, you need to put the TTF files in ~/.fonts and the configuration in ~/.config/fontconfig.
 &lt;match&gt;
  &lt;test name=&quot;family&quot;&gt;
   &lt;string&gt;sans-serif&lt;/string&gt;
  &lt;/test&gt;
  &lt;edit binding=&quot;strong&quot; name=&quot;family&quot; mode=&quot;prepend&quot;&gt;
   &lt;string&gt;Segoe UI Emoji&lt;/string&gt;
  &lt;/edit&gt;
 &lt;/match&gt;
 &lt;match&gt;
  &lt;test name=&quot;family&quot;&gt;
   &lt;string&gt;serif&lt;/string&gt;
  &lt;/test&gt;
  &lt;edit binding=&quot;strong&quot; name=&quot;family&quot; mode=&quot;prepend&quot;&gt;
   &lt;string&gt;Segoe UI Emoji&lt;/string&gt;
  &lt;/edit&gt;
 &lt;/match&gt;
 &lt;match&gt;
  &lt;test name=&quot;family&quot;&gt;
   &lt;string&gt;monospace&lt;/string&gt;
  &lt;/test&gt;
  &lt;edit binding=&quot;strong&quot; name=&quot;family&quot; mode=&quot;prepend&quot;&gt;
   &lt;string&gt;Segoe UI Emoji&lt;/string&gt;
  &lt;/edit&gt;
 &lt;/match&gt;
 &lt;!-- optionally override system emoji font --&gt;
 &lt;match&gt;
  &lt;test name=&quot;family&quot;&gt;
   &lt;string&gt;Noto Emoji&lt;/string&gt;
  &lt;/test&gt;
  &lt;edit binding=&quot;strong&quot; name=&quot;family&quot; mode=&quot;prepend&quot;&gt;
   &lt;string&gt;Segoe UI Emoji&lt;/string&gt;
  &lt;/edit&gt;
 &lt;/match&gt;

Here is the test node:
var eta = new TextArea(&quot;&quot;&quot;
                ðŸ˜€ ðŸ˜ƒ ðŸ˜„ ðŸ˜ ðŸ˜† ðŸ˜… ðŸ˜‚ ðŸ¤£ ðŸ¥² ðŸ¥¹ ðŸ˜Š ðŸ˜‡ ðŸ™‚ ðŸ™ƒ ðŸ˜‰ ðŸ˜Œ ðŸ˜ ðŸ¥° ðŸ˜˜ ðŸ˜— ðŸ˜™ ðŸ˜š ðŸ˜‹ ðŸ˜› ðŸ˜ ðŸ˜œ ðŸ¤ª 
                ðŸ¤¨ ðŸ§ ðŸ¤“ ðŸ˜Ž ðŸ¥¸ ðŸ¤© ðŸ¥³ ðŸ™‚â€ ðŸ˜ ðŸ˜’ ðŸ™‚â€ ðŸ˜ž ðŸ˜” ðŸ˜Ÿ ðŸ˜• ðŸ™ â˜¹ï¸ ðŸ˜£ ðŸ˜– ðŸ˜« ðŸ˜© ðŸ¥º ðŸ˜¢ ðŸ˜­ ðŸ˜® ðŸ˜¤ ðŸ˜  
                ðŸ˜¡ ðŸ¤¬ ðŸ¤¯ ðŸ˜³ ðŸ¥µ ðŸ¥¶ ðŸ˜± ðŸ˜¨ ðŸ˜° ðŸ˜¥ ðŸ˜“ ðŸ«£ ðŸ¤— ðŸ«¡ ðŸ¤” ðŸ«¢ ðŸ¤­ ðŸ¤« ðŸ¤¥ ðŸ˜¶ ðŸ˜¶ ðŸ˜ ðŸ˜‘ ðŸ˜¬ ðŸ«¨ ðŸ«  ðŸ™„ 
                ðŸ˜¯ ðŸ˜¦ ðŸ˜§ ðŸ˜® ðŸ˜² ðŸ¥± ðŸ˜´ ðŸ¤¤ ðŸ˜ª ðŸ˜µ ðŸ˜µ ðŸ«¥ ðŸ¤ ðŸ¥´ ðŸ¤¢ ðŸ¤® ðŸ¤§ ðŸ˜· ðŸ¤’ ðŸ¤• ðŸ¤‘ ðŸ¤  ðŸ˜ˆ ðŸ‘¿ ðŸ‘¹ ðŸ‘º ðŸ¤¡ 
                ðŸ’© ðŸ‘» ðŸ’€ ðŸ‘½ ðŸ‘¾ ðŸ¤– ðŸŽƒ ðŸ˜º ðŸ˜¸ ðŸ˜¹ ðŸ˜» ðŸ˜¼ ðŸ˜½ ðŸ™€ ðŸ˜¿ ðŸ˜¾ 
                &quot;&quot;&quot;);

.root {
    -fx-font-family: &quot;serif&quot;;
}


Twitter Emoji, https://github.com/13rac1/twemoji-color-font
Apple Emoji, https://github.com/samuelngs/apple-emoji-linux
OpenSans Emoji, https://github.com/MorbZ/OpenSansEmoji
Noto Emoji, https://fonts.google.com/noto/specimen/Noto+Color+Emoji

I also found a very similar bug for MacOS JDK-8290866, but unfortunately setting -Dprism.lcdtext=true as well as changing -fx-smoothing-type didn't help.
And the results:

","An example to demonstrate current behavior and summarize comments, showing how various emoji fonts are handled (not a comprehensive cross-platform answer).
I only ran the example app on OS X, other OS environments may exhibit different behavior.
OS X output
Environment

OS X 14.5
openJDK 22.0.2
JavaFX 22.0.2


Sample app
In the example app, the fonts required are downloaded dynamically at runtime, so they don't need to be pre-downloaded and installed in the OS for you to use them.  However, the drawback of this approach is that it takes a few seconds for the app to start as it gathers and initializes the fonts off of the net.
import javafx.application.Application;
import javafx.geometry.Insets;
import javafx.scene.Scene;
import javafx.scene.control.Label;
import javafx.scene.control.TextArea;
import javafx.scene.layout.Pane;
import javafx.scene.layout.VBox;
import javafx.scene.text.Font;
import javafx.stage.Stage;

import java.io.IOException;
import java.util.List;

public class EmoApp extends Application {
    private static final String CSS_DATA = &quot;data:text/css,&quot;;

    private static final String CSS = CSS_DATA + &quot;&quot;&quot;
        .root {
           -fx-font-size: 16px;
        }
        .default {}
        .noto-color-emoji {
          -fx-font-family: &quot;Noto Color Emoji&quot;;
        }
        .open-sans-emoji {
          -fx-font-family: &quot;OpenSansEmoji&quot;;
        }
        .segoe-emoji {
          -fx-font-family: &quot;Segoe UI Emoji&quot;;
        }
        .apple-color-emoji {
          -fx-font-family: &quot;Apple Color Emoji&quot;;
        }
        &quot;&quot;&quot;;

    private static final String EMOJI_TEXT = &quot;&quot;&quot;
        😀 😃 😄 😁 😆 😅 😂 🤣 🥲 🥹 😊 😇 🙂 🙃 😉 😌 😍 🥰 😘 😗 😙 😚 😋 😛 😝 😜 🤪 
        🤨 🧐 🤓 😎 🥸 🤩 🥳 🙂‍ 😏 😒 🙂‍ 😞 😔 😟 😕 🙁 ☹️ 😣 😖 😫 😩 🥺 😢 😭 😮 😤 😠 
        😡 🤬 🤯 😳 🥵 🥶 😱 😨 😰 😥 😓 🫣 🤗 🫡 🤔 🫢 🤭 🤫 🤥 😶 😶 😐 😑 😬 🫨 🫠 🙄 
        😯 😦 😧 😮 😲 🥱 😴 🤤 😪 😵 😵 🫥 🤐 🥴 🤢 🤮 🤧 😷 🤒 🤕 🤑 🤠 😈 👿 👹 👺 🤡 
        💩 👻 💀 👽 👾 🤖 🎃 😺 😸 😹 😻 😼 😽 🙀 😿 😾 
        &quot;&quot;&quot;;


    private static final String NOTO_COLOR_EMOJI_CSS =
            &quot;https://fonts.googleapis.com/css2?family=Noto+Color+Emoji&amp;display=swap&quot;;

    private static final List&lt;String&gt; emojiStyleClasses = List.of(
            &quot;default&quot;,
            &quot;noto-color-emoji&quot;,
            &quot;open-sans-emoji&quot;,
            &quot;segoe-emoji&quot;,
            &quot;apple-color-emoji&quot;
    );

    @Override
    public void start(Stage stage) throws IOException {
        Font.loadFont(
                &quot;https://github.com/MorbZ/OpenSansEmoji/raw/master/OpenSansEmoji.ttf&quot;,
                10
        );
        Font.loadFont(
                &quot;https://github.com/mrbvrz/segoe-ui/raw/master/font/seguiemj.ttf&quot;,
                10
        );

        VBox layout = new VBox(10);
        layout.getChildren().addAll(
                emojiStyleClasses.stream()
                        .map(this::emojiSample)
                        .toList()
        );
        layout.setPadding(new Insets(10));

        Scene scene = new Scene(layout);
        scene.getStylesheets().addAll(
                NOTO_COLOR_EMOJI_CSS,
                CSS
        );
        stage.setScene(scene);
        stage.show();
    }

    public Pane emojiSample(String styleClass) {
        TextArea textArea = new TextArea(EMOJI_TEXT);
        textArea.setPrefSize(750, 160);
        textArea.getStyleClass().add(styleClass);

        return new VBox(5,
                new Label(styleClass),
                textArea
        );
    }
}

Notes

From my testing (on Mac), the color Emojis only work with the default Mac Apple Color Emoji font and Mac appears to fall back to use that if the appropriate glyphs aren't in the requested font, but no other color fonts will work with JavaFX (they will be rendered monochrome or not at all).

If I run this code
stage.setScene(new Scene(new TextArea(&quot;\uD83D\uDE00\uD83D\uDE03\uD83D\uDE04&quot;))); 

on a Mac, it works just fine, and the emojis you have in your question display. Also copy and paste of your emojis into the text area works fine as well as editing caret positioning, selection highlighting and the delete key.

On Mac the Emojis display in color, with the default system font. The standard &quot;command + ctrl + space&quot; keyboard sequence will bring up an emoji input selection in most Mac apps, but not in JavaFX apps. So I am not sure how to use the keyboard to enter new emojis.

To get multicolored emoji characters, you probably need to use a color font, e.g. on Windows, you could try &quot;Segoe UI Emoji&quot;.  However, from my testing on Mac, JavaFX didn't seem to handle color fonts (except the &quot;Apple Color Emoji&quot; font that ships with the OS).

I know the asker doesn't want to use a third-party library, but for others who come across this question who do, you could consider pavlobu / emoji-text-flow-javafx, mentioned in this similar question and answer, which &quot;allow users to use extended TextFlow to display emojis. It helps to display consistent emoji images on different platforms where JavaFX application runs.&quot;

I also tried it on Mac with the OpenSansEmoji font -&gt; output was similar to that in the question, but (most glyphs rendered monochrome, but glyphs rendered garbled in the question were displayed in color, probably just falling back to the default Apple Emoji Font, which works). Similarly for Segoe UI Emoji, which renders many more black and white glyphs as shown in the question, and color default Apple Emoji Fonts for the glyphs not rendered in Segoe. Noto Color Emoji rendered blank characters on Mac.

So what that means to me is that JavaFX can only render the color emojis from the default Apple Emoji Font on a Mac, but not a custom color font. Probably there is some special fallback on Macs to use the Apple Emoji font if the current font doesn't have the appropriate Emoji glyphs. Still, outside of that, color glyphs aren't rendered (similar to Linux), so it is probably a cross-platform limitation of JavaFX font processing and not Linux-specific.


FAQ

How did you get the behavior of rendered monochrome or not at all if the Apple Color Emoji font is used as a fallback? Is there a way to prevent the fallback?

I didn't do anything to have that fallback behavior, it just seems to happen.  You can see in the result that some emoji fonts are rendering monochrome for glyphs, which is coming from the emoji font.  But when the emoji font does not seem to have an appropriate glyph you see color glyphs coming from the Apple emoji font.
The Noto Color Emoji font has all the glyphs, but they don't render with JavaFX, ending up with blank text.
Unrelated to JavaFX, the default emoji font used by Apple (Apple Emoji Font) can be replaced in the OS, see:

https://superuser.com/questions/968509/is-is-possible-to-change-the-emoji-set-on-os-x

The link (which is quite old), noted that the &quot;Noto Color Emoji&quot; font does not work on OS X, so the failure of the font may be at the OS level, rather than directly applicable to JavaFX.
"
"When I click on Card, the color changes sometimes and sometimes it doesn't. Many a time it happens that the changed color does not retain. I want to create something like Facebook Notifications page where we come to know about read notifications by the changed color of the card. But there is some problem with my adapter class. Please help me.
I tried saving the color states in shared preferences, but the desired output is not achieved. I think the issue must be with my toggle logic. I don't have any professional experience with Android coding. Please help me. My app is in final stage.
public class PyqAdapter extends RecyclerView.Adapter&lt;PyqAdapter.ViewHolder&gt; {
    private final Context mCtx;
    private final List&lt;PyqModel&gt; pyqModelList;
    private final int defaultBackgroundColor;
    private final int selectedBackgroundColor;
    private final Set&lt;Integer&gt; selectedPositions;

    private static final String PREFS_NAME = &quot;PyqAllItems&quot;;
    private static final String SELECTED_ITEMS_KEY = &quot;PyqSelectedItems&quot;;

    public PyqAdapter(Context mCtx, List&lt;PyqModel&gt; pyqModelList) {
        if (mCtx == null) {
            throw new IllegalArgumentException(&quot;Context cannot be null&quot;);
        }
        this.mCtx = mCtx;
        this.pyqModelList = pyqModelList;
        this.selectedPositions = new HashSet&lt;&gt;();

        // Load colors based on the current theme
        Resources res = mCtx.getResources();
        int nightModeFlags = res.getConfiguration().uiMode &amp; Configuration.UI_MODE_NIGHT_MASK;
        if (nightModeFlags == Configuration.UI_MODE_NIGHT_YES) {
            defaultBackgroundColor = ContextCompat.getColor(mCtx, R.color.defaultBackgroundDark);
            selectedBackgroundColor = ContextCompat.getColor(mCtx, R.color.selectedBackgroundDark);
        } else {
            defaultBackgroundColor = ContextCompat.getColor(mCtx, R.color.defaultBackgroundLight);
            selectedBackgroundColor = ContextCompat.getColor(mCtx, R.color.selectedBackgroundLight);
        }

        // Load selected states from SharedPreferences
        SharedPreferences prefs = mCtx.getSharedPreferences(PREFS_NAME, Context.MODE_PRIVATE);
        Set&lt;String&gt; selectedItems = prefs.getStringSet(SELECTED_ITEMS_KEY, new HashSet&lt;&gt;());
        for (String position : selectedItems) {
            selectedPositions.add(Integer.parseInt(position));
        }

        // Set selection state on models based on loaded positions
        for (int i = 0; i &lt; pyqModelList.size(); i++) {
            PyqModel model = pyqModelList.get(i);
            model.setSelected(selectedPositions.contains(i));
        }
    }
    @NonNull
    @Override
    public ViewHolder onCreateViewHolder(@NonNull ViewGroup parent, int viewType) {
        LayoutInflater inflater = LayoutInflater.from(parent.getContext());
        View view = inflater.inflate(R.layout.pyq_rv_layout, parent, false);
        return new ViewHolder(view);
    }

    @Override
    public void onBindViewHolder(@NonNull ViewHolder holder, int position) {
        final PyqModel temp = pyqModelList.get(position);

        // Set text views
        holder.textView.setText(temp.getPdfName());
        holder.serialNumber.setText(String.valueOf(position + 1));
        holder.pyqTopics.setText(temp.getPyqTopics());

        // Use holder.getAdapterPosition() to get the current position
        int adapterPosition = holder.getAdapterPosition();
        if (adapterPosition == RecyclerView.NO_POSITION) {
            return;
        }

        // Set background color based on selection state
        if (temp.isSelected()) {
            holder.cardView.setCardBackgroundColor(selectedBackgroundColor);
        } else {
            holder.cardView.setCardBackgroundColor(defaultBackgroundColor);
        }

        holder.cardView.setOnClickListener(v -&gt; {
            SharedPreferences prefs = mCtx.getSharedPreferences(PREFS_NAME, Context.MODE_PRIVATE);
            SharedPreferences.Editor editor = prefs.edit();
            Set&lt;String&gt; selectedItems = new HashSet&lt;&gt;();
            for (int pos : selectedPositions) {
                selectedItems.add(String.valueOf(pos));
            }
            selectedPositions.add(adapterPosition);
            editor.putStringSet(SELECTED_ITEMS_KEY, selectedItems);
            // Notify adapter to refresh views
            notifyItemChanged(adapterPosition);
            editor.apply();

            // Launch ViewPdf activity
            Intent i = new Intent(holder.cardView.getContext(), ViewPdf.class);
            i.putExtra(&quot;pdfName&quot;, temp.getPdfName());
            i.putExtra(&quot;pdfUrl&quot;, temp.getPdfUri());
            i.setFlags(Intent.FLAG_ACTIVITY_NEW_TASK);
            holder.cardView.getContext().startActivity(i);
        });
    }






    @Override
    public int getItemCount() {
        return pyqModelList.size();
    }

    public static class ViewHolder extends RecyclerView.ViewHolder {
        CardView cardView;
        TextView textView, serialNumber, pyqTopics;

        public ViewHolder(View itemView) {
            super(itemView);
            cardView = itemView.findViewById(R.id.pyqCardView);
            textView = itemView.findViewById(R.id.pyqTitle);
            serialNumber = itemView.findViewById(R.id.serialNumber);
            pyqTopics = itemView.findViewById(R.id.pyqTopics);
        }
    }
}

","Add this :
private List&lt;Integer&gt; highlightedPositions;

instead of this:
private final Set&lt;Integer&gt; selectedPositions;

in your adapter class.
Then use these two function to save and then load the positions of the cards selected by the User.
private void loadHighlightedPositions() {
    SharedPreferences sharedPreferences = getSharedPreferences(&quot;MyPrefs&quot;, MODE_PRIVATE);
    String positionsString = sharedPreferences.getString(&quot;highlightedPositions&quot;, &quot;&quot;);
    if (!positionsString.isEmpty()) {
        String[] positionsArray = positionsString.split(&quot;,&quot;);
        for (String position : positionsArray) {
            highlightedPositions.add(Integer.parseInt(position));
        }
    }
}

private void saveHighlightedPositions() {
    SharedPreferences sharedPreferences = getSharedPreferences(&quot;MyPrefs&quot;, MODE_PRIVATE);
    SharedPreferences.Editor editor = sharedPreferences.edit();
    StringBuilder positionsString = new StringBuilder();
    for (int position : highlightedPositions) {
        positionsString.append(position).append(&quot;,&quot;);
    }
    if (positionsString.length() &gt; 0) {
        positionsString.deleteCharAt(positionsString.length() - 1); // Remove the trailing comma
    }
    editor.putString(&quot;highlightedPositions&quot;, positionsString.toString());
    editor.apply();
}

Then use this in onBindviewHolder() Method :
if (highlightedPositions.contains(position)) {
    holder.itemView.setBackgroundColor(selectedBackgroundColor); // Change to your desired color
} else {
    holder.itemView.setBackgroundColor(defaultBackgroundColor); // Default color
}

Then use these two Functions in the adapter class to add and remove position based on whether position is already selected or not.
public void addPosition(int position) {
    if (!highlightedPositions.contains(position)) {
        highlightedPositions.add(position);
        saveHighlightedPositions();
        notifyItemChanged(position);
    }
}

public void removePosition(int position) {
    if (highlightedPositions.contains(position)) {
        highlightedPositions.remove((Integer) position);
        saveHighlightedPositions();
        notifyItemChanged(position);
    }
}

Then use this on onClickListener for your card item:
holder.itemView.setOnClickListener(v -&gt; {
    if (highlightedPositions.contains(position)) {
        highlightedPositions.remove((Integer) position);
    } else {
        highlightedPositions.add(position);
    }
    saveHighlightedPositions();
    notifyItemChanged(position);
});

P.S You need to make changes in Shared Preferences storage approach. Hope that helps.
"
"I'm using Spring and testing with JUnit5 and mockito to test a service layer method that makes a call to a JPA repository method. The service layer should make a query to the database and if a record is present then an exception must be throw.
Bellow the classes that are being used.
ItemServiceTest:
@TestInstance(TestInstance.Lifecycle.PER_CLASS)
@ExtendWith(MockitoExtension.class)
class ItemServiceTest {

    MockItem input;

    @InjectMocks
    ItemService itemService;

    @Mock
    ItemRepository itemRepository;

    @Mock
    CategorieRepository categorieRepository;

    @Mock
    ItemDTOMapper itemDTOMapper;
    
    @Mock
    private UriComponentsBuilder uriBuilder;

    @Mock
    private UriComponents uriComponents;

    @Captor
    private ArgumentCaptor&lt;Long&gt; longCaptor;

    @Captor
    private ArgumentCaptor&lt;String&gt; stringCaptor;

    @BeforeEach
    void setUpMocks() {
        input = new MockItem();
        MockitoAnnotations.openMocks(this);
    }

    @Test
    void testCase() throws ItemAlreadyCreatedException {
        Item item = input.mockEntity();
        CreateItemData data = input.mockDTO();
        ItemListData listData = input.mockItemListData();

        when(itemRepository.findByItemNameIgnoreCase(any())).thenReturn(Optional.of(item));
        given(uriBuilder.path(stringCaptor.capture())).willReturn(uriBuilder);
        given(uriBuilder.buildAndExpand(longCaptor.capture())).willReturn(uriComponents);

        Exception ex = assertThrows(ItemAlreadyCreatedException.class, () -&gt; {
            itemService.createItem(data, uriBuilder);
        });

        String expectedMessage = &quot;There is an item created with this name&quot;;
        String actualMessage = ex.getMessage();

        assertEquals(expectedMessage, actualMessage);
    }
}

ItemRepository:
public interface ItemRepository extends JpaRepository&lt;Item, Long&gt; {

    Optional&lt;Item&gt; findByItemNameIgnoreCase(String name);
}

ItemService:
@Service
public class ItemService {

    private final ItemRepository itemRepository;
    private final CategorieRepository categorieRepository;
    private final ItemDTOMapper itemDTOMapper;
    private final ImageService imageService;

    public ItemService(ItemRepository itemRepository, CategorieRepository categorieRepository, ItemDTOMapper itemDTOMapper, ImageService imageService) {
        this.itemRepository = itemRepository;
        this.categorieRepository = categorieRepository;
        this.itemDTOMapper = itemDTOMapper;
        this.imageService = imageService;
    }
    
    @Transactional
    public CreateRecordUtil createItem(CreateItemData data, UriComponentsBuilder uriBuilder) throws ItemAlreadyCreatedException {
        
        Optional&lt;Item&gt; isNameInUse = itemRepository.findByItemNameIgnoreCase(data.itemName());

        if (isNameInUse.isPresent()) {
            throw new ItemAlreadyCreatedException(&quot;There is an item created with this name&quot;);
        }

        //some logic after if statement
 
        return new CreateRecordUtil();
    }
}

MockItem (it is a class to mock Item entity and its DTOs):
public class MockItem {

    public Item mockEntity() {
        return mockEntity(0);
    }

    public CreateItemData mockDTO() {
        return mockDTO(0);
    }

    public ItemListData mockItemListData() {
        return itemListData(0);
    }

    public Item mockEntity(Integer number) {
        Item item = new Item();
        Categorie category = new Categorie(11L, &quot;mockCategory&quot;, &quot;mockDescription&quot;);

        item.setId(number.longValue());
        item.setItemName(&quot;Name Test&quot; + number);
        item.setDescription(&quot;Name Description&quot; + number);
        item.setCategory(category);
        item.setPrice(BigDecimal.valueOf(number));
        item.setNumberInStock(number);

        return item;
    }

    public CreateItemData mockDTO(Integer number) {
        CreateItemData data = new CreateItemData(
                &quot;Name Test&quot; + number,
                &quot;Name Description&quot; + number,
                11L,
                BigDecimal.valueOf(number),
                number);

        return data;
    }

    private ItemListData itemListData(Integer number) {
        CategoryListData category = new CategoryListData(11L, &quot;mockCategory&quot;);

        ItemListData data = new ItemListData(
                number.longValue(),
                &quot;First Name Test&quot; + number,
                category,
                &quot;Name Description&quot; + number,
                BigDecimal.valueOf(number),
                number
        );

        return data;
    }
}

I've tried to use mockito when like the following:
when(itemRepository.findByItemNameIgnoreCase(any())).thenReturn(Optional.of(item));
With this line I expect that when my itemService calls itemRepository.findByItemNameIgnoreCase() inside createItem() method, it should return the mock record.
That works fine when I call itemRepository directly in the test case body, the problem begins when I tried to call itemRepository in the service layer as I said. It does not returned the expected when() that was being expected and the if statement was not reached at all, and the test case fails with:
org.opentest4j.AssertionFailedError: Expected com.inventory.server.infra.exception.ItemAlreadyCreatedException to be thrown, but nothing was thrown.

    at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:152)
    at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:73)
    at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:35)
    at org.junit.jupiter.api.Assertions.assertThrows(Assertions.java:3115)
    at com.inventory.server.service.ItemServiceTest.testCase(ItemServiceTest.java:84)
    at java.base/java.lang.reflect.Method.invoke(Method.java:580)
    at java.base/java.util.ArrayList.forEach(ArrayList.java:1596)
    at java.base/java.util.ArrayList.forEach(ArrayList.java:1596)

So, after that I've tried to use verify to see if there were some interaction being made with itemRepository inside itemService, like the following:
verify(itemRepository).findByItemNameIgnoreCase(any());
But with that call I get the following error:
Wanted but not invoked:
itemRepository.findByItemNameIgnoreCase(
    &lt;any&gt;
);
-&gt; at com.inventory.server.service.ItemServiceTest.testCase(ItemServiceTest.java:92)
Actually, there were zero interactions with this mock.

Wanted but not invoked:
itemRepository.findByItemNameIgnoreCase(
    &lt;any&gt;
);
-&gt; at com.inventory.server.service.ItemServiceTest.testCase(ItemServiceTest.java:92)
Actually, there were zero interactions with this mock.

    at com.inventory.server.service.ItemServiceTest.testCase(ItemServiceTest.java:92)
    at java.base/java.lang.reflect.Method.invoke(Method.java:580)
    at java.base/java.util.ArrayList.forEach(ArrayList.java:1596)
    at java.base/java.util.ArrayList.forEach(ArrayList.java:1596)


How can I reach the if statement so I can assert that the exception was throw?
I've tried A LOT of other similar problems solutions here in SO, but none of then worked in my case, a help in this one would be really appreciated.
","@ExtendWith(MockitoExtension.class)
class ItemServiceTest {

    @InjectMocks
    ItemService itemService;

    @Mock
    ItemRepository itemRepository;

    // ...

    @BeforeEach
    void setUpMocks() {
        MockitoAnnotations.openMocks(this);
    }

    @Test
    void test() {
      // ...
    }
}

What is happening step by step when executing your test?

A new instance of ItemServiceTest is created
MockitoExtension initializes and assigns Mockito mock objects to each field annotated with @Mock
MockitoExtension creates a new instance of each field annotated with @InjectMocks and injects the mock objects from step 2
The @BeforeEach method is called

MockitoAnnotations.openMocks(this) initializes and assigns Mockito mock objects to each field annotated with @Mock
(itemService already has a reference assigned, so it is ignored by Mockito)


Your test method is called

Methods are stubbed on the reassigned mock instances from step 4.1
Your service is called, invoking methods on the mock instances assigned in step 3



After step 4.1. the mocks referenced by your fields and the mocks injected into itemService are different instances. Your test method stubs the instances referenced by the fields, but your service invokes methods on the instances injected into your instance.
Solution:
Remove @ExtendWith(MockitoExtension.class) or remove MockitoAnnotations.openMocks(this) (preferred).
You don't have to trust strangers on the internet on this one. Add the following logs to your test:
@BeforeEach
void setUpMocks() {
    input = new MockItem();
    System.out.println(&quot;before openMocks &quot; + System.identityHashCode(itemRepository));
    MockitoAnnotations.openMocks(this);
    System.out.println(&quot;after openMocks&quot; + System.identityHashCode(itemRepository));
}


@Test
void testCase() throws ItemAlreadyCreatedException {
    System.out.println(&quot;testCase() &quot; + System.identityHashCode(itemRepository));
    // ...
}

and service:
public ItemService(ItemRepository itemRepository, CategorieRepository categorieRepository, ItemDTOMapper itemDTOMapper, ImageService imageService) {
    System.out.println(&quot;new ItemService() &quot; + System.identityHashCode(itemRepository));

    this.itemRepository = itemRepository;
    this.categorieRepository = categorieRepository;
    this.itemDTOMapper = itemDTOMapper;
    this.imageService = imageService;
}

@Transactional
public Object createItem(CreateItemData data, UriComponentsBuilder uriBuilder) throws ItemAlreadyCreatedException {
    System.out.println(&quot;createItem() &quot; +  System.identityHashCode(itemRepository));

    // ...
}

You will then see output similar to:
new ItemService() 930641076  // step 3
before openMocks 930641076   // step 4
after openMocks 280541440    // step 4.1
testCase() 280541440         // step 5
createItem() 930641076       // step 5.2

What can you derive from that output?

ItemService is only instantiated once
ItemService is instantiated with an instance of itemRepository that is assigned before your @BeforeEach method
openMocks initializes a new mock instance and reassigns the field in your test
Your test method stubs methods on the second, reassigned instance
Your service still references the first mock instance and createItem() invokes methods on that service

In essence, this is another incarnation of Why are my mocked methods not called when executing a unit test? – not manually reassigning the fields, but having Mockito reassign new instances via openMocks(this).
"
"Up until Spring 5.x I was creating the multipart files that way (using now deprecated CommonsMultipartFile):
OutputStream outputStream;
final DiskFileItem diskFileItem = new DiskFileItem(&quot;file&quot;, mimeType, false, fileName, fileSize, repo));
try (InputStream inputStream = new FileInputStream(actualFile)) {
    outputStream = diskFileItem.getOutputStream();
    IOUtils.copy(inputStream, outputStream);
    return new CommonsMultipartFile(diskFileItem);
} catch (Exception e) {
    throw new GoogleConversionFailedException(&quot;Cannot build MultipartFile&quot;, e);
}

How to achieve the same result (create MultipartFile out of java.io.File) on Spring 6 without using MockMultipartFile (documentation states that it's supposed to be used for testing i really want to avoid that route)?
","You could always just implement the interface which is straight-forward in your case:
public class FileMultipartFile implements MultipartFile{
    private Path path;
    public FileMultipartFile(Path path){
        this.path=path;
    }
    @Override 
    public String getName(){
        return path.getFileName().toString();
    }
    @Override 
    public String getOriginalFilename() {
        return getName();
    }
    @Override
    public String getContentType() {
        try{
             //not ideal as mentioned in the comments of https://stackoverflow.com/a/8973468/10871900 
             return Files.probeContentType(path); 
        }catch(IOException e){
             return null;
        }
    }
    @Override
    public long getSize() {
        return Files.size(path);
    }
    @Override
    public boolean isEmpty() {
        return getSize()==0;
    }
    @Override 
    public byte[] getBytes() throws IOException {
        return Files.readAllBytes(path);
    }
    @Override 
    public InputStream getInputStream() throws IOException {
         return Files.newInputStream(path);
    }
    @Override 
    public void transferTo(File dest) throws IOException, IllegalStateException {
        transferTo(dest.toPath());
    }
    @Override 
    public void transferTo(Path dest) throws IOException, IllegalStateException {
        Files.copy(path, dest);
    }
}

You can create instances using new FileMultipartFile(yourFile.toPath());
In case you want to use a custom name, mime type or similar, you can add additional fields.
"
"I'm facing little problem with proper alignment of strings in my TextView. I have 2 strings (left and right) and TextView that has match_parent width. The point is that every solution I find doesn't worked (or doesn't work as I want).
Here's my code:
        String LeftText = &quot;Left&quot;;
        String RightText = &quot;Right&quot;;
        SpannableString finalString = new SpannableString(LeftText+ &quot; &quot; + RightText);
        finalString.setSpan(new AlignmentSpan.Standard(Layout.Alignment.ALIGN_OPPOSITE), LeftText.length() , LeftText.length() +RightText.length(), Spannable.SPAN_EXCLUSIVE_EXCLUSIVE);

        textView.setText(finalString);

I find similar code that works here: solution that works partly but there is new line sing &quot;\n&quot;. I noticed that If I replace &quot; &quot; with &quot;\n&quot; &quot;right&quot; String is on the right side of TextView but little lower (because of the new line sing) but I want this whole text to be in the same line. Can I do something about this?
","If you want to achieve this in a single TextView without splitting it into two, you can try something like this:
String leftText = &quot;Left&quot;;
String rightText = &quot;Right&quot;;
SpannableString finalString = new SpannableString(leftText+ &quot; &quot; + rightText);

Drawable drawable = new ColorDrawable(Color.TRANSPARENT);

textView.getViewTreeObserver().addOnPreDrawListener(new ViewTreeObserver.OnPreDrawListener() {
    @Override
    public boolean onPreDraw() {
        textView.getViewTreeObserver().removeOnPreDrawListener(this);

        float textViewContentWidth = textView.getWidth() - textView.getPaddingStart() - textView.getCompoundPaddingEnd();
        float leftTextWidth = textView.getPaint().measureText(leftText);
        float rightTextWidth = textView.getPaint().measureText(rightText);

        drawable.setBounds(0, 0, (int)(textViewContentWidth - rightTextWidth - leftTextWidth), 1);

        finalString.setSpan(new ImageSpan(drawable, ImageSpan.ALIGN_BASELINE), leftText.length(), leftText.length() + 1, Spannable.SPAN_EXCLUSIVE_EXCLUSIVE);

        textView.setText(finalString);
        return true;
    }
});

The idea is as follows (assuming that the left and right texts do not overlap):

Concatenate strings with a space, as in the original question.
Measure the distance between the left and right texts and create a transparent drawable of this width.
Use ImageSpan to replace the space from step 1.

"
"Purpose, just a POC (for now) to automatically and periodically find some CVE tags in the maven repository.
I can access maven just fine through browser and mvn, but am unable to do the same via Java, what am I missing? I've tried UrlConnection, HttpsURLConnection, with and without GET, Content-type, User-Agent, and Accept, it always returns a 403 for all addresses that I try, the same code works fine on other websites like &quot;cve.mitre.org&quot; or &quot;nvd.nist.gov&quot;, but fails for &quot;https://mvnrepository.com/artifact/log4j/apache-log4j-extras/1.2.17&quot;.
My URL is been built dynamically, with the start &quot;**https://mvnrepository.com/artifact/**&quot;, then adding the group, name, and version are added, turning it into a valid address like &quot;https://mvnrepository.com/artifact/log4j/apache-log4j-extras/1.2.17&quot;
    System.setProperty(&quot;https.proxyHost&quot;, &quot;xxxx&quot;);
    System.setProperty(&quot;https.proxyPort&quot;, &quot;xxxx&quot;);

    String content = null;
    try {
        URL obj = new URL(address);
        HttpsURLConnection con = (HttpsURLConnection) obj.openConnection();
        con.setRequestMethod(&quot;GET&quot;);
        con.setRequestProperty(&quot;Content-Type&quot;, &quot;application/json&quot;);
        con.setRequestProperty(&quot;User-Agent&quot;, &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36&quot;);
        con.setRequestProperty(&quot;Accept&quot;, &quot;*/*&quot;);

        con.connect();
        
        BufferedReader br;
        
        if (con.getResponseCode() &lt; 300) {
            br = new BufferedReader(new InputStreamReader(con.getInputStream(), StandardCharsets.UTF_8));
        } else {
            br = new BufferedReader(new InputStreamReader(con.getErrorStream(), StandardCharsets.UTF_8));
        }            

        final StringBuilder sb = new StringBuilder();
        String line;
        while ((line = br.readLine()) != null) {
            sb.append(line);
        }
        br.close();

","This web use anti-bot security CloudFlare.
How to bypass CloudFlare bot protection?
It depends.... Sometimes it is very difficult task or impossible. That you need to do, is simulate a real user with the browser.
With htmlunit browser you can bypass it in this case only and with a good IP address. (i use my own ip address and did only one request)
You need maven dependency:
&lt;dependency&gt;
    &lt;groupId&gt;net.sourceforge.htmlunit&lt;/groupId&gt;
    &lt;artifactId&gt;htmlunit&lt;/artifactId&gt;
    &lt;version&gt;2.57.0&lt;/version&gt;
&lt;/dependency&gt;

Here you have some java example:
import com.gargoylesoftware.htmlunit.WebClient;
import com.gargoylesoftware.htmlunit.html.HtmlAnchor;
import com.gargoylesoftware.htmlunit.html.HtmlPage;
import java.io.IOException;
import java.net.URL;
import java.util.List;

public class Maven {

    public static void main(String[] args) throws IOException {

        try (final WebClient webClient = new WebClient()) {
            webClient.getOptions().setJavaScriptEnabled(false);
            URL target = new URL(&quot;https://mvnrepository.com/artifact/log4j/apache-log4j-extras/1.2.17&quot;);
            final HtmlPage page = webClient.getPage(target);
            List&lt;HtmlAnchor&gt; elements = page.getByXPath(&quot;//a[contains(@class, 'vuln')]&quot;);
            elements.forEach(element -&gt; System.out.println(element.getTextContent()));
        }
    }
}


OUTPUT:
CVE-2022-23305
CVE-2022-23302
CVE-2021-4104
CVE-2019-17571
View 1 more ...
4 vulnerabilities 

I hope I have been able to help you.
"
"I am trying to implement an DMN (Decision Model and Notation) evaluation service, where the user can upload a csv file with test cases to be evaluated and receive results also as a csv file for every test cases in the input file.
Reading the input csv file and evaluating the test cases works without problems. But I have some issues in writing the results to a csv file using OpenCsv.
Here is the mapped bean, which should be converted to csv row:
@Data
@AllArgsConstructor
@NoArgsConstructor
public class DmnTestCaseResult {

   private Map&lt;String, Object&gt; testInput;

   private Map&lt;String, Object&gt; expectedOutput;

   private List&lt;Map&lt;String, Object&gt;&gt; testOutput;

   private String errorMessage;
}

As you can see here, the test case result can have in some situations multiple testOutputs, defined as a list of map.
What I want is to write for every map entry in the testOutput, a seperate row in the csv file. But with the code I wrote below, only the first entry of the testOutput is written as only one row in the csv file.
 public String convertDmnRuleTestResultToCsv(DmnRuleTestResult result) {
    List&lt;DmnTestCaseResult&gt; results = result.getTestCases();
    try(StringWriter sw = new StringWriter(); CSVWriter writer = new CSVWriter(sw, CSVWriter.DEFAULT_SEPARATOR, CSVWriter.NO_QUOTE_CHARACTER, CSVWriter.NO_ESCAPE_CHARACTER, CSVWriter.DEFAULT_LINE_END)) {
        StatefulBeanToCsv&lt;DmnTestCaseResult&gt; beanToCsv = new StatefulBeanToCsvBuilder&lt;DmnTestCaseResult&gt;(writer)
                .withApplyQuotesToAll(false)
                .build();
        beanToCsv.write(results);
        return sw.toString();
    } catch(Exception ex){
        throw new CsvParseException(ex.getMessage());
    }
}

How can I tell the OpenCsv that it should create seperate row for each entry in the testOutputs ?
EDIT: Added more information
UI: 
Resulted incorrect CSV:

Expected correct CSV:

As you can see from the screenshots, one input can have multiple test outputs. Therefore I want to create for every test output a seperate line in csv file.
","As StatefulBeanToCsv does not seem to be capable to generating multiple lines for a single bean, I suggest implementing a custom mapping function. This also requires you to manually print the header line as well.
public static String convertDmnRuleTestResultToCsv(DmnRuleTestResult result) {
    List&lt;DmnTestCaseResult&gt; results = result.getTestCases();
    try (StringWriter sw = new StringWriter();
            CSVWriter writer = new CSVWriter(sw, CSVWriter.DEFAULT_SEPARATOR,
                    CSVWriter.NO_QUOTE_CHARACTER, CSVWriter.NO_ESCAPE_CHARACTER,
                    CSVWriter.DEFAULT_LINE_END)) {
        writeHeader(writer);
        for (DmnTestCaseResult r : results) {
            for (Map&lt;String, Object&gt; map : r.getTestOutput())
                writer.writeNext(map(r, map));
        }
        return sw.toString();
    } catch (Exception ex) {
        throw new RuntimeException(ex.getMessage());
    }
}

private static void writeHeader(CSVWriter writer) {
    List&lt;String&gt; header = new ArrayList&lt;&gt;();
    header.add(&quot;ERRORMESSAGE&quot;);
    header.add(&quot;EXPECTEDOUTPUT&quot;);
    header.add(&quot;INPUT&quot;);
    header.add(&quot;OUTPUT&quot;);
    writer.writeNext(header.toArray(new String[] {}));

}

private static String[] map(DmnTestCaseResult r, Map&lt;String, Object&gt; testOutput) {
    // you can manually adjust formats here as well; entrySet() call can be left out, it does change the format. do what you like more
    List&lt;String&gt; line = new ArrayList&lt;&gt;();
    line.add(r.getErrorMessage());
    line.add(r.getExpectedOutput().entrySet().toString());
    line.add(r.getTestInput().entrySet().toString());
    line.add(testOutput.entrySet().toString());
    return line.toArray(new String[] {});
}

And this prints:
ERRORMESSAGE,EXPECTEDOUTPUT,INPUT,OUTPUT
errorMessage,[expectedOutput1=expectedOutput1, expectedOutput2=expectedOutput2],[input2=testInput2, input1=testInput1],[testOut2=testOut2, testOut=testOut1]
errorMessage,[expectedOutput1=expectedOutput1, expectedOutput2=expectedOutput2],[input2=testInput2, input1=testInput1],[testOut3=testOut3, testOut4=testOut4]

"
"I have a column with paths where all paths belong to one root folder. When paths do not fit in the column (they can be rather long and column width is limited) I want to show user only the end of the path with leading ellipsis. For example:
..someverylongdirectoryname/file.txt
..omeverylongdirectoryname/file2.txt

I tried this code, but it didn't work.
public class JavaFxTest extends Application {


    public static class FilePath {
        private final String path;

        public FilePath(String path) {
            this.path = path;
        }

        public String getPath() {
            return path;
        }
    }

    @Override
    public void start(Stage primaryStage) {
        TableView&lt;FilePath&gt; tableView = new TableView&lt;&gt;();

        TableColumn&lt;FilePath, String&gt; pathColumn = new TableColumn&lt;&gt;(&quot;Path&quot;);
        pathColumn.setCellValueFactory(new PropertyValueFactory&lt;&gt;(&quot;path&quot;));
        pathColumn.setPrefWidth(200);

        pathColumn.setCellFactory(column -&gt; new javafx.scene.control.TableCell&lt;&gt;() {
            private final Text text = new Text();

            @Override
            protected void updateItem(String item, boolean empty) {
                super.updateItem(item, empty);
                if (empty || item == null) {
                    setGraphic(null);
                } else {
                    text.setText(item);
                    text.setStyle(&quot;-fx-text-alignment: right;&quot;);
                    setGraphic(text);
                }
            }
        });

        tableView.getColumns().add(pathColumn);
        tableView.getItems().addAll(
            new FilePath(&quot;/usr/local/bin/someverylongdirectoryname/file.txt&quot;),
            new FilePath(&quot;/usr/local/bin/someverylongdirectoryname/file2.txt&quot;)
        );

        primaryStage.setScene(new Scene(tableView, 300, 200));
        primaryStage.show();
    }

    public static void main(String[] args) {
        launch(args);
    }
}

Could anyone say how to do it?
","It looks like you are looking to have the text overrun ellipsis at the beginning of the label, instead of the end. Just use the fact that the Cell is a Labeled and set the overrun style:
        pathColumn.setCellFactory(column -&gt; new javafx.scene.control.TableCell&lt;&gt;() {

            {
                setTextOverrun(OverrunStyle.LEADING_ELLIPSIS);
            }
            @Override
            protected void updateItem(String item, boolean empty) {
                super.updateItem(item, empty);
                setText(item);
            }
        });

Of course, you can also just manually replace the root folder if you prefer:
        String commonPath = &quot;/usr/local/bin/someverylongdirectoryname&quot;;

        pathColumn.setCellFactory(column -&gt; new javafx.scene.control.TableCell&lt;&gt;() {


            @Override
            protected void updateItem(String item, boolean empty) {
                super.updateItem(item, empty);
                if (empty || item == null) {
                    setText(null);
                } else {
                    setText(item.replace(commonPath, &quot;...&quot;));
                }
            }
        });

"
"I hope to implement Dinic's algorithm using Java, and I have found a strange problem.
My graph vertex name use string type, and when this string uses pure numbers, such as 1, 2, 3 ,,, 200, At this point, its execution speed is very fast.
However, if I add a prefix to the node name, the execution speed of this code will become very slow with the length of the prefix string, which is difficult to understand.
My algorithm implementation code:
package org.apache.misc.alg.dag;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.HashSet;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Queue;
import java.util.Set;

public class DinicCalculator&lt;T&gt; implements MaxAntichainCalculator&lt;T&gt; {

    private final Map&lt;String, Map&lt;String, Integer&gt;&gt; network;
    private List&lt;String&gt; nodes;
    private int[] level;

    public DinicCalculator() {
        network = new HashMap&lt;&gt;();
        nodes = new ArrayList&lt;&gt;();
        nodes.add(&quot;src&quot;);
        nodes.add(&quot;sink&quot;);
    }

    private void bfs(String source) {
        level = new int[nodes.size()];
        Arrays.fill(level, -1);
        level[nodes.indexOf(source)] = 0;

        Queue&lt;String&gt; queue = new LinkedList&lt;&gt;();
        queue.offer(source);

        while (!queue.isEmpty()) {
            String u = queue.poll();
            for (Map.Entry&lt;String, Integer&gt; entry : network.get(u).entrySet()) {
                String v = entry.getKey();
                int capacity = entry.getValue();
                if (capacity &gt; 0 &amp;&amp; level[nodes.indexOf(v)] == -1) {
                    level[nodes.indexOf(v)] = level[nodes.indexOf(u)] + 1;
                    queue.offer(v);
                }
            }
        }
    }

    private int dfs(String u, int flow, String sink) {
        if (u.equals(sink)) {
            return flow;
        }

        for (Map.Entry&lt;String, Integer&gt; entry : network.get(u).entrySet()) {
            String v = entry.getKey();
            int capacity = entry.getValue();
            if (capacity &gt; 0 &amp;&amp; level[nodes.indexOf(u)] &lt; level[nodes.indexOf(v)]) {
                int sent = dfs(v, Math.min(flow, capacity), sink);
                if (sent &gt; 0) {
                    network.get(u).put(v, capacity - sent);
                    network.get(v).put(u, network.get(v).getOrDefault(u, 0) + sent);
                    return sent;
                }
            }
        }
        return 0;
    }

    private void addEdge(String from, String to, int capacity) {
        network.computeIfAbsent(from, k -&gt; new HashMap&lt;&gt;()).put(to, capacity);
        network.computeIfAbsent(to, k -&gt; new HashMap&lt;&gt;()).put(from, 0);
        if (!nodes.contains(from)) nodes.add(from);
        if (!nodes.contains(to)) nodes.add(to);
    }

    private Set&lt;String&gt; reach(Map&lt;T, Set&lt;T&gt;&gt; graph, T t, Set&lt;String&gt; visited) {
        Queue&lt;T&gt; queue = new LinkedList&lt;&gt;();
        queue.add(t);

        while (!queue.isEmpty()) {
            T current = queue.poll();
            String currentKey = &quot;A&quot; + current.toString();
            visited.add(currentKey);
            for (T neighbor : graph.get(current)) {
                String neighborKey = &quot;B&quot; + neighbor.toString();
                if (!visited.contains(neighborKey)) {
                    queue.add(neighbor);
                    visited.add(neighborKey);
                }
            }
        }

        return visited;
    }

    // entrance
    public int calculator(Map&lt;T, Set&lt;T&gt;&gt; graph) {

        for (T t : graph.keySet()) {
            addEdge(&quot;src&quot;, &quot;A&quot; + t.toString(), 1);
            addEdge(&quot;B&quot; + t, &quot;sink&quot;, 1);
            Set&lt;String&gt; visitedSubset = new HashSet&lt;&gt;();
            for (String u : reach(graph, t, visitedSubset)) {
                addEdge(&quot;A&quot; + t, u, 1);
            }
        }

        int maxFlow = 0;
        while (true) {
            bfs(&quot;src&quot;);
            if (level[nodes.indexOf(&quot;sink&quot;)] == -1) {
                break;
            }

            int flow;
            while ((flow = dfs(&quot;src&quot;, Integer.MAX_VALUE, &quot;sink&quot;)) &gt; 0) {
                maxFlow += flow;
            }
        }

        return graph.size() - maxFlow;
    }
}

My test code:
package org.apache.misc.alg.dag;

import java.util.Arrays;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Map;
import java.util.Set;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class DagTests {

    private static final Logger logger = LoggerFactory.getLogger(DagTests.class);
   
    @Test
    public void test() {
        // Test prefixes of different lengths
        // like 1,2,3,4,,,,,200
        test1(&quot;&quot;);
        // like A1,A2,A3,A4,,,,,A200
        test1(&quot;A&quot;);
        test1(&quot;AA&quot;);
        test1(&quot;AAA&quot;);
        test1(&quot;x&quot;);
        test1(&quot;xx&quot;);
        // like xx_1,xx_2,xx_3,,,,xx_200
        test1(&quot;xx_&quot;);
    }

    public void test1(String prefix) {
        Map&lt;String, Set&lt;String&gt;&gt; graph = genGraph(prefix);
        long t1 = System.currentTimeMillis();
        int result = new DinicCalculator&lt;String&gt;().calculator(graph);
        logger.info(&quot;DinicCalculator with prefix: &quot; + prefix + &quot;, result: &quot; + result + &quot;, time: &quot; + (System.currentTimeMillis() - t1));
    }

    private Map&lt;String, Set&lt;String&gt;&gt; genGraph(String prefix) {
        Map&lt;String, Set&lt;String&gt;&gt; graph = new HashMap&lt;&gt;();
        String end = null;
        for (int i = 0; i &lt; 200; i++) {
            String i1 = prefix + i;
            String i2 = prefix + (i + 1);
            graph.put(i1, new HashSet&lt;&gt;(Arrays.asList(i2)));
            end = i2;
        }

        graph.put(end, new HashSet&lt;&gt;());
        return graph;
    }
}

My test code output:

18:21:24.609 [main] INFO org.apache.misc.alg.dag.DagTests -- DinicCalculator with prefix: , result: 1, time: 503
18:21:27.137 [main] INFO org.apache.misc.alg.dag.DagTests -- DinicCalculator with prefix: A, result: 1, time: 2526
18:21:48.843 [main] INFO org.apache.misc.alg.dag.DagTests -- DinicCalculator with prefix: AA, result: 1, time: 21706
18:21:55.826 [main] INFO org.apache.misc.alg.dag.DagTests -- DinicCalculator with prefix: AAA, result: 1, time: 6983
18:21:57.199 [main] INFO org.apache.misc.alg.dag.DagTests -- DinicCalculator with prefix: x, result: 1, time: 1373
19:35:07.166 [main] INFO org.apache.misc.alg.dag.DagTests -- DinicCalculator with prefix: xx, result: 1, time: 4389965
19:45:18.590 [main] INFO org.apache.misc.alg.dag.DagTests -- DinicCalculator with prefix: xx_, result: 1, time: 611424

Test info:

OS: macOS Sonoma 14.6.1

chip: apple m1 pro


JDK version: openjdk-21.0.2

I have a similar effect when using x64+Ubuntu 22.04+JDK 1.8, also in x64+centos7.5 + jdk1.8.
So where exactly is the problem, could it be caused by CPU cache?
","Your dfs method is missing a visited check. You're in practice doing an unrestricted search, which is very slow.
Also, I changed your nodes field to be a Map&lt;String, Integer&gt;, instead of List&lt;String&gt;, to avoid costly nodes.indexOf calls.
After that, this is what I see:
DinicCalculator with prefix: , result: 1, time: 85
DinicCalculator with prefix: A, result: 1, time: 102
DinicCalculator with prefix: AA, result: 1, time: 104
DinicCalculator with prefix: AAA, result: 1, time: 91
DinicCalculator with prefix: x, result: 1, time: 67
DinicCalculator with prefix: xx, result: 1, time: 66
DinicCalculator with prefix: xx_, result: 1, time: 83

Here's the code for DinicCalculator:
public class DinicCalculator&lt;T&gt; {

    private final Map&lt;String, Map&lt;String, Integer&gt;&gt; network;
    private Map&lt;String, Integer&gt; nodes;
    private int[] level;

    public DinicCalculator() {
        network = new HashMap&lt;&gt;();
        nodes = new HashMap&lt;&gt;();
        nodes.put(&quot;src&quot;, nodes.size());
        nodes.put(&quot;sink&quot;, nodes.size());
    }

    private void bfs(String source) {
        level = new int[nodes.size()];
        Arrays.fill(level, -1);
        level[nodes.get(source)] = 0;

        Queue&lt;String&gt; queue = new LinkedList&lt;&gt;();
        queue.offer(source);

        while (!queue.isEmpty()) {
            String u = queue.poll();
            for (Map.Entry&lt;String, Integer&gt; entry : network.get(u).entrySet()) {
                String v = entry.getKey();
                int capacity = entry.getValue();
                if (capacity &gt; 0 &amp;&amp; level[nodes.get(v)] == -1) {
                    level[nodes.get(v)] = level[nodes.get(u)] + 1;
                    queue.offer(v);
                }
            }
        }
    }

    private int dfs(String u, int flow, String sink, HashSet&lt;String&gt; visited) {
        if (visited.contains(u)) {
            return 0;
        }
        visited.add(u);

        if (u.equals(sink)) {
            return flow;
        }

        for (Map.Entry&lt;String, Integer&gt; entry : network.get(u).entrySet()) {
            String v = entry.getKey();
            int capacity = entry.getValue();
            if (capacity &gt; 0 &amp;&amp; level[nodes.get(u)] &lt; level[nodes.get(v)]) {
                int sent = dfs(v, Math.min(flow, capacity), sink, visited);
                if (sent &gt; 0) {
                    network.get(u).put(v, capacity - sent);
                    network.get(v).put(u, network.get(v).getOrDefault(u, 0) + sent);
                    return sent;
                }
            }
        }
        return 0;
    }

    private void addEdge(String from, String to, int capacity) {
        network.computeIfAbsent(from, k -&gt; new HashMap&lt;&gt;()).put(to, capacity);
        network.computeIfAbsent(to, k -&gt; new HashMap&lt;&gt;()).put(from, 0);
        if (!nodes.containsKey(from)) nodes.put(from, nodes.size());
        if (!nodes.containsKey(to)) nodes.put(to, nodes.size());
    }

    private Set&lt;String&gt; reach(Map&lt;T, Set&lt;T&gt;&gt; graph, T t, Set&lt;String&gt; visited) {
        Queue&lt;T&gt; queue = new LinkedList&lt;&gt;();
        queue.add(t);

        while (!queue.isEmpty()) {
            T current = queue.poll();
            String currentKey = &quot;A&quot; + current.toString();
            visited.add(currentKey);
            for (T neighbor : graph.get(current)) {
                String neighborKey = &quot;B&quot; + neighbor.toString();
                if (!visited.contains(neighborKey)) {
                    queue.add(neighbor);
                    visited.add(neighborKey);
                }
            }
        }

        return visited;
    }

    // entrance
    public int calculator(Map&lt;T, Set&lt;T&gt;&gt; graph) {

        for (T t : graph.keySet()) {
            addEdge(&quot;src&quot;, &quot;A&quot; + t.toString(), 1);
            addEdge(&quot;B&quot; + t, &quot;sink&quot;, 1);
            Set&lt;String&gt; visitedSubset = new HashSet&lt;&gt;();
            for (String u : reach(graph, t, visitedSubset)) {
                addEdge(&quot;A&quot; + t, u, 1);
            }
        }

        int maxFlow = 0;
        while (true) {
            bfs(&quot;src&quot;);
            if (level[nodes.get(&quot;sink&quot;)] == -1) {
                break;
            }

            int flow;
            while ((flow = dfs(&quot;src&quot;, Integer.MAX_VALUE, &quot;sink&quot;, new HashSet&lt;&gt;())) &gt; 0) {
                maxFlow += flow;
            }
        }

        return graph.size() - maxFlow;
    }
}

"
"I have project2 that depends on project1. They are both next to each other on my file system.
When I try to build project2 (after successfully building project1) I get the error:
Could not determine the dependencies of task ':app:distTar'.
&gt; Could not resolve all task dependencies for configuration ':app:runtimeClasspath'.
   &gt; Could not resolve project :project1.
     Required by:
         project :app
      &gt; No matching configuration of project :project1 was found. The consumer was configured to find a runtime of a library compatible with Java 11, packaged as a jar, preferably optimized for standard JVMs, and its dependencies declared externally but:
          - None of the consumable configurations have attributes.

Project2 adds the dependency to project1 as follows...
build.gradle
/*
 * This file was generated by the Gradle 'init' task.
 *
 * This generated file contains a sample Java application project to get you started.
 * For more details take a look at the 'Building Java &amp; JVM projects' chapter in the Gradle
 * User Manual available at https://docs.gradle.org/7.2/userguide/building_java_projects.html
 */

plugins {
    // Apply the application plugin to add support for building a CLI application in Java.
    id 'application'
}

repositories {
    // Use Maven Central for resolving dependencies.
    mavenCentral()
}

dependencies {
    // Use JUnit test framework.
    testImplementation 'junit:junit:4.13.2'

    // This dependency is used by the application.
    implementation 'com.google.guava:guava:30.1.1-jre'

    implementation project(':project1')
    implementation files('../../project1/lib/build/libs/lib.jar')
}

application {
    // Define the main class for the application.
    mainClass = 'project2.App'
}

Settings.gradle
rootProject.name = 'project2'
include('app')

include   ':project1'
project(':project1').projectDir = new File(settingsDir, '../project1')

The source for project1...
/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package project1;

public class Library {
    public boolean someLibraryMethod() {
        return true;
    }
}

The source for project2
/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package project2;

import project1.*;

public class App {
    public String getGreeting() {
        return &quot;Hello World!&quot;;
    }

    public static void main(String[] args) {
        System.out.println(new App().getGreeting());
        bool someBool = Library.someLibraryMethod();
    }
}

the complete folder structure of the two projects. Although it looks like project2 is under project1, that is just how the copy pasted output looks, they are indeed sibling folders.
â”€â”€ project1
â”‚Â Â  â”œâ”€â”€ gradle
â”‚Â Â  â”‚Â Â  â””â”€â”€ wrapper
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ gradle-wrapper.jar
â”‚Â Â  â”‚Â Â      â””â”€â”€ gradle-wrapper.properties
â”‚Â Â  â”œâ”€â”€ gradlew
â”‚Â Â  â”œâ”€â”€ gradlew.bat
â”‚Â Â  â”œâ”€â”€ lib
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ bin
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ main
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ project1
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â      â””â”€â”€ Library.class
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ test
â”‚Â Â  â”‚Â Â  â”‚Â Â      â””â”€â”€ project1
â”‚Â Â  â”‚Â Â  â”‚Â Â          â””â”€â”€ LibraryTest.class
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ build

â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ libs
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ lib.jar

â”‚Â Â  â”‚Â Â  â”œâ”€â”€ build.gradle
â”‚Â Â  â”‚Â Â  â””â”€â”€ src
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ main
â”‚Â Â  â”‚Â Â      â”‚Â Â  â”œâ”€â”€ java
â”‚Â Â  â”‚Â Â      â”‚Â Â  â”‚Â Â  â””â”€â”€ project1
â”‚Â Â  â”‚Â Â      â”‚Â Â  â”‚Â Â      â””â”€â”€ Library.java
â”‚Â Â  â”‚Â Â      â”‚Â Â  â””â”€â”€ resources
â”‚Â Â  â”‚Â Â      â””â”€â”€ test
â”‚Â Â  â”‚Â Â          â”œâ”€â”€ java
â”‚Â Â  â”‚Â Â          â”‚Â Â  â””â”€â”€ project1
â”‚Â Â  â”‚Â Â          â”‚Â Â      â””â”€â”€ LibraryTest.java
â”‚Â Â  â”‚Â Â          â””â”€â”€ resources
â”‚Â Â  â””â”€â”€ settings.gradle
â””â”€â”€ project2
    â”œâ”€â”€ app
    â”‚Â Â  â”œâ”€â”€ build.gradle
    â”‚Â Â  â””â”€â”€ src
    â”‚Â Â      â”œâ”€â”€ main
    â”‚Â Â      â”‚Â Â  â”œâ”€â”€ java
    â”‚Â Â      â”‚Â Â  â”‚Â Â  â””â”€â”€ project2
    â”‚Â Â      â”‚Â Â  â”‚Â Â      â””â”€â”€ App.java
    â”‚Â Â      â”‚Â Â  â””â”€â”€ resources
    â”‚Â Â      â””â”€â”€ test
    â”‚Â Â          â”œâ”€â”€ java
    â”‚Â Â          â”‚Â Â  â””â”€â”€ project2
    â”‚Â Â          â”‚Â Â      â””â”€â”€ AppTest.java
    â”‚Â Â          â””â”€â”€ resources
    â”œâ”€â”€ gradle
    â”‚Â Â  â””â”€â”€ wrapper
    â”‚Â Â      â”œâ”€â”€ gradle-wrapper.jar
    â”‚Â Â      â””â”€â”€ gradle-wrapper.properties
    â”œâ”€â”€ gradlew
    â”œâ”€â”€ gradlew.bat
    â””â”€â”€ settings.gradle

69 directories, 37 files
","Types of Builds
First you have to decide if this should be a multi-project build or a composite build.
Multi-Project Build
This is when you have a single Gradle project that is made up of multiple sub-projects. You should use a multi-project build if the various modules are highly coupled.
You create a sub-project by using include in the settings.gradle[.kts] file. But you should only have the one settings file. Each individual sub-project may have its own build.gradle[.kts] file, and typically does, but does not necessarily have to. There are various ways to cross-configure sub-projects. See the documentation linked above for details.
I do not provide an example of a multi-project build in this answer.
Composite Build
This is when you have two (or more?) relatively independent projects, but one depends on the other like any other external dependency. Yet you want to use the artifacts of the Gradle project directly rather than grab the binaries from a repository. At least some of the time, anyway.
There are a few ways to create a composite build.

Use --include-build on the command line. This requires the least amount of modification to either project (none).

Use includeBuild in the settings.gradle[.kts] file (not include, as that's for multi-project builds). This requires modifying at least one of the project's configurations, which may not be desirable.

Create a &quot;parent&quot; Gradle project for the &quot;real&quot; projects with a settings.gradle[.kts] file. Then add includeBuild for the &quot;real&quot; projects that you want to composite.


In a composite build, the way dependencies are resolved is described in the documentation linked above. Here's an excerpt:

Included builds interact with other builds via dependency substitution. If any build in the composite has a dependency that can be satisfied by the included build, then that dependency will be replaced by a project dependency on the included build. Because of the reliance on dependency substitution, composite builds may force configurations to be resolved earlier, when composing the task execution graph. This can have a negative impact on overall build performance, because these configurations are not resolved in parallel.

Note you don't use project(&quot;...&quot;) to define a dependency on an included build. You simply use the regular Maven coordinates like normal, and if the included build has a project matching those coordinates, then it will be substituted in.

Example Composite Build
This example uses --include-build to create a composite build. Although this example can be reproduced by following the instructions below, it can be a little error-prone and tedious, so here is a GitLab repository of the final example. I make no promise to keep the repository alive.
Source Code
Here is how the two projects were generated and any changes I made to the build configurations and or the code. Note I used Gradle 7.6 when running the init task. Also, when prompted with:

Generate build using new APIs and behavior (some features may change in the next minor release)? (default: no) [yes, no]


I answered, &quot;no&quot;.
Project 1
This project is the &quot;library&quot;. It was generated with the following command:
...\demo\project1&gt; gradle init --type java-library --test-framework junit-jupiter --project-name project1 --package sample.project1 --dsl kotlin

I only made modifications to the following files:
lib\build.gradle.kts
I added:
group = &quot;sample.project1&quot;
version = &quot;1.0&quot;

lib\src\main\java\sample\project1\Library.java
Changed it to:
/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package sample.project1;

public class Library {

    public static void printMessage() {
        System.out.println(&quot;Hello, this is a composite build!&quot;);
    }
}

lib\src\test\java\sample\project1\LibraryTest.java
I deleted this file.
Project 2
This project is the &quot;application&quot;. It depends on project 1. It was generated with the following command:
...\demo\project2&gt; gradle init --type java-application --test-framework junit-jupiter --project-name project2 --package sample.project2 --dsl kotlin

I only made modifications to the following files:
app\build.gradle.kts
I added:
group = &quot;sample.project2&quot;
version = &quot;1.0&quot;

And added the following to the dependencies block:
// ADD DEPENDENCY ON PROJECT 1
implementation(&quot;sample.project1:lib:1.0&quot;)

app\src\main\java\sample\project2\App.java
I changed it to:
/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package sample.project2;

import sample.project1.Library;

public class App {

    public static void main(String[] args) {
        Library.printMessage();
    }
}

app\src\test\java\sample\project2\AppTest.java
I deleted this file.
Directory Structure
Here is the directory structure of the two projects after generating them and making the above modifications. The output excludes the .gradle directory.
...\DEMO
├───project1
│   │   .gitattributes
│   │   .gitignore
│   │   gradlew
│   │   gradlew.bat
│   │   settings.gradle.kts
│   │
│   ├───gradle
│   │   └───wrapper
│   │           gradle-wrapper.jar
│   │           gradle-wrapper.properties
│   │
│   └───lib
│       │   build.gradle.kts
│       │
│       └───src
│           ├───main
│           │   ├───java
│           │   │   └───sample
│           │   │       └───project1
│           │   │               Library.java
│           │   │
│           │   └───resources
│           └───test
│               ├───java
│               └───resources
└───project2
    │   .gitattributes
    │   .gitignore
    │   gradlew
    │   gradlew.bat
    │   settings.gradle.kts
    │
    ├───app
    │   │   build.gradle.kts
    │   │
    │   └───src
    │       ├───main
    │       │   ├───java
    │       │   │   └───sample
    │       │   │       └───project2
    │       │   │               App.java
    │       │   │
    │       │   └───resources
    │       └───test
    │           ├───java
    │           └───resources
    └───gradle
        └───wrapper
                gradle-wrapper.jar
                gradle-wrapper.properties

Running Example
Running the example with the following command:
...\demo\project2&gt; .\gradlew app:run --include-build ..\project1 --console plain

Note: I used --console plain in order to see all the task output. That way you can see tasks from the included build were executed. You do not need to include this option if you don't want to.
Gave the following output:
&gt; Task :app:processResources NO-SOURCE
&gt; Task :project1:lib:compileJava
&gt; Task :project1:lib:processResources NO-SOURCE
&gt; Task :project1:lib:classes
&gt; Task :project1:lib:jar
&gt; Task :app:compileJava
&gt; Task :app:classes

&gt; Task :app:run
Hello, this is a composite build!

BUILD SUCCESSFUL in 1s
4 actionable tasks: 4 executed

"
"PUSH-NOTIFICATION PROBLEM
I am building a Calendar application and I added an alarm that user can choose specific date and time. The alarm works fine. My problem is that when the notification is shown and I tap on the notification bar, I get in the app and the music play until I quit the app. The only wayI found, is to add a time delay to play for 10seconds for example, but I want to stop music on tap.
Vibrator problem
The vibrator doesn't work, right now this isn't such a big problem, so if anyone can solve me at least the vibrator problem it would be very helpfuld.
My AlarmReceiver class
public class AlarmReceiver extends BroadcastReceiver {

    @Override
    public void onReceive(Context context, Intent intent) {




        Vibrator vibrator = (Vibrator) context.getSystemService(Context.VIBRATOR_SERVICE);
        vibrator.vibrate(VibrationEffect.DEFAULT_AMPLITUDE);



        String event = intent.getStringExtra(&quot;title&quot;);
        String comment = intent.getStringExtra(&quot;comment&quot;);
        Intent activityIntent = new Intent(context, MainActivity.class);

        PendingIntent pendingIntent = PendingIntent.getActivity( context,0,activityIntent,0 );


        String text = &quot;Reminder for the Event: &quot; + &quot;\n&quot; + event + &quot;\n&quot; + &quot;Comments: &quot; + &quot;\n&quot; + comment;
        NotificationCompat.Builder builder = new NotificationCompat.Builder(context, &quot;myandroid&quot;)
                .setSmallIcon(R.drawable.alarm)
                .setContentTitle(event)
                .setContentText(text)
                .setAutoCancel(true)
                .setStyle(new NotificationCompat.BigTextStyle()
                        .bigText(text))
                .setContentIntent(pendingIntent)
                .setDeleteIntent(pendingIntent)

                .setDefaults(NotificationCompat.DEFAULT_ALL)
                .setPriority(NotificationCompat.PRIORITY_HIGH);




        NotificationManagerCompat notificationManagerCompat = NotificationManagerCompat.from(context);
        notificationManagerCompat.notify(123,builder.build());

        Notification notification1 = builder.build();
        notification1.flags |= Notification.FLAG_AUTO_CANCEL;


        Uri notification = RingtoneManager.getDefaultUri(RingtoneManager.TYPE_ALARM);

        Ringtone r = RingtoneManager.getRingtone(context, notification);
        r.play();

        final Handler handler = new Handler();
        handler.postDelayed(new Runnable() {
            @Override
            public void run() {
                if (r.isPlaying())
                    r.stop();
            }
        },1000*10 );

    }

}

","I made an extra class:
 public class StopRingtone extends BroadcastReceiver {
@Override
public void onReceive(Context context, Intent intent) {
    // Stop the ringtone playback
    Uri notification = RingtoneManager.getDefaultUri(RingtoneManager.TYPE_ALARM);
    Ringtone r = RingtoneManager.getRingtone(context, notification);
    Log.v(&quot;test2&quot;,&quot;stop2&quot;);
    r.stop();

    // Call onNotificationDeleted to remove the notification
    NotificationManagerCompat notificationManager = NotificationManagerCompat.from(context);
    notificationManager.cancelAll();
}

}
And i modified my AlarmReceiver like this
import android.app.PendingIntent;
import android.content.BroadcastReceiver;
import android.content.Context;
import android.content.Intent;
import android.content.pm.PackageManager;
import android.media.Ringtone;
import android.media.RingtoneManager;
import android.net.Uri;
import android.os.Build;
import android.os.Bundle;
import android.os.VibrationEffect;
import android.os.Vibrator;
import androidx.core.app.ActivityCompat;
import androidx.core.app.NotificationCompat;
import androidx.core.app.NotificationManagerCompat;

public class AlarmReceiver extends BroadcastReceiver {

    private String text;
    public static Ringtone r;

    private Vibrator vibrator;


    public static final int NOTIFICATION_ID = 123;


    @Override
    public void onReceive(Context context, Intent intent) {
        Bundle b = intent.getExtras();
        String event = &quot;&quot;;
        String comment;


        vibrator = (Vibrator) context.getSystemService(Context.VIBRATOR_SERVICE);
        if (vibrator != null &amp;&amp; vibrator.hasVibrator()) {
            if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.O) {
                vibrator.vibrate(VibrationEffect.createOneShot(1000, VibrationEffect.DEFAULT_AMPLITUDE));
            } else {
                vibrator.vibrate(1000);
            }
        }

        if (b != null) {
            event = b.getString(&quot;title&quot;);
            comment = b.getString(&quot;comment&quot;);
            text = &quot;Υπενθύμιση για το συμβάν: &quot; + &quot;\n&quot; + event + &quot;\n&quot; + &quot;Σχόλια: &quot; + &quot;\n&quot; + comment;
        }




        Intent stopIntent = new Intent(context, StopReceiver.class);
        PendingIntent stopPendingIntent = PendingIntent.getBroadcast(context, 0, stopIntent, PendingIntent.FLAG_IMMUTABLE);


        NotificationCompat.Builder builder = new NotificationCompat.Builder(context, &quot;myandroid&quot;)
                .setSmallIcon(R.drawable.alarm)
                .setPriority(NotificationCompat.PRIORITY_HIGH)
                .setContentTitle(event)
                .setContentText(text)
                .setContentIntent(stopPendingIntent)
                .setAutoCancel(true)
                .setStyle(new NotificationCompat.BigTextStyle().bigText(text))
                .setDefaults(NotificationCompat.DEFAULT_ALL);


        NotificationManagerCompat notificationManagerCompat = NotificationManagerCompat.from(context);
        if (ActivityCompat.checkSelfPermission(context, android.Manifest.permission.POST_NOTIFICATIONS) != PackageManager.PERMISSION_GRANTED) {
            // TODO: Consider calling
            //    ActivityCompat#requestPermissions
            // here to request the missing permissions, and then overriding
            //   public void onRequestPermissionsResult(int requestCode, String[] permissions,
            //                                          int[] grantResults)
            // to handle the case where the user grants the permission. See the documentation
            // for ActivityCompat#requestPermissions for more details.
            return;
        }
        notificationManagerCompat.notify(NOTIFICATION_ID, builder.build());
        Uri notification = RingtoneManager.getDefaultUri(RingtoneManager.TYPE_ALARM);
        r = RingtoneManager.getRingtone(context, notification);
        r.play();




    }

    public void stopRingtone() {

        if (r != null &amp;&amp; r.isPlaying()) {
            r.stop();
        }
        if (vibrator != null) {
            vibrator.cancel();
        }


    }

    public void getInApplication(Context context)
    {
        Intent i = new Intent(context.getApplicationContext(),MainActivity.class);
        i.addFlags(Intent.FLAG_ACTIVITY_NEW_TASK);
        context.startActivity(i);
    }

    public static class StopReceiver extends BroadcastReceiver {
        @Override
        public void onReceive(Context context, Intent intent) {
            // Stop the ringtone
            AlarmReceiver alarmReceiver = new AlarmReceiver();
            alarmReceiver.stopRingtone();
            alarmReceiver.getInApplication(context);



            // Dismiss the notification
            NotificationManagerCompat notificationManager = NotificationManagerCompat.from(context);

            notificationManager.cancel(NOTIFICATION_ID);

        }
    }



}

Also i modified my manifest to listen the StopReceiver
&lt;application&gt;

 &lt;receiver
            android:name=&quot;.AlarmReceiver&quot;
            android:enabled=&quot;true&quot;
            android:exported=&quot;true&quot;
            android:label=&quot;AlarmReceiver&quot;
            android:permission=&quot;TODO&quot; /&gt;
        &lt;receiver
            android:name=&quot;.AlarmReceiver$StopReceiver&quot;
            android:enabled=&quot;true&quot;
            android:exported=&quot;true&quot;
            android:permission=&quot;TODO&quot; /&gt;
        &lt;/application&gt;

"
"I write a program that has a list of numbers. You need to add code to find a specific number in the list. If the number is found, the program will show its location. If the number is not found, the program will say that it couldn't be found.
The problem is, the output is looping, and I don't want that.
int[] array = new int[10];
array[0] = 6;
array[1] = 2;
array[2] = 8;
array[3] = 1;
array[4] = 3;
array[5] = 0;
array[6] = 9;
array[7] = 7;

System.out.print(&quot;Search for? &quot;);
int searching = in.nextInt();

for(int i=0; i&lt;array.length; i++){
    if(searching == array[i]){
        System.out.println(searching + &quot; is at index &quot; + i + &quot;.&quot;);
        break;
    }
    else{
        System.out.println(searching + &quot; was not found.&quot;);
    }
}

My output:
Search for? 1
1 was not found.
1 was not found.
1 was not found.
1 is at index 3.

Expected output:
1 is at index 3.

","import java.util.Scanner;

public class IndexWasNotFound {

    public static void main(String[] args) {
        Scanner scanner = new Scanner(System.in);
        int[] array = new int[10];
        array[0] = 6;
        array[1] = 2;
        array[2] = 8;
        array[3] = 1;
        array[4] = 3;
        array[5] = 0;
        array[6] = 9;
        array[7] = 7;

        int index = 0;
        String ans = null;
        boolean yn;
        System.out.print(&quot;Search for? &quot;);
        int searching = scanner.nextInt();

        for(int i=0; i&lt;array.length; i++) {
            if (searching == array[i]) {
                index = i;
                ans = searching + &quot; is at index &quot; + index + &quot;.&quot;;
                yn = true;
                break;
            } else {
                ans = searching + &quot; was not found.&quot;;
                yn = false;
            }
        }
        System.out.println(ans);
    }
}

"
"I have an application that displays several Line Charts with several Series like this:

I'd like to change the color of each Series but haven't found a way to achieve this. The only thing I found is how to change the default colors but that doesn't solve my problem.
Is there really now way to achieve individual colors for chart series?
","The JavaFX CSS Reference Guide says the following for LineChart:





Style class
Comments
Properties




&quot;chart-series-line series&lt;i&gt; default-color&lt;j&gt;&quot;
Where &lt;i&gt; is the index of the series and &lt;j&gt; is the series’ color index
Node


&quot;chart-line-symbol series&lt;i&gt; data&lt;j&gt; default-color&lt;k&gt;&quot;
Where &lt;i&gt; is the index of the series, &lt;j&gt; is the index of the data within the series, and &lt;k&gt; is the series’ color index
Node


&quot;chart-line-symbol series&lt;i&gt; default-color&lt;j&gt;&quot;
Where &lt;i&gt; is the index of the series and &lt;j&gt; is the series’ color index
LegendItem




Note: Although the line is only documented as a Node, by default it is actually a javafx.scene.shape.Path.
If you want to target a specific series' line, use .chart-series-line.series&lt;i&gt;, where &lt;i&gt; is replaced with the index of the series in the chart's data. And if you want to give a series of a specific chart a certain color, then simply give the chart an ID and use that in the CSS selector.
Here's an example. It uses so-called looked-up colors, which makes the CSS a little more scalable and organized. Also, they can have their values changed via the setStyle method in code, allowing you to dynamically change the color programmatically. Another approach for that is to use a &quot;data URL&quot;, showcased in this other Stack Overflow answer.
style.css:
/*
 * The '-fx-stroke' is used to set the color of the line. The line is
 * targeted by the '.chart-series-line.series&lt;i&gt;' selectors.
 *
 * The '-fx-background-color' is used to set the color of the legend
 * symbol so it matches the line. This would also color the symbols
 * on the line if they were shown. These symbol nodes are targeted
 * by the '.chart-line-symbol.series&lt;i&gt;' selectors.
 *
 * Both the '-fx-series0-color' and '-fx-series1-color' &quot;properties&quot;
 * are looked-up colors. You can change the value of a looked-up color
 * in code by calling 'setStyle(...)' on the appropriate node.
 */

 #firstChart {
    -fx-series0-color: magenta;
    -fx-series1-color: dodgerblue;
 }

#secondChart {
    -fx-series0-color: red;
    -fx-series1-color: black;
}

#firstChart .chart-series-line.series0,
#firstChart .chart-line-symbol.series0 {
    -fx-stroke: -fx-series0-color;
    -fx-background-color: -fx-series0-color;
}

#firstChart .chart-series-line.series1,
#firstChart .chart-line-symbol.series1 {
    -fx-stroke: -fx-series1-color;
    -fx-background-color: -fx-series1-color;
}

#secondChart .chart-series-line.series0,
#secondChart .chart-line-symbol.series0 {
    -fx-stroke: -fx-series0-color;
    -fx-background-color: -fx-series0-color;
}

#secondChart .chart-series-line.series1,
#secondChart .chart-line-symbol.series1 {
    -fx-stroke: -fx-series1-color;
    -fx-background-color: -fx-series1-color;
}

Main.java:
import java.util.function.UnaryOperator;
import javafx.application.Application;
import javafx.geometry.Insets;
import javafx.scene.Scene;
import javafx.scene.chart.LineChart;
import javafx.scene.chart.NumberAxis;
import javafx.scene.chart.XYChart;
import javafx.scene.layout.VBox;
import javafx.stage.Stage;

public class Main extends Application {

    @Override
    public void start(Stage primaryStage) {
        var chart1 = createLineChart(&quot;Chart 1&quot;);
        chart1.setId(&quot;firstChart&quot;);
        chart1.getData().add(createSeries(&quot;f(x) = x&quot;, x -&gt; x));
        chart1.getData().add(createSeries(&quot;f(x) = 2x&quot;, x -&gt; 2 * x));

        /*
         * Uncomment the line of code below to demonstrate dynamically changing
         * the value of a looked-up color.
         */
        // chart1.setStyle(&quot;-fx-series0-color: goldenrod;&quot;);

        var chart2 = createLineChart(&quot;Chart 2&quot;);
        chart2.setId(&quot;secondChart&quot;);
        chart2.getData().add(createSeries(&quot;f(x) = x^2&quot;, x -&gt; x * x));
        chart2.getData().add(createSeries(&quot;f(x) = x^3&quot;, x -&gt; x * x * x));

        var root = new VBox(10, chart1, chart2);
        root.setPadding(new Insets(10));

        primaryStage.setScene(new Scene(root, 1000, 720));
        primaryStage.getScene()
                .getStylesheets()
                .add(Main.class.getResource(&quot;/style.css&quot;).toString());
        primaryStage.show();
    }

    private LineChart&lt;Number, Number&gt; createLineChart(String title) {
        var chart = new LineChart&lt;&gt;(new NumberAxis(), new NumberAxis());
        chart.setTitle(title);
        chart.getXAxis().setLabel(&quot;x&quot;);
        chart.getYAxis().setLabel(&quot;f(x)&quot;);
        chart.setCreateSymbols(false);
        return chart;
    }

    private XYChart.Series&lt;Number, Number&gt; createSeries(
            String name, 
            UnaryOperator&lt;Double&gt; func) {
        var series = new XYChart.Series&lt;Number, Number&gt;();
        series.setName(name);

        for (int x = 0; x &lt; 20; x++) {
            series.getData().add(new XYChart.Data&lt;&gt;(x, func.apply((double) x)));
        }

        return series;
    }

}

"
"Should spring security filters call authentication providers directly?
I am trying Pattern 2, from the above post, where essentially my custom filter intercepts a request, takes all the credentials and puts it in the SecurityContext with authenticated=false.
Then my CustomAuthenticationProvider should pick up these credentials and validate it.
In my project my filter intercepts the request and does its work but my auth provider is not getting called.
UsernamePasswordAuthFilter.java
package com.springsecurity.learning.config;

import java.io.IOException;

import org.springframework.http.HttpMethod;
import org.springframework.security.authentication.UsernamePasswordAuthenticationToken;
import org.springframework.security.core.context.SecurityContextHolder;
import org.springframework.web.filter.OncePerRequestFilter;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.springsecurity.learning.dto.CredentialsDto;

import jakarta.servlet.FilterChain;
import jakarta.servlet.ServletException;
import jakarta.servlet.http.HttpServletRequest;
import jakarta.servlet.http.HttpServletResponse;

public class UsernamePasswordAuthFilter extends OncePerRequestFilter {
    
    private final String END_POINT = &quot;/api/login&quot;;
    private static final ObjectMapper MAPPER = new ObjectMapper();

    @Override
    protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain)
            throws ServletException, IOException {
        // TODO Auto-generated method stub
        if(END_POINT.equals(request.getRequestURI()) 
                &amp;&amp; HttpMethod.POST.matches(request.getMethod())) {
            CredentialsDto credentialsDto = MAPPER.readValue(request.getInputStream(), CredentialsDto.class);
            
            SecurityContextHolder.getContext().setAuthentication(
                    new UsernamePasswordAuthenticationToken(credentialsDto.getUsername(), 
                            credentialsDto.getPassword())
            );
        }
        
        
        
        filterChain.doFilter(request, response);
    }

}


CustomAuthentcationProvider.java
package com.springsecurity.learning.config;

import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.security.authentication.AuthenticationProvider;
import org.springframework.security.authentication.BadCredentialsException;
import org.springframework.security.authentication.UsernamePasswordAuthenticationToken;
import org.springframework.security.core.Authentication;
import org.springframework.security.core.AuthenticationException;
import org.springframework.security.core.authority.SimpleGrantedAuthority;
import org.springframework.stereotype.Component;

import com.springsecurity.learning.dto.CredentialsDto;
import com.springsecurity.learning.dto.UserDto;
import com.springsecurity.learning.services.AuthenticationService;

import lombok.AllArgsConstructor;


@Component
@AllArgsConstructor
public class CustomAuthenticationProvider implements AuthenticationProvider {

    private final AuthenticationService authenticationService;
    
    @Override
    public Authentication authenticate(Authentication authentication) throws AuthenticationException {
        // TODO Auto-generated method stub
        
        UserDto userDto = null;
        if(authentication instanceof UsernamePasswordAuthenticationToken) {
            userDto = authenticationService.authenticate(
                    new CredentialsDto((String)authentication.getPrincipal(),
                            (String)authentication.getCredentials()));
        }
        
        if(userDto==null)return null;
        
        return new UsernamePasswordAuthenticationToken(userDto.getUsername(), 
                null,
                List.of(new SimpleGrantedAuthority(userDto.getRole())
        )); 
    }

    @Override
    public boolean supports(Class&lt;?&gt; authentication) {
        // TODO Auto-generated method stub(UsernamePasswordAuthenticationToken.class.isAssignableFrom(authentication));
        return true;
    }

}


SecurityConfig.java
package com.springsecurity.learning.config;

import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.security.authentication.AuthenticationManager;
import org.springframework.security.authentication.ProviderManager;
import org.springframework.security.config.annotation.web.builders.HttpSecurity;
import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;
import org.springframework.security.config.http.SessionCreationPolicy;
import org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder;
import org.springframework.security.web.SecurityFilterChain;
import org.springframework.security.web.authentication.logout.LogoutFilter;
import org.springframework.security.web.authentication.www.BasicAuthenticationFilter;

@Configuration
@EnableWebSecurity(debug = true)
public class SecurityConfig {
    
    @Bean
    public BCryptPasswordEncoder passwordEncoder() {
        return new BCryptPasswordEncoder();
    }
    
    @Bean
    public AuthenticationManager authenticationManager(CustomAuthenticationProvider customAuthenticationProvider) {
        return new ProviderManager(customAuthenticationProvider);
    }

    @Bean
    public SecurityFilterChain filterChain(HttpSecurity httpSecurity, CustomAuthenticationProvider customAuthenticationProvider) throws Exception {
        httpSecurity
            .authenticationProvider(customAuthenticationProvider)
            .addFilterAfter(new UsernamePasswordAuthFilter(), LogoutFilter.class)
            .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.STATELESS)
            .and()
            .csrf().disable()
            .authorizeHttpRequests()
            .requestMatchers(&quot;/home/public&quot;)
            .permitAll()
            .anyRequest()
            .authenticated()
            .and()
            .httpBasic().disable();
        
        return httpSecurity.build();
    }
}


This is my security filter chain

","Your filter is the culprit as you aren't calling the authenticate method on the AuthenticationManager (which in your case will call the provider(s)).
If you look at some default Spring Security filters, like the UsernamePasswordAuthenticationFilter and BasicAuthenticationFilter you can see that they do call that method and set the resulting Authentication in the SecurityContext. Where as you just set it without attempting any authentication.
Your filter needs to call it and to have more functionality I would also suggest to extend the Spring Security provided AbstractAuthenticationProcessingFilter and implement the attemptAuthenticationMethod.
public class UsernamePasswordAuthFilter extends AbstractAuthenticationProcessingFilter {
    
  private final String END_POINT = &quot;/api/login&quot;;
  private static final ObjectMapper MAPPER = new ObjectMapper();

  UsernamePasswordAuthFilter() {
    super(END_POINT); 
  }

  @Override
  public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response)
            throws AuthenticationException, IOException, ServletException;

    CredentialsDto credentialsDto = MAPPER.readValue(request.getInputStream(), CredentialsDto.class);
    String username = credentialsDto.getUsername();
    String password = credentialsDto.getPassword();
    
    UsernamePasswordAuthenticationToken authRequest = UsernamePasswordAuthenticationToken.unauthenticated(username,
                password);
    // Allow subclasses to set the &quot;details&quot; property
    setDetails(request, authRequest);
    return this.getAuthenticationManager().authenticate(authRequest);
  }
}

This will read your object, call authentication and integrate with the other parts of Spring Security (like firing events, session management etc.).
You could make it even a bit cleaner by having your CredentialsDto implement the Authentication interface. Your CustomAuthenticationProvider can then check if it supports it and just cast it. Saves you creating intermediate objects.
public class UsernamePasswordAuthFilter extends AbstractAuthenticationProcessingFilter {
    
  private final String END_POINT = &quot;/api/login&quot;;
  private static final ObjectMapper MAPPER = new ObjectMapper();

  UsernamePasswordAuthFilter() {
    super(END_POINT); 
  }

  @Override
  public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response)
            throws AuthenticationException, IOException, ServletException;

    CredentialsDto credentialsDto = MAPPER.readValue(request.getInputStream(), CredentialsDto.class);   
    // Allow subclasses to set the &quot;details&quot; property
    setDetails(request, credentialsDto);
    return this.getAuthenticationManager().authenticate(credentialsDto);
  }
}

Your CustomAuthenticationProvider is now pretty simple.
@Component
@AllArgsConstructor
public class CustomAuthenticationProvider implements AuthenticationProvider {

    private final AuthenticationService authenticationService;
    
    @Override
    public Authentication authenticate(Authentication authentication) throws AuthenticationException {       
        UserDto userDto = null;
        if(authentication instanceof CredentialsDto) {
          userDto = authenticationService.authenticate((CredentialsDto) authentication);
                
        }
        
        if(userDto==null)return null;       
        return new UsernamePasswordAuthenticationToken(userDto.getUsername(), 
                null,
                List.of(new SimpleGrantedAuthority(userDto.getRole())
        )); 
    }

    @Override
    public boolean supports(Class&lt;?&gt; authentication) {    
        return CredentialsDto.class.isAssignableFrom(authentication);
    }
}

Something along those lines.
For configuration your UsernamePasswordAuthFilter now needs to be a proper bean to get the AuthenticationManager injected, if you keep the same strategy as you have now you will run into NullPointerExceptions due to no dependencies being injected.
"
"Many 3d programs uses an outline to hint the user when 3d object is selected .
is there a way to mimic that behavior in javafx ?
","Outline with a scaled 3d object with front faces culled

In this aproach . A 3d object is instansiated with the class type of PickResult node when mouse is entered on a Group node , that object has the same sizes of the picked one and the same translations ( in this case is just x axis )  but is slightly scaled (1.1). So , the 3d object is bigger and it is in the same spot but is not overlaping the shape3d underneath beacuse any face looking toward the camera is culled with Culface.FRONT enum in Shape3d.setCullFace() . Finally , when mouse exits pickresult node the 3d object is removed from group's children allowing next iterations . This is a single class  functional javafx app you can try
App.java
public class App extends Application {

    private Shape3D outLine;

    @Override
    public void start(Stage stage) {
        Shape3D sphere = new Sphere(0.45);
        PhongMaterial sMaterial = new PhongMaterial(Color.CORAL);
        sphere.setMaterial(sMaterial);
        sphere.setTranslateX(-1.1);
        Shape3D box = new Box(0.9, 0.9, 0.9);
        box.setTranslateX(1.1);
        box.setMaterial(new PhongMaterial(Color.PALEGREEN));
        Shape3D cylinder = new Cylinder(0.45, 0.8);
        cylinder.setMaterial(new PhongMaterial(Color.PALEVIOLETRED));
        PerspectiveCamera camera = new PerspectiveCamera(true);
        camera.setTranslateZ(-5.5);

        Group group3d = new Group(camera, box, sphere, cylinder);

        Scene scene = new Scene(group3d, 640, 480, true, SceneAntialiasing.BALANCED);
        group3d.setOnMouseExited((t) -&gt; {
            group3d.getChildren().remove(outLine);
        });
        group3d.setOnMouseEntered((t) -&gt; {

            PickResult pickResult = t.getPickResult();
            Node intersectedNode = pickResult.getIntersectedNode();

            if (intersectedNode instanceof Sphere) {

                outLine = new Sphere(((Sphere) intersectedNode).getRadius());
                outLine.setTranslateX(intersectedNode.getTranslateX());
                group3d.getChildren().add(outLine);

                outLine.setCullFace(CullFace.FRONT);
                outLine.setScaleX(1.1);
                outLine.setScaleY(1.1);
                outLine.setScaleZ(1.1);

            }
            if (intersectedNode instanceof Cylinder) {
                Cylinder c = (Cylinder) intersectedNode;
                outLine = new Cylinder(c.getRadius(), c.getHeight(), c.getDivisions());
                outLine.setTranslateX(c.getTranslateX());
                group3d.getChildren().add(outLine);

                outLine.setCullFace(CullFace.FRONT);
                outLine.setScaleX(1.1);
                outLine.setScaleY(1.1);
                outLine.setScaleZ(1.1);

            }
            if (intersectedNode instanceof Box) {
                Box b = (Box) intersectedNode;
                outLine = new Box(b.getWidth(), b.getHeight(), b.getDepth());
                outLine.setTranslateX(b.getTranslateX());
                group3d.getChildren().add(outLine);

                outLine.setCullFace(CullFace.FRONT);
                outLine.setScaleX(1.1);
                outLine.setScaleY(1.1);
                outLine.setScaleZ(1.1);

            }
        });
        scene.setCamera(camera);
        scene.setFill(Color.AQUA);
        stage.setScene(scene);
        stage.setTitle(&quot;outline javafx&quot;);
        stage.show();
    }

    public static void main(String[] args) {
        launch();
    }
}

Note  Shape3d objects scale from its very center . If this method will be implemented with custom MeshView objects , those meshes needs to scale at its center as well
"
"I have a project running with JWT authentication, it works, but now I need to implement Multi-Tenancy using the following approach:

Requirements:

A user can have access to one or more tenants
Access permissions are defined by user and tenant
Getting subdomain through @RequestAttribute in requests
Generate the token containing the tenant ID (subdomain).
Validate the tenant on all requests

Implemented:

Created JWT Autentication.
Created TenantInterceptor.
Getting subdomain using @RequestAttribute on requests.
Created existsByUsernameAndSubdomain validation.

I'm having trouble implementing this new feature, can you point me to an implementation example or tutorial that can help me?
I thank you for your help!
Below are my classes or if you prefer clone on GitHub!
My classes
Models:


/** ERole **/
    public enum ERole {
    ROLE_USER,
    ROLE_MODERATOR,
    ROLE_ADMIN
}

/** Role **/
@Entity
@Data
@Builder
@AllArgsConstructor
@NoArgsConstructor
@Table(name = ""roles"")
public class Role {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Integer id;

    @Enumerated(EnumType.STRING)
    @Column(length = 20)
    private ERole name;
}

/** Tenant **/
@Entity
@Data
@Builder
@AllArgsConstructor
@NoArgsConstructor
@Table(name = ""tenants"",
        uniqueConstraints = {
                @UniqueConstraint(columnNames = ""subdomain"", name = ""un_subdomain"")
        })
public class Tenant {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @NotBlank
    @Size(max = 20)
    private String subdomain;

    @NotBlank
    private String name;

}

/** User **/
@Entity
@Data
@Builder
@AllArgsConstructor
@NoArgsConstructor
@Table(name = ""users"",
        uniqueConstraints = {
                @UniqueConstraint(columnNames = ""username"", name = ""un_username"")
        })
public class User {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @NotBlank
    @Size(max = 20)
    private String username;

    @NotBlank
    @Size(max = 120)
    @JsonIgnore
    private String password;

//    Remove
    @ManyToMany(fetch = FetchType.LAZY)
    @JoinTable(name = ""users_roles"",
            joinColumns = {@JoinColumn(name = ""user_id"",
                    foreignKey = @ForeignKey(name = ""fk_users_roles_users1""))},
            inverseJoinColumns = {@JoinColumn(name = ""role_id"",
                    foreignKey = @ForeignKey(name = ""fk_users_roles_roles1""))})
    private Set&lt;Role&gt; roles = new HashSet&lt;&gt;();

//    Include
    @EqualsAndHashCode.Exclude
    @OneToMany(mappedBy = ""user"",
            cascade = CascadeType.ALL,
            orphanRemoval = true,
            fetch = FetchType.LAZY)
    @JsonManagedReference
    private List&lt;UserTenant&gt; tenants = new ArrayList&lt;&gt;();

    public User(String username, String password) {
        this.username = username;
        this.password = password;
    }

}

/** UserTenant **/
@Entity
@Data
@Builder
@AllArgsConstructor
@NoArgsConstructor
@Table(name = ""users_tenants"",
        uniqueConstraints = {
                @UniqueConstraint(columnNames = ""user_id"", name = ""un_user_id""),
                @UniqueConstraint(columnNames = ""tenant_id"", name = ""un_tenant_id"")
        })
public class UserTenant {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @ManyToOne(fetch = FetchType.LAZY)
    @JoinColumn(name = ""user_id"",
            nullable = false,
            foreignKey = @ForeignKey(
                    name = ""fk_users_tenants_user1""))
    @JsonBackReference
    private User user;

    @ManyToOne(fetch = FetchType.LAZY)
    @JoinColumn(name = ""tenant_id"",
            nullable = false,
            foreignKey = @ForeignKey(
                    name = ""fk_users_tenants_tenant1""))
    @JsonBackReference
    private Tenant tenant;

    @ManyToMany(fetch = FetchType.LAZY)
    @JoinTable(name = ""users_tenants_roles"",
            joinColumns = {@JoinColumn(name = ""user_tenant_id"",
                    foreignKey = @ForeignKey(name = ""fk_users_tenants_user_tenant1""))},
            inverseJoinColumns = {@JoinColumn(name = ""role_id"",
                    foreignKey = @ForeignKey(name = ""fk_users_tenants_roles1""))})
    private Set&lt;Role&gt; roles = new HashSet&lt;&gt;();

}



Payloads:


/** LoginRequest **/
@Data
@Builder
@AllArgsConstructor
@NoArgsConstructor
public class LoginRequest {
    @NotBlank
    private String username;

    @NotBlank
    private String password;

}

/** SignupRequest **/
@Data
@Builder
@AllArgsConstructor
@NoArgsConstructor
public class SignupRequest {
    @NotBlank
    @Size(max = 20)
    private String username;

    @NotBlank
    @Size(max = 40)
    private String password;
    private Set&lt;String&gt; role;

}

/** JwtResponse **/
@Data
@Builder
@AllArgsConstructor
@NoArgsConstructor
public class JwtResponse {
    private Long id;
    private String username;
    private List&lt;String&gt; roles;
    private String tokenType = ""Bearer"";
    private String accessToken;

    public JwtResponse(String accessToken, Long id, String username,
                       List&lt;String&gt; roles) {
        this.id = id;
        this.username = username;
        this.roles = roles;
        this.accessToken = accessToken;
    }

}

/** MessageResponse **/
@Data
@Builder
@NoArgsConstructor
public class MessageResponse {
    private String message;

    public MessageResponse(String message) {
        this.message = message;
    }
}



Repositories:


/** RoleRepository **/
@Repository
public interface RoleRepository extends JpaRepository&lt;Role, Long&gt; {
    Optional&lt;Role&gt; findByName(ERole name);
}

/** UserRepository **/
@Repository
public interface UserRepository extends JpaRepository&lt;User, Long&gt; {
    Optional&lt;User&gt; findByUsername(String username);

    Boolean existsByUsername(String username);

}

/** UserTenantRepository **/
@Repository
public interface UserTenantRepository extends JpaRepository&lt;UserTenant, Long&gt; {

    @Query(""SELECT ut FROM UserTenant ut WHERE ut.user.username = :username AND ut.tenant.subdomain = :subdomain "")
    Optional&lt;UserTenant&gt; findByUserAndSubdomain(String username, String subdomain);

    @Query(""SELECT "" +
            ""CASE WHEN COUNT(ut) &gt; 0 THEN true ELSE false END "" +
            ""FROM UserTenant ut "" +
            ""WHERE ut.user.username = :username "" +
            ""AND ut.tenant.subdomain = :subdomain "")
    Boolean existsByUsernameAndSubdomain(String subdomain, String username);

}



Services:


/** AuthService **/
@Service
@RequiredArgsConstructor
public class AuthService {

    private final UserRepository userRepository;
    private final AuthenticationManager authenticationManager;
    private final JwtUtils jwtUtils;
    private final PasswordEncoder encoder;
    private final RoleRepository roleRepository;

    public JwtResponse authenticateUser(String subdomain, LoginRequest loginRequest) {

        System.out.println(subdomain);

        Authentication authentication = authenticationManager.authenticate(
                new UsernamePasswordAuthenticationToken(loginRequest.getUsername(), loginRequest.getPassword()));
        System.out.println(authentication);

        SecurityContextHolder.getContext().setAuthentication(authentication);
        String jwt = jwtUtils.generateJwtToken(authentication);

        UserDetailsImpl userDetails = (UserDetailsImpl) authentication.getPrincipal();
        List&lt;String&gt; roles = userDetails.getAuthorities().stream()
                .map(GrantedAuthority::getAuthority)
                .collect(Collectors.toList());

        return new JwtResponse(jwt,
                userDetails.getId(),
                userDetails.getUsername(),
                roles);
    }

    @Transactional
    public MessageResponse registerUser(SignupRequest signUpRequest) {

        // Create new user's account
        User user = new User(
                signUpRequest.getUsername(),
                encoder.encode(signUpRequest.getPassword()));

        Set&lt;String&gt; strRoles = signUpRequest.getRole();
        Set&lt;Role&gt; roles = new HashSet&lt;&gt;();

        if (strRoles == null) {
            Role userRole = roleRepository.findByName(ERole.ROLE_USER)
                    .orElseThrow(() -&gt; new RuntimeException(""Error: Role is not found.""));
            roles.add(userRole);
        } else {
            strRoles.forEach(role -&gt; {
                switch (role) {
                    case ""admin"":
                        Role adminRole = roleRepository.findByName(ERole.ROLE_ADMIN)
                                .orElseThrow(() -&gt; new RuntimeException(""Error: Role is not found.""));
                        roles.add(adminRole);
                        break;
                    case ""mod"":
                        Role modRole = roleRepository.findByName(ERole.ROLE_MODERATOR)
                                .orElseThrow(() -&gt; new RuntimeException(""Error: Role is not found.""));
                        roles.add(modRole);
                        break;
                    default:
                        Role userRole = roleRepository.findByName(ERole.ROLE_USER)
                                .orElseThrow(() -&gt; new RuntimeException(""Error: Role is not found.""));
                        roles.add(userRole);
                }
            });
        }
        user.setRoles(roles);
        userRepository.save(user);
        return new MessageResponse(""User registered successfully!"");
    }

}

/** UserDetailsImpl **/
public class UserDetailsImpl implements UserDetails {
    private static final long serialVersionUID = 1L;

    private final Long id;

    private final String username;

    @JsonIgnore
    private final String password;

    private final Collection&lt;? extends GrantedAuthority&gt; authorities;

    public UserDetailsImpl(Long id, String username, String password,
                           Collection&lt;? extends GrantedAuthority&gt; authorities) {
        this.id = id;
        this.username = username;
        this.password = password;
        this.authorities = authorities;
    }

    public static UserDetailsImpl build(User user) {
        List&lt;GrantedAuthority&gt; authorities = user.getRoles().stream()
                .map(role -&gt; new SimpleGrantedAuthority(role.getName().name()))
                .collect(Collectors.toList());

        return new UserDetailsImpl(
                user.getId(),
                user.getUsername(),
                user.getPassword(),
                authorities);
    }

    @Override
    public Collection&lt;? extends GrantedAuthority&gt; getAuthorities() {
        return authorities;
    }

    public Long getId() {
        return id;
    }

    @Override
    public String getPassword() {
        return password;
    }

    @Override
    public String getUsername() {
        return username;
    }

    @Override
    public boolean isAccountNonExpired() {
        return true;
    }

    @Override
    public boolean isAccountNonLocked() {
        return true;
    }

    @Override
    public boolean isCredentialsNonExpired() {
        return true;
    }

    @Override
    public boolean isEnabled() {
        return true;
    }

    @Override
    public boolean equals(Object o) {
        if (this == o)
            return true;
        if (o == null || getClass() != o.getClass())
            return false;
        UserDetailsImpl user = (UserDetailsImpl) o;
        return Objects.equals(id, user.id);
    }
}

/** UserDetailsServiceImpl **/
@Service
@RequiredArgsConstructor
public class UserDetailsServiceImpl implements UserDetailsService {

    private final UserRepository userRepository;
    private final UserTenantRepository userTenantRepository;

    @Override
    @Transactional
    public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException {
        User user = userRepository.findByUsername(username)
                .orElseThrow(() -&gt; new UsernameNotFoundException(""User Not Found with username: "" + username));


        return UserDetailsImpl.build(user);
    }

}



Controller


/** AuthController **/
@RestController
@RequestMapping(""/auth"")
@RequiredArgsConstructor
public class AuthController {

    private final AuthService authService;
    private final UserRepository userRepository;
    private final UserTenantRepository userTenantRepository;

    @PostMapping(""/signin"")
    public ResponseEntity&lt;?&gt; authenticateUser(
            @RequestAttribute String subdomain,
            @Valid @RequestBody LoginRequest loginRequest
    ) {
        if (!userTenantRepository.existsByUsernameAndSubdomain(subdomain, loginRequest.getUsername())) {
            return ResponseEntity
                    .badRequest()
                    .body(new MessageResponse(""Unauthorized: This username and tenant is not authorized!""));
        }
        return ResponseEntity.ok(authService.authenticateUser(subdomain, loginRequest));
    }


    @PostMapping(""/signup"")
    public ResponseEntity&lt;?&gt; registerUser(@Valid @RequestBody SignupRequest signUpRequest) {
        if (userRepository.existsByUsername(signUpRequest.getUsername())) {
            return ResponseEntity
                    .badRequest()
                    .body(new MessageResponse(""Error: Username is already taken!""));
        }
        return ResponseEntity.ok(authService.registerUser(signUpRequest));
    }
}



JWT:


/** AuthEntryPointJwt **/
@Component
public class AuthEntryPointJwt implements AuthenticationEntryPoint {

    private static final Logger logger = LoggerFactory.getLogger(AuthEntryPointJwt.class);

    @Override
    public void commence(HttpServletRequest request, HttpServletResponse response,
                         AuthenticationException authException) throws IOException {
        logger.error(""Unauthorized error: {}"", authException.getMessage());
        response.sendError(HttpServletResponse.SC_UNAUTHORIZED, ""Unauthorized: incorrect username or password"");
    }

}

/** AuthTokenFilter **/
public class AuthTokenFilter extends OncePerRequestFilter {
    @Autowired
    private JwtUtils jwtUtils;

    @Autowired
    private UserDetailsServiceImpl userDetailsService;


    @Override
    protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response
            , FilterChain filterChain)
            throws ServletException, IOException {
        try {
            String jwt = parseJwt(request);
            if (jwt != null &amp;&amp; jwtUtils.validateJwtToken(jwt)) {
                String username = jwtUtils.getUserNameFromJwtToken(jwt);

                String serverName = request.getServerName();
                String subdomain = serverName.substring(0, serverName.indexOf("".""));

                UserDetails userDetails = userDetailsService.loadUserByUsername(username);
                System.out.println(userDetails);
                UsernamePasswordAuthenticationToken authentication = new UsernamePasswordAuthenticationToken(
                        userDetails, null, userDetails.getAuthorities());
                authentication.setDetails(new WebAuthenticationDetailsSource().buildDetails(request));

                SecurityContextHolder.getContext().setAuthentication(authentication);
            }
        } catch (Exception e) {
            e.printStackTrace();
        }

        filterChain.doFilter(request, response);
    }

    private String parseJwt(HttpServletRequest request) {
        String headerAuth = request.getHeader(""Authorization"");

        if (StringUtils.hasText(headerAuth) &amp;&amp; headerAuth.startsWith(""Bearer "")) {
            return headerAuth.substring(7);
        }
        return null;
    }
}

/** JwtUtils **/
@Component
public class JwtUtils {
    private static final Logger logger = LoggerFactory.getLogger(JwtUtils.class);

    @Value(""${example.app.jwtSecret}"")
    private String jwtSecret;

    @Value(""${example.app.jwtExpirationMs}"")
    private int jwtExpirationMs;

    public String generateJwtToken(Authentication authentication) {

        UserDetailsImpl userPrincipal = (UserDetailsImpl) authentication.getPrincipal();

        return Jwts.builder()
                .setSubject((userPrincipal.getUsername()))
                .setIssuedAt(new Date())
                .setExpiration(new Date((new Date()).getTime() + jwtExpirationMs))
                .signWith(SignatureAlgorithm.HS512, jwtSecret)
                .compact();
    }

    public String getUserNameFromJwtToken(String token) {
        return Jwts.parser().setSigningKey(jwtSecret).parseClaimsJws(token).getBody().getSubject();
    }

    public boolean validateJwtToken(String authToken) {
        try {
            Jwts.parser().setSigningKey(jwtSecret).parseClaimsJws(authToken);
            return true;
        } catch (SignatureException e) {
            logger.error(""Invalid JWT signature: {}"", e.getMessage());
        } catch (MalformedJwtException e) {
            logger.error(""Invalid JWT token: {}"", e.getMessage());
        } catch (ExpiredJwtException e) {
            logger.error(""JWT token is expired: {}"", e.getMessage());
        } catch (UnsupportedJwtException e) {
            logger.error(""JWT token is unsupported: {}"", e.getMessage());
        } catch (IllegalArgumentException e) {
            logger.error(""JWT claims string is empty: {}"", e.getMessage());
        }

        return false;
    }
}



Utils:


/** TenantInterceptor **/
public class TenantInterceptor implements HandlerInterceptor {

    @Override
    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) {
        String serverName = request.getServerName();
        String tenantId = serverName.substring(0, serverName.indexOf("".""));

        request.setAttribute(""subdomain"", tenantId);

        return true;
    }
}

/** WebSecurityConfig **/
@Configuration
@EnableWebSecurity
@EnableGlobalMethodSecurity(prePostEnabled = true)
public class WebSecurityConfig implements WebMvcConfigurer {

    final
    UserDetailsServiceImpl userDetailsService;

    private final AuthEntryPointJwt unauthorizedHandler;

    public WebSecurityConfig(UserDetailsServiceImpl userDetailsService, AuthEntryPointJwt unauthorizedHandler) {
        this.userDetailsService = userDetailsService;
        this.unauthorizedHandler = unauthorizedHandler;
    }

    @Bean
    public AuthTokenFilter authenticationJwtTokenFilter() {
        return new AuthTokenFilter();
    }

    @Bean
    public DaoAuthenticationProvider authenticationProvider() {
        DaoAuthenticationProvider authProvider = new DaoAuthenticationProvider();

        authProvider.setUserDetailsService(userDetailsService);
        authProvider.setPasswordEncoder(passwordEncoder());

        return authProvider;
    }

    @Bean
    public AuthenticationManager authenticationManager(AuthenticationConfiguration authConfig) throws Exception {
        return authConfig.getAuthenticationManager();
    }

    @Bean
    public PasswordEncoder passwordEncoder() {
        return new BCryptPasswordEncoder();
    }

    @Bean
    public SecurityFilterChain filterChain(HttpSecurity http) throws Exception {
        http.cors().and().csrf().disable()
                .exceptionHandling().authenticationEntryPoint(unauthorizedHandler).and()
                .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.STATELESS).and()
                .authorizeRequests()
                .antMatchers(
                        ""/auth/**"",
                        ""/v3/api-docs/**"",
                        ""/swagger-ui/**"",
                        ""/swagger-ui.html"",
                        ""/configuration/**"",
                        ""/swagger-resources/**"",
                        ""/webjars/**"",
                        ""/api-docs/**"").permitAll()
                .antMatchers(""/api/**"").authenticated()
                .anyRequest().authenticated();

        http.authenticationProvider(authenticationProvider());

        http.addFilterBefore(authenticationJwtTokenFilter(), UsernamePasswordAuthenticationFilter.class);

        return http.build();
    }

    @Override
    public void addInterceptors(InterceptorRegistry registry) {
        registry.addInterceptor(new TenantInterceptor());
    }

}



","I was able to solve the problem by modifying the loadUserByUsername method in the UserDetailsServiceImpl.
See the implementation details on GitHub!
@Service
@RequiredArgsConstructor
public class UserDetailsServiceImpl implements UserDetailsService {

    private final UserTenantRepository userTenantRepository;

    @Override
    @Transactional
    public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException {

        // Getting subdomain from request attributes
        HttpServletRequest request =
                ((ServletRequestAttributes) Objects.requireNonNull(RequestContextHolder.getRequestAttributes()))
                        .getRequest();

        String serverName = request.getServerName();
        String subdomain = serverName.substring(0, serverName.indexOf(&quot;.&quot;));
        UserTenant userTenant = userTenantRepository.findByUserAndSubdomain(username, subdomain)
                .orElseThrow(() -&gt; new UsernameNotFoundException(
                        &quot;UserTenant Not Found with username: &quot; + username + &quot; and &quot; + subdomain));

        // Getting Rules from the UserTenant
        List&lt;GrantedAuthority&gt; authorities = userTenant.getRoles().stream()
                .map(role -&gt; new SimpleGrantedAuthority(role.getName().name()))
                .collect(Collectors.toList());

        return new UserDetailsImpl(
                userTenant.getUser().getId(),
                userTenant.getUser().getUsername(),
                userTenant.getUser().getPassword(),
                authorities
        );
    }
}

Inserts in Database
INSERT INTO roles(id, name)
VALUES (1 ,'ROLE_USER'),
       (2, 'ROLE_MODERATOR'),
       (3, 'ROLE_ADMIN');

INSERT INTO tenants (id, name, subdomain)
VALUES (1, 'Tenant 1', 'tenant1'),
       (2, 'Tenant 2', 'tenant2');

# user, password
# user1, user1
# user2, user2
INSERT INTO users (id, username, password)
VALUES (1, 'user1', '$2a$10$wFMJLxdXKGRa8lJO6k2DAOnW9HstAPoHecXUNkDyYNeaNnZJAz.hy'),
       (2, 'user2', '$2a$10$Z9/wLkmf5IwfjJqIQU6X.OBFg3TCBUyk3bdfgkGjU0.HI5kVibZxG');

INSERT INTO users_tenants (id,  tenant_id, user_id)
VALUES (1, 1, 1),
       (2, 2, 2);

INSERT INTO users_tenants_roles (user_tenant_id,  role_id)
VALUES (1, 2),
       (1, 3),
       (2, 1);


INSERT INTO items (id,  tenant_id, name)
VALUES (1, 1, 'Product 1 in Tenant 1'),
       (2, 1, 'Product 2 in Tenant 1'),
       (3, 2, 'Product 1 in Tenant 2'),
       (4, 2, 'Product 2 in Tenant 2');

Validations in Postman
Created token variable in Postman:

Set token value in Postman variable:

Added Authorization variable in requests headers:

Validating if the domain exists in sign in:

Validating user access permission on tenant:

Authorized login:

Getting items of tenant1:

Trying to get tenant2 list of items without being logged in tenant2 and having access permissions:

Logging in with user2 in tenant2:

Getting items list tenant2

Trying to get tenant1 list of items without being logged in tenant1 and having access permissions:

"
"I have two lists of custom objects. And I want to merge both lists by id, using Java 8.
I have a class Employee with the fields (All String): id, name, city.
And I have another class Person with the fields (All String): id, city.
Here is an example :
   List&lt;Employee&gt; employeeList = Stream.of(
                        new Employee(&quot;100&quot;,&quot;Alex&quot;,&quot;&quot;),
                        new Employee(&quot;200&quot;,&quot;Rida&quot;,&quot;&quot;),
                        new Employee(&quot;300&quot;,&quot;Ganga&quot;,&quot;&quot;))
                .collect(Collectors.toList());

        List&lt;Person&gt; personList = Stream.of(
                        new Person(&quot;100&quot;,&quot;Atlanta&quot;),
                        new Person(&quot;300&quot;,&quot;Boston&quot;),
                        new Person(&quot;400&quot;,&quot;Pleasanton&quot;))
                .collect(Collectors.toList());


After merging the two lists I want to get the result shown below.
How can I do it?
List&lt;Employee&gt; 
[
Employee(id=100, name=Alex, city=Atlanta), 
Employee(id=200, name=Rida, city=null), 
Employee(id=300, name=Ganga, city=Boston),
Employee(id=400, name=null, city=Pleasanton)
]

","In order to achieve that, firstly, you can create two maps based on the two lists using id as a key.
Then create a stream over the key sets of these maps. Then inside the map() operation you need to create a new Employee object for every key by using passing a name extracted from the employeeList city taken from the personById .
When id is not present in either of the maps the object returned by get() will be null and attempt to invoke method on it will triger the NullPointerException. In order to handle this situation, we can make use of Null-object pattern, by defining two variables that could be safely accessed and will be provided as an argument to getOfDefault().
Then collect the stream element into a list with Collectors.toList().
public static void main(String[] args) {
    List&lt;Employee&gt; employeeList = Stream.of(
                    new Employee(&quot;100&quot;,&quot;Alex&quot;,&quot;&quot;),
                    new Employee(&quot;200&quot;,&quot;Rida&quot;,&quot;&quot;),
                    new Employee(&quot;300&quot;,&quot;Ganga&quot;,&quot;&quot;))
            .collect(Collectors.toList());

    List&lt;Person&gt; personList = Stream.of(
                    new Person(&quot;100&quot;,&quot;Atlanta&quot;),
                    new Person(&quot;300&quot;,&quot;Boston&quot;),
                    new Person(&quot;400&quot;,&quot;Pleasanton&quot;))
            .collect(Collectors.toList());

    Map&lt;String, Employee&gt; employeeById = employeeList.stream()
            .collect(Collectors.toMap(Employee::getId, Function.identity()));

    Map&lt;String, Person&gt; personById = personList.stream()
            .collect(Collectors.toMap(Person::getId, Function.identity()));

    Person nullPerson = new Person(&quot;&quot;, null);             // null-object
    Employee nullEmployee = new Employee(&quot;&quot;, null, null); // null-object

    List&lt;Employee&gt; result = Stream.concat(employeeById.keySet().stream(),
                                          personById.keySet().stream())
            .distinct() // eliminating duplicated keys
            .map(key -&gt; new Employee(key,
                    employeeById.getOrDefault(key, nullEmployee).getName(),
                    personById.getOrDefault(key, nullPerson).getCity()))
            .collect(Collectors.toList());
    
    result.forEach(System.out::println);
}

Output
Employee{id='100', name='Alex', city='Atlanta'}
Employee{id='200', name='Rida', city='null'}
Employee{id='300', name='Ganga', city='Boston'}
Employee{id='400', name='null', city='Pleasanton'}

"
"I'm writing a word-guessing game code. The main calls the inputTake method, which asks for input of a word consisting 5 English letters only, and returns is. Before returning the word, it calls another method, checkInput, to make sure the input is valid. If the input isn't valid, the checkInput method prints an error message and calls inputTake to let the user try again.
But when the first input is invalid, checkInput calls inputTake and then the second input is valid everything seems to work alright. The problem is that the method returns the first, invalid input, and not the valid input.
I tried initializing Scanner in the main and giving it to the method as parameter, but that doesn't help.
Below is the code I wrote, any thoughts? Any help is welcome

Main:
Board board1 = new Board();
        
String guess = board1.inputTake();

Board:
// take input - print a message and calls the checkInput method with the String inputed.
public String inputTake(){
    Scanner scan = new Scanner(System.in);
    String guess;

    System.out.println(&quot;choose a word, pick carefully: &quot;);
    guess = scan.next();
    
    // we gotta check whether the input's valid before we return it!
    checkInput(guess);
        
    return guess;
    }
    
    /* checks whether a given String is made out of 5 english language letters. 
     * if it is, program continues normally.
     * if not, it prints error message and calls the InputTake method again.
     */
public void checkInput(String input) {
    boolean isGood = true;
        
    // check if 5 letters
    if(input.length() != 5)
        isGood = false;
        
    // check if all are english
    if(!input.matches(&quot;[a-zA-Z]+&quot;)) 
          isGood = false;
        
    if(isGood == false) {
        System.out.println(&quot;make sure your guess consists of 5 english letters, try again.&quot;);
        inputTake();
    }
}

","As mentioned in the comments, the problem is that your inputTake() call inside checkInput() doesn't do what you want. You can try this:
// take input - print a message and calls the checkInput method with the String inputed.
public String inputTake(){
    Scanner scan = new Scanner(System.in);
    String guess;

    System.out.println(&quot;choose a word, pick carefully: &quot;);
    guess = scan.next();
        
    // we gotta check whether the input's valid before we return it!
    if(!isGoodInput(guess)) {
        System.out.println(&quot;make sure your guess consists of 5 english letters, try again.&quot;);
        guess = inputTake();
    }
    return guess;
}
        
/* checks whether a given String is made out of 5 english language letters. 
* if it is, program continues normally.
*/
public boolean isGoodInput(String input) {
    return input.length() == 5 &amp;&amp; input.matches(&quot;[a-zA-Z]+&quot;);
}

"
"Small question for SpringBoot, and how to configure the bean using @Qualifier please.
I have a very straightforward piece of code:
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;3.0.1&lt;/version&gt;
        &lt;relativePath/&gt;
    &lt;/parent&gt;

    &lt;groupId&gt;com.question&lt;/groupId&gt;
    &lt;artifactId&gt;language&lt;/artifactId&gt;
    &lt;version&gt;1.1&lt;/version&gt;

    &lt;name&gt;language&lt;/name&gt;
    &lt;description&gt;Spring Boot&lt;/description&gt;

    &lt;properties&gt;
        &lt;java.version&gt;17&lt;/java.version&gt;
        &lt;maven.compiler.source&gt;17&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;17&lt;/maven.compiler.target&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;


package com.question;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class LanguageApplication {

    public static void main(String[] args) {
        SpringApplication.run(LanguageApplication.class, args);
    }

}

package com.question.service;

public interface LanguageService {

    String process(String name);

}


package com.question.service;

import org.springframework.stereotype.Service;

@Service(&quot;french&quot;)
public class FrenchLanguageServiceImpl implements LanguageService {

    @Override
    public String process(String name) {
        return &quot;Bonjour &quot; + name;
    }

}


package com.question.service;

import org.springframework.stereotype.Service;

@Service(&quot;english&quot;)
public class EnglishLanguageServiceImpl implements LanguageService {

    @Override
    public String process(String name) {
        return &quot;Welcome &quot; + name;
    }

}


package com.question.controller;

import com.question.service.LanguageService;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.web.bind.annotation.*;

@RestController
public class LanguageController {

    private final LanguageService languageService;

    @Value(&quot;${configuration}&quot;)
    public String configuration;

    public LanguageController(@Qualifier(configuration) LanguageService languageService) {
        this.languageService = languageService;
    }

    @GetMapping(&quot;/test&quot;)
    public String test(@RequestParam String name) {
        return languageService.process(name);
    }

}


Expected:
What I hope to achieve is equally straightforward. I would like to pass some sort of configuration to application.properties, something like configuration=french or configuration=english.
At the controller layer, to use (@Qualifier(configuration) LanguageService languageService) And the correct concrete service will be used.
Actual:
Unfortunately,
@Qualifier(configuration) + @Value(&quot;${configuration}&quot;) public String configuration;
will yield Attribute Value must be constant.
Is there a way we can configure the concrete Bean via a configurable @Qualifier please?
I understand there is a way to workaround this by using ApplicationContext getBean.
But having this construct: @Qualifier(configuration) makes the code clean and easily understandable. How to achieve this please?
Thank you
","If you only need 1 of the LanguageService beans active at a time, then you can use @ConditionalOnProperty on each of them, each using a unique havingValue. Like this (warning, untested code written from memory):
interface ConfigKeys {
    public static final String LANGUAGE = &quot;my.app.prefix.language&quot;;
}


@Service
@ConditionalOnProperty(name = ConfigKeys.LANGUAGE, havingValue = &quot;english&quot;)
public class EnglishLanguageServiceImpl implements LanguageService {

    @Override
    public String process(String name) {
        return &quot;Welcome &quot; + name;
    }

}

@Service
@ConditionalOnProperty(name = ConfigKeys.LANGUAGE, havingValue = &quot;french&quot;)
public class FrenchLanguageServiceImpl implements LanguageService {

    @Override
    public String process(String name) {
        return &quot;Bonjour &quot; + name;
    }

}

With that, you don't need any qualifiers, just set the my.app.prefix.language property in your config (application.properties or application.yaml) to the value you want, and there will just one LanguageService bean in the context. You can inject that bean wherever you need it without needing a qualifier.
"
"env:

jdk: 17.0.1
mapstruct: 1.5.1.Final

Using the default configuration I generated the following code
        protected AgentInfo wealthProdAccountInfoDTOToAgentInfo(WealthProdAccountInfoDTO wealthProdAccountInfoDTO) {
        if ( wealthProdAccountInfoDTO == null ) {
            return null;
        }

        String agentName = null;
        String agentIdentityType = null;
        String agentIdentityNo = null;
        String agentIdentityExpireAt = null;

        agentName = wealthProdAccountInfoDTO.getAgentName();
        agentIdentityType = wealthProdAccountInfoDTO.getAgentIdentityType();
        agentIdentityNo = wealthProdAccountInfoDTO.getAgentIdentityNo();
        agentIdentityExpireAt = wealthProdAccountInfoDTO.getAgentIdentityExpireAt();

        AgentInfo agentInfo = new AgentInfo( agentName, agentIdentityType, agentIdentityNo, agentIdentityExpireAt );

        return agentInfo;
    }

But I want to return null when all field of source are null, like this
    protected AgentInfo wealthProdAccountInfoDTOToAgentInfo(WealthProdAccountInfoDTO wealthProdAccountInfoDTO) {
        if ( wealthProdAccountInfoDTO == null ) {
            return null;
        }
        // add check logic
        if (agentName == null &amp;&amp; agentIdentityType == null &amp;&amp; agentIdentityNo == null &amp;&amp; agentIdentityExpireAt == null) {
            return null;
        }

        String agentName = null;
        String agentIdentityType = null;
        String agentIdentityNo = null;
        String agentIdentityExpireAt = null;

        agentName = wealthProdAccountInfoDTO.getAgentName();
        agentIdentityType = wealthProdAccountInfoDTO.getAgentIdentityType();
        agentIdentityNo = wealthProdAccountInfoDTO.getAgentIdentityNo();
        agentIdentityExpireAt = wealthProdAccountInfoDTO.getAgentIdentityExpireAt();

        AgentInfo agentInfo = new AgentInfo( agentName, agentIdentityType, agentIdentityNo, agentIdentityExpireAt );

        return agentInfo;
    }

how should I configure it?
","Thanks to ArtemAgaev's idea, I ended up considering using @AfterMapping and java reflection for this type of scenario
    @AfterMapping
    default void cleanData(@MappingTarget AccountInfoDomain domain) {
        Optional.ofNullable(domain).ifPresent(c -&gt; {
            if (isAllFieldNull(domain.getAgentInfo())) {
                domain.setAgentInfo(null);
            }
        });
    }

    public static boolean isAllFieldNull(Object o) {
        Object[] fieldsValue = getFieldsValue(o);
        return Optional.ofNullable(fieldsValue).map(f -&gt; Arrays.stream(f).allMatch(Objects::isNull)).orElse(true);
    }

    public static Object[] getFieldsValue(Object obj) {
        if (null != obj) {
            final Field[] fields = getFields(obj instanceof Class ? (Class&lt;?&gt;) obj : obj.getClass());
            if (null != fields) {
                final Object[] values = new Object[fields.length];
                for (int i = 0; i &lt; fields.length; i++) {
                    values[i] = getFieldValue(obj, fields[i]);
                }
                return values;
            }
        }
        return null;
    }


"
"I am trying to read a table from a csv text file and generate a table (List of Hashmaps) in java.
For which I'm reading each line of text file, constructing a Hashmap&lt;String, String&gt; record out of the line and appending it to a ArrayList at the end of each iteration.
I am expecting a single instance of each line from text file to appear only once in the List, but all getting is the last row from text file appearing n+1 times, n being the last row number.
Here's the code:
public static void main(String[] args) throws IOException {
    
    FileReader filObj = null;
    try {
        filObj = new FileReader(new File(System.getProperty(&quot;user.home&quot;) + &quot;\\Desktop\\testData.txt&quot;));
    } catch (FileNotFoundException e) {
        e.printStackTrace();
    }
    
    BufferedReader br = new BufferedReader(filObj);
    
    List&lt;String&gt; headers = new ArrayList&lt;String&gt;();
    List&lt;HashMap&lt;String, String&gt;&gt; myTable = new ArrayList&lt;HashMap&lt;String, String&gt;&gt;();
    HashMap&lt;String, String&gt; myRecord = new HashMap&lt;String, String&gt;();
    
    String line = null;
    int ext = 0;
    while ((line = br.readLine()) != null) {
        //System.out.println(line);
        if (ext == 0) {
            headers = Arrays.asList(line.split(&quot;,&quot;));
        } else {
            int index = 0;
            for (String each : line.split(&quot;,&quot;)) {
                myRecord.put(headers.get(index), each);
                index++;
            }
            System.out.println(&quot;myrecord:&quot; + myRecord);
        }
        myTable.add(myRecord);
        ext++;
        System.out.println(&quot;My Table:&quot; + myTable);
    }
}

the testData.txt file contents are as below
TransactionNumber,TransactionType,Amount,TransactionDate,TransactionRemarks
123456,Credit,4000,07/10/2021,Salary Credited
123333,Debit,7000,05/10/2021,Fuel
123446,Credit,3000,01/10/2021,Refund

and the console output is as below:
My Table:[{}]
myrecord:{TransactionType=Credit, TransactionNumber=123456, Amount=4000, TransactionRemarks=Salary Credited, TransactionDate=07/10/2021}
My Table:[{TransactionType=Credit, TransactionNumber=123456, Amount=4000, TransactionRemarks=Salary Credited, TransactionDate=07/10/2021}, {TransactionType=Credit, TransactionNumber=123456, Amount=4000, TransactionRemarks=Salary Credited, TransactionDate=07/10/2021}]
myrecord:{TransactionType=Debit, TransactionNumber=123333, Amount=7000, TransactionRemarks=Fuel, TransactionDate=05/10/2021}
My Table:[{TransactionType=Debit, TransactionNumber=123333, Amount=7000, TransactionRemarks=Fuel, TransactionDate=05/10/2021}, {TransactionType=Debit, TransactionNumber=123333, Amount=7000, TransactionRemarks=Fuel, TransactionDate=05/10/2021}, {TransactionType=Debit, TransactionNumber=123333, Amount=7000, TransactionRemarks=Fuel, TransactionDate=05/10/2021}]
myrecord:{TransactionType=Credit, TransactionNumber=123446, Amount=3000, TransactionRemarks=Refund, TransactionDate=01/10/2021}
My Table:[{TransactionType=Credit, TransactionNumber=123446, Amount=3000, TransactionRemarks=Refund, TransactionDate=01/10/2021}, {TransactionType=Credit, TransactionNumber=123446, Amount=3000, TransactionRemarks=Refund, TransactionDate=01/10/2021}, {TransactionType=Credit, TransactionNumber=123446, Amount=3000, TransactionRemarks=Refund, TransactionDate=01/10/2021}, {TransactionType=Credit, TransactionNumber=123446, Amount=3000, TransactionRemarks=Refund, TransactionDate=01/10/2021}]


","You're adding the same instance of the map to the list at each iteration step. And this map gets added to the list n + 1 times because the line myTable.add(myRecord); gets executed after reading the headers (I guess that wasn't your intention).
Instead, you need to move the creation of the map into the while loop, to instantiate a new map for every line of text.
There are few more issues in your code:

You're not closing the stream, always use try with resources to make sure that resources would be properly closed.

Don't throw from the main(), it's not a good practice;

Write your code against interfaces What does it mean to &quot;program to an interface&quot;?

Since Java 7 we can use diamond and since Java 10 var to reduce verbosity of code.


Your code might be rewritten like that:
public static void main(String[] args) {
    
    List&lt;Map&lt;String, String&gt;&gt; myTable = new ArrayList&lt;&gt;();
    
    try (var reader = new BufferedReader(new FileReader(System.getProperty(&quot;user.home&quot;) + &quot;\\Desktop\\testData.txt&quot;))) {
        List&lt;String&gt; headers = new ArrayList&lt;&gt;();
        
        String line = null;
        int ext = 0;

        while ((line = reader.readLine()) != null) {
            //System.out.println(line);
            if (ext == 0) {
                headers = Arrays.asList(line.split(&quot;,&quot;));
                ext++;
                continue;
            }
            
            Map&lt;String, String&gt; myRecord = new HashMap&lt;&gt;();

            int index = 0;
            for (String each : line.split(&quot;,&quot;)) {
                myRecord.put(headers.get(index), each);
                index++;
            }
            
            myTable.add(myRecord);
            ext++;

            System.out.println(&quot;myrecord:&quot; + myRecord);
            System.out.println(&quot;My Table:&quot; + myTable);
        }
    } catch (IOException e) {
        e.printStackTrace();
    }
}

"
"Hi I have the following webfilter
@Component
public class TMPFilter implements WebFilter {
    private long requestTime = System.nanoTime();
    
    @Override
    public Mono&lt;Void&gt; filter(ServerWebExchange exchange, WebFilterChain chain) {
        long deltaTime = System.nanoTime() - requestTime;
        exchange.getResponse().getHeaders().add(&quot;server-timing-test&quot;, &quot;test&quot;);
        System.out.println(deltaTime);
        return chain.filter(exchange)
                .doOnRequest(request -&gt; {
                    requestTime = System.nanoTime();
                })
                .doOnSuccess( arg -&gt; {
                    long delta = System.nanoTime() - requestTime;
                    exchange.getResponse().getHeaders().add(&quot;server-timing-success&quot;, Long.toString(delta));
                })
                .doOnError((arg) -&gt; {
                    long delta = System.nanoTime() - requestTime;
                    exchange.getResponse().getHeaders().add(&quot;server-timing-error&quot;, Long.toString(delta));
                })
                .doFinally((arg) -&gt; {
                    long delta = System.nanoTime() - requestTime;
                    exchange.getResponse().getHeaders().add(&quot;server-timing-finally&quot;, Long.toString(delta));
                });
    }
}


I'd like to send back a response with a header with the time it took resolve that response. Am not sure how to go about this, am getting  the following error.
java.lang.UnsupportedOperationException: null
    at org.springframework.http.ReadOnlyHttpHeaders.add(ReadOnlyHttpHeaders.java:91)
    Suppressed: java.lang.UnsupportedOperationException: null
        at org.springframework.http.ReadOnlyHttpHeaders.add(ReadOnlyHttpHeaders.java:91)
        at com...tmp.api.filters.TMPFilter.lambda$filter$1(TMPFilter.java:28)

","This seems to do the trick:
import org.apache.commons.lang3.time.StopWatch;
import org.springframework.core.annotation.Order;
import org.springframework.http.server.reactive.ServerHttpResponse;
import org.springframework.stereotype.Component;
import org.springframework.web.server.ServerWebExchange;
import org.springframework.web.server.WebFilter;
import org.springframework.web.server.WebFilterChain;
import reactor.core.publisher.Mono;

import java.util.concurrent.TimeUnit;

@Component
public class TMPFilter implements WebFilter {
    private StopWatch stopWatch = new StopWatch();

    @Override
    public Mono&lt;Void&gt; filter(ServerWebExchange exchange, WebFilterChain chain) {
        ServerHttpResponse response =  exchange.getResponse();

        startWatch();
        response.beforeCommit(() -&gt; {
            stopWatch();
            String duration = Long.toString(stopWatch.getTime(TimeUnit.MILLISECONDS));
            response.getHeaders().add(&quot;server-timing-total&quot;, duration);
            return Mono.empty();
        });

        return chain.filter(exchange)
                .doOnRequest(request -&gt; {
                    startWatch();
                });
    }

    private void startWatch() {
        if(this.stopWatch.isStopped()) {
            this.stopWatch.start();
        }
    }

    private void stopWatch() {
        if(this.stopWatch.isStarted()) {
            this.stopWatch.stop();
        }
    }
}


"
"Python ints are objects that encapsulate the actual number value. Can we mess with that value, for example setting the value of the object 1 to 2? So that 1 == 2 becomes True?
","Yes, we can. But don't do this at home. Seriously, the 1 object is used in many places and I have no clue what this might break and what that might do to your computer. I reject all responsibility. But I found it interesting to learn about these things.
The id function gives us the memory address and the ctypes module lets us mess with memory:
import ctypes

ctypes.memmove(id(1) + 24, id(2) + 24, 4)

print(1 == 2)

x = 40
print(x + 1)

Output:
True
42

Try it online!. I tried it there because such sites have got to be protected from our hacking anyway.

More explanation / analysis:
The memmove copied the value from the 2 object into the 1 object. Their size is 28 bytes each, but I skipped the first 24 bytes, because that's the object's reference count, type address, and value size, as we can view/verify as well:
import ctypes, struct, sys

x = 1
data = ctypes.string_at(id(x), 28)
ref_count, type_address, number_of_digits, lowest_digit = \
    struct.unpack('qqqi', data)

print('reference count: ', ref_count, sys.getrefcount(x))
print('type address:    ', type_address, id(type(x)))
print('number of digits:', number_of_digits, -(-x.bit_length() // 30))
print('lowest digit:    ', lowest_digit, x % 2**30)

Output (Try it online!):
reference count:  135 138
type address:     140259718753696 140259718753696
number of digits: 1 1
lowest digit:     1 1

The reference count gets increased by the getrefcount call, but I don't know why by 3. Anyway, ~134 things other than us reference the 1 object, and we're potentially messing all of them up, so... really don't try this at home.
The &quot;digits&quot; refer to how CPython stores ints as digits in base 230. For example, x = 2 ** 3000 has 101 such digits. Output for x = 123 ** 456 for a better test:
reference count:  1 2
type address:     140078560107936 140078560107936
number of digits: 106 106
lowest digit:     970169057 970169057

"
"Here are two measurements:
timeit.timeit('&quot;toto&quot;==&quot;1234&quot;', number=100000000)
1.8320042459999968
timeit.timeit('&quot;toto&quot;==&quot;toto&quot;', number=100000000)
1.4517491540000265

As you can see, comparing two strings that match is faster than comparing two strings with the same size that do not match.
This is quite disturbing: During a string comparison, I believed that Python was testing strings character by character, so &quot;toto&quot;==&quot;toto&quot; should be longer to test than &quot;toto&quot;==&quot;1234&quot; as it requires four tests against one for the non-matching comparison. Maybe the comparison is hash-based, but in this case, timings should be the same for both comparisons.
Why?
","Combining my comment and the comment by @khelwood:
TL;DR:
When analysing the bytecode for the two comparisons, it reveals the 'time' and 'time' strings are assigned to the same object. Therefore, an up-front identity check (at C-level) is the reason for the increased comparison speed.
The reason for the same object assignment is that, as an implementation detail, CPython interns strings which contain only 'name characters' (i.e. alpha and underscore characters). This enables the object's identity check.

Bytecode:
import dis

In [24]: dis.dis(&quot;'time'=='time'&quot;)
  1           0 LOAD_CONST               0 ('time')  # &lt;-- same object (0)
              2 LOAD_CONST               0 ('time')  # &lt;-- same object (0)
              4 COMPARE_OP               2 (==)
              6 RETURN_VALUE

In [25]: dis.dis(&quot;'time'=='1234'&quot;)
  1           0 LOAD_CONST               0 ('time')  # &lt;-- different object (0)
              2 LOAD_CONST               1 ('1234')  # &lt;-- different object (1)
              4 COMPARE_OP               2 (==)
              6 RETURN_VALUE


Assignment Timing:
The 'speed-up' can also be seen in using assignment for the time tests. The assignment (and compare) of two variables to the same string, is faster than the assignment (and compare) of two variables to different strings. Further supporting the hypothesis the underlying logic is performing an object comparison. This is confirmed in the next section.
In [26]: timeit.timeit(&quot;x='time'; y='time'; x==y&quot;, number=1000000)
Out[26]: 0.0745926329982467

In [27]: timeit.timeit(&quot;x='time'; y='1234'; x==y&quot;, number=1000000)
Out[27]: 0.10328884399496019


Python source code:
As helpfully provided by @mkrieger1 and @Masklinn in their comments, the source code for unicodeobject.c performs a pointer comparison first and if True, returns immediately.
int
_PyUnicode_Equal(PyObject *str1, PyObject *str2)
{
    assert(PyUnicode_CheckExact(str1));
    assert(PyUnicode_CheckExact(str2));
    if (str1 == str2) {                  // &lt;-- Here
        return 1;
    }
    if (PyUnicode_READY(str1) || PyUnicode_READY(str2)) {
        return -1;
    }
    return unicode_compare_eq(str1, str2);
}


Appendix:

Reference answer nicely illustrating how to read the disassembled bytecode output. Courtesy of @Delgan
Reference answer which nicely describes CPython's string interning. Coutresy of @ShadowRanger

"
"I need to calculate the square root of some numbers, for example âˆš9 = 3 and âˆš2 = 1.4142. How can I do it in Python?
The inputs will probably be all positive integers, and relatively small (say less than a billion), but just in case they're not, is there anything that might break?


Note: This is an attempt at a canonical question after a discussion on Meta about an existing question with the same title.
Related

Integer square root in python

How to find integer nth roots?


Is there a short-hand for nth root of x in Python?
Difference between **(1/2), math.sqrt and cmath.sqrt?
Why is math.sqrt() incorrect for large numbers?
Python sqrt limit for very large numbers?

square root of a number greater than 10^2000 in Python 3


Which is faster in Python: x**.5 or math.sqrt(x)?
Why does Python give the &quot;wrong&quot; answer for square root? (specific to Python 2)
calculating n-th roots using Python 3&#39;s decimal module
How can I take the square root of -1 using python? (focused on NumPy)
Arbitrary precision of square roots


","Option 1: math.sqrt()
The math module from the standard library has a sqrt function to calculate the square root of a number. It takes any type that can be converted to float (which includes int) and returns a float.
&gt;&gt;&gt; import math
&gt;&gt;&gt; math.sqrt(9)
3.0

Option 2: Fractional exponent
The power operator (**) or the built-in pow() function can also be used to calculate a square root. Mathematically speaking, the square root of a equals a to the power of 1/2.
The power operator requires numeric types and matches the conversion rules for binary arithmetic operators, so in this case it will return either a float or a complex number.
&gt;&gt;&gt; 9 ** (1/2)
3.0
&gt;&gt;&gt; 9 ** .5  # Same thing
3.0
&gt;&gt;&gt; 2 ** .5
1.4142135623730951

(Note: in Python 2, 1/2 is truncated to 0, so you have to force floating point arithmetic with 1.0/2 or similar. See Why does Python give the &quot;wrong&quot; answer for square root?)
This method can be generalized to nth root, though fractions that can't be exactly represented as a float (like 1/3 or any denominator that's not a power of 2) may cause some inaccuracy:
&gt;&gt;&gt; 8 ** (1/3)
2.0
&gt;&gt;&gt; 125 ** (1/3)
4.999999999999999

Edge cases
Negative and complex
Exponentiation works with negative numbers and complex numbers, though the results have some slight inaccuracy:
&gt;&gt;&gt; (-25) ** .5  # Should be 5j
(3.061616997868383e-16+5j)
&gt;&gt;&gt; 8j ** .5  # Should be 2+2j
(2.0000000000000004+2j)

(Note: the parentheses are required on -25, otherwise it's parsed as -(25**.5) because exponentiation is more tightly binding than negation.)
Meanwhile, math is only built for floats, so for x&lt;0, math.sqrt(x) will raise ValueError: math domain error and for complex x, it'll raise TypeError: can't convert complex to float. Instead, you can use cmath.sqrt(x), which is more more accurate than exponentiation (and will likely be faster too):
&gt;&gt;&gt; import cmath
&gt;&gt;&gt; cmath.sqrt(-25)
5j
&gt;&gt;&gt; cmath.sqrt(8j)
(2+2j)

Precision
Both options involve an implicit conversion to float, so floating point precision is a factor. For example let's try a big number:
&gt;&gt;&gt; n = 10**30
&gt;&gt;&gt; x = n**2
&gt;&gt;&gt; root = x**.5
&gt;&gt;&gt; root == n
False
&gt;&gt;&gt; root - n  # how far off are they?
0.0
&gt;&gt;&gt; int(root) - n  # how far off is the float from the int?
19884624838656

Very large numbers might not even fit in a float and you'll get OverflowError: int too large to convert to float. See Python sqrt limit for very large numbers?
Other types
Let's look at Decimal for example:
Exponentiation fails unless the exponent is also Decimal:
&gt;&gt;&gt; decimal.Decimal('9') ** .5
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
TypeError: unsupported operand type(s) for ** or pow(): 'decimal.Decimal' and 'float'
&gt;&gt;&gt; decimal.Decimal('9') ** decimal.Decimal('.5')
Decimal('3.000000000000000000000000000')

Meanwhile, math and cmath will silently convert their arguments to float and complex respectively, which could mean loss of precision.
decimal also has its own .sqrt(). See also calculating n-th roots using Python 3&#39;s decimal module
"
"I've read about and understand floating point round-off issues such as:
&gt;&gt;&gt; sum([0.1] * 10) == 1.0
False

&gt;&gt;&gt; 1.1 + 2.2 == 3.3
False

&gt;&gt;&gt; sin(radians(45)) == sqrt(2) / 2
False

I also know how to work around these issues with math.isclose() and cmath.isclose().
The question is how to apply those work arounds to Python's match/case statement.  I would like this to work:
match 1.1 + 2.2:
    case 3.3:
        print('hit!')  # currently, this doesn't match

","The key to the solution is to build a wrapper that overrides the __eq__ method and replaces it with an approximate match:
import cmath

class Approximately(complex):

    def __new__(cls, x, /, **kwargs):
        result = complex.__new__(cls, x)
        result.kwargs = kwargs
        return result

    def __eq__(self, other):
        try:
            return isclose(self, other, **self.kwargs)
        except TypeError:
            return NotImplemented

It creates approximate equality tests for both float values and complex values:
&gt;&gt;&gt; Approximately(1.1 + 2.2) == 3.3
True
&gt;&gt;&gt; Approximately(1.1 + 2.2, abs_tol=0.2) == 3.4
True
&gt;&gt;&gt; Approximately(1.1j + 2.2j) == 0.0 + 3.3j
True

Here is how to use it in a match/case statement:
for x in [sum([0.1] * 10), 1.1 + 2.2, sin(radians(45))]:
    match Approximately(x):
        case 1.0:
            print(x, 'sums to about 1.0')
        case 3.3:
            print(x, 'sums to about 3.3')
        case 0.7071067811865475:
            print(x, 'is close to sqrt(2) / 2')
        case _:
            print('Mismatch')

This outputs:
0.9999999999999999 sums to about 1.0
3.3000000000000003 sums to about 3.3
0.7071067811865475 is close to sqrt(2) / 2

"
"Normally, if you try to pass multiple values for the same keyword argument, you get a TypeError:
In [1]: dict(id=1, **{'id': 2})
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Input In [1], in &lt;cell line: 1&gt;()
----&gt; 1 dict(id=1, **{'id': 2})

TypeError: dict() got multiple values for keyword argument 'id'

But if you do it while handling another exception, you get a KeyError instead:
In [2]: try:
   ...:     raise ValueError('foo') # no matter what kind of exception
   ...: except:
   ...:     dict(id=1, **{'id': 2}) # raises: KeyError: 'id'
   ...: 
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Input In [2], in &lt;cell line: 1&gt;()
      1 try:
----&gt; 2     raise ValueError('foo') # no matter what kind of exception
      3 except:

ValueError: foo

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
Input In [2], in &lt;cell line: 1&gt;()
      2     raise ValueError('foo') # no matter what kind of exception
      3 except:
----&gt; 4     dict(id=1, **{'id': 2})

KeyError: 'id'

What's going on here? How could a completely unrelated exception affect what kind of exception dict(id=1, **{'id': 2}) throws?
For context, I discovered this behavior while investigating the following bug report: https://github.com/tortoise/tortoise-orm/issues/1583
This has been reproduced on CPython 3.11.8, 3.10.5, and 3.9.5.
","This looks like a Python bug.
The code that's supposed to raise the TypeError works by detecting and replacing an initial KeyError, but this code doesn't work right. When the exception occurs in the middle of another exception handler, the code that should raise the TypeError fails to recognize the KeyError. It ends up letting the KeyError through, instead of replacing it with a TypeError.
The bug appears to be gone on 3.12, due to changes in the exception implementation.

Here's the deep dive, for the CPython 3.11.8 source code. Similar code exists on 3.10 and 3.9.
As we can see by using the dis module to examine the bytecode for dict(id=1, **{'id': 2}):
In [1]: import dis

In [2]: dis.dis(&quot;dict(id=1, **{'id': 2})&quot;)
  1           0 LOAD_NAME                0 (dict)
              2 LOAD_CONST               3 (())
              4 LOAD_CONST               0 ('id')
              6 LOAD_CONST               1 (1)
              8 BUILD_MAP                1
             10 LOAD_CONST               0 ('id')
             12 LOAD_CONST               2 (2)
             14 BUILD_MAP                1
             16 DICT_MERGE               1
             18 CALL_FUNCTION_EX         1
             20 RETURN_VALUE

Python uses the DICT_MERGE opcode to merge two dicts, to build the final keyword argument dict.
The relevant part of the DICT_MERGE code is as follows:
            if (_PyDict_MergeEx(dict, update, 2) &lt; 0) {
                format_kwargs_error(tstate, PEEK(2 + oparg), update);
                Py_DECREF(update);
                goto error;
            }

It uses _PyDict_MergeEx to attempt to merge two dicts, and if that fails (and raises an exception), it uses format_kwargs_error to try to raise a different exception.
When the third argument to _PyDict_MergeEx is 2, that function will raise a KeyError for duplicate keys, inside the dict_merge helper function. This is where the KeyError comes from.
Once the KeyError is raised, format_kwargs_error has the job of replacing it with a TypeError. It tries to do so with the following code:
    else if (_PyErr_ExceptionMatches(tstate, PyExc_KeyError)) {
        PyObject *exc, *val, *tb;
        _PyErr_Fetch(tstate, &amp;exc, &amp;val, &amp;tb);
        if (val &amp;&amp; PyTuple_Check(val) &amp;&amp; PyTuple_GET_SIZE(val) == 1) {

but this code is looking for an unnormalized exception, an internal way of representing exceptions that isn't exposed to Python-level code. It expects the exception value to be a 1-element tuple containing the key that the KeyError was raised for, instead of an actual exception object.
Exceptions raised inside C code are usually unnormalized, but not if they occur while Python is handling another exception. Unnormalized exceptions cannot handle exception chaining, which occurs automatically for exceptions raised inside an exception handler. In this case, the internal _PyErr_SetObject routine will automatically normalize the exception:
    exc_value = _PyErr_GetTopmostException(tstate)-&gt;exc_value;
    if (exc_value != NULL &amp;&amp; exc_value != Py_None) {
        /* Implicit exception chaining */
        Py_INCREF(exc_value);
        if (value == NULL || !PyExceptionInstance_Check(value)) {
            /* We must normalize the value right now */

Since the KeyError has been normalized, format_kwargs_error doesn't understand what it's looking at. It lets the KeyError through, instead of raising the TypeError it's supposed to.

On Python 3.12, things are different. The internal exception representation has been changed, so any raised exception is always normalized. Thus, the Python 3.12 version of format_kwargs_error looks for a normalized exception instead of an unnormalized exception, and if _PyDict_MergeEx has raised a KeyError, the code will recognize it:
    else if (_PyErr_ExceptionMatches(tstate, PyExc_KeyError)) {
        PyObject *exc = _PyErr_GetRaisedException(tstate);
        PyObject *args = ((PyBaseExceptionObject *)exc)-&gt;args;
        if (exc &amp;&amp; PyTuple_Check(args) &amp;&amp; PyTuple_GET_SIZE(args) == 1) {

"
"Currently i'm trying to work more with numpy typing to make my code clearer however i've somehow reached a limit that i can't currently override.
Is it possible to specify a specific shape and also the corresponding data type?
Example:
Shape=(4,)
datatype= np.int32

My attempts so far look like the following (but all just threw errors):
First attempt:
import numpy as np

def foo(x: np.ndarray[(4,), np.dtype[np.int32]]):
...
result -&gt; 'numpy._DTypeMeta' object is not subscriptable

Second attempt:
import numpy as np
import numpy.typing as npt

def foo(x: npt.NDArray[(4,), np.int32]):
...
result -&gt; Too many arguments for numpy.ndarray[typing.Any, numpy.dtype[+ScalarType]]

Also, unfortunately, I can't find any information about it in the documentation or I only get errors when I implement it the way it is documented.
","Currently, numpy.typing.NDArray only accepts a dtype, like so: numpy.typing.NDArray[numpy.int32]. You have some options though.
Use typing.Annotated
typing.Annotated allows you to create an alias for a type and to bundle some extra information with it.
In some my_types.py you would write all variations of shapes you want to hint:
from typing import Annotated, Literal, TypeVar
import numpy as np
import numpy.typing as npt


DType = TypeVar(&quot;DType&quot;, bound=np.generic)

Array4 = Annotated[npt.NDArray[DType], Literal[4]]
Array3x3 = Annotated[npt.NDArray[DType], Literal[3, 3]]
ArrayNxNx3 = Annotated[npt.NDArray[DType], Literal[&quot;N&quot;, &quot;N&quot;, 3]]

And then in foo.py, you can supply a numpy dtype and use them as typehint:
import numpy as np
from my_types import Array4


def foo(arr: Array4[np.int32]):
    assert arr.shape == (4,)

MyPy will recognize arr to be an np.ndarray and will check it as such. The shape checking can be done at runtime only, like in this example with an assert.
If you don't like the assertion, you can use your creativity to define a function to do the checking for you.
def assert_match(arr, array_type):
    hinted_shape = array_type.__metadata__[0].__args__
    hinted_dtype_type = array_type.__args__[0].__args__[1]
    hinted_dtype = hinted_dtype_type.__args__[0]
    assert np.issubdtype(arr.dtype, hinted_dtype), &quot;DType does not match&quot;
    assert arr.shape == hinted_shape, &quot;Shape does not match&quot;


assert_match(some_array, Array4[np.int32])

Use nptyping
Another option would be to use 3th party lib nptyping (yes, I am the author).
You would drop my_types.py as it would be of no use anymore.
Your foo.py would become something like:
from nptyping import NDArray, Shape, Int32


def foo(arr: NDArray[Shape[&quot;4&quot;], Int32]):
    assert isinstance(arr, NDArray[Shape[&quot;4&quot;], Int32])

Use beartype + typing.Annotated
There is also another 3th party lib called beartype that you could use. It can take a variant of the typing.Annotated approach and will do the runtime checking for you.
You would reinstate your my_types.py with content similar to:
from beartype import beartype
from beartype.vale import Is
from typing import Annotated
import numpy as np


Int32Array4 = Annotated[np.ndarray, Is[lambda array:
    array.shape == (4,) and np.issubdtype(array.dtype, np.int32)]]
Int32Array3x3 = Annotated[np.ndarray, Is[lambda array:
    array.shape == (3,3) and np.issubdtype(array.dtype, np.int32)]]

And your foo.py would become:
import numpy as np
from beartype import beartype
from my_types import Int32Array4 


@beartype
def foo(arr: Int32Array4):
    ...  # Runtime type checked by beartype.

Use beartype + nptyping
You could also stack up both libraries.
Your my_types.py can be removed again and your foo.py would become something like:
from nptyping import NDArray, Shape, Int32
from beartype import beartype


@beartype
def foo(arr: NDArray[Shape[&quot;4&quot;], Int32]):
    ...  # Runtime type checked by beartype.

"
"I am using pydantic for schema validations and I would like to throw an error when any extra field that isn't defined is added to a schema.
from typing import Literal, Union

from pydantic import BaseModel, Field, ValidationError


class Cat(BaseModel):
    pet_type: Literal['cat']
    meows: int


class Dog(BaseModel):
    pet_type: Literal['dog']
    barks: float


class Lizard(BaseModel):
    pet_type: Literal['reptile', 'lizard']
    scales: bool


class Model(BaseModel):
    pet: Union[Cat, Dog, Lizard] = Field(..., discriminator='pet_type')
    n: int


print(Model(pet={'pet_type': 'dog', 'barks': 3.14, 'eats': 'biscuit'}, n=1))
&quot;&quot;&quot; try:
    Model(pet={'pet_type': 'dog'}, n=1)
except ValidationError as e:
    print(e) &quot;&quot;&quot;


In the above code, I have added the eats field which is not defined. The pydantic validations are applied and the extra values that I defined are removed in response. I want to throw an error saying eats is not allowed for Dog or something like that. Is there any way to achieve that?
And is there any chance that we can provide the input directly instead of the pet object?
print(Model({'pet_type': 'dog', 'barks': 3.14, 'eats': 'biscuit', n=1})). I tried without descriminator but those specific validations are missing related to pet_type. Can someone guide me how to achieve either one of that?
","Pydantic v2
You can use the extra field in the model_config class attribute to forbid extra attributes during model initialisation (by default, additional attributes will be ignored).
For example:
from pydantic import BaseModel, ConfigDict

class Pet(BaseModel):
    model_config = ConfigDict(extra=&quot;forbid&quot;)

    name: str

data = {
    &quot;name&quot;: &quot;some name&quot;,
    &quot;some_extra_field&quot;: &quot;some value&quot;,
}

my_pet = Pet.model_validate(data)   # &lt;- effectively the same as Pet(**pet_data)

will raise a ValidationError:
ValidationError: 1 validation error for Pet
some_extra_field
  Extra inputs are not permitted [type=extra_forbidden, input_value='some value', input_type=str]
    For further information visit https://errors.pydantic.dev/2.7/v/extra_forbidden

Works as well when the model is &quot;nested&quot;, e.g.:
class PetModel(BaseModel):
    my_pet: Pet
    n: int

pet_data = {
    &quot;my_pet&quot;: {&quot;name&quot;: &quot;Some Name&quot;, &quot;invalid_field&quot;: &quot;some value&quot;},
    &quot;n&quot;: 5,
}

pet_model = PetModel.model_validate(pet_data)
# Effectively the same as
# pet_model = PetModel(my_pet={&quot;name&quot;: &quot;Some Name&quot;, &quot;invalid_field&quot;: &quot;some value&quot;}, n=5)

will raise:
ValidationError: 1 validation error for PetModel
my_pet.invalid_field
  Extra inputs are not permitted [type=extra_forbidden, input_value='some value', input_type=str]
    For further information visit https://errors.pydantic.dev/2.7/v/extra_forbidden

NB: As you can see, extra has the type ExtraValues now, and its value will get validated by ConfigDict. This means it's not possible to accidentally provide an unsupported value for extra (e.g. having a typo), i.e. something like ConfigDict(extra=&quot;fordib&quot;) will fail with a SchemaError.
Pydantic v1
You can use the extra field in the Config class to forbid extra attributes during model initialisation (by default, additional attributes will be ignored).
For example:
from pydantic import BaseModel, Extra

class Pet(BaseModel):
    name: str

    class Config:
        extra = Extra.forbid

data = {
    &quot;name&quot;: &quot;some name&quot;,
    &quot;some_extra_field&quot;: &quot;some value&quot;,
}

my_pet = Pet.parse_obj(data)   # &lt;- effectively the same as Pet(**pet_data)

will raise a VaidationError:
ValidationError: 1 validation error for Pet
some_extra_field
  extra fields not permitted (type=value_error.extra)

Works as well when the model is &quot;nested&quot;, e.g.:
class PetModel(BaseModel):
    my_pet: Pet
    n: int

pet_data = {
    &quot;my_pet&quot;: {&quot;name&quot;: &quot;Some Name&quot;, &quot;invalid_field&quot;: &quot;some value&quot;},
    &quot;n&quot;: 5,
}

pet_model = PetModel.parse_obj(pet_data)
# Effectively the same as
# pet_model = PetModel(my_pet={&quot;name&quot;: &quot;Some Name&quot;, &quot;invalid_field&quot;: &quot;some value&quot;}, n=5)

will raise:
ValidationError: 1 validation error for PetModel
my_pet -&gt; invalid_field
  extra fields not permitted (type=value_error.extra)

"
"I have a python script that reads in data from a csv file
The code runs fine, but everytime it runs I get this Deprecation message:
DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.

the warning stems from this piece of code:
fprice = df.groupby(['StartDate', 'Commodity', 'DealType']).apply(lambda group: -(group['MTMValue'].sum() - (group['FixedPriceStrike'] * group['Quantity']).sum()) / group['Quantity'].sum()).reset_index(name='FloatPrice')

to my understanding, I am performing the apply function on my groupings,but then I am disregarding the groupings and not using them anymore to be apart of my dataframe. I am confused about the directions to silence the warning
here is some sample data that this code uses:
TradeID  TradeDate  Commodity  StartDate   ExpiryDate FixedPrice Quantity MTMValue
-------- ---------- ---------  ---------   ---------- ---------- -------- ---------
 aaa   01/01/2024   (com1,com2) 01/01/2024  01/01/2024    10        10      100.00 
 bbb   01/01/2024   (com1,com2) 01/01/2024  01/01/2024    10        10      100.00 
 ccc   01/01/2024   (com1,com2) 01/01/2024  01/01/2024    10        10      100.00  

and here is the expected output from this data:
TradeID  TradeDate  Commodity  StartDate   ExpiryDate FixedPrice Quantity MTMValue  FloatPrice
-------- ---------- ---------  ---------   ---------- ---------- -------- --------- ----------
 aaa   01/01/2024   (com1,com2) 01/01/2024  01/01/2024    10        10      100.00      0
 bbb   01/01/2024   (com1,com2) 01/01/2024  01/01/2024    10        10      100.00      0
 ccc   01/01/2024   (com1,com2) 01/01/2024  01/01/2024    10        10      100.00      0 

","About include_groups parameter
The include_groups parameter of DataFrameGroupBy.apply is new in pandas version 2.2.0. It is basically a transition period (2.2.0 -&gt; 3.0) parameter added to help communicating a changing behavior (with warnings) and to tackle pandas Issue 7155. In most cases you should be able to just set it to False to silent the warning (see below).
Setup
Let's say you have a pandas DataFrame df and a dummy function myfunc for apply, and you want to

Group by column 'c'
Apply myfunc on each group

&gt;&gt;&gt; df
      a  value     c
0   foo     10  cat1
1   bar     20  cat2
2   baz     30  cat1
3  quux     40  cat2


&gt;&gt;&gt; def myfunc(x):
    print(x, '\n')
    

include_groups = True (Old behavior)

This is the default behavior in pandas &lt;2.2.0 (there is no include_groups parameter)
pandas 2.2.0 and above (likely until 3.0) will still default to this but issue a DeprecationWarning.
The grouping column(s), here 'c' is included in the DataFrameGroupBy

&gt;&gt;&gt; df.groupby('c').apply(myfunc)
     a  value     c
0  foo     10  cat1
2  baz     30  cat1 

      a  value     c
1   bar     20  cat2
3  quux     40  cat2 

Now as mentioned in Issue 7155, keeping the grouping column c in the dataframe passed to apply is unwanted behavior. Most people will not expect c to be present here. The answer of bue has actually an example how this could lead to bugs; apply on np.mean and expect there be less columns (causes a bug if your grouping column is numerical).
include_groups = False (new behavior)

This will remove the warning in the pandas &gt; 2.2.0 (&lt;3.0)
This will be the default in future version of pandas (likely 3.0)
This is what you likely would want to have; drop the grouping column 'c':

&gt;&gt;&gt; df.groupby('c').apply(myfunc, include_groups=False)
     a  value
0  foo     10
2  baz     30 

      a  value
1   bar     20
3  quux     40 

Circumventing need to use include_groups at all
Option 1: Explicitly giving column names
You may also skip the need for using the include_groups parameter at all by explicitly giving the list of the columns (as pointed out by the warning itself; &quot;..or explicitly select the grouping columns after groupby to silence this warning..&quot;,  and Cahit in their answer), like this:
&gt;&gt;&gt; df.groupby('c')[['a', 'value', 'c']].apply(myfunc)
     a  value     c
0  foo     10  cat1
2  baz     30  cat1 

      a  value     c
1   bar     20  cat2
3  quux     40  cat2 

Empty DataFrame
Columns: []
Index: []

Option 2: Setting the index before groupby
You may also set the groupby column to the index, as pointed out by Stefan in the comments.
&gt;&gt;&gt; df.set_index('c').groupby(level='c').apply(myfunc)
        a  value
c               
cat1  foo     10
cat1  baz     30 

         a  value
c                
cat2   bar     20
cat2  quux     40 

Empty DataFrame
Columns: []
Index: []



Details just for this use case
Your grouping columns are
['StartDate', 'Commodity', 'DealType']

In the apply function you use the following columns:
['MTMValue',  'FixedPriceStrike', 'Quantity']

i.e., you do not need any of the grouping columns in your apply, and therefore you can use include_groups=False which also removes the warning.
fprice = df.groupby(['StartDate', 'Commodity', 'DealType']).apply(lambda group: -(group['MTMValue'].sum() - (group['FixedPriceStrike'] * group['Quantity']).sum()) / group['Quantity'].sum(), include_groups=False).reset_index(name='FloatPrice')

"
"I am new to SQLAlchemy and I see that in the documentation the older version (Column) can be swapped directly with the newer &quot;mapped_column&quot;.
Is there any advantage to using mapped_column over Column? Could you stick to the older 'Column'?
","I think originally Column was used in the lower &quot;core&quot;/sqlalchemy.sql layer AND the higher ORM layer.  This created a conflict of purpose.  So mapped_column now supersedes Column when using the ORM layer to add more functionality that can't be used by the core layer.  The core layer will keep using Column.  So I think it is just meant to help you do more faster or more succinctly with the ORM.
There is a blurb about them titled &quot;mapped_column() supersedes the use of Column()&quot; below declarative-table-with-mapped-column.
Here are some basic examples, using postgresql.
See SQL output at end.
class Base(DeclarativeBase):
    pass

class Controller(Base):
    __tablename__ = &quot;controllers&quot;

    id: Mapped[int] = mapped_column(primary_key=True)
    name: Mapped[str] = mapped_column() # Example 1
    index: Mapped[int] # Example 2
    configured: Mapped[Optional[bool]] # Example 3
    setup_mode: Mapped[bool] # Example 4
    created_at = Column(DateTime(timezone=True)) # Example 5

Example 1
The column type is derived from the type hint, VARCHAR is derived from str in this case.
name: Mapped[str] = mapped_column()

Example 2
When mapped_column would be empty it can be left out entirely and this still works, ie. INTEGER is derived from int.
index: Mapped[int]

Example 3
When Optional from typing is used then a column will allow NULL, ie. nullable=True.
configured: Mapped[Optional[bool]]

Example 4
When Optional is NOT used then a column will not allow NULL, ie. nullable=False.
setup_mode: Mapped[bool]

Example 5
Column can still be used alongside mapped_column without using type hints at all.
created_at = Column(DateTime(timezone=True))

Example of type checking
Running this code through mypy will produce an error similar to error: &quot;Controller&quot; has no attribute &quot;unknown_attribute&quot;  [attr-defined]

with Session(engine) as session:
    controller = session.scalars(select(Controller).limit(1)).first()
    if controller is not None:
        assert controller.created_at
        assert controller.unknown_attribute

Final CREATE TABLE output
CREATE TABLE controllers (
    id SERIAL NOT NULL, 
    name VARCHAR NOT NULL, 
    index INTEGER NOT NULL, 
    configured BOOLEAN, 
    setup_mode BOOLEAN NOT NULL, 
    created_at TIMESTAMP WITH TIME ZONE, 
    PRIMARY KEY (id)
)

Some &quot;Why&quot;s

allow more orm specific functionality that does not make sense to be in Column()
reduce boilerplate code

int, str, datetime, Optional, etc. are available from python without needing sqlalchemy imports
In cases where the type can be derived from the typehint and no special configuration is necessary the entire column/mapped_column definition can be left out, ie. index: Mapped[int]


allow type checkers to better check types

the checks can be expanded by using something like the data-class integration but is beyond this question



"
"I just read PEP 393 and learned that Python's str type uses different internal representations, depending on the content. So, I experimented a little bit and was a bit surprised by the results:
&gt;&gt;&gt; sys.getsizeof('')
41
&gt;&gt;&gt; sys.getsizeof('H')
42
&gt;&gt;&gt; sys.getsizeof('Hi')
43
&gt;&gt;&gt; sys.getsizeof('Ã–')
61
&gt;&gt;&gt; sys.getsizeof('Ã–l')
59

I understand that in the first three cases, the strings don't contain any non-ASCII characters, so an encoding with 1 byte per char can be used. Putting a non-ASCII character like Ã– in a string forces the interpreter to use a different encoding. Therefore, I'm not surprised that 'Ã–' takes more space than 'H'.
However, why does 'Ã–l' take less space than 'Ã–'? I assumed that whatever internal representation is used for 'Ã–l' allows for an even shorter representation of 'Ã–'.
I'm using Python 3.12, apparently it is not reproducible in earlier versions.
","This test code (the structures are only correct according to 3.12.4 source, and even so I didn't quite double-check them)
import ctypes
import sys


class PyUnicodeObject(ctypes.Structure):
    _fields_ = [
        (&quot;ob_refcnt&quot;, ctypes.c_ssize_t),
        (&quot;ob_type&quot;, ctypes.c_void_p),
        (&quot;length&quot;, ctypes.c_ssize_t),
        (&quot;hash&quot;, ctypes.c_ssize_t),
        (&quot;state&quot;, ctypes.c_uint64),
    ]


class StateBitField(ctypes.LittleEndianStructure):
    _fields_ = [
        (&quot;interned&quot;, ctypes.c_uint, 2),
        (&quot;kind&quot;, ctypes.c_uint, 3),
        (&quot;compact&quot;, ctypes.c_uint, 1),
        (&quot;ascii&quot;, ctypes.c_uint, 1),
        (&quot;statically_allocated&quot;, ctypes.c_uint, 1),
        (&quot;_padding&quot;, ctypes.c_uint, 24),
    ]

    def __repr__(self):
        return &quot;, &quot;.join(f&quot;{k}: {getattr(self, k)}&quot; for k, *_ in self._fields_ if not k.startswith(&quot;_&quot;))


def dump_s(s: str):
    o = PyUnicodeObject.from_address(id(s))
    state_int = o.state
    state = StateBitField.from_buffer(ctypes.c_uint64(state_int))
    print(f&quot;{s!r}&quot;.ljust(8), f&quot;{o.length=}, {sys.getsizeof(s)=}, {state}&quot;)


dump_s('5')
dump_s('a')
dump_s('ä')
dump_s('vvv')
dump_s('ÖÖÖ')
dump_s(str(chr(214)))  # avoid the string having been interned into module source
dump_s(str(chr(214) + chr(108)))  # avoid the string having been interned into module source

prints out
'5'      o.length=1, sys.getsizeof(s)=42, interned: 3, kind: 1, compact: 1, ascii: 1, statically_allocated: 1
'a'      o.length=1, sys.getsizeof(s)=42, interned: 3, kind: 1, compact: 1, ascii: 1, statically_allocated: 1
'ä'      o.length=1, sys.getsizeof(s)=61, interned: 0, kind: 1, compact: 1, ascii: 0, statically_allocated: 1
'vvv'    o.length=3, sys.getsizeof(s)=44, interned: 2, kind: 1, compact: 1, ascii: 1, statically_allocated: 0
'ÖÖÖ'    o.length=3, sys.getsizeof(s)=60, interned: 0, kind: 1, compact: 1, ascii: 0, statically_allocated: 0
'Ö'      o.length=1, sys.getsizeof(s)=61, interned: 0, kind: 1, compact: 1, ascii: 0, statically_allocated: 1
'Öl'     o.length=2, sys.getsizeof(s)=59, interned: 0, kind: 1, compact: 1, ascii: 0, statically_allocated: 0
'Ö'      o.length=1, sys.getsizeof(s)=61, interned: 0, kind: 1, compact: 1, ascii: 0, statically_allocated: 1

– the smoking gun seems to be statically_allocated on Ö etc..
I think that stems from this line in pycore_runtime_init_generated where it looks like the runtime statically objects for all Latin-1 strings (among others). As discussed in the comments, this CPython PR added UTF-8 representations of all of these statically allocated strings, so Ö is statically stored as both Latin-1 (1 character) and UTF-8 (2 characters).
Also, I should note getsizeof() actually forwards to unicode_sizeof_impl, it's not just measuring memory.
"
"Consider this:
&gt;&gt;&gt; '{x[1]}'.format(x=&quot;asd&quot;)
's'
&gt;&gt;&gt; '{x[1:3]}'.format(x=&quot;asd&quot;)
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
TypeError: string indices must be integers

What could be the cause for this behavior?
","An experiment based on your comment, checking what value the object's __getitem__ method actually receives:
class C:
    def __getitem__(self, index):
        print(repr(index))

'{c[4]}'.format(c=C())
'{c[4:6]}'.format(c=C())
'{c[anything goes!@#$%^&amp;]}'.format(c=C())
C()[4:6]

Output (Try it online!):
4
'4:6'
'anything goes!@#$%^&amp;'
slice(4, 6, None)

So while the 4 gets converted to an int, the 4:6 isn't converted to slice(4, 6, None) as in usual slicing. Instead, it remains simply the string '4:6'. And that's not a valid type for indexing/slicing a string, hence the TypeError: string indices must be integers you got.
Update:
Is that documented? Well... I don't see something really clear, but @GACy20 pointed out something subtle. The grammar has these rules
field_name        ::=  arg_name (&quot;.&quot; attribute_name | &quot;[&quot; element_index &quot;]&quot;)*
element_index     ::=  digit+ | index_string
index_string      ::=  &lt;any source character except &quot;]&quot;&gt; +

Our c[4:6] is the field_name, and we're interested in the element_index part 4:6. I think it would be clearer if digit+ had its own rule with meaningful name:
field_name        ::=  arg_name (&quot;.&quot; attribute_name | &quot;[&quot; element_index &quot;]&quot;)*
element_index     ::=  index_integer | index_string
index_integer     ::=  digit+
index_string      ::=  &lt;any source character except &quot;]&quot;&gt; +

I'd say having index_integer and index_string would more clearly indicate that digit+ is converted to an integer (instead of staying a digit string), while &lt;any source character except &quot;]&quot;&gt; + would stay a string.
That said, looking at the rules as they are, perhaps we should think &quot;what would be the point of separating the digits case out of the any-characters case which would match it as well?&quot; and think that the point is to treat pure digits differently, presumably to convert them to an integer. Or maybe some other part of the documentation even states that digit or digits+ in general gets converted to an integer.
"
"I have installed Python 3.9 in the Ubuntu 20.04 LTS. Now the system has both Python 3.8 and Python 3.9.
# which python
# which python3
/usr/bin/python3
# which python3.8
/usr/bin/python3.8
# which python3.9
/usr/bin/python3.9
# ls -alith /usr/bin/python3
12583916 lrwxrwxrwx 1 root root 9 Jul 19  2021 /usr/bin/python3 -&gt; python3.8

But the pip3 command will still install everything into the Python 3.8 directory.
# pip3 install --upgrade --find-links file:///path/to/directory &lt;...&gt;

I want to change that default pip3 behavior by updating the symbolic link /usr/bin/python3 to /usr/bin/python3.9.
How to do that?
# update-alternatives --set python3 /usr/bin/python3.9
This command will not work as expected.

Here is the pip3 info:
# which pip3
/usr/bin/pip3
# ls -alith /usr/bin/pip3
12589712 -rwxr-xr-x 1 root root 367 Jul 13  2021 /usr/bin/pip3
# pip3 -V
pip 20.0.2 from /usr/lib/python3/dist-packages/pip (python 3.8)
# 

The alias command will not work:
# alias python3=python3.9
# ls -alith /usr/bin/python3
12583916 lrwxrwxrwx 1 root root 9 Jul 19  2021 /usr/bin/python3 -&gt; python3.8

","You should be able to use python3.9 -m pip install &lt;package&gt; to run pip with a specific python version, in this case 3.9.
The full docs on this are here: https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/
If you want python3 to point to python3.9 you could use the quick and dirty.
alias python3=python3.9

EDIT:
Tried to recreate your problem,
# which python3
/usr/bin/python3
# python3 --version
Python 3.8.10
# which python3.8
/usr/bin/python3.8
# which python3.9
/usr/bin/python3.9

Then update the alternatives, and set new priority:
# sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1
# sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 2
# sudo update-alternatives --config python3
There are 2 choices for the alternative python3 (providing /usr/bin/python3).

  Selection    Path                Priority   Status
------------------------------------------------------------
  0            /usr/bin/python3.9   2         auto mode
  1            /usr/bin/python3.8   2         manual mode
* 2            /usr/bin/python3.9   2         manual mode

Press &lt;enter&gt; to keep the current choice[*], or type selection number: 0

Check new version:
# ls -alith /usr/bin/python3
3338 lrwxrwxrwx 1 root root 25 Feb  8 14:33 /usr/bin/python3 -&gt; /etc/alternatives/python3
# python3 -V
Python 3.9.5
# ls -alith /usr/bin/pip3
48482 -rwxr-xr-x 1 root root 367 Jul 13  2021 /usr/bin/pip3
# pip3 -V
pip 20.0.2 from /usr/lib/python3/dist-packages/pip (python 3.9)

Hope this helps (tried it in wsl2 Ubuntu 20.04 LTS)
"
"The web interface for ChatGPT has an easy pdf upload. Is there an API from openAI that can receive pdfs?
I know there are 3rd party libraries that can read pdf but given there are images and other important information in a pdf, it might be better if a model like GPT 4 Turbo was fed the actual pdf directly.
I'll state my use case to add more context. I intent to do RAG. In the code below I handle the PDF and a prompt. Normally I'd append the text at the end of the prompt. I could still do that with a pdf if I extract its contents manually.
The following code is taken from here https://platform.openai.com/docs/assistants/tools/code-interpreter. Is this how I'm supposed to do it?
# Upload a file with an &quot;assistants&quot; purpose
file = client.files.create(
  file=open(&quot;example.pdf&quot;, &quot;rb&quot;),
  purpose='assistants'
)

# Create an assistant using the file ID
assistant = client.beta.assistants.create(
  instructions=&quot;You are a personal math tutor. When asked a math question, write and run code to answer the question.&quot;,
  model=&quot;gpt-4-1106-preview&quot;,
  tools=[{&quot;type&quot;: &quot;code_interpreter&quot;}],
  file_ids=[file.id]
)

There is an upload endpoint as well, but it seems the intent of those endpoints are for fine-tuning and assistants. I think the RAG use case is a normal one and not necessarily related to assistants.
","As of today (openai.__version__==1.42.0) using OpenAI Assistants + GPT-4o allows to extract content of (or answer questions on) an input pdf file foobar.pdf stored locally, with a solution along the lines of
from openai import OpenAI
from openai.types.beta.threads.message_create_params import (
    Attachment,
    AttachmentToolFileSearch,
)
import os

filename = &quot;foobar.pdf&quot;
prompt = &quot;Extract the content from the file provided without altering it. Just output its exact content and nothing else.&quot;

client = OpenAI(api_key=os.environ.get(&quot;MY_OPENAI_KEY&quot;))

pdf_assistant = client.beta.assistants.create(
    model=&quot;gpt-4o&quot;,
    description=&quot;An assistant to extract the contents of PDF files.&quot;,
    tools=[{&quot;type&quot;: &quot;file_search&quot;}],
    name=&quot;PDF assistant&quot;,
)

# Create thread
thread = client.beta.threads.create()

file = client.files.create(file=open(filename, &quot;rb&quot;), purpose=&quot;assistants&quot;)

# Create assistant
client.beta.threads.messages.create(
    thread_id=thread.id,
    role=&quot;user&quot;,
    attachments=[
        Attachment(
            file_id=file.id, tools=[AttachmentToolFileSearch(type=&quot;file_search&quot;)]
        )
    ],
    content=prompt,
)

# Run thread
run = client.beta.threads.runs.create_and_poll(
    thread_id=thread.id, assistant_id=pdf_assistant.id, timeout=1000
)

if run.status != &quot;completed&quot;:
    raise Exception(&quot;Run failed:&quot;, run.status)

messages_cursor = client.beta.threads.messages.list(thread_id=thread.id)
messages = [message for message in messages_cursor]

# Output text
res_txt = messages[0].content[0].text.value
print(res_txt)

The prompt can of course be replaced with the desired user request and I assume that the openai key is stored in a env var named MY_OPENAI_KEY.
Limitations:

it's not (yet) possible to enforce JSON structure (other than with instructions in the prompt). This solution is inspired by https://medium.com/@erik-kokalj/effectively-analyze-pdfs-with-gpt-4o-api-378bd0f6be03.

this relies on text content in the PDF (i.e. searchable text content), and the queries won't be able to access e.g. image content in the pdf.


"
"I am currently creating a new column in a polars data frame using
predictions = [10, 20, 30, 40, 50]
df['predictions'] = predictions

where predictions is a numpy array or list containing values I computed with another tool.
However, polars throws a warning, that this option will be deprecated.
How can the same result be achieved using .with_columns()?
","You can now also pass numpy arrays in directly. E.g,
df = pl.DataFrame({&quot;x&quot;: [0, 1, 2, 3, 4]})
p1 = [10, 20, 30, 40, 50]
p2 = np.array(p1)
df.with_columns(
    p1=pl.Series(p1), # For python lists, construct a Series
    p2=p2, # For numpy arrays, you can pass them directly
)
# shape: (5, 3)
# ┌─────┬─────┬─────┐
# │ x   ┆ p1  ┆ p2  │
# │ --- ┆ --- ┆ --- │
# │ i64 ┆ i64 ┆ i64 │
# ╞═════╪═════╪═════╡
# │ 0   ┆ 10  ┆ 10  │
# │ 1   ┆ 20  ┆ 20  │
# │ 2   ┆ 30  ┆ 30  │
# │ 3   ┆ 40  ┆ 40  │
# │ 4   ┆ 50  ┆ 50  │
# └─────┴─────┴─────┘

"
"Using FastAPI in a sync, not async mode, I would like to be able to receive the raw, unchanged body of a POST request.
All examples I can find show async code, when I try it in a normal sync way, the request.body() shows up as a coroutine object.
When I test it by posting some XML to this endpoint, I get a 500 &quot;Internal Server Error&quot;.
from fastapi import FastAPI, Response, Request, Body

app = FastAPI()

@app.get(&quot;/&quot;)
def read_root():
    return {&quot;Hello&quot;: &quot;World&quot;}

@app.post(&quot;/input&quot;)
def input_request(request: Request):
    # how can I access the RAW request body here?  
    body = request.body()

    # do stuff with the body here  

    return Response(content=body, media_type=&quot;application/xml&quot;)

Is this not possible with FastAPI?
Note: a simplified input request would look like:
POST http://127.0.0.1:1083/input
Content-Type: application/xml

&lt;XML&gt;
    &lt;BODY&gt;TEST&lt;/BODY&gt;
&lt;/XML&gt;

and I have no control over how input requests are sent, because I need to replace an existing SOAP API.
","Using async def endpoint
If an object is a co-routine, it needs to be awaited. FastAPI is actually Starlette underneath, and Starlette methods for returning the request body are async methods (see the source code here as well); thus, one needs to await them (inside an async def endpoint). For example:
from fastapi import Request

@app.post(&quot;/input&quot;)
async def input_request(request: Request):
    return await request.body()

Update 1 - Using def endpoint
Alternatively, if you are confident that the incoming data is a valid JSON, you can define your endpoint with def instead, and use the Body field, as shown below (for more options on how to post JSON data, see this answer):
from fastapi import Body

@app.post(&quot;/input&quot;)
def input_request(payload: dict = Body(...)):
    return payload

If, however, the incoming data are in XML format, as in the example you provided, one option is to pass them using Files instead, as shown below—as long as you have control over how client data are sent to the server (have a look here as well). Example:
from fastapi import File

@app.post(&quot;/input&quot;) 
def input_request(contents: bytes = File(...)): 
    return contents

Update 2 - Using def endpoint and async dependency
As described in this post, you can use an async dependency function to pull out the body from the request. You can use async dependencies on non-async (i.e., def) endpoints as well. Hence, if there is some sort of blocking code in this endpoint that prevents you from using async/await—as I am guessing this might be the reason in your case—this is the way to go.
Note: I should also mention that this answer—which explains the difference between def and async def endpoints (that you might be aware of)—also provides solutions when you are required to use async def (as you might need to await for coroutines inside a route), but also have some synchronous expensive CPU-bound operation that might be blocking the server. Please have a look.
Example of the approach described earlier can be found below. You can uncomment the time.sleep() line, if you would like to confirm yourself that a request won't be blocking other requests from going through, as when you declare an endpoint with normal def instead of async def, it is run in an external threadpool (regardless of the async def dependency function).
from fastapi import FastAPI, Depends, Request
import time

app = FastAPI()

async def get_body(request: Request):
    return await request.body()

@app.post(&quot;/input&quot;)
def input_request(body: bytes = Depends(get_body)):
    print(&quot;New request arrived.&quot;)
    #time.sleep(5)
    return body

"
"Consider a Polars data frame with a column of str type that indicates the date in the format '27 July 2020'.
I would like to convert this column to the polars.datetime type, which is distinct from the Python standard datetime.
import polars as pl
from datetime import datetime

df = pl.DataFrame({
    &quot;id&quot;: [1, 2], 
     &quot;event_date&quot;: [&quot;27 July 2020&quot;, &quot;31 December 2020&quot;]
})

df = df.with_columns( 
    pl.col(&quot;event_date&quot;).map_elements(lambda x: x.replace(&quot; &quot;, &quot;-&quot;))
                        .map_elements(lambda x: datetime.strptime(x, &quot;%d-%B-%Y&quot;))
)

shape: (2, 2)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ id  â”† event_date          â”‚
â”‚ --- â”† ---                 â”‚
â”‚ i64 â”† datetime[Î¼s]        â”‚
â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ 1   â”† 2020-07-27 00:00:00 â”‚
â”‚ 2   â”† 2020-12-31 00:00:00 â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Suppose we try to process df further to create a new column indicating the quarter of the year an event took place.
df.with_columns(
    pl.col(&quot;event_date&quot;).map_elements(lambda x: x.month)
                        .map_elements(lambda x: 1 if x in range(1,4) else 2 if x in range(4,7) else 3 if x in range(7,10) else 4)
                        .alias(&quot;quarter&quot;)
)

shape: (2, 3)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ id  â”† event_date          â”† quarter â”‚
â”‚ --- â”† ---                 â”† ---     â”‚
â”‚ i64 â”† datetime[Î¼s]        â”† i64     â”‚
â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•¡
â”‚ 1   â”† 2020-07-27 00:00:00 â”† 3       â”‚
â”‚ 2   â”† 2020-12-31 00:00:00 â”† 4       â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

How would I do this in Polars without applying custom lambdas through map_elements?
","The easiest way to convert strings to Date/Datetime is to use Polars' own functions:

.str.to_date()
.str.to_datetime()

df.with_columns( 
    pl.col(&quot;event_date&quot;).str.to_datetime(&quot;%d %B %Y&quot;)
)

shape: (2, 2)
┌─────┬─────────────────────┐
│ id  ┆ event_date          │
│ --- ┆ ---                 │
│ i64 ┆ datetime[μs]        │
╞═════╪═════════════════════╡
│ 1   ┆ 2020-07-27 00:00:00 │
│ 2   ┆ 2020-12-31 00:00:00 │
└─────┴─────────────────────┘

The Temporal section of the docs shows the supported functions in the .dt namespace.
In the case of your second example, there is a dedicated quarter expression:

.dt.quarter()

df = df.with_columns( 
    pl.col(&quot;event_date&quot;).str.to_datetime(&quot;%d %B %Y&quot;)
).with_columns(
    pl.col(&quot;event_date&quot;).dt.quarter().alias(&quot;quarter&quot;)
)

shape: (2, 3)
┌─────┬─────────────────────┬─────────┐
│ id  ┆ event_date          ┆ quarter │
│ --- ┆ ---                 ┆ ---     │
│ i64 ┆ datetime[μs]        ┆ i8      │
╞═════╪═════════════════════╪═════════╡
│ 1   ┆ 2020-07-27 00:00:00 ┆ 3       │
│ 2   ┆ 2020-12-31 00:00:00 ┆ 4       │
└─────┴─────────────────────┴─────────┘

"
"I'm learning how to package Python projects for PyPI according to the tutorial (https://packaging.python.org/en/latest/tutorials/packaging-projects/). For the example project, they use the folder structure:
packaging_tutorial/
â”œâ”€â”€ LICENSE
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ src/
â”‚   â””â”€â”€ example_package_YOUR_USERNAME_HERE/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ example.py
â””â”€â”€ tests/

I am just wondering why the src/ folder is needed? Does it serve a particular purpose? Could one instead include the package directly in the top folder? E.g. would
packaging_tutorial/
â”œâ”€â”€ LICENSE
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ example_package_YOUR_USERNAME_HERE/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ example.py
â””â”€â”€ tests/

have any disadvantages or cause complications?
","There is an interesting blog post about this topic; basically, using src prevents that when running tests from within the project directory, the package source folder gets imported instead of the installed package (and tests should always run against installed packages, so that the situation is the same as for a user).
Consider the following example project where the name of the package under development is mypkg. It contains an __init__.py file and another DATA.txt non-code resource:
.
├── mypkg
│   ├── DATA.txt
│   └── __init__.py
├── pyproject.toml
├── setup.cfg
└── test
    └── test_data.py

Here, mypkg/__init__.py accesses the DATA.txt resource and loads its content:
from importlib.resources import read_text
  
data = read_text('mypkg', 'DATA.txt').strip()  # The content is 'foo'.

The script test/test_data.py checks that mypkg.data actually contains 'foo':
import mypkg
  
def test():
    assert mypkg.data == 'foo'

Now, running coverage run -m pytest from within the base directory gives the impression that everything is alright with the project:
$ coverage run -m pytest
[...]
test/test_data.py .                                             [100%]

========================== 1 passed in 0.01s ==========================

However, there's a subtle issue. Running coverage run -m pytest invokes pytest via python -m pytest, i.e. using the -m switch. This has a &quot;side effect&quot;, as mentioned in the docs:

[...] As with the -c option, the current directory will be added to the start of sys.path. [...]

This means that when importing mypkg in test/test_data.py, it didn't import the installed version but it imported the package from the source tree in mypkg instead.
Now, let's further assume that we forgot to include the DATA.txt resource in our project specification (after all, there is no MANIFEST.in). So this file is actually not included in the installed version of mypkg (installation e.g. via python -m pip install .). This is revealed by running pytest directly:
$ pytest
[...]
======================= short test summary info =======================
ERROR test/test_data.py - FileNotFoundError: [Errno 2] No such file ...
!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!
========================== 1 error in 0.13s ===========================

Hence, when using coverage the test passed despite the installation of mypkg being broken. The test didn't capture this as it was run against the source tree rather than the installed version. If we had used a src directory to contain the mypkg package, then adding the current working directory via -m would have caused no problems, as there is no package mypkg in the current working directory anymore.
But in the end, using src is not a requirement but more of a convention/best practice. For example requests doesn't use src and they still manage to be a popular and successful project.
"
"I follow the FastAPI Tutorial and am not quite sure what the exact relationship between the proposed data objects is.
We have the models.py file:
from sqlalchemy import Boolean, Column, ForeignKey, Integer, String
from sqlalchemy.orm import relationship

from .database import Base


class User(Base):
    __tablename__ = &quot;users&quot;

    id = Column(Integer, primary_key=True, index=True)
    email = Column(String, unique=True, index=True)
    hashed_password = Column(String)
    is_active = Column(Boolean, default=True)

    items = relationship(&quot;Item&quot;, back_populates=&quot;owner&quot;)


class Item(Base):
    __tablename__ = &quot;items&quot;

    id = Column(Integer, primary_key=True, index=True)
    title = Column(String, index=True)
    description = Column(String, index=True)
    owner_id = Column(Integer, ForeignKey(&quot;users.id&quot;))

    owner = relationship(&quot;User&quot;, back_populates=&quot;items&quot;)

And the schemas.py file:
from typing import List, Union

from pydantic import BaseModel


class ItemBase(BaseModel):
    title: str
    description: Union[str, None] = None


class ItemCreate(ItemBase):
    pass


class Item(ItemBase):
    id: int
    owner_id: int

    class Config:
        orm_mode = True


class UserBase(BaseModel):
    email: str


class UserCreate(UserBase):
    password: str


class User(UserBase):
    id: int
    is_active: bool
    items: List[Item] = []

    class Config:
        orm_mode = True

Those classes are then used to define db queries like in the crud.py file:
from sqlalchemy.orm import Session

from . import models, schemas


def get_user(db: Session, user_id: int):
    return db.query(models.User).filter(models.User.id == user_id).first()


def get_user_by_email(db: Session, email: str):
    return db.query(models.User).filter(models.User.email == email).first()


def get_users(db: Session, skip: int = 0, limit: int = 100):
    return db.query(models.User).offset(skip).limit(limit).all()


def create_user(db: Session, user: schemas.UserCreate):
    fake_hashed_password = user.password + &quot;notreallyhashed&quot;
    db_user = models.User(email=user.email, hashed_password=fake_hashed_password)
    db.add(db_user)
    db.commit()
    db.refresh(db_user)
    return db_user

def get_items(db: Session, skip: int = 0, limit: int = 100):
    return db.query(models.Item).offset(skip).limit(limit).all()

def create_user_item(db: Session, item: schemas.ItemCreate, user_id: int):
    db_item = models.Item(**item.dict(), owner_id=user_id)
    db.add(db_item)
    db.commit()
    db.refresh(db_item)
    return db_item

And in the FastAPI code main.py:
from typing import List

from fastapi import Depends, FastAPI, HTTPException
from sqlalchemy.orm import Session

from . import crud, models, schemas
from .database import SessionLocal, engine

models.Base.metadata.create_all(bind=engine)

app = FastAPI()


# Dependency
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


@app.post(&quot;/users/&quot;, response_model=schemas.User)
def create_user(user: schemas.UserCreate, db: Session = Depends(get_db)):
    db_user = crud.get_user_by_email(db, email=user.email)
    if db_user:
        raise HTTPException(status_code=400, detail=&quot;Email already registered&quot;)
    return crud.create_user(db=db, user=user)


@app.get(&quot;/users/&quot;, response_model=List[schemas.User])
def read_users(skip: int = 0, limit: int = 100, db: Session = Depends(get_db)):
    users = crud.get_users(db, skip=skip, limit=limit)
    return users


@app.get(&quot;/users/{user_id}&quot;, response_model=schemas.User)
def read_user(user_id: int, db: Session = Depends(get_db)):
    db_user = crud.get_user(db, user_id=user_id)
    if db_user is None:
        raise HTTPException(status_code=404, detail=&quot;User not found&quot;)
    return db_user


@app.post(&quot;/users/{user_id}/items/&quot;, response_model=schemas.Item)
def create_item_for_user(
    user_id: int, item: schemas.ItemCreate, db: Session = Depends(get_db)
):
    return crud.create_user_item(db=db, item=item, user_id=user_id)


@app.get(&quot;/items/&quot;, response_model=List[schemas.Item])
def read_items(skip: int = 0, limit: int = 100, db: Session = Depends(get_db)):
    items = crud.get_items(db, skip=skip, limit=limit)
    return items

From what I understand:

The models data classes define the SQL tables.
The schemas data classes define the API that FastAPI uses to interact with the database.
They must be convertible into each other so that the set-up works.

What I don't understand:

In crud.create_user_item I expected the return type to be schemas.Item, since that return type is used by FastAPI again.
According to my understanding the response model of @app.post(&quot;/users/{user_id}/items/&quot;, response_model=schemas.Item) in the main.py is wrong, or how can I understand the return type inconsistency?
However inferring from the code, the actual return type must be models.Item, how is that handled by FastAPI?
What would be the return type of crud.get_user?

","I'll go through your bullet points one by one.


The models data classes define the SQL tables.

Yes. More precisely, the orm classes that map to actual database tables are defined in the models module.


The schemas data classes define the API that FastAPI uses to interact with the database.

Yes and no. The Pydantic models in the schemas module define the data schemas relevant to the API, yes. But that has nothing to do with the database yet. Some of these schemas define what data is expected to be received by certain API endpoints for the request to be considered valid. Others define what the data returned by certain endpoints will look like.


They must be convertible into each other so that the set-up works.

While the database table schemas and the API data schemas are usually very similar, that is not necessarily the case. In the tutorial however, they correspond quite neatly, which allows succinct crud code, like this:
db_item = models.Item(**item.dict(), owner_id=user_id)

Here item is a Pydantic model instance, i.e. one of your API data schemas schemas.ItemCreate containing data you decided is necessary for creating a new item. Since its fields (their names and types) correspond to those of the database model models.Item, the latter can be instantiated from the dictionary representation of the former (with the addition of the owner_id).


In crud.create_user_item I expected the return type to be schemas.Item, since that return type is used by FastAPI again.

No, this is exactly the magic of FastAPI. The function create_user_item returns an instance of models.Item, i.e. the ORM object as constructed from the database (after calling session.refresh on it):
def create_user_item(db: Session, item: schemas.ItemCreate, user_id: int):
    db_item = models.Item(**item.dict(), owner_id=user_id)
    ...
    return db_item

And the API route handler function create_item_for_user actually does return that same object (of class models.Item).
@app.post(&quot;/users/{user_id}/items/&quot;, response_model=schemas.Item)
def create_item_for_user(
    user_id: int, item: schemas.ItemCreate, db: Session = Depends(get_db)
):
    return crud.create_user_item(db=db, item=item, user_id=user_id)

However, the @app.post decorator takes that object and uses it to construct an instance of the response_model you defined for that route, which is schemas.Item in this case. This is why you set orm_mode in your schemas.Item model:
class Config:
    orm_mode = True

This allows an instance of that class to be created via the .from_orm method. (This only applies to Pydantic v1 Models.) That all happens behind the scenes and again depends on the SQLAlchemy model corresponding to the Pydantic model with regards to field names and types. Otherwise validation fails.


According to my understanding the response model [...] is wrong

No, see above. The decorated route function actually returns an instance of the schemas.Item model.


However inferring from the code, the actual return type must be models.Item

Yes, see above. The return type of the undecorated route handler function create_item_for_user is in fact models.Item. But its return type is not the response model.
I assume that to reduce confusion the documentation example does not annotate the return type of those route functions. If it did, it would look like this:
@app.post(&quot;/users/{user_id}/items/&quot;, response_model=schemas.Item)
def create_item_for_user(
    user_id: int, item: schemas.ItemCreate, db: Session = Depends(get_db)
) -&gt; models.Item:
    return crud.create_user_item(db=db, item=item, user_id=user_id)

It may help to remember that a function decorator is just syntactic sugar for a function that takes a function as argument and (usually) returns a function. Typically the returned function actually internally calls the function passed to it as argument and does additional things before and/or after that call. I could rewrite the route above like this and it would be exactly the same:
def create_item_for_user(
    user_id: int, item: schemas.ItemCreate, db: Session = Depends(get_db)
) -&gt; models.Item:
    return crud.create_user_item(db=db, item=item, user_id=user_id)


create_item_for_user = app.post(
    &quot;/users/{user_id}/items/&quot;, response_model=schemas.Item
)(create_item_for_user)



What would be the return type of crud.get_user?

That would be models.User because that is the database model and is what the first method of that query returns.
def get_user(db: Session, user_id: int) -&gt; models.User:
    return db.query(models.User).filter(models.User.id == user_id).first()

This is then again returned by the read_user API route function in the same fashion as I explained above for models.Item.
@app.get(&quot;/users/{user_id}&quot;, response_model=schemas.User)
def read_user(user_id: int, db: Session = Depends(get_db)) -&gt; models.User:
    db_user = crud.get_user(db, user_id=user_id)
    ...
    return db_user  # &lt;-- instance of `models.User`

That is, the models.User object is intercepted by the internal function of the decorator and (because of the defined response_model) passed to schemas.User.from_orm, which returns a schemas.User object.
Hope this helps.
"
"Attached is a picture with curved lines, how can you find the Baseline of the text?

The goal is to get lines like I drew by hand in the following picture:

I tried the following code, but letters like g p q y and similar break the line.
import cv2 as cv
import numpy as np

src = cv.imread(&quot;boston_cooking_a.jpg&quot;, cv.IMREAD_GRAYSCALE)
src = cv.adaptiveThreshold(src=src, maxValue=255, blockSize=55, C=11, thresholdType=cv.THRESH_BINARY, adaptiveMethod=cv.ADAPTIVE_THRESH_MEAN_C)
src = cv.dilate(src, cv.getStructuringElement(ksize=(3, 3), shape=cv.MORPH_RECT))
src = cv.erode(src, cv.getStructuringElement(ksize=(50, 3), shape=cv.MORPH_RECT))
src = cv.Sobel(src, ddepth=0, dx=0, dy=1, ksize=5)
cv.imwrite(&quot;test.jpg&quot;, src)
cv.imshow(&quot;src&quot;, src)
cv.waitKey(0)


EDIT:
Attached is another image to test your answer on, so we can make sure the answer doesn't suffer from &quot;overfitting&quot; to a single image.

","I found an approach which is a possibility to find your lines in „pure“ opencv. The  suggested solution is not perfect, but demonstrates a first direction.
Maybe you should use pytesseract to follow up your overall goal ?
In general the suggested solution below is quite
sensitive to the parameters of the first filter A.
The basics pseudo code steps are:

A) apply filters to merge letters to words
B) select contours of words (filter by: ratio heights vs widths , area size)
C) get random points from word-contours using gaussian distribution and the center point centroid of contour
D) use linear regression to find middle line of word-contours
E) merge all word-contours which are neighbors to line-contours (outer middle line points are close together)
F) do polynomial regression 2nd order to estimate middle line of line-contours
G) write the found merged lines from our estimaded group line

The main output for example 2 shows robust output but still has some artifacts from step 1 merge all letter to words.

import cv2
import math
import uuid
import numpy as np
from scipy import stats

def resizeImageByPercentage(img,scalePercent = 60):
    width = int(img.shape[1] * scalePercent / 100)
    height = int(img.shape[0] * scalePercent / 100)
    dim = (width, height)
    # resize image
    return cv2.resize(img, dim, interpolation = cv2.INTER_AREA)

def calcMedianContourWithAndHeigh(contourList):
    hs = list()
    ws = list()
    for cnt in contourList:
        (x, y, w, h) = cv2.boundingRect(cnt)
        ws.append(w)
        hs.append(h)
    return np.median(ws),np.median(hs)

def calcCentroid(contour):
    houghMoments = cv2.moments(contour)
    # calculate x,y coordinate of centroid
    if houghMoments[&quot;m00&quot;] != 0: #case no contour could be calculated
        cX = int(houghMoments[&quot;m10&quot;] / houghMoments[&quot;m00&quot;])
        cY = int(houghMoments[&quot;m01&quot;] / houghMoments[&quot;m00&quot;])
    else:
    # set values as what you need in the situation
        cX, cY = -1, -1
    return cX,cY

def applyDilateImgFilter(img,kernelSize= 3,iterations=1):
    img_bin = 255 - img #invert
    kernel = np.ones((kernelSize,kernelSize),np.uint8)
    img_dilated = cv2.dilate(img_bin, kernel, iterations = iterations)
    return (255- img_dilated) #invert back

def randomColor():
    return tuple(np.random.randint(0, 255, 3).tolist())

def drawGaussianValuesInsideRange(start, end, center, stdDev, amountValues):
    values = []
    if center &lt; 0:
        return values
    if start &gt; end:
        return values
    while len(values) &lt; amountValues:
        valueListPotencial = np.random.normal(center, stdDev, amountValues)
        valueListFiltered = [value for value in valueListPotencial if start &lt;= value &lt;= end]
        values.extend(valueListFiltered)
    return values[:amountValues]

def drawRandomPointsInPolygon(amountPoints, cntFactObj):
    pointList = list()
    if not isinstance(cntFactObj, ContourFacts):
        return pointList
    #we calc basic parameter from random point selection
    horizontalStart = cntFactObj.x
    horizontalEnd = cntFactObj.x + cntFactObj.w
    verticalStart = cntFactObj.y
    verticalEnd = cntFactObj.y + cntFactObj.h  
    #calc std deviation connected to length and ratio
    horitonalStdDeviation = 1 / cntFactObj.ratioHeightoWidth * (horizontalEnd-horizontalStart)
    verticalStdDeviation = 1 / cntFactObj.ratioHeightoWidth * (verticalEnd-verticalStart)
    while len(pointList)&lt;amountPoints:
        if cntFactObj.centoird[0] &lt; 0 or cntFactObj.centoird[1] &lt; 0:
            return pointList
        drawXValues = drawGaussianValuesInsideRange(horizontalStart, horizontalEnd, cntFactObj.centoird[0],
                                          horitonalStdDeviation, amountPoints)
        drawYValues = drawGaussianValuesInsideRange(verticalStart, verticalEnd, cntFactObj.centoird[1], 
                                         verticalStdDeviation, amountPoints)
        #we create the points and check if they are inside the polygon
        for i in range(0,len(drawXValues)):
            #create points
            point = (drawXValues[i],drawYValues[i])
            # check if the point is inside the polygon
            if cv2.pointPolygonTest(cntFactObj.contour, point, False) &gt; 0:
                pointList.append(point)
    return pointList[:amountPoints]

def drawCountourOn(img,contours,color=None):
    imgContour = img.copy()
    for i in range(len(contours)):
        if color is None:
            color = randomColor()
        cv2.drawContours(imgContour, contours, i, color, 2)
    return imgContour

DEBUGMODE = True
fileIn = &quot;bZzzEeCU.jpg&quot;#&quot;269aSnEM.jpg&quot;
img = cv2.imread(fileIn)

## A) apply filters to merge letters to words
# prepare img load
imgGrey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
#gaussian filter
imgGaussianBlur = cv2.GaussianBlur(imgGrey,(3,3),1)
#make binary img, black and white via filter
_, imgBinThres = cv2.threshold(imgGaussianBlur, 140, 230, cv2.THRESH_BINARY)
if DEBUGMODE:
    cv2.imwrite(&quot;img01bw.jpg&quot;,resizeImageByPercentage(imgBinThres,30))

## 3 steps merged by helper class ContourFacts
## B) select contours of words (filter by: ratio heights vs widths , area size)
## C) get random points from wordcontours with gaussian distribution and center point centroid of contour
## D) use linear regression to find middle line of wordcontours

#apply dilate filter to merge letter to words
imgDilated = applyDilateImgFilter(imgBinThres,5,3)
if DEBUGMODE:
    cv2.imwrite(&quot;img02dilated.jpg&quot;,resizeImageByPercentage(imgDilated,30))

# detect contours
contourList, _ = cv2.findContours(imgDilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
if DEBUGMODE:
    imgContour = drawCountourOn(img,contourList)
    cv2.imwrite(&quot;img03contourAll.jpg&quot;,resizeImageByPercentage(imgContour,30))
    
#do a selection of contours by rule
#A) ratio h vs w
#B) area size
mediaWordWidth, medianWordHigh = calcMedianContourWithAndHeigh(contourList)
print(&quot;median word width: &quot;, mediaWordWidth)
print(&quot;median word high: &quot;, medianWordHigh)
contourSelectedByRatio=list()
#we calc for every contour ratio h vs w
ratioThresholdHeightToWidth = 1.1 #thresold ratio should be a least be 1 to 1
# e.g word to --&gt;  10 pixel / 13 pixel

#helper class for contour atrributess
class ContourFacts:
    def __init__(self,contour):
        if contour is None:
            return
        self.uid = uuid.uuid4()
        (self.x, self.y, self.w, self.h) = cv2.boundingRect(contour)
        self.minRect = cv2.minAreaRect(contour)
        self.angle = self.minRect[-1]
        _, (rectWidth, rectHeight), _ = self.minRect
        self.minRectArea = rectWidth * rectHeight
        self.ratioHeightoWidth = self.h / self.w
        self.contour = contour
        self.centoird = calcCentroid(contour)
        self.randomPoinsInCnt = self.DrawRandomPoints()
        if len(self.randomPoinsInCnt) &gt; 0:
            (self.bottomSlope, self.bottomIntercept) = self.EstimateCenterLineViaLinearReg()
            self.bottomMinX = min([x for x,y in self.randomPoinsInCnt])
            self.bottomMaxX = max([x for x,y in self.randomPoinsInCnt])

    def EstimateCenterLineViaLinearReg(self):
        if self.contour is None:
            return (0,0)
        slope = 0
        intercept = 0
        #model = slope (x) + intercept
        xValues = [x for x,y in self.randomPoinsInCnt]
        yValues = [y for x,y in self.randomPoinsInCnt]
        if len(xValues) &lt; 2:
            return (0,0)
        elif len(xValues) ==2:
            #we calc a line with 2 points
            # y = m*x + b
            deltaX = xValues[1]-xValues[0]
            if deltaX == 0:
                return (0,0)
            slope = (yValues[1]-yValues[0])/(deltaX)
            intercept = yValues[0] - (slope*xValues[0])
        else:
            #normal linear regression above 2 points
            slope, intercept, r, p, std_err = stats.linregress(xValues, yValues)
        #TODO check std_err
        return slope, intercept
    
    def DrawRandomPoints(self,pointFactor=2):
        pointList = list()
        #calc area to amount point relation  -&gt; bigger area more points
        amountPointsNeeded = int(self.minRectArea/pointFactor)
        pointList = drawRandomPointsInPolygon(amountPointsNeeded,self)
        return pointList
    
    def GetCenterLineLeftCorner(self):
        if self.contour is None or len(self.randomPoinsInCnt) == 0:
            return (0,0)    
        # calc via  y = m*x + b with min
        return (int(self.bottomMinX), int(self.bottomSlope*self.bottomMinX + self.bottomIntercept))
    def GetCenterLineRightCorner(self):
        if self.contour is None or len(self.randomPoinsInCnt) == 0:
            return (0,0)    
        # calc via via y = m*x + b with max
        return (int(self.bottomMaxX), int(self.bottomSlope*self.bottomMaxX + self.bottomIntercept))
    def __eq__(self, other):
        if isinstance(other, ContourFacts):
            return self.uid == other.uid
        return False
    def __hash__(self):
        return hash(self.uid)



#calc mean area size from area size
vectorOfAreaSize = np.array([cv2.contourArea(cnt) for cnt in contourList])
meanAreaSize = np.mean(vectorOfAreaSize)
print(&quot;mean area size: &quot;, meanAreaSize)
stdDevAreaSize = np.std(vectorOfAreaSize)
print(&quot;std dev area size: &quot;, stdDevAreaSize)
thresoldDiffAreaSize = stdDevAreaSize/4
#we iterate all contours and select by ratio and size
for cnt in contourList:
    #construct helper class instance
    contourFactObj = ContourFacts(cnt)
    #calc abs diff to mean area size
    diffArea = abs(cv2.contourArea(cnt) - meanAreaSize)
    if contourFactObj.ratioHeightoWidth &lt; ratioThresholdHeightToWidth and diffArea &lt; (thresoldDiffAreaSize):
        contourSelectedByRatio.append(contourFactObj)

#debug print 
if DEBUGMODE:
    #we print words
    imgContourSelection = img.copy() 
    for cnt in contourSelectedByRatio:
        contourColor = randomColor()
        imgContourSelection = drawCountourOn(imgContourSelection,[cnt.contour],contourColor)
        #we print centroid 
        cv2.circle(imgContourSelection, cnt.centoird, 5, (0, 0, 255), -1)
        p1 = cnt.GetCenterLineLeftCorner()
        p2 = cnt.GetCenterLineRightCorner()
        if p1 != (0,0) or p2 != (0,0):
            cv2.circle(imgContourSelection, p1, 5, (0, 0, 255), -1)
            cv2.circle(imgContourSelection, p2, 5, (0, 0, 255), -1)
            cv2.line(imgContourSelection, p1, p2, (0, 255, 0), 2)
    cv2.imwrite(&quot;img04contourSelection.jpg&quot;,resizeImageByPercentage(imgContourSelection,30))


## E) merge all wordcontours which are neighbours to linecontours (outer middle line points are close together)  
#define distance function, differences in height is negativ weighted
def euclidianDistanceWithNegativHeightWeight(cnt1,cnt2,negativeHeightWeight=2.0):
    if cnt1 is None or cnt2 is None:
        return 1000000
    if not isinstance(cnt1, ContourFacts) or not isinstance(cnt2, ContourFacts):
        return 1000000
    p1 = cnt1.GetCenterLineRightCorner()
    p2 = cnt2.GetCenterLineLeftCorner()
    return math.sqrt((p2[0] - p1[0])**2 + (negativeHeightWeight*(p2[1] - p1[1]))**2)

# helper class to group contours
class ContourGroup:
    def __init__(self):
        self.uuid = uuid.uuid4()
        self.contourList = list()
    def GetLastElement(self):
        if len(self.contourList) == 0:
            return None
        return self.contourList[-1]
    def Add(self,cnt):
        self.contourList.append(cnt)   
    def __eq__(self, other):
        if isinstance(other, ContourGroup):
            return self.uuid == other.uuid
        return False
    
groupMap = dict()
lineGroupList = list()
## we grouping the contours to lines
maxDistanceThresholNextWord= medianWordHigh *0.9 #TODO get better estimate
#recursive function to get nearest neighbors
def getNearestNeighbors(cnt1,depthCounter,contourSelectedByRatio,maxDistanceThresholNextWord):
    maxDepth = 10 #var for max recursion depth 
    nearestCnt = None
    nearestDist = maxDistanceThresholNextWord
    for j in range(0,len(contourSelectedByRatio)):
        cnt2 = contourSelectedByRatio[j]
        if cnt1 == cnt2:#skip same
            continue
        dist = euclidianDistanceWithNegativHeightWeight(cnt1,cnt2)
        if dist &lt; nearestDist:
            nearestDist = dist
            nearestCnt = cnt2
    if nearestCnt is not None:#call recursive
        nearaestListWeHave = [nearestCnt] #new list
        depthCounter += 1
        if depthCounter &lt; maxDepth:# all to call
            nearListWeGet =getNearestNeighbors(nearestCnt,depthCounter,contourSelectedByRatio,maxDistanceThresholNextWord)
            if nearListWeGet is None:
                return nearaestListWeHave
            else:
                nearListWeGet.extend(nearaestListWeHave)   
                return nearListWeGet
        else:#limit reached of recursion skip
            return nearaestListWeHave
    else:      
        return None
## E) merge all wordcontours which are neighbours to linecontours (outer middle line points are close together)      
#we group all contours
for i in range(0,len(contourSelectedByRatio)):
    cnt1 = contourSelectedByRatio[i]
    if cnt1 in groupMap:
        continue
    lineGroup = ContourGroup()
    lineGroup.Add(cnt1)
    groupMap[cnt1] = lineGroup
    depthCounter = 0
    nearaestList = getNearestNeighbors(cnt1,depthCounter,
                                       contourSelectedByRatio,maxDistanceThresholNextWord)
    if nearaestList is None:
        lineGroupList.append(lineGroup) #no neighbor found
        continue
    for cnt in nearaestList:
        groupMap[cnt] = lineGroup
        lineGroup.Add(cnt)
    lineGroupList.append(lineGroup)

if DEBUGMODE:
    imgContourGroup = img.copy()
    for group in lineGroupList:
        #print(f&quot;group({group.uuid} size: {len(group.contourList)}&quot;)
        #we print all corner points
        for cnt in group.contourList:
            leftCorner = cnt.GetCenterLineLeftCorner()
            rigthCorner = cnt.GetCenterLineRightCorner()
            cv2.circle(imgContourGroup, leftCorner, 5, (0, 0, 255), -1)
            cv2.circle(imgContourGroup, rigthCorner, 5, (140, 0, 0), -1)
        #we print estimated underlines
        for cnt in group.contourList:
            leftCorner = cnt.GetCenterLineLeftCorner()
            rigthCorner = cnt.GetCenterLineRightCorner()
            cv2.line(imgContourGroup, leftCorner, rigthCorner, (0, 255, 0), 2)
        # we print all contours
        groupColor = randomColor()
        cntList = [cnt.contour for cnt in group.contourList]
        imgContourGroup = drawCountourOn(imgContourGroup,cntList,groupColor)
    cv2.imwrite(&quot;img05contourGroup.jpg&quot;,resizeImageByPercentage(imgContourGroup,30))

## F) do polynomial regression 2nd order to estimate middle line of linecontours
# calc line from stable group points
minAmountRegressionElements = 12
movingWindowSize = 3
letterCenterOffset = medianWordHigh * 0.5
lineListCollection = list()
for group in lineGroupList:
    stablePoints = list()
    for cnt in group.contourList:
        stablePoints.extend(cnt.randomPoinsInCnt)
    if len(stablePoints) &gt;= minAmountRegressionElements :
        xValues = [x for x,y in stablePoints]
        yValues = [y for x,y in stablePoints]
        # perform polynomial regression of degree 2
        coefffientValues = np.polyfit(np.array(xValues), np.array(yValues), 2)
        # create a polynomial function with the coefficients
        polynomial = np.poly1d(coefffientValues)
        #we filter to build something like a line
        xValuesNewLineFilter = list()
        xMin =int( min(xValues))
        xMax = int(max(xValues))
        for xNew in range(xMin,xMax,movingWindowSize):
                xValuesNewLineFilter.append(xNew)
        #we predict new points with all old x values
        yValuesNew = polynomial(xValuesNewLineFilter)
        yValuesNewHighCorrect =np.array(yValuesNew) + letterCenterOffset
        lineList = list()
        #we create a list of points
        for i in range(0,len(xValuesNewLineFilter)):
            pointInt = (int(xValuesNewLineFilter[i]),int(yValuesNewHighCorrect[i]))
            lineList.append(pointInt)
        lineListCollection.append(lineList)
## G) write the lines 
imgLines = img.copy()
for lineList in lineListCollection:
    p1 = lineList[0]
    for j in range(1,len(lineList)):
        p2 = lineList[j]
        #cv2.circle(imgLines, p2Int, 5, (0, 0, 255), -1)
        cv2.line(imgLines, p1, p2, (0, 255, 0), 2)
        p1 = p2
cv2.imwrite(&quot;img06Lines.jpg&quot;,resizeImageByPercentage(imgLines,30))

if DEBUGMODE:
    cv2.waitKey(0)

more debug output is:



The picture below shows word contours with green middle lines and red outer points for neighborhood analysis.


"
"I have the following model
class Window(BaseModel):
    size: tuple[int, int]

and I would like to instantiate it like this:
fields = {'size': '1920x1080'}
window = Window(**fields)

Of course this fails since the value of 'size' is not of the correct type. However, I would like to add logic so that the value is split at x, i.e.:
def transform(raw: str) -&gt; tuple[int, int]:
    x, y = raw.split('x')
    return int(x), int(y)

Does Pydantic support this?
","Pydantic 2.x (edit)
Pydantic 2.0 introduced the field_validator decorator which lets you implement such a behaviour in a very simple way. Given the original parsing function:
from pydantic import BaseModel, field_validator

class Window(BaseModel):
    size: tuple[int, int]

    @field_validator(&quot;size&quot;, mode=&quot;before&quot;)
    @classmethod
    def transform(cls, raw: str) -&gt; tuple[int, int]:
        x, y = raw.split(&quot;x&quot;)
        return int(x), int(y)

Note:

The validator method is a class method, as denoted by the cls first argument. Implementing it as an instance method (with self) will raise an error.
The mode=&quot;before&quot; in the decorator is critical here, as expected this is what makes the method run before checking &quot;size&quot; is a tuple.


Pydantic 1.x (original answer)
You can implement such a behaviour with pydantic's validator. Given your predefined function:
def transform(raw: str) -&gt; tuple[int, int]:
    x, y = raw.split('x')
    return int(x), int(y)

You can implement it in your class like this:
from pydantic import BaseModel, validator


class Window(BaseModel):
    
    size: tuple[int, int]
    _extract_size = validator('size', pre=True, allow_reuse=True)(transform)


Note the pre=True argument passed to the validator. It means that it will be run before the default validator that checks if size is a tuple.

Now:
fields = {'size': '1920x1080'}
window = Window(**fields)
print(window)
# output: size=(1920, 1080)

Note that after that, you won't be able to instantiate your Window with a tuple for size.
fields2 = {'size': (800, 600)}
window2 = Window(**fields2)
# AttributeError: 'tuple' object has no attribute 'split'

In order to overcome that, you could simply bypass the function if a tuple is passed by altering slightly your code:
Pydantic 2.x
class Window(BaseModel):
    size: tuple[int, int]

    @field_validator(&quot;size&quot;, mode=&quot;before&quot;)
    def transform(cls, raw: str | tuple[int, int]) -&gt; tuple[int, int]:
        if isinstance(raw, tuple):
            return raw
        x, y = raw.split(&quot;x&quot;)
        return int(x), int(y)

Pydantic 1.x
def transform(raw: str | tuple[int, int]) -&gt; tuple[int, int]:
    if isinstance(raw, tuple):
        return raw
    x, y = raw.split('x')
    return int(x), int(y)

class Window(BaseModel):

    size: tuple[int, int]
    _extract_size = validator('size', pre=True, allow_reuse=True)(transform)

Which should give:
fields2 = {'size': (800, 600)}
window2 = Window(**fields2)
print(window2)
# output: size:(800, 600)

"
"I have some log data like:
logs = [
 {'id': '1234', 'error': None, 'fruit': 'orange'},
 {'id': '12345', 'error': None, 'fruit': 'apple'}
]

Each dict has the same keys: 'id', 'error' and 'fruit' (in this example).
I want to remove duplicates from this list, but straightforward dict and set based approaches do not work because my elements are themselves dicts, which are not hashable:
&gt;&gt;&gt; set(logs)
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
TypeError: unhashable type: 'dict'

Another approach is to sort and use itertools.groupby - but dicts are also not comparable, so this also does not work:
&gt;&gt;&gt; from itertools import groupby
&gt;&gt;&gt; [k for k, _ in groupby(sorted(logs))]
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
TypeError: '&lt;' not supported between instances of 'dict' and 'dict'

I had the idea to calculate a hash value for each log entry, and store it in a set for comparison, like so:
def compute_hash(log_dict: dict):
    return hash(log_dict.values())

def deduplicate(logs):
    already_seen = set()
    for log in logs:
        log_hash = compute_hash(log)
        if log_hash in already_seen:
            continue
        already_seen.add(log_hash)
        yield log

However, I found that compute_hash would give the same hash for different dictionaries, even ones with completely bogus contents:
&gt;&gt;&gt; logs = [{'id': '123', 'error': None, 'fruit': 'orange'}, {}]
&gt;&gt;&gt; # The empty dict will be removed; every dict seems to get the same hash.
&gt;&gt;&gt; list(deduplicate(logs))
[{'id': '123', 'error': None, 'fruit': 'orange'}]

After some experimentation, I was seemingly able to fix the problem by modifying compute_hash like so:
def compute_hash(log_dict: dict):
    return hash(frozenset(log_dict.values()))

However, I cannot understand why this makes a difference. Why did the original version seem to give the same hash for every input dict? Why does converting the .values result to a frozenset first fix the problem?
Aside from that: is this algorithm correct? Or is there some counterexample where the wrong values will be removed?

This question discusses how hashing works in Python, in depth, as well as considering other data structures that might be more appropriate than dictionaries for the list elements. See List of unique dictionaries instead if you simply want to remove duplicates from a list of dictionaries.
","What went wrong
The first thing I want to point out about the original attempt is that it seems over-engineered. When the inputs are hashable, manually iterating is only necessary to preserve order, and even then, in 3.7 and up we can rely on the order-preserving property of dicts.
Just because it's hashable doesn't mean the hash is useful
It also isn't especially useful to call hash on log_dict.values(). While log_dict is not hashable, its .values() (in 3.x) is an instance of the dict_values type (the name is not defined in the builtins, but that is how instances identify themselves), which is hashable:
&gt;&gt;&gt; dv = {1:2, 3:4}.values()
&gt;&gt;&gt; dv
dict_values([2, 4])
&gt;&gt;&gt; {dv}
{dict_values([2, 4])}

So we could just as easily have used the .values() directly as a &quot;hash&quot;:
def compute_hash(log_dict: dict):
    return log_dict.values()

... but this would have given a new bug - now every hash would be different:
&gt;&gt;&gt; {1:2}.values() == {1:2}.values()
False

But why?
Because dict_values type doesn't define __hash__, nor __eq__. object is the immediate superclass, so calls to those methods fall back to the object defaults:
&gt;&gt;&gt; dv.__class__.__bases__
(&lt;class 'object'&gt;,)
&gt;&gt;&gt; dv.__class__.__hash__
&lt;slot wrapper '__hash__' of 'object' objects&gt;
&gt;&gt;&gt; dv.__class__.__eq__
&lt;slot wrapper '__eq__' of 'object' objects&gt;

In fact, dict_values cannot sensibly implement these methods because it is (indirectly) mutable - as a view, it is dependent on the underlying dict:
&gt;&gt;&gt; d = {1:2}
&gt;&gt;&gt; dv = d.values()
&gt;&gt;&gt; d[3] = 4
&gt;&gt;&gt; dv
dict_values([2, 4])

Since there isn't an obvious generic way to hash any object that also isn't exceedingly slow, while also caring about its actual attributes, the default simply doesn't care about attributes and is simply based on object identity. For example, on my platform, the results look like:
Python 3.8.10 (default, Nov 14 2022, 12:59:47) 
[GCC 9.4.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; dv = {1:2, 3:4}.values()
&gt;&gt;&gt; bin(id(dv))
'0b11111110101110011010010110000001010101011110000'
&gt;&gt;&gt; bin(hash(dv))
'0b1111111010111001101001011000000101010101111'

In other words:
&gt;&gt;&gt; hash(dv) == id(dv) // 16
True

Thus, if compute_hash in the original code is repeatedly called with temporary objects, it won't give useful results - the results don't depend on the contents of the object, and will commonly be the same, as temporary (i.e., immediately GCd) objects in a loop will often end up in the same memory location.
(Yes, this means that objects default to being hashable and equality-comparable. The dict type itself overrides __hash__ to explicitly disallow it, while - curiously - overriding __eq__ to compare contents.)
frozenset has a useful hash
On the other hand, frozenset is intended for long-term storage of some immutable data. Consequently, it's important and useful for it to define a __hash__, and it does:
&gt;&gt;&gt; f = frozenset(dv)
&gt;&gt;&gt; bin(id(f))
'0b11111110101110011010001011101000110001011100000'
&gt;&gt;&gt; bin(hash(f))
'0b101111010001101001001111100001000001100111011101101100000110001'

Dictionaries, hashing and collision detection
Although there have been many tweaks and optimizations over the years, Pythons dict and set types are both fundamentally based on hash tables. When a value is inserted, its hash is first computed (normally an integer value), and then that value is reduced (normally using modulo) into an index into the underlying table storage. Similarly, when a value is looked up, the hash is computed and reduced in order to determine where to look in the table for that value.
Of course, it is possible that some other value is already stored in that spot. There are multiple possible strategies for dealing with this (and last I checked, the literature is inconsistent about naming them). But most importantly for our purposes: when looking up a value in a dict by key, or checking for the presence of a value in a set, the container will also have to do equality checks after figuring out where to look, in order to confirm that the right thing has actually been found.
Consequently, any approach that simply computes a hash manually, and naively associates those hashes with the original values, will fail. It is easy for two of the input dicts to have the same computed hash value, even if their contents are actually being considered. For example, the hash of a frozenset is based on an XOR of hashes for the elements. So if two of our input dicts had all the same values assigned to keys in a different order, the hash would be the same:
&gt;&gt;&gt; def show_hash(d):
...     return bin(hash(frozenset(d.values())))
... 
&gt;&gt;&gt; show_hash({'id': '1', 'error': None, 'value': 'apple'})
'0b101010010100001000111001000001000111101111110100010000010101110'
&gt;&gt;&gt; # Changing a value changes the hash...
&gt;&gt;&gt; show_hash({'id': '1', 'error': None, 'value': 'orange'})
'0b11111111001000011101011001001011100010100100010010110000100100'
&gt;&gt;&gt; # but rearranging them does not:
&gt;&gt;&gt; show_hash({'id': '1', 'error': 'orange', 'value': None})
'0b11111111001000011101011001001011100010100100010010110000100100'

It's also possible for such a hash collision to occur by coincidence with totally unrelated values. It's extremely unlikely for 64-bit hashes (since this value will not be reduced and used as a hash table index, despite the name)
Fixing it explicitly
So, in order to have correct code, we would need to do our own checking afterwards, explicitly checking whether the value which hashed to something in our already_seen set was actually equal to previous values that had that hash. And there could theoretically be multiple of those, so we'd have to remember multiple values for each of those external hashes, perhaps by using a dict for already_seen instead. Something like:
from collections import defaultdict

def deduplicate(logs):
    already_seen = defaultdict(list)
    for log in logs:
        log_hash = compute_hash(log)
        if log in already_seen.get(log_hash, ()):
            continue
        already_seen[log_hash].append(log)
        yield log

Hopefully this immediately looks unsatisfactory. With this approach, we are essentially re-implementing the core logic of sets and dictionaries - we compute hashes ourselves, retrieve corresponding values from internal storage (already_seen) and then manually check for equality (if log in ...).
Looking at it from another angle
The reason we're doing all of this in the first place - looking for a hash value to represent the original dict in our own storage - is because the dict isn't hashable. But we could address that problem head-on, instead, by explicitly converting the data into a hashable form (that preserves all the information), rather than trying to relate a hashable value to the data.
In other words, let's use a different type to represent the data, rather than a dict.
Since all our input dicts have the same keys, the natural thing to do would be to convert those into the attributes of a user-defined class. In 3.7 and up, a simple, natural and explicit way to do this is using a dataclass, like so:
from dataclasses import dataclass
from typing import Optional

@dataclass(frozen=True, slots=True)
class LogEntry:
    id: str
    error: Optional[str]
    fruit: str

It's not explained very well in the documentation, but using frozen=True (the main purpose is to make the instances immutable) will cause a __hash__ to be generated as well, taking the fields into account as desired. Using slots=True causes __slots__ to be generated for the type as well, avoiding memory overhead.
From here, it's trivial to convert the existing logs:
logs = [LogEntry(**d) for d in logs]

And we can directly deduplicate with a set:
set(logs)

or, preserving order using a dict (in 3.7 and up):
list(dict.fromkeys(logs))

There are other options, of course. The simplest is to make a tuple from the .values - assuming each log dict has its keys in the same order (again, assuming Python 3.7 and up, where keys have an order), this preserves all the useful information - the .keys are just for convenience. Slightly more sophisticated, we could use collections.namedtuple:
from collections import namedtuple

LogEntry = namedtuple('LogEntry', 'id error fruit')
# from here, use the LogEntry type as before

This is simpler than the dataclass approach, but less explicit (and doesn't offer an elegant way to document field types).
"
"Is there any way to get the response content in a middleware?
The following code is a copy from here.
@app.middleware(&quot;http&quot;)
async def add_process_time_header(request: Request, call_next):
    start_time = time.time()

    response = await call_next(request)

    process_time = time.time() - start_time
    response.headers[&quot;X-Process-Time&quot;] = str(process_time)
    return response

","The response body is an iterator, which once it has been iterated through, it cannot be re-iterated again. Thus, you either have to save all the iterated data to a list (or bytes variable) and use that to return a custom Response, or initiate the iterator again. The options below demonstrate both approaches. In case you would like to get the request body inside the middleware as well, please have a look at this answer.
Option 1
Save the data to a list and use iterate_in_threadpool to initiate the iterator again, as described here—which is what StreamingResponse uses, as shown here.
from starlette.concurrency import iterate_in_threadpool

@app.middleware(&quot;http&quot;)
async def some_middleware(request: Request, call_next):
    response = await call_next(request)
    response_body = [chunk async for chunk in response.body_iterator]
    response.body_iterator = iterate_in_threadpool(iter(response_body))
    print(f&quot;response_body={response_body[0].decode()}&quot;)
    return response

Note 1: If your code uses StreamingResponse, response_body[0] would return only the first chunk of the response. To get the entire response body, you should join that list of bytes (chunks), as shown below (.decode() returns a string representation of the bytes object):
print(f&quot;response_body={(b''.join(response_body)).decode()}&quot;)

Note 2: If you have a StreamingResponse streaming a body that wouldn't fit into your server's RAM (for example, a response of 30GB), you may run into memory errors when iterating over the response.body_iterator (this applies to both options listed in this answer), unless you loop through response.body_iterator (as shown in Option 2), but instead of storing the chunks in an in-memory variable, you store it somewhere on the disk. However, you would then need to retrieve the entire response data from that disk location and load it into RAM, in order to send it back to the client (which could extend the delay in responding to the client even more)—in that case, you could load the contents into RAM in chunks and use StreamingResponse, similar to what has been demonstrated here, here, as well as here, here and here (in Option 1, you can just pass your iterator/generator function to iterate_in_threadpool). However, I would not suggest following that approach, but instead have such endpoints returning large streaming responses excluded from the middleware, as described in this answer.
Option 2
The below demosntrates another approach, where the response body is stored in a bytes object (instead of a list, as shown above), and is used to return a custom Response directly (along with the status_code, headers and media_type of the original response).
@app.middleware(&quot;http&quot;)
async def some_middleware(request: Request, call_next):
    response = await call_next(request)
    chunks = []
    async for chunk in response.body_iterator:
        chunks.append(chunk)
    response_body = b''.join(chunks)
    print(f&quot;response_body={response_body.decode()}&quot;)
    return Response(content=response_body, status_code=response.status_code, 
        headers=dict(response.headers), media_type=response.media_type)

"
"I'd like to construct an object that works like a random number generator, but generates numbers in a specified sequence.
# a random number generator
rng = lambda : np.random.randint(2,20)//2

# a non-random number generator
def nrng():
    numbers = np.arange(1,10.5,0.5)
    for i in range(len(numbers)):
        yield numbers[i]

for j in range(10):
    print('random number', rng())
    print('non-random number', nrng())

The issue with the code above that I cannot call nrng in the last line because it is a generator. I know that the most straightforward way to rewrite the code above is to simply loop over the non-random numbers instead of defining the generator. I would prefer getting the example above to work because I am working with a large chunk of code that include a function that accepts a random number generator as an argument, and I would like to add the functionality to pass non-random number sequences without rewriting the entire code.
EDIT: I see some confusion in the comments. I am aware that python's random number generators generate pseudo-random numbers. This post is about replacing a pseudo-random-number generator by a number generator that generates numbers from a non-random, user-specified sequence (e.g., a generator that generates the number sequence 1,1,2,2,1,0,1 if I want it to).
","Edit:
The cleanest way to do this would be to use a lambda to wrap your call to next(nrng) as per great comment from @GACy20:
def nrng_gen():
    yield from range(10)

nrng = nrng_gen()

nrng_func = lambda: next(nrng)

for i in range(10):
    print(nrng_func())

Original answer:
If you want your object to keep state and look like a function, create a custom class with __call__ method.
eg.
class NRNG:
    def __init__(self):
        self.numbers = range(10)
        self.state = -1
    def __call__(self):
        self.state += 1
        return self.numbers[self.state]
        
nrng = NRNG()


for i in range(10):
    print(nrng())

However, I wouldn't recommend this unless absolutely necessary, as it obscures the fact that your nrng keeps a state (although technically, most rngs keep their state internally).
It's best to just use a regular generator with yield by calling next on it or to write a custom iterator (also class-based). Those will work with things like for loops and other python tools for iteration (like the excellent itertools package).
"
"I have been familiarizing with pytest lately and on how you can use conftest.py to define fixtures that are automatically discovered and imported within my tests. It is pretty clear to me how conftest.py works and how it can be used, but I'm not sure about why this is considered a best practice in some basic scenarios.
Let's say my tests are structured in this way:
tests/
--test_a.py
--test_b.py

The best practice, as suggested by the documentation and various articles about pytest around the web, would be to define a conftest.py file with some fixtures to be used in both test_a.py and test_b.py. In order to better organize my fixtures, I might have the need of splitting them into separate files in a semantically meaningful way, ex. db_session_fixtures.py, dataframe_fixtures.py, and then import them as plugins in conftest.py.
tests/
--test_a.py
--test_b.py
--conftest.py
--db_session_fixtures.py
--dataframe_fixtures.py

In conftest.py I would have:
import pytest
    
pytest_plugins = [&quot;db_session_fixtures&quot;, &quot;dataframe_fixtures&quot;]

and I would be able to use db_session_fixtures and dataframe_fixtures seamlessly in my test cases without any additional code.
While this is handy, I feel it might hurt readability. For example, if I would not use conftest.py as described above, I might write in test_a.py
from .dataframe_fixtures import my_dataframe_fixture

def test_case_a(my_dataframe_fixture):
   #some tests

and use the fixtures as usual.
The downside is that it requires me to import the fixture, but the explicit import improves the readability of my test case, letting me know in a glance where the fixture come from, just as any other python module.
Are there downsides I am overlooking on about this solution or other advantages that conftest.py brings to the table, making it the best practice when setting up pytest test suites?
","There's not a huge amount of difference, it's mainly just down to preference. I mainly use conftest.py to pull in fixures that are required, but not directly used by your test. So you may have a fixture that does something useful with a database, but needs a database connection to do so. So you make the db_connection fixture available in conftest.py, and then your test only has to do something like:
conftest.py
from tests.database_fixtures import db_connection

__all__ = ['db_connection']

tests/database_fixtures.py
import pytest

@pytest.fixture
def db_connection():
    ...

@pytest.fixture
def new_user(db_connection):
    ...

test/test_user.py
from tests.database_fixtures import new_user

def test_user(new_user):
    assert new_user.id &gt; 0  # or whatever the test needs to do

If you didn't make db_connection available in conftest.py or directly import it then pytest would fail to find the db_connection fixture when trying to use the new_user fixture. If you directly import db_connection into your test file, then linters will complain that it is an unused import. Worse, some may remove it, and cause your tests to fail. So making the db_connection available in conftest.py, to me, is the simplest solution.
Overriding Fixtures
The one significant difference is that it is easier to override fixtures using conftest.py. Say you have a directory layout of:
./
├─ conftest.py
└─ tests/
   ├─ test_foo.py
   └─ bar/
      ├─ conftest.py
      └─ test_foobar.py

In conftest.py you could have:
import pytest

@pytest.fixture
def some_value():
    return 'foo'

And then in tests/bar/conftest.py you could have:
import pytest

@pytest.fixture
def some_value(some_value):
    return some_value + 'bar'

Having multiple conftests allows you to override a fixture whilst still maintaining access to the original fixture. So following tests would all work.
tests/test_foo.py
def test_foo(some_value):
    assert some_value == 'foo'

tests/bar/test_foobar.py
def test_foobar(some_value):
    assert some_value == 'foobar'

You can still do this without conftest.py, but it's a bit more complicated. You'd need to do something like:
import pytest

# in this scenario we would have something like:
#   mv contest.py tests/custom_fixtures.py
from tests.custom_fixtures import some_value as original_some_value

@pytest.fixture
def some_value(original_some_value):
    return original_some_value + 'bar'

def test_foobar(some_value):
    assert some_value == 'foobar'

"
"As the title says, I am trying to generate a refresh token, and then I would like to use the refresh token to get short lived Access tokens.
There is a problem though, in that I'm not smart enough to understand the docs on the dropbox site, and all the other information I've found hasn't worked for me
(A, B, C) or is in a language I don't understand.
I have tried out all three examples from the github page, as well as user code from other questions on this site.
I haven't got anything to work.
The most I got was

Error: 400 Client Error: Bad Request for url: api.dropboxapi.com/oauth2/token

and

dropbox.rest.RESTSocketError: Error connecting to &quot;api.dropbox.com&quot;: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1123)

:(
","Here is how I did it. I'll try to keep it simple and precise
Replace &lt;APP_KEY&gt; with your dropbox app key in the below Authorization URL
https://www.dropbox.com/oauth2/authorize?client_id=&lt;APP_KEY&gt;&amp;token_access_type=offline&amp;response_type=code
Complete the code flow on the Authorization URL. You will receive an AUTHORIZATION_CODE at the end.
Go to Postman and create a new POST request with below configuration

Request URL- https://api.dropboxapi.com/oauth2/token
Authorization -&gt; Type = Basic Auth -&gt; Username = &lt;APP_KEY&gt; , Password = &lt;APP_SECRET&gt;
(Refer this answer for cURL -u option)
Body -&gt; Select &quot;x-www-form-urlencoded&quot;





Key
Value




code
&lt;AUTHORIZATION_CODE&gt;


grant_type
authorization_code




After you send the request, you will receive JSON payload containing refresh_token.
{
    &quot;access_token&quot;: &quot;sl.****************&quot;,
    &quot;token_type&quot;: &quot;bearer&quot;,
    &quot;expires_in&quot;: 14400,
    &quot;refresh_token&quot;: &quot;*********************&quot;,
    &quot;scope&quot;: &lt;SCOPES&gt;,
    &quot;uid&quot;: &quot;**********&quot;,
    &quot;account_id&quot;: &quot;***********************&quot;
}

In your python application,
import dropbox

dbx = dropbox.Dropbox(
            app_key = &lt;APP_KEY&gt;,
            app_secret = &lt;APP_SECRET&gt;,
            oauth2_refresh_token = &lt;REFRESH_TOKEN&gt;
        )

Hope this works for you too!
"
"This question is probably me not understanding architecture of (new) sqlalchemy, typically I use code like this:
query = select(models.Organization).where(
    models.Organization.organization_id == organization_id
)
result = await self.session.execute(query)

return result.scalars().all()

Works fine, I get a list of models (if any).
With a query with specific columns only:
query = (
    select(
        models.Payment.organization_id,
        models.Payment.id,
        models.Payment.payment_type,
    )
    .where(
        models.Payment.is_cleared.is_(True),
    )
    .limit(10)
)

result = await self.session.execute(query)

return result.scalars().all()

I am getting first row, first column only. Same it seems to:   https://docs.sqlalchemy.org/en/14/core/connections.html?highlight=scalar#sqlalchemy.engine.Result.scalar
My understanding so far was that in new sqlalchemy we should always call scalars() on the query, as described here: https://docs.sqlalchemy.org/en/14/changelog/migration_20.html#migration-orm-usage
But with specific columns, it seems we cannot use scalars() at all. What is even more confusing is that result.scalars() returns sqlalchemy.engine.result.ScalarResult that has fetchmany(), fechall() among other methods that I am unable to iterate in any meaningful way.
My question is, what do I not understand?
","
My understanding so far was that in new sqlalchemy we should always call scalars() on the query

That is mostly true, but only for queries that return whole ORM objects.
Just a regular .execute()
query = select(Payment)

results = sess.execute(query).all()
print(results)  # [(Payment(id=1),), (Payment(id=2),)]
print(type(results[0]))  # &lt;class 'sqlalchemy.engine.row.Row'&gt;

returns a list of Row objects, each containing a single ORM object. Users found that awkward since they needed to unpack the ORM object from the Row object. So .scalars() is now recommended
results = sess.scalars(query).all()
print(results)  # [Payment(id=1), Payment(id=2)]
print(type(results[0]))  # &lt;class '__main__.Payment'&gt;

However, for queries that return individual attributes (columns) we don't want to use .scalars() because that will just give us one column from each row, normally the first column
query = select(
    Payment.id,
    Payment.organization_id,
    Payment.payment_type,
)

results = sess.scalars(query).all()
print(results)  # [1, 2]

Instead, we want to use a regular .execute() so we can see all the columns
results = sess.execute(query).all()
print(results)  # [(1, 123, None), (2, 234, None)]

Notes:

.scalars() is doing the same thing in both cases: return a list containing a single (scalar) value from each row (default is index=0).

sess.scalars() is the preferred construct. It is simply shorthand for sess.execute().scalars().


"
"Currently when I try to retrieve date from a polars datetime column, I have to write something similar to:
import polars as pl
import datetime as dt

df = pl.DataFrame({
    'time': [dt.datetime.now()]
})

df = df.with_columns(
    pl.col(&quot;time&quot;).map_elements(lambda x: x.date()).alias(&quot;date&quot;)
)

shape: (1, 2)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ time                       â”† date       â”‚
â”‚ ---                        â”† ---        â”‚
â”‚ datetime[Î¼s]               â”† date       â”‚
â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ 2024-07-20 11:41:04.265539 â”† 2024-07-20 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Is there a different way, something closer to:
pl.col(&quot;time&quot;).dt.date().alias(&quot;date&quot;)

","You can use .dt.date()
import datetime
import polars as pl

df = pl.DataFrame({
    &quot;time&quot;: [datetime.datetime.now()]
})

df.with_columns(
    pl.col(&quot;time&quot;).dt.date().alias(&quot;date&quot;)
)

shape: (1, 2)
┌────────────────────────────┬────────────┐
│ time                       ┆ date       │
│ ---                        ┆ ---        │
│ datetime[μs]               ┆ date       │
╞════════════════════════════╪════════════╡
│ 2024-07-21 16:17:41.489579 ┆ 2024-07-21 │
└────────────────────────────┴────────────┘

"
"This is the simplified version of my code:
main is a coroutine which stops after the second iteration.
get_numbers is an async generator which yields numbers but within an async context manager.
import asyncio


class MyContextManager:
    async def __aenter__(self):
        print(&quot;Enter to the Context Manager...&quot;)
        return self

    async def __aexit__(self, exc_type, exc_value, exc_tb):
        print(exc_type)
        print(&quot;Exit from the Context Manager...&quot;)
        await asyncio.sleep(1)
        print(&quot;This line is not executed&quot;)  # &lt;-------------------
        await asyncio.sleep(1)


async def get_numbers():
    async with MyContextManager():
        for i in range(30):
            yield i


async def main():
    async for i in get_numbers():
        print(i)
        if i == 1:
            break


asyncio.run(main())

And the output is:
Enter to the Context Manager...
0
1
&lt;class 'asyncio.exceptions.CancelledError'&gt;
Exit from the Context Manager...

I have two questions actually:

From my understanding, AsyncIO schedules a Task to be called soon in the next cycle of the event loop and gives __aexit__ a chance to execute. But the line print(&quot;This line is not executed&quot;) is not executed. Why is that? Is it correct to assume that if we have an await statement inside the __aexit__, the code after that line is not going to execute at all and we shouldn't rely on that for cleaning?



Output of the help() on async generators shows that:

 |  aclose(...)
 |      aclose() -&gt; raise GeneratorExit inside generator.

so why I get &lt;class 'asyncio.exceptions.CancelledError'&gt; exception inside the __aexit__ ?
* I'm using Python 3.10.4
","This is not specific to __aexit__ but to all async code: When an event loop shuts down it must decide between cancelling remaining tasks or preserving them. In the interest of cleanup, most frameworks prefer cancellation instead of relying on the programmer to clean up preserved tasks later on.
This kind of shutdown cleanup is a separate mechanism from the graceful unrolling of functions, contexts and similar on the call stack during normal execution. A context manager that must also clean up during cancellation must be specifically prepared for it. Still, in many cases it is fine not to be prepared for this since many resources fail safe by themselves.

In contemporary event loop frameworks there are usually three levels of cleanup:

Unrolling: The __aexit__ is called when the scope ends and might receive an exception that triggered the unrolling as an argument. Cleanup is expected to be delayed as long as necessary. This is comparable to __exit__ running synchronous code.
Cancellation: The __aexit__ may receive a CancelledError1 as an argument or as an exception on any await/async for/async with. Cleanup may delay this, but is expected to proceed as fast as possible. This is comparable to KeyboardInterrupt cancelling synchronous code.
Closing: The __aexit__ may receive a GeneratorExit as an argument or as an exception on any await/async for/async with. Cleanup must proceed as fast as possible. This is comparable to GeneratorExit closing a synchronous generator.

To handle cancellation/closing, any async code – be it in __aexit__ or elsewhere – must expect to handle CancelledError or GeneratorExit. While the former may be delayed or suppressed, the latter should be dealt with immediately and synchronously2.
    async def __aexit__(self, exc_type, exc_value, exc_tb):
        print(&quot;Exit from the Context Manager...&quot;)
        try:
            await asyncio.sleep(1)  # an exception may arrive here
        except GeneratorExit:
            print(&quot;Exit stage left NOW&quot;)
            raise
        except asyncio.CancelledError:
            print(&quot;Got cancelled, just cleaning up a few things...&quot;)
            await asyncio.sleep(0.5)
            raise
        else:
            print(&quot;Nothing to see here, taking my time on the way out&quot;)
            await asyncio.sleep(1)

Note: It is generally not possible to exhaustively handle these cases. Different forms of cleanup may interrupt one another, such as unrolling being cancelled and then closed. Handling cleanup is only possible on a best effort basis; robust cleanup is achieved by fail safety, for example via transactions, instead of explicit cleanup.

Cleanup of asynchronous generators in specific is a tricky case since they can be cleaned up by all cases at once: Unrolling as the generator finishes, cancellation as the owning task is destroyed or closing as the generator is garbage collected. The order at which the cleanup signals arrive is implementation dependent.
The proper way to address this is not to rely on implicit cleanup in the first place. Instead, every coroutine should make sure that all its child resources are closed before the parent exits. Notably, an async generator may hold resources and needs closing.
async def main():
    # create a generator that might need cleanup
    async_iter = get_numbers()
    async for i in async_iter:
        print(i)
        if i == 1:
            break
    # wait for generator clean up before exiting
    await async_iter.aclose()

In recent versions, this pattern is codified via the aclosing context manager.
from contextlib import aclosing

async def main():
    # create a generator and prepare for its cleanup
    async with aclosing(get_numbers()) as async_iter:
        async for i in async_iter:
            print(i)
            if i == 1:
                break


1The name and/or identity of this exception may vary.
2While it is possible to await asynchronous things during GeneratorExit, they may not yield to the event loop. A synchronous interface is advantageous to enforce this.
"
"I have a dataclass and I want to iterate over in in a loop to spit out each of the values. I'm able to write a very short __iter__() within it easy enough, but is that what I should be doing? I don't see anything in the documentation about an 'iterable' parameter or anything, but I just feel like there ought to be...
Here is what I have which, again, works fine.
from dataclasses import dataclass

@dataclass
class MyDataClass:
    a: float
    b: float
    c: float

    def __iter__(self):
        for value in self.__dict__.values():
            yield value

thing = MyDataclass(1,2,3)
for i in thing:
    print(i)
# outputs 1,2,3 on separate lines, as expected

Is this the best / most direct way to do this?
","The simplest approach is probably to make a iteratively extract the fields following the guidance in the dataclasses.astuple function for creating a shallow copy, just omitting the call to tuple (to leave it a generator expression, which is a legal iterator for __iter__ to return:
def __iter__(self):
    return (getattr(self, field.name) for field in dataclasses.fields(self))

# Or writing it directly as a generator itself instead of returning a genexpr:
def __iter__(self):
    for field in dataclasses.fields(self):
        yield getattr(self, field.name)

Unfortunately, astuple itself is not suitable (as it recurses, unpacking nested dataclasses and structures), while asdict (followed by a .values() call on the result), while suitable, involves eagerly constructing a temporary dict and recursively copying the contents, which is relatively heavyweight (memory-wise and CPU-wise); better to avoid unnecessary O(n) eager work.
asdict would be suitable if you want/need to avoid using live views (if later attributes of the instance are replaced/modified midway through iterating, asdict wouldn't change, since it actually guarantees they're deep copied up-front, while the genexpr would reflect the newer values when you reached them). The implementation using asdict is even simpler (if slower, due to the eager pre-deep copy):
def __iter__(self):
    yield from dataclasses.asdict(self).values()

# or avoiding a generator function:
def __iter__(self):
    return iter(dataclasses.asdict(self).values())

There is a third option, which is to ditch dataclasses entirely. If you're okay with making your class behave like an immutable sequence, then you get iterability for free by making it a typing.NamedTuple (or the older, less flexible collections.namedtuple) instead, e.g.:
from typing import NamedTuple

class MyNotADataClass(NamedTuple):
    a: float
    b: float
    c: float

thing = MyNotADataClass(1,2,3)
for i in thing:
    print(i)
# outputs 1,2,3 on separate lines, as expected

and that is iterable automatically (you can also call len on it, index it, or slice it, because it's an actual subclass of tuple with all the tuple behaviors, it just also exposes its contents via named properties as well).
"
"I wonder how i can transform Spark dataframe to Polars dataframe.
Let's say i have this code on PySpark:
df = spark.sql('''select * from tmp''')

I can easily transform it to pandas dataframe using .toPandas.
Is there something similar in polars, as I need to get a polars dataframe for further processing?
","Context
Pyspark uses arrow to convert to pandas. Polars is an abstraction over arrow memory. So we can hijack the API that spark uses internally to create the arrow data and use that to create the polars DataFrame.
TLDR
Given an spark context we can write:
import pyarrow as pa
import polars as pl

sql_context = SQLContext(spark)

data = [('James',[1, 2]),]
spark_df = sql_context.createDataFrame(data=data, schema = [&quot;name&quot;,&quot;properties&quot;])

df = pl.from_arrow(pa.Table.from_batches(spark_df._collect_as_arrow()))

print(df)

shape: (1, 2)
┌───────┬────────────┐
│ name  ┆ properties │
│ ---   ┆ ---        │
│ str   ┆ list[i64]  │
╞═══════╪════════════╡
│ James ┆ [1, 2]     │
└───────┴────────────┘

Serialization steps
This will actually be faster than the toPandas provided by spark itself, because it saves an extra copy.
toPandas() will lead to this serialization/copy step:
spark-memory -&gt; arrow-memory -&gt; pandas-memory
With the query provided we have:
spark-memory -&gt; arrow/polars-memory
"
"In Python3.11 it's suggested to use TaskGroup for spawning Tasks rather than using gather. Given Gather will also return the result of a co-routine, what's the best approach with TaskGroup.
Currently I have
async with TaskGroup() as tg:
      r1 = tg.create_task(foo())
      r2 = tg.create_task(bar())
res = [r1.result(), r2.result()]

Is there a more concise approach that can be used to achieve the same result?
","The task groups were implemented more as a cleaner way to handle task lifetimes and exception handling, enabled greatly by the new exceptions group rework. I think its a popular misconception right now, but the TaskGroup is not a drop in replacement for all of the gather use-cases. For cases where you do not care about the results (which seems to be the only example I am seeing in new documentation and tutorials) it feels much more terse.
When the task group has completed, it's still required by the user to pull results out of the completed coruntines. If you need values immediately then you can write it as you have it as res = [r1.result(), r2.result()] after the TaskGroup completes. A more terse syntax might be to gather the results after the TaskGroup completes with res = await asyncio.gather(r1, r2) (this will release the execution of your function which may or may not be desirable).
This may look redundant to use both TaskGroup and gather, but the TaskGroup is solving a different purpose than what is provided by gather alone, being that it allows for waiting for your tasks with strong safety guarantees, logic around cancellation for failures, and grouping of exceptions.
It might be possible to extend the default TaskGroup class to make this pattern easier. Here's one such idea that can keep track of which tasks were issued in the task group and provides a helper to fish out the results:
class GatheringTaskGroup(asyncio.TaskGroup):
    def __init__(self):
        super().__init__()
        self.__tasks = []

    def create_task(self, coro, *, name=None, context=None):
        task = super().create_task(coro, name=name, context=context)
        self.__tasks.append(task)
        return task

    def results(self):
        return [task.result() for task in self.__tasks]

async def foo(): return 1
async def bar(): return 2
async with GatheringTaskGroup() as tg:
    task1 = tg.create_task(foo())
    task2 = tg.create_task(bar())
print(tg.results())

[1, 2]

"
"I have a Numpy as a dependency in Poetry pyproject.toml file and it fails to install.
  error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly
              error: Command &quot;clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX12.sdk -DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Ibuild/src.macosx-12-arm64-3.9/numpy/core/src/umath -Ibuild/src.macosx-12-arm64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-12-arm64-3.9/numpy/core/src/common -Inumpy/core/include -Ibuild/src.macosx-12-arm64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Users/moo/Library/Caches/pypoetry/virtualenvs/dex-ohlcv-qY1n4duk-py3.9/include -I/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-12-arm64-3.9/numpy/core/src/common -Ibuild/src.macosx-12-arm64-3.9/numpy/core/src/npymath -c numpy/core/src/multiarray/array_assign_scalar.c -o build/temp.macosx-12-arm64-3.9/numpy/core/src/multiarray/array_assign_scalar.o -MMD -MF build/temp.macosx-12-arm64-3.9/numpy/core/src/multiarray/array_assign_scalar.o.d -faltivec -I/System/Library/Frameworks/vecLib.framework/Headers&quot; failed with exit status 1
              [end of output]
        
          note: This error originates from a subprocess, and is likely not a problem with pip.
          ERROR: Failed building wheel for numpy
        Failed to build numpy


macOS Big Sur
Python 3.9 installed through Homebrew

How to solve it?
If I install Numpy with pip it installs fine.
","Make sure you have OpenBLAS installed from Homebrew:
brew install openblas

Then before running any installation script, make sure you tell your shell environment to use Homebrew OpenBLAS installation
export OPENBLAS=&quot;$(brew --prefix openblas)&quot; 
poetry install

If you get an error
                File &quot;/private/var/folders/tx/50wn88yd40v2_6_7fvfr98z00000gn/T/pip-build-env-uq7qd2ba/overlay/lib/python3.9/site-packages/wheel/bdist_wheel.py&quot;, line 252, in get_tag
                  plat_name = get_platform(self.bdist_dir)
                File &quot;/private/var/folders/tx/50wn88yd40v2_6_7fvfr98z00000gn/T/pip-build-env-uq7qd2ba/overlay/lib/python3.9/site-packages/wheel/bdist_wheel.py&quot;, line 48, in get_platform
                  result = calculate_macosx_platform_tag(archive_root, result)
                File &quot;/private/var/folders/tx/50wn88yd40v2_6_7fvfr98z00000gn/T/pip-build-env-uq7qd2ba/overlay/lib/python3.9/site-packages/wheel/macosx_libfile.py&quot;, line 356, in calculate_macosx_platform_tag
                  assert len(base_version) == 2
              AssertionError

This should have been fixed in the recent enough Python packaging tools.
Make sure

Poetry is recent enough version
Numpy is recent enough version
Any dependency using Numpy, like Scipy or Pyarrrow are also the most recent version

For example in your pyproject.toml
[tool.poetry.dependencies]
# For Scipy compatibility
python = &quot;&gt;=3.9,&lt;3.11&quot;

scipy = &quot;^1.8.0&quot;
pyarrow = &quot;^7.0.0&quot;

Even if this still fails you can try to preinstall scipy with pip before running poetry install in Poetry virtualenv (enter with poetry shell) This should pick up the precompiled scipy wheel. When the precompiled wheel is present, Poetry should not try to install it again and then fail it the build step.
poetry shell
pip install scipy

Collecting scipy
  Downloading scipy-1.8.0-cp39-cp39-macosx_12_0_arm64.whl (28.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 28.7/28.7 MB 6.0 MB/s eta 0:00:00
Requirement already satisfied: numpy&lt;1.25.0,&gt;=1.17.3 in /Users/moo/Library/Caches/pypoetry/virtualenvs/dex-ohlcv-qY1n4duk-py3.9/lib/python3.9/site-packages (from scipy) (1.22.3)
Installing collected packages: scipy
Successfully installed scipy-1.8.0

After this run Poetry normally:
poetry install

"
"I am trying to debug my Python Pytest tests in VS Code, using the Testing Activity on the left bar. I am able to run my tests as expected, with some passing and some failing. I would like to debug the failing tests to more accurately determine what is causing the failures.
When I run an individual test in debug mode VS Code is properly hitting a breakpoint and stopping, and the Run and Debug pane shows the local variables. I can observe the status of local variables either in the Variables &gt; Local pane or through the REPL, by typing the name of the variable.
When I try to print out any statement, such as using &gt; print(&quot;here&quot;) I do not get any output to the Debug Console. When I reference a variable, or put the string directly using &gt; &quot;here&quot; I do see the output to the Debug Console.
It seems to me that the stdout of my REPL is not displaying to the Debug Console. A number of answers online have been suggesting to add options like &quot;redirectOutput&quot;: true or &quot;console&quot;: &quot;integratedTerminal&quot;, but neither of those seem to have worked. My full launch.json is below:
{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    &quot;version&quot;: &quot;0.2.0&quot;,
    &quot;configurations&quot;: [
        {
            &quot;name&quot;: &quot;Python: Current File&quot;,
            &quot;type&quot;: &quot;python&quot;,
            &quot;request&quot;: &quot;launch&quot;,
            &quot;program&quot;: &quot;${file}&quot;,
            &quot;debugOptions&quot;: [
                &quot;WaitOnAbnormalExit&quot;,
                &quot;WaitOnNormalExit&quot;
            ],
            &quot;console&quot;: &quot;integratedTerminal&quot;,
            &quot;stopOnEntry&quot;: false,
            &quot;redirectOutput&quot;: true,
            &quot;outputCapture&quot;: &quot;std&quot;
        }
    ]
}

Is there another setting I'm missing to enable this output? Have I got the wrong console type?
","So After a lot of frustrating &quot;debugging&quot; I found a solution that worked for me (if you are using pytest as me):
tldr
Two solutions:

downgrade your vscode python extension to v2022.2.1924087327 that will do the trick (or any version that had the debugpy&lt;=1.5.1).


Or, Launch the debbuger from the debug tab not the testing tab. And use a configuration like the following one
{
    &quot;name&quot;: &quot;Python: Current File (Integrated Terminal)&quot;,
    &quot;type&quot;: &quot;python&quot;,
    &quot;request&quot;: &quot;launch&quot;,
    &quot;program&quot;: &quot;${file}&quot;,
    &quot;console&quot;: &quot;integratedTerminal&quot;,
    &quot;purpose&quot;: [&quot;debug-test&quot;], 
    &quot;redirectOutput&quot;: true,
    &quot;env&quot;: {&quot;PYTHONPATH&quot;: &quot;${workspaceRoot}&quot;}
}


Bonus. If you are using pytest you can temporarily disable the capture of the stdout of pytest  so your print statements, and the print function, *if you breakpoint inside the contextmanager, will work too. This is very cumbersome but points out the original problem of why the prints are not longer working.
def test_disabling_capturing(capsys):
    print('this output is captured')
    with capsys.disabled():
        print('output not captured, going directly to sys.stdout')
    print('this output is also captured')



the long explanation
so the problem apparently is that the debugpy (which is the library used by vscode python debugger) in is last version v1.6.0 fixed this &quot;bug (827)&quot;. In a nutshell, this &quot;bug&quot; was that vscode &quot;duplicated&quot; all the stdout when debugging because it captures the pytest stdout and replicate it in the debugger console.
This is because, by default, pytest captures all the stdout and store it (so when running all test in parallel it doesn't create a mess).
After &quot;fixing&quot; this issue, now, when you launch the test via the testing tab, by default, pytest is capturing all the stdout and the &quot;new&quot; (&gt;=v1.6.1) debugpy ignores it. Therefore, all the print statements are not shown anymore on the debug console, even when you call print() in a breakpoint, because are captured by pytest (IDK where the pytest captured stdout is showing/stored if it is anywhere). which, in my case is a PITA.
You can disable the pytest capture option using the flag -s or --capture=no when launching pytest in a console or even from the debug tab as a custom configuration. but the problem is that there is no way (apparently) to add these parameters in vscode for the testing tab so pytest is executed using that option.
Therefore the solution that I found was to downgrade the python extension to a version that uses an older version of debugpy v1.5.1, you can see in the python extension changelog that from the  version 2022.4.0 they update the debugpy version, so going before that did the trick for me, you will have the double stdout &quot;bug&quot; in the console, but the print statement will work.
ref: The issue that lead me to the solution

You may make your voice heard here in the vscode-python issues
"
"When working with modular imports with FastAPI and SQLModel, I am getting the following error if I open /docs:

TypeError: issubclass() arg 1 must be a class


Python 3.10.6
pydantic 1.10.2
fastapi 0.85.2
sqlmodel 0.0.8
macOS 12.6

Here is a reproducible example.
user.py
from typing import List, TYPE_CHECKING, Optional
from sqlmodel import SQLModel, Field

if TYPE_CHECKING:
    from item import Item

class User(SQLModel):
    id: int = Field(default=None, primary_key=True)
    age: Optional[int]
    bought_items: List[&quot;Item&quot;] = []

item.py
from sqlmodel import SQLModel, Field

class Item(SQLModel):
    id: int = Field(default=None, primary_key=True)
    price: float
    name: str

main.py
from fastapi import FastAPI

from user import User

app = FastAPI()

@app.get(&quot;/&quot;, response_model=User)
def main():
    return {&quot;message&quot;: &quot;working just fine&quot;}

I followed along the tutorial from sqlmodel https://sqlmodel.tiangolo.com/tutorial/code-structure/#make-circular-imports-work.
If I would put the models in the same file, it all works fine. As my actual models are quite complex, I need to rely on the modular imports though.
Traceback:
Traceback (most recent call last):
  File &quot;/Users/felix/opt/anaconda3/envs/fastapi_test/lib/python3.10/site-packages/fastapi/utils.py&quot;, line 45, in get_model_definitions
    m_schema, m_definitions, m_nested_models = model_process_schema(
  File &quot;pydantic/schema.py&quot;, line 580, in pydantic.schema.model_process_schema
  File &quot;pydantic/schema.py&quot;, line 621, in pydantic.schema.model_type_schema
  File &quot;pydantic/schema.py&quot;, line 254, in pydantic.schema.field_schema
  File &quot;pydantic/schema.py&quot;, line 461, in pydantic.schema.field_type_schema
  File &quot;pydantic/schema.py&quot;, line 847, in pydantic.schema.field_singleton_schema
  File &quot;pydantic/schema.py&quot;, line 698, in pydantic.schema.field_singleton_sub_fields_schema
  File &quot;pydantic/schema.py&quot;, line 526, in pydantic.schema.field_type_schema
  File &quot;pydantic/schema.py&quot;, line 921, in pydantic.schema.field_singleton_schema
  File &quot;/Users/felix/opt/anaconda3/envs/fastapi_test/lib/python3.10/abc.py&quot;, line 123, in __subclasscheck__
    return _abc_subclasscheck(cls, subclass)
TypeError: issubclass() arg 1 must be a class

","TL;DR
You need to call User.update_forward_refs(Item=Item) before the OpenAPI setup.

Explanation
So, this is actually quite a bit trickier and I am not quite sure yet, why this is not mentioned in the docs. Maybe I am missing something. Anyway...
If you follow the traceback, you'll see that the error occurs because in line 921 of pydantic.schema in the field_singleton_schema function a check is performed to see if issubclass(field_type, BaseModel) and at that point field_type is not in fact a type instance.
A bit of debugging reveals that this occurs, when the schema for the User model is being generated and the bought_items field is being processed. At that point the annotation is processed and the type argument for List is still a forward reference to Item. Meaning it is not the actual Item class itself. And that is what is passed to issubclass and causes the error.
This is a fairly common problem, when dealing with recursive or circular relationships between Pydantic models, which is why they were so kind to provide a special method just for that. It is explained in the Postponed annotations section of the documentation. The method is update_forward_refs and as the name suggests, it is there to resolve forward references.
What is tricky in this case, is that you need to provide it with an updated namespace to resolve the Item reference. To do that you need to actually have the real Item class in scope because that is what needs to be in that namespace. Where you do it does not really matter. You could for example import User model into your item module and call it there (obviously below the definition of Item):
from sqlmodel import SQLModel, Field

from .user import User

class Item(SQLModel):
    id: int = Field(default=None, primary_key=True)
    price: float
    name: str

User.update_forward_refs(Item=Item)

But that call needs to happen before an attempt is made to set up that schema. Thus you'll at least need to import the item module in your main module:
from fastapi import FastAPI

from .user import User
from . import item

api = FastAPI()

@api.get(&quot;/&quot;, response_model=User)
def main():
    return {&quot;message&quot;: &quot;working just fine&quot;}

At that point it is probably simpler to have a sub-package with just the model modules and import all of them in the __init__.py of that sub-package.
The reason I gave the example of putting the User.update_forward_refs call in below your Item definition is that these situations typically occur, when you actually have a circular relationship, i.e. if your Item class had a users field for example, which was typed as list[User]. Then you'd have to import User there anyway and might as well just update the references there.
In your specific example, you don't actually have any circular dependencies, so there is strictly speaking no need for the TYPE_CHECKING escape. You can simply do from .item import Item inside user.py and put the actual class in your annotation as bought_items: list[Item]. But I assume you simplified the actual use case and simply forgot to include the circular dependency.

Maybe I am missing something and someone else here can find a way to call update_forward_refs without the need to provide Item explicitly, but this way should definitely work.
"
"I have a class decorator, which adds a few functions and fields to decorated class.
@mydecorator
@dataclass
class A:
    a: str = &quot;&quot;

Added (via setattr()) is a .save() function and a set of info for dataclass fields as a separate dict.
I'd like VScode and mypy to properly recognize that, so that when I use:
a=A()
a.save()

or a.my_fields_dict those 2 are properly recognized.
Is there any way to do that? Maybe modify class A type annotations at runtime?
","TL;DR
What you are trying to do is not possible with the current type system.

1. Intersection types
If the attributes and methods you are adding to the class via your decorator are static (in the sense that they are not just known at runtime), then what you are describing is effectively the extension of any given class T by mixing in a protocol P. That protocol defines the method save and so on.
To annotate this you would need an intersection of T &amp; P. It would look something like this:
from typing import Protocol, TypeVar


T = TypeVar(&quot;T&quot;)


class P(Protocol):
    @staticmethod
    def bar() -&gt; str: ...


def dec(cls: type[T]) -&gt; type[Intersection[T, P]]:
    setattr(cls, &quot;bar&quot;, lambda: &quot;x&quot;)
    return cls  # type: ignore[return-value]


@dec
class A:
    @staticmethod
    def foo() -&gt; int:
        return 1

You might notice that the import of Intersection is conspicuously missing. That is because despite being one of the most requested features for the Python type system, it is still missing as of today. There is currently no way to express this concept in Python typing.

2. Class decorator problems
The only workaround right now is a custom implementation alongside a corresponding plugin for the type checker(s) of your choice. I just stumbled across the typing-protocol-intersection package, which does just that for mypy.
If you install that and add plugins = typing_protocol_intersection.mypy_plugin to your mypy configuration, you could write your code like this:
from typing import Protocol, TypeVar

from typing_protocol_intersection import ProtocolIntersection


T = TypeVar(&quot;T&quot;)


class P(Protocol):
    @staticmethod
    def bar() -&gt; str: ...


def dec(cls: type[T]) -&gt; type[ProtocolIntersection[T, P]]:
    setattr(cls, &quot;bar&quot;, lambda: &quot;x&quot;)
    return cls  # type: ignore[return-value]


@dec
class A:
    @staticmethod
    def foo() -&gt; int:
        return 1

But here we run into the next problem. Testing this with reveal_type(A.bar()) via mypy will yield the following:
error: &quot;Type[A]&quot; has no attribute &quot;bar&quot;  [attr-defined]
note: Revealed type is &quot;Any&quot;

Yet if we do this instead:
class A:
    @staticmethod
    def foo() -&gt; int:
        return 1


B = dec(A)

reveal_type(B.bar())

we get no complaints from mypy and note: Revealed type is &quot;builtins.str&quot;. Even though what we did before was equivalent!
This is not a bug of the plugin, but of the mypy internals. It is another long-standing issue, that mypy does not handle class decorators correctly.
A person in that issue thread even mentioned your use case in conjunction with the desired intersection type.

DIY
In other words, you'll just have to wait until those two holes are patched. Or you can hope that at least the decorator issue by mypy is fixed soon-ish and write your own VSCode plugin for intersection types in the meantime. Maybe you can get together with the person behind that mypy plugin I mentioned above.
"
"I have a system of equations where each equation is a linear equation with boolean constraints. For  example:
x1 + x2 + x3 = 2
x1 + x4 = 1
x2 + x1 = 1

And each x_i is either 0 or 1. Sometimes there might be a small positive (&lt;5) coefficient (for example x1 + 2 * x3 + x4 = 3. Basically a standard linear programming task. What I need to do is to find all x_i which are guaranteed to be 0 and all x_j which are guaranteed to be 1. Sorry if my terminology is not correct here but by guaranteed I mean that if you generate all possible solutions you in all of them all x_i will be 0 and in all of them x_j will be 1.
For example my equation has only 2 solutions:

1, 0, 1, 0
0, 1, 1, 1

So you do not have guaranteed 0 and have x_3 as a guaranteed 1.
I know how to solve this problem with or-tools by generating all solutions and it works for my usecases (equations are pretty constrained so usually there are &lt; 500 solutions although the number of variables is big enough to make the whole combinatorial search impossible).
The big problem is that I can't use that library (system restrictions above my control) and only libraries available in my case are numpy and scipy. I found that scipy has scipy.optimize.linprog.
It seems like I have found a way to generate one solution
import numpy as np
from scipy.optimize import linprog

A_eq = np.array([
    [1, 1, 1, 0],  # x1 + x2 + x3 = 2
    [1, 0, 0, 1],  # x1 + x4 = 1
    [1, 1, 0, 0]   # x1 + x2 = 1
])
b_eq = np.array([2, 1, 1])
c = np.zeros(4)
bounds = [(0, 1)] * 4

res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs-ipm')
if res.success:
    print(res.x)

But I can't find a way to generate all solutions. Also I am not sure whether there is a better way to do it as all I need to know is to find guaranteed values

P.S. this problem is important to me. I guarantee to add a 500 bounty on it, but system prevents me from doing it until 2 days will pass.
","You don't need to (fully) brute-force, and you don't need to find all of your solutions. You just need to find solutions for which each of your variables meets each of their extrema. The following is a fairly brain-off LP approach with 2n² columns and 2mn rows. It's sparse, and for your inputs does not need to be integral. That said, I somewhat doubt it will be the most efficient method possible.
import numpy as np
from scipy.optimize import milp, Bounds, LinearConstraint
import scipy.sparse as sp

lhs = np.array((
    (1, 1, 1, 0),
    (1, 0, 0, 1),
    (1, 1, 0, 0),
))
rhs = np.array((2, 1, 1))
m, n = lhs.shape

# Variables: n * 2 (minimize, maximize) * n
c = sp.kron(
    sp.eye_array(n),
    np.array((
        (+1,),
        (-1,),
    )),
)

b = np.tile(rhs, 2*n)
system_constraint = LinearConstraint(
    A=sp.kron(sp.eye_array(2*n), lhs, format='csc'),
    lb=b, ub=b,
)

result = milp(
    c=c.toarray().ravel(),  # must be dense
    integrality=0,
    bounds=Bounds(lb=0, ub=1),
    constraints=system_constraint,
)
assert result.success
extrema = result.x.reshape((n, 2, n))
mins = extrema[:, 0]
maxs = extrema[:, 1]
vmins = np.diag(mins)
vmaxs = np.diag(maxs)

print('Solutions for minima on the diagonal:')
print(mins)
print('Solutions for maxima on the diagonal:')
print(maxs)
print('Variable minima:', vmins)
print('Variable maxima:', vmaxs)
print('Guaranteed 0:', vmaxs &lt; 0.5)
print('Guaranteed 1:', vmins &gt; 0.5)

Solutions for minima on the diagonal:
[[-0.  1.  1.  1.]
 [ 1.  0.  1. -0.]
 [ 1.  0.  1. -0.]
 [ 1.  0.  1. -0.]]
Solutions for maxima on the diagonal:
[[ 1.  0.  1. -0.]
 [-0.  1.  1.  1.]
 [ 1.  0.  1. -0.]
 [-0.  1.  1.  1.]]
Variable minima: [-0.  0.  1. -0.]
Variable maxima: [1. 1. 1. 1.]
Guaranteed 0: [False False False False]
Guaranteed 1: [False False  True False]

There is a variant on this idea where

rather than using sparse modelling, you just loop
don't use LP at all
fix each variable at each of its extrema, and iteratively column-eliminate from the left-hand side
attempt a least-squares solution of the linear system, and infer a high residual to mean that there is no solution

This somewhat naively assumes that all solutions will see integer values, and (unlike milp) does not have the option to set integrality=1. For demonstration I was forced to add a row to get a residual.
import numpy as np

lhs = np.array((
    (1, 1, 1, 0),
    (1, 0, 0, 1),
    (1, 1, 0, 0),
    (0, 0, 1, 1),
))
rhs = np.array((2, 1, 1, 1))
m, n = lhs.shape
epsilon = 1e-12
lhs_select = np.ones(n, dtype=bool)

for i in range(n):
    lhs_select[i] = False
    x0, (residual,), rank, singular = np.linalg.lstsq(lhs[:, lhs_select], rhs)
    zero_solves = residual &lt; epsilon
    x1, (residual,), rank, singular = np.linalg.lstsq(lhs[:, lhs_select], rhs - lhs[:, i])
    one_solves = residual &lt; epsilon
    lhs_select[i] = True

    if zero_solves and not one_solves:
        print(f'x{i}=0, solution {x0.round(12)}')
    elif one_solves and not zero_solves:
        print(f'x{i}=1, solution {x1.round(12)}')

x0=1, solution [-0.  1.  0.]
x1=0, solution [ 1.  1. -0.]
x2=1, solution [1. 0. 0.]
x3=0, solution [ 1. -0.  1.]

"
"I would like to replace Pandas with Polars but I was not able to find out how to use Polars with Plotly without converting to Pandas. I wonder if there is a way to completely cut Pandas out of the process.
Consider the following test data:
import polars as pl
import numpy as np
import plotly.express as px

df = pl.DataFrame(
    {
        &quot;nrs&quot;: [1, 2, 3, None, 5],
        &quot;names&quot;: [&quot;foo&quot;, &quot;ham&quot;, &quot;spam&quot;, &quot;egg&quot;, None],
        &quot;random&quot;: np.random.rand(5),
        &quot;groups&quot;: [&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;B&quot;],
    }
)

fig = px.bar(df, x='names', y='random')
fig.show()

I would like this code to show the bar chart in a Jupyter notebook but instead it returns an error:
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/polars/internals/frame.py:1483: UserWarning: accessing series as Attribute of a DataFrame is deprecated
  warnings.warn(&quot;accessing series as Attribute of a DataFrame is deprecated&quot;)

It is possible to transform the Polars data frame to a Pandas data frame with df = df.to_pandas(). Then, it works. However, is there another, simpler and more elegant solution?
","Yes, no need for converting to a Pandas dataframe. Someone (sa-) has requested supporting a better option here and included a workaround for it.

&quot;The workaround that I use right now is px.line(x=df[&quot;a&quot;], y=df[&quot;b&quot;]), but it gets unwieldy if the name of the data frame is too big&quot;

For the OP's code example, the approach of specifying the dataframe columns explicitly works.
I find in addition to specifying the dataframe columns with px.bar(x=df[&quot;names&quot;], y=df[&quot;random&quot;]) - or - px.bar(df, x=df[&quot;names&quot;], y=df[&quot;random&quot;]), casting to a list can also work:
import polars as pl
import numpy as np
import plotly.express as px

df = pl.DataFrame(
    {
        &quot;nrs&quot;: [1, 2, 3, None, 5],
        &quot;names&quot;: [&quot;foo&quot;, &quot;ham&quot;, &quot;spam&quot;, &quot;egg&quot;, None],
        &quot;random&quot;: np.random.rand(5),
        &quot;groups&quot;: [&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;B&quot;],
    }
)

px.bar(df, x=list(df[&quot;names&quot;]), y=list(df[&quot;random&quot;]))

Knowing polars better, you may see some other options once you see the idea of the workaround.
The example posted there is simpler, instead of px.line(df, x=&quot;a&quot;, y=&quot;b&quot;) like you could use for a Pandas dataframe, you use px.line(x=df[&quot;a&quot;], y=df[&quot;b&quot;]). With polars, that is:
import polars as pl
import plotly.express as px

df = pl.DataFrame({&quot;a&quot;:[1,2,3,4,5], &quot;b&quot;:[1,4,9,16,25]})

px.line(x=df[&quot;a&quot;], y=df[&quot;b&quot;])

(Note that using plotly.express requires Pandas to be installed, see here and here. I used plotly.express in my answer because it was closer to the OP. The code could be adapted to using plotly.graph_objects if there was a desire to not have Pandas installed &amp; involved at all.)
"
"When I write code in VS Code, beginning with:
import os
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
from langchain.document_loaders import TextLoader

I am met with the error: ModuleNotFoundError: No module named 'langchain'
I have updated my Python to version 3.11.4, have updated pip, and reinstalled langchain. I have also checked sys.path and the folder C:\\Python311\\Lib\\site-packages in which the Langchain folder is, is appended.
EDIT: Langchain import works when I run it in the Python console (functionality works too), but when I run the code from the VSCode run button it still provides the ModuleNotFoundError.
Has anyone else run into this issue and found a solution?
","I had installed packages with python 3.9.7 but this version was causing issues so I switched to Python 3.10. When I installed the langhcain it was in python 3.9.7 directory. If yo run pip show langchain, you get this
Name: langchain
Version: 0.0.220
Summary: Building applications with LLMs through composability
Home-page: https://www.github.com/hwchase17/langchain
Author: 
Author-email: 
License: MIT
Location: /home/anaconda3/lib/python3.9/site-packages
Requires: aiohttp, async-timeout, dataclasses-json, langchainplus-sdk, numexpr, numpy, openapi-schema-pydantic, pydantic, PyYAML, requests, SQLAlchemy, tenacity
Required-by: jupyter_ai, jupyter_ai_magics

If you look at the Location property, you see this /home/anaconda3/lib/python3.9/site-packages. But since I am using Pyhton3.10 I had to make sure langchain is in the directory of Python 3.10. so installed the langhchain with
python3.10 -m pip install langchain   

now when I run, python3.10 -m pip show langchain I get this
Name: langchain
Version: 0.0.264
Summary: Building applications with LLMs through composability
Home-page: https://www.github.com/hwchase17/langchain
Author: 
Author-email: 
License: MIT
Location: /home/.local/lib/python3.10/site-packages
Requires: aiohttp, async-timeout, dataclasses-json, langsmith, numexpr, numpy, openapi-schema-pydantic, pydantic, PyYAML, requests, SQLAlchemy, tenacity
Required-by: 

Now new Location is referring to Python3.10 directory
"
"Recently I have started to use hydra to manage the configs in my application. I use Structured Configs to create schema for .yaml config files. Structured Configs in Hyda uses dataclasses for type checking. However, I also want to use some kind of validators for some of the parameter I specify in my Structured Configs (something like this).
Do you know if it is somehow possible to use Pydantic for this purpose? When I try to use Pydantic, OmegaConf complains about it:
omegaconf.errors.ValidationError: Input class 'SomeClass' is not a structured config. did you forget to decorate it as a dataclass?

","For those of you wondering how this works exactly, here is an example of it:
import hydra
from hydra.core.config_store import ConfigStore
from omegaconf import OmegaConf
from pydantic.dataclasses import dataclass
from pydantic import validator


@dataclass
class MyConfigSchema:
    some_var: float

    @validator(&quot;some_var&quot;)
    def validate_some_var(cls, some_var: float) -&gt; float:
        if some_var &lt; 0:
            raise ValueError(f&quot;'some_var' can't be less than 0, got: {some_var}&quot;)
        return some_var


cs = ConfigStore.instance()
cs.store(name=&quot;config_schema&quot;, node=MyConfigSchema)


@hydra.main(config_path=&quot;/path/to/configs&quot;, config_name=&quot;config&quot;)
def my_app(config: MyConfigSchema) -&gt; None:
    # The 'validator' methods will be called when you run the line below
    OmegaConf.to_object(config)


if __name__ == &quot;__main__&quot;:    
    my_app()

And config.yaml :
defaults:
  - config_schema

some_var: -1  # this will raise a ValueError

"
"PEP 622 introduced match statement as an alternative to if-elif-else. However, one thing I can't find in the proposal or in any of the material online is whether the match statement can be used as an expression and not just as a statement.
A couple of examples to make it clear:
Example 1:
def make_point_2d(pt):
    match pt:
        case (x, y):
            return Point2d(x, y)
        case _:
            raise TypeError(&quot;not a point we support&quot;)

Example 2:
match response.status:
    case 200:
        do_something(response.data)
    case 301 | 302:
        retry(response.location)

In the first example, the function returns from inside a case clause, and in the second example, nothing is returned. But I want to be able to do something like the following hypothetical example:
spouse = match name:
    case &quot;John&quot;:
        &quot;Jane&quot;
    case &quot;David&quot;:
        &quot;Alice&quot;
print(spouse)

But it doesn't compile.
","Not in Python.
In Rust and Haskell, matches are expressions composed of expressions:
let spouse = match name {
    // expr =&gt; expr,
    &quot;John&quot; =&gt; &quot;Jane&quot;,

    // expr =&gt; {stmt; stmt; expr},
    &quot;David&quot; =&gt; {let s = &quot;Alice&quot;; println!(&quot;Matched David&quot;); s},

    _ =&gt; panic!(&quot;Unknown name&quot;),
};

do
  spouse &lt;- case name of
    &quot;John&quot; -&gt; return &quot;Jane&quot;
    &quot;David&quot; -&gt; do
      let s = &quot;Alice&quot;
      putStrLn &quot;Matched David&quot;
      return s

...so it's not technically impossible that the Python match statement couldn't have been designed to work as an expression. Presumably, the reason it was avoided for Python are syntactical reasons:

Python does not have syntax for expression statements (e.g. {stmt; stmt; expr}).
spouse = match name:
    case &quot;John&quot;:
        &quot;Jane&quot;
    case &quot;David&quot;:  # stmt; stmt; expr
        s = &quot;Alice&quot;
        print(&quot;Matched David&quot;)
        s


Indentation to indicate a block being treated as an expression may look unnatural to some people. (See also: match grammar.)


That said, it is possible your proposed syntax could be accepted in the future.
"
"I have a string like
aaabbbbcca

And I'd like to parse all possible uniform substrings from that. So my expected substrings for this string are
['a', 'aa', 'aaa', 'b', 'bb', 'bbb', 'bbbb', 'c', 'cc', 'a']

I tried the following
import re

print(re.findall(r&quot;([a-z])(?=\1*)&quot;, &quot;aaabbbbcca&quot;))
# Output: ['a', 'a', 'a', 'b', 'b', 'b', 'b', 'c', 'c', 'a']

Is it possible trough regular expressions? If yes, then how?
","You can achieve what you need without a regex here:
result = []
text = &quot;aaabbbbcca&quot;
prev = ''
for c in text:
  if c == prev:
    result.append(result[-1] + c)
  else:
    result.append(c)
    prev = c
 
print(result)
# =&gt; ['a', 'aa', 'aaa', 'b', 'bb', 'bbb', 'bbbb', 'c', 'cc', 'a']

See the Python demo.
In short, you can iterate over the string and append new item to a result list when the new char is not equal to the previous char, otherwise, append a new item with the value equal to the previous item + the same char concatenated to the value.
With regex, the best you can do is
import re
text = &quot;aaabbbbcca&quot;
print( [x.group(1) for x in re.finditer(r'(?=((.)\2*))', text)] )
# =&gt; ['aaa', 'aa', 'a', 'bbbb', 'bbb', 'bb', 'b', 'cc', 'c', 'a']

See this Python demo. Here, (?=((.)\2*)) matches any location inside the string that is immediately preceded with any one char (other than line break chars if you do not use re.DOTALL option) that is followed with zero or more occurrences of the same char (capturing the char(s) into Group 1).
"
"Yolov8 and I suspect Yolov5 handle non-square images well. I cannot see any evidence of cropping the input image, i.e. detections seem to go to the enge of the longest side. Does it resize to a square 640x604 which would change the aspect ratio of objects making them more difficult to detect?
When training on a custom dataset starting from a pre-trained model, what does the imgsz (image size) parameter actually do?
","Modern Yolo versions, from v3 onwards, can handle arbitrary sized images as long as both sides are a multiple of 32. This is because the maximum stride of the backbone is 32 and it is a fully convolutional network. But there are clearly two different cases for how input images to the model are preprocessed:
Training
An example. Let's say you start a training by:
from ultralytics.yolo.engine.model import YOLO
  
model = YOLO(&quot;yolov8n.pt&quot;)
results = model.train(data=&quot;coco128.yaml&quot;, imgsz=512) 

By printing what is fed to the model (im) in trainer.py you will obtain the following output:
Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
  0%|          | 0/8 [00:00&lt;?, ?it/s]
 torch.Size([16, 3, 512, 512])
      1/100      1.67G      1.165      1.447      1.198        226        512:  12%|█▎        | 1/8 [00:01&lt;00:08,  1.15s/it]
 torch.Size([16, 3, 512, 512])
      1/100      1.68G      1.144      1.511       1.22        165        512:  25%|██▌       | 2/8 [00:02&lt;00:06,  1.10s/it]
 torch.Size([16, 3, 512, 512])

So, during training, images have to be reshaped to the same size in order to be able to create mini-batches as you cannot concatenate tensors of different shapes. To preserve the aspect ratio of the images, in order to avoid distortion, they are usually &quot;letterbox'ed&quot;. imgsz selects the size of the images to train on.
Prediction
Now, let's have a look at prediction. Let's say you select the images under assets as source and imgsz 512 by
from ultralytics.yolo.engine.model import YOLO
  
model = YOLO(&quot;yolov8n.pt&quot;)
results = model.predict(stream=True, imgsz=512) # source already setup

By printing the original image shape (im0) and the one fed to the model (im) in predictor.py you will obtain the following output:
(yolov8) ➜  ultralytics git:(main) ✗ python new.py 
Ultralytics YOLOv8.0.23 🚀 Python-3.8.15 torch-1.11.0+cu102 CUDA:0 (Quadro P2000, 4032MiB)
YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs
im0s (1080, 810, 3)
im torch.Size([1, 3, 512, 384])
image 1/2 /home/mikel.brostrom/ultralytics/ultralytics/assets/bus.jpg: 512x384 4 persons, 1 bus, 7.4ms
im0s (720, 1280, 3)
im torch.Size([1, 3, 288, 512])
image 2/2 /home/mikel.brostrom/ultralytics/ultralytics/assets/zidane.jpg: 288x512 3 persons, 2 ties, 5.8ms
Speed: 0.4ms pre-process, 6.6ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 512)

You can see that the longest image side is reshaped to 512. The short side is reshaped to the closest multiple of 32 while maintaining the aspect ratio. As you are not feeding multiple images at the same time you don't need to reshape images into the same shape and stack them, making it possible to avoid padding.
"
"In ctransformers library, I can only load around a dozen supported models. How can I run local inference on CPU (not just on GPU) from any open-source LLM quantized in the GGUF format (e.g. Llama 3, Mistral, Zephyr, i.e. ones unsupported in ctransformers)?
","llama-cpp-python is my personal choice, because it is easy to use and it is usually one of the first to support quantized versions of new models. To install it for CPU, just run pip install llama-cpp-python. Compiling for GPU is a little more involved, so I'll refrain from posting those instructions here since you asked specifically about CPU inference. I also recommend installing huggingface_hub (pip install huggingface_hub) to easily download models.
Once you have both llama-cpp-python and huggingface_hub installed, you can download and use a model (e.g. mixtral-8x7b-instruct-v0.1-gguf) like so:
## Imports
from huggingface_hub import hf_hub_download
from llama_cpp import Llama

## Download the GGUF model
model_name = &quot;TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF&quot;
model_file = &quot;mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf&quot; # this is the specific model file we'll use in this example. It's a 4-bit quant, but other levels of quantization are available in the model repo if preferred
model_path = hf_hub_download(model_name, filename=model_file)

## Instantiate model from downloaded file
llm = Llama(
    model_path=model_path,
    n_ctx=16000,  # Context length to use
    n_threads=32,            # Number of CPU threads to use
    n_gpu_layers=0        # Number of model layers to offload to GPU
)

## Generation kwargs
generation_kwargs = {
    &quot;max_tokens&quot;:20000,
    &quot;stop&quot;:[&quot;&lt;/s&gt;&quot;],
    &quot;echo&quot;:False, # Echo the prompt in the output
    &quot;top_k&quot;:1 # This is essentially greedy decoding, since the model will always return the highest-probability token. Set this value &gt; 1 for sampling decoding
}

## Run inference
prompt = &quot;The meaning of life is &quot;
res = llm(prompt, **generation_kwargs) # Res is a dictionary

## Unpack and the generated text from the LLM response dictionary and print it
print(res[&quot;choices&quot;][0][&quot;text&quot;])
# res is short for result

Keep in mind that mixtral is a fairly large model for most laptops and requires ~25+ GB RAM, so if you need a smaller model, try using one like llama-13b-chat-gguf (model_name=&quot;TheBloke/Llama-2-13B-chat-GGUF&quot;; model_file=&quot;llama-2-13b-chat.Q4_K_M.gguf&quot;) or mistral-7b-openorca-gguf (model_name=&quot;TheBloke/Mistral-7B-OpenOrca-GGUF&quot;; model_file=&quot;mistral-7b-openorca.Q4_K_M.gguf&quot;).
"
"I need to create a schema but it has a column called global, and when I try to write this, I got an error.
class User(BaseModel):

    id:int
    global:bool

I try to use another name, but gives another error when try to save in db.
","It looks like you are using a pydantic module. You can't use the name global because it's a reserved keyword so you need to use this trick to convert it.
pydantic v1:
class User(BaseModel):
    id: int
    global_: bool

    class Config:
        fields = {
            'global_': 'global'
        }

or
pydantic v1 &amp; v2:
class User(BaseModel):
    id: int
    global_: bool = Field(..., alias='global')

To create a class you have to use a dictionary (because User(id=1, global=False) also throws an error):
user = User(id=1, global=False)

&gt; Traceback (most recent call last):
&gt; (...)
&gt; File &quot;&lt;input&gt;&quot;, line 1
&gt;  User(id=1, global=False)
&gt;             ^^^^^^
&gt; SyntaxError: invalid syntax

user = User(**{'id': 1, 'global': False})

Set allow_population_by_field_name = True or populate_by_name=True  in config to allow creating models using both global and global_ names (thanks @GooDeeJAY).
pydantic v1:
class User(BaseModel):
    id: int
    global_: bool = Field(..., alias='global')

    class Config:
        allow_population_by_field_name = True

pydantic v2:
class User(BaseModel):
    id: int
    global_: bool = Field(..., alias='global')
    model_config = ConfigDict(populate_by_name=True)


user1 = User(**{'id': 1, 'global': False})
user2 = User(id=1, global_=False)
assert user1 == user2

By default schema dump will not use aliased fields:
user.dict() # for pydantic v1
user.model_dump() # for pydantic v2
&gt; {'id': 1, 'global_': False}

To get data in the correct schema use by_alias:
user.dict(by_alias=True) # for pydantic v1
user.model_dump(by_alias=True) # for pydantic v2
&gt; {'id': 1, 'global': False}

"
"Whenever you extend a class in JavaScript or Python, the derived class must use the super keyword in order to set attributes and/or invoke methods and constructor in the base class. For example:
class Rectangle {
    constructor(length, width) {
        this.name = &quot;Rectangle&quot;;
        this.length = length;
        this.width = width;
    }

    shoutArea() {
        console.log(
            `I AM A ${this.name.toUpperCase()} AND MY AREA IS ${this.length * this.width}`
        );
    }
    
    rectHello() {
        return &quot;Rectanglish: hello&quot;;
    }
}

class Square extends Rectangle {
    constructor(length) {
        super(length, length);
        this.name = &quot;Square&quot;
    }
    
    squaHello() {
        const h = super.rectHello();
        return &quot;Squarish:&quot; + h.split(':')[1];
    }
}

const rect = new Rectangle(6, 4);
rect.shoutArea(); //=&gt; I AM A RECTANGLE AND MY AREA IS 24

const squa = new Square(5);
squa.shoutArea(); //=&gt; I AM A SQUARE AND MY AREA IS 25

console.log(squa.squaHello()); //=&gt; Squarish: hello

","
What's the Raku equivalent of the super keyword as used in JavaScript and Python?

One of Raku's re-dispatching functions.¹
Basics of redispatch
First some code that does not include a redispatch function:
class Rectangle {
  has ($.length, $.width)
}

Rectangle.new: length =&gt; 6, width =&gt; 4;

The Rectangle declaration does not even include construction code, just a declaration of two attributes and that's it.
So what is the Rectangle.new call doing? It's inheriting the default new method provided by Raku's Mu class which initializes any class attributes whose names match any named arguments.

If you want a custom constructor that accepts positional arguments, then you typically write a new method which lists which arguments you want in its signature, and then have that method call suitably invoke the default new, which requires named arguments, by calling an appropriate redispatch function with the arguments converted to named arguments:
class Rectangle {
  has ($.length, $.width);
  method new ($length, $width) { callwith length =&gt; $length, width =&gt; $width }
}

Rectangle.new: 6, 4;

callwith is a redispatch function which does a:

call of the next matching candidate based on the original call.²

with a fresh set of arguments.


In this simple case the original call was Rectangle.new: 6, 4, and the next candidate is the new method inherited from Mu.
A Rectangle class based on yours
Rather than mimic your code I'll write an idiomatic Raku translation of it and comment on it.
class Rectangle {
  has ($!length, $!width) is required is built;
  method new ($length, $width) { callwith :$length, :$width }
  method shoutArea { put uc &quot;I am a {self.^name} and my area is {$!length * $!width}&quot; }
  method rectHello { 'Rectanglish: hello' }
}

constant rect = Rectangle.new: 6, 4;
rect.shoutArea; #=&gt; I AM A RECTANGLE AND MY AREA IS 24

Commentary:

It's a good habit to default to writing code that limits problems that can arise as code evolves. For this reason I've used $!length for the length attribute rather than $.length.³

I've added an is required annotation to the attributes. This means a failure to initialize attributes by the end of an instance's construction will mean an exception gets thrown.

I've added an is built annotation to the attributes. This means that even an attribute without a public accessor -- as is the case for $!length and $!width due to my use of ! instead of . in the &quot;twigil&quot; -- can/will still be automatically initialized if there is a matching named argument in the construction call.

:$length is short for length =&gt; $length.

self.^name avoids unnecessary overhead. It's not important and quite possibly distracting to read about so feel free to ignore my footnote explaining it.⁴


A Square class based on yours
I'll make the new for Square redispatch:
class Square is Rectangle {
  method new ($side-length) { callwith $side-length, $side-length }
  method squaHello { &quot;Squarish: {self.rectHello.split(':')[1].trim}&quot; }
}

constant squa = Square.new: 5;
squa.shoutArea; #=&gt; I AM A SQUARE AND MY AREA IS 25
put squa.squaHello; #=&gt; Squarish: hello

Commentary:

I picked the name $side-length for the Square's .new parameter, but the name doesn't matter because it's a positional parameter/argument.

The redispatch is to the next candidate, just as it was before, abstractly speaking. Concretely speaking the next candidate this time is the method I had just defined in Rectangle (which in turn redispatches to the new of Mu).

self.rectHello suffices because the method being called has a different name than the originally called method (squaHello). If you renamed the two methods in Rectangle and Square to have the same name Hello then a redispatch would again be appropriate, though this time I'd have written just callsame rather than callwith ... because callsame just redispatches to the next candidate using the same arguments that were provided in the original call, which would save bothering to write out the arguments again.


Footnotes
¹ Redispatching is a generalization of features like super. Redispatch functions are used for a range of purposes, including ones that have nothing to do with object orientation.
² In Raku a function or method call may result in the compiler generating a list of possibly matching candidates taking into account factors such as invocants for method calls, and multiple dispatch and function wrappers for both functions and methods. Having constructed a candidate list it then dispatches to the leading candidate (or the next one in the case of redispatch to the next candidate).
³ If you really want a getter/setter to be automatically generated for a given attribute, then declare it with a ., eg $.length instead of $!length, and Raku will generate both a $!length attribute and a .length getter. (And a setter too if you add an is rw to the $.length declaration.) I did this in the first code example to keep things a bit simpler.
⁴ The ^ in a method call like foo.^bar means a bar method call is redirected &quot;upwards&quot; (hence the ^) to the Higher Order Workings object that knows how a foo functions as a particular kind of type. In this case a Rectangle is a class and the HOW object is an instance of Perl6::Metamodel::ClassHOW, which knows how classes work, including that each class has a distinct name and has a .name method that retrieves that name. And the name of the Rectangle class is of course 'Rectangle', so self.^name saves having to create something else with the class's name.
"
"This is my DataFrame:
import pandas as pd
import numpy as np
df = pd.DataFrame(
    {
        'x': [1, np.nan, 3, np.nan, 5],
        'y': [np.nan, 7, 8, 9, np.nan],
        'x_a': [1, 2, 3, 4, 5],
        'y_a': [6, 7, 8, 9, 10]

    }
)

Expected output is fill_na columns x and y:
     x     y  x_a  y_a
0  1.0   6.0    1    6
1  2.0   7.0    2    7
2  3.0   8.0    3    8
3  4.0   9.0    4    9
4  5.0  10.0    5   10

Basically I want to fillna x with x_a and y with y_a. In other words each column should be paired with another column that has the suffix _a and the column name.
I can get this output by using this code:
for col in ['x', 'y']:
    df[col] = df[col].fillna(df[f'{col}_a'])

But I wonder if it is the best/most efficient way? Suppose I got hundreds of columns like these
","What about using an Index to select all columns at once and set_axis to realign the DataFrame:
cols = pd.Index(['x', 'y'])
df[cols] = df[cols].fillna(df[cols+'_a'].set_axis(cols, axis=1))

NB. this is assuming all columns in cols and all '_a' columns exist. If you're not sure you could be safe and use intersection and reindex:
cols = pd.Index(['x', 'y']).intersection(df.columns)
df[cols] = df[cols].fillna(df.reindex(columns=cols+'_a').set_axis(cols, axis=1))

Or for an approach that is fully independent of explicitly passing input columns and just relying on the suffix (_a):
suffix = '_a'

# find columns &quot;xyz&quot; that have a &quot;xyz_a&quot; counterpart
c1 = df.columns.intersection(df.columns+suffix)
c2 = c1.str.removesuffix(suffix)
# select, fillna, update
df[c2] = df[c2].fillna(df[c1].set_axis(c2, axis=1))

Output:
     x     y  x_a  y_a
0  1.0   6.0    1    6
1  2.0   7.0    2    7
2  3.0   8.0    3    8
3  4.0   9.0    4    9
4  5.0  10.0    5   10

Example for which the second approach would be needed:
df = pd.DataFrame(
    {
        'x': [1, np.nan, 3, np.nan, 5],
        'z': [np.nan, 7, 8, 9, np.nan],
        'p_a': [1, 2, 3, 4, 5],
        'y_a': [6, 7, 8, 9, 10]

    }
)

"
"I am trying to use the pandas.cumsum() function, but in a way that ignores rows with a value in the ID column that is duplicated and specifically only adds the last value to the cumulative sum, ignoring all earlier values.
Example code below (I couldn't share the real code, which is for work).
import pandas as pd, numpy as np
import random as rand
id = ['a','b','c','a','b','e','f','a','b','k']
value = [12,14,3,13,16,7,4,6,10,18]

df = pd.DataFrame({'id':id, 'value':value})
df[&quot;cumsum_of_value&quot;] = df['value'].cumsum()
df[&quot;desired_output&quot;] = [
    12,26,29,30,32,39,43,36,30,48
]
df[&quot;comments&quot;] = [&quot;&quot;]*len(df)
df.loc[df.index==0, &quot;comments&quot;]=&quot;standard cumsum&quot;
df.loc[df.index==1, &quot;comments&quot;]=&quot;standard cumsum&quot;
df.loc[df.index==2, &quot;comments&quot;]=&quot;standard cumsum&quot;
df.loc[df.index==3, &quot;comments&quot;]=&quot;cumsum of rows 1-3, ignore row 0&quot;
df.loc[df.index==4, &quot;comments&quot;]=&quot;cumsum of rows 2-4, ignore rows 0, 1&quot;
df.loc[df.index==5, &quot;comments&quot;]=&quot;cumsum of rows 2-5, ignore rows 0, 1&quot;
df.loc[df.index==6, &quot;comments&quot;]=&quot;cumsum of rows 2-6, ignore rows 0, 1&quot;
df.loc[df.index==7, &quot;comments&quot;]=&quot;cumsum of rows 2,4-7, ignore rows 0, 1, 3&quot;
df.loc[df.index==8, &quot;comments&quot;]=&quot;cumsum of rows 2,5-8, ignore rows 0, 1, 3, 4&quot;
df.loc[df.index==9, &quot;comments&quot;]=&quot;cumsum of rows 2,5-9, ignore rows 0, 1, 3, 4&quot;
print(df)

In this example, there are seven (7) unique values in the ID column (a, b, c ,d, e, f, g), so the cumsum should only ever sum a max of seven (7) records as its output on any row.
Is this possible using combinations of functions such as cumsum(), groupby(), duplicated(), drop_duplicates(), and avoiding the use of an iterative loop?
I've tried the below
df[&quot;duped&quot;] = np.where(df[&quot;id&quot;].duplicated(keep='last'),0,1)
df[&quot;value_duped&quot;] = df[&quot;duped&quot;] * df[&quot;value&quot;]
df[&quot;desired_output_attempt&quot;] = df[&quot;cumsum_of_value&quot;] - df[&quot;value_duped&quot;]

But it doesn't come close to the correct answer. I can't think of how to get something like this to result in the desired output without iterating.
","Try:
df[&quot;out&quot;] = (
    df.groupby(&quot;id&quot;)[&quot;value&quot;].transform(&quot;diff&quot;).fillna(df[&quot;value&quot;]).cumsum().astype(int)
)

print(df)

Prints:
  id  value  cumsum_of_value  desired_output  out
0  a     12               12              12   12
1  b     14               26              26   26
2  c      3               29              29   29
3  a     13               42              30   30
4  b     16               58              32   32
5  e      7               65              39   39
6  f      4               69              43   43
7  a      6               75              36   36
8  b     10               85              30   30
9  k     18              103              48   48

"
"Below is my code-
Elasticsearch is not using https protocol, it's using http protocol.
pip uninstall elasticsearch
pip install elasticsearch==7.13.4
import elasticsearch.helpers
from elasticsearch import Elasticsearch
# from elasticsearch import Elasticsearch, RequestsHttpConnection

es_host = '&lt;&gt;'
es_port = '&lt;&gt;'
es_username = '&lt;&gt;'
es_password = '&gt;&lt;'
es_index = '&lt;&gt;'

es = Elasticsearch([{'host':str(es_host),'port':str(es_port)}], http_auth=(str(es_username), str(es_password)))

es.indices.refresh(index=es_index)

Error-
10 es = Elasticsearch([{'host': str(es_host), 'port': str(es_port)}],http_auth=(str(es_username), str(es_password)))
     11 
     12 es.indices.refresh(index=es_index)

3 frames
/usr/local/lib/python3.7/dist-packages/elasticsearch/_sync/client/__init__.py in __init__(self, hosts, cloud_id, api_key, basic_auth, bearer_auth, opaque_id, headers, connections_per_node, http_compress, verify_certs, ca_certs, client_cert, client_key, ssl_assert_hostname, ssl_assert_fingerprint, ssl_version, ssl_context, ssl_show_warn, transport_class, request_timeout, node_class, node_pool_class, randomize_nodes_in_pool, node_selector_class, dead_node_backoff_factor, max_dead_node_backoff, serializer, serializers, default_mimetype, max_retries, retry_on_status, retry_on_timeout, sniff_on_start, sniff_before_requests, sniff_on_node_failure, sniff_timeout, min_delay_between_sniffing, sniffed_node_callback, meta_header, timeout, randomize_hosts, host_info_callback, sniffer_timeout, sniff_on_connection_fail, http_auth, maxsize, _transport)

/usr/local/lib/python3.7/dist-packages/elasticsearch/_sync/client/utils.py in client_node_configs(hosts, cloud_id, **kwargs)

/usr/local/lib/python3.7/dist-packages/elasticsearch/_sync/client/utils.py in hosts_to_node_configs(hosts)

/usr/local/lib/python3.7/dist-packages/elasticsearch/_sync/client/utils.py in host_mapping_to_node_config(host)

TypeError: __init__() missing 1 required positional argument: 'scheme'

When I add &quot;scheme&quot;
Code-
es = Elasticsearch([{'host':str(es_host),'port':str(es_port)}], http_auth=(str(es_username), str(es_password)), scheme=&quot;http&quot;,verify_certs=False)

Error-
__init__() got an unexpected keyword argument 'scheme'

I checked and tried connection to ES but its not connecting.
","I ran into a similar error. I am using elasticsearch==8.3.1. When you construct your url with the list of dictionaries, you need to define the schema. Add &quot;scheme&quot;: &quot;https&quot; to your dictionary and that will solve the missing argument.
es = Elasticsearch(
    [
        {'host': 'localhost', 'port': '9200', &quot;scheme&quot;: &quot;https&quot;}
    ],
        basic_auth=('elastic', '&lt;password&gt;')
)

In your case, you should convert your instantiation to as follows:
es = Elasticsearch(
    [
        {
            'host':str(es_host),
            'port':str(es_port),
            'scheme': &quot;https&quot;
        }
    ], 
    http_auth=(str(es_username), str(es_password))
)

I am not sure if the scheme is http or https, that is something you'll need to dig into.
"
"I have this application:
import enum
from typing import Annotated, Literal

import uvicorn
from fastapi import FastAPI, Query, Depends
from pydantic import BaseModel

app = FastAPI()


class MyEnum(enum.Enum):
    ab = &quot;ab&quot;
    cd = &quot;cd&quot;


class MyInput(BaseModel):
    q: Annotated[MyEnum, Query(...)]


@app.get(&quot;/&quot;)
def test(inp: MyInput = Depends()):
    return &quot;Hello world&quot;


def main():
    uvicorn.run(&quot;run:app&quot;, host=&quot;0.0.0.0&quot;, reload=True, port=8001)


if __name__ == &quot;__main__&quot;:
    main()

curl http://127.0.0.1:8001/?q=ab or curl http://127.0.0.1:8001/?q=cd returns &quot;Hello World&quot;
But any of these

curl http://127.0.0.1:8001/?q=aB
curl http://127.0.0.1:8001/?q=AB
curl http://127.0.0.1:8001/?q=Cd
etc

returns 422Unprocessable Entity which makes sense.
How can I make this validation case insensitive?
","You could make case insensitive enum values by overriding the Enum's _missing_ method . As per the documentation, this classmethod—which by default does nothing—can be used to look up for values not found in cls; thus, allowing one to try and find the enum member by value.
Note that one could extend from the str class when declaring the enumeration class (e.g., class MyEnum(str, Enum)), which would indicate that all members in the enum must have values of the specified type (e.g., str). This would also allow comparing a string to an enum member (using the equality operator ==), without having to use the .value attribute on the enum member (e.g., if member.lower() == value). Otherwise, if the enumeration class was declared as class MyEnum(Enum) (without str subclass), one would need to use the .value attribute on the enum member (e.g., if member.value.lower() == value) to safely compare the enum member to a string.
Also, note that calling the lower() function on the enum member (i.e., member.lower()) would not be necessary, unless the enum member values of your class include uppercase (or a combination of uppercase and lowercase) letters as well (e.g., ab = 'aB', cd = 'Cd', etc.). Hence, for the example below, where only lowercase letters are used, you could avoid using it, and instead simply use  if member == value to compare the enum member to a value; thus, saving you from calling the lower() funciton on every member in the class.
Example 1
from enum import Enum

class MyEnum(str, Enum):
    ab = 'ab'
    cd = 'cd'
    
    @classmethod
    def _missing_(cls, value):
        value = value.lower()
        for member in cls:
            if member.lower() == value:
                return member
        return None

Generic Version (with FastAPI example)
from fastapi import FastAPI
from enum import Enum


app = FastAPI()


class CaseInsensitiveEnum(str, Enum):
    @classmethod
    def _missing_(cls, value):
        value = value.lower()
        for member in cls:
            if member.lower() == value:
                return member
        return None
        

class MyEnum(CaseInsensitiveEnum):
    ab = 'aB'
    cd = 'Cd'


@app.get(&quot;/&quot;)
def main(q: MyEnum):
    return q

In case you needed the Enum query parameter to be defined using Pydantic's BaseModel, you could then use the below (see this answer and this answer for more details):
from fastapi import Query, Depends
from pydantic import BaseModel

...

class MyInput(BaseModel):
    q: MyEnum = Query(...)


@app.get(&quot;/&quot;)
def main(inp: MyInput = Depends()):
    return inp.q

In both cases, the endpoint could be called as follows:
http://127.0.0.1:8000/?q=ab
http://127.0.0.1:8000/?q=aB
http://127.0.0.1:8000/?q=cD
http://127.0.0.1:8000/?q=CD
...

Example 2
In Python 3.11+, one could instead use the newly introduced StrEnum, which allows using the auto() feature, resulting in the lower-cased version of the member's name as the value.
from enum import StrEnum, auto

class MyEnum(StrEnum):    
    AB = auto()
    CD = auto()
    
    @classmethod
    def _missing_(cls, value):
        value = value.lower()
        for member in cls:
            if member == value:
                return member
        return None

"
"I am trying to understand how exactly code-wise the hooks operate in PyTorch. I have a model and I would like to set a forward and backward hook in my code. I would like to set a hook in my model after a specific layer and I guess the easiest way is to set a hook to this specific module. This introductory video warns that the backward module contains a bug, but I am not sure if that is still the case.
My code looks as follows:
def __init__(self, model, attention_layer_name='desired_name_module',discard_ratio=0.9):
  self.model = model
  self.discard_ratio = discard_ratio
  for name, module in self.model.named_modules():
    if attention_layer_name in name:
        module.register_forward_hook(self.get_attention)
        module.register_backward_hook(self.get_attention_gradient)

  self.attentions = []
  self.attention_gradients = []

def get_attention(self, module, input, output):
  self.attentions.append(output.cpu())

def get_attention_gradient(self, module, grad_input, grad_output):
  self.attention_gradients.append(grad_input[0].cpu())

def __call__(self, input_tensor, category_index):
  self.model.zero_grad()
  output = self.model(input_tensor)
  loss = ...
  loss.backward()

I am puzzled to understand how code-wise the following lines work:
module.register_forward_hook(self.get_attention)
module.register_backward_hook(self.get_attention_gradient)

I am registering a hook to my desired module, however, then, I am calling a function in each case without any input. My question is Python-wise, how does this call work exactly? How the arguments of the register_forward_hook and register_backward_hook operate when the function it's called?
","How does a hook work?
A hook allows you to execute a specific function - referred to as a &quot;callback&quot; - when a particular action has been performed. In this case, you are expecting self.get_attention to be called once the forward function of module has been accessed. To give a minimal example of how a hook would look like. I define a simple class on which you can register new callbacks through register_hook, then when the instance is called (via __call__), all hooks will be called with the provided arguments:
class Obj:
    def __init__(self):
        self.hooks = []
    
    def register_hook(self, hook):
        self.hooks.append(hook)

    def __call__(self, x, y):
        print('instance called')
        for hook in self.hooks:
            hook(x, y)

First, implement two hooks for demonstration purposes:
def foo(x, y):
    print(f'foo called with {x} and {y}')
def bar(x, _):
    print(f'bar called with {x}')

And initialize an instance of Obj:
obj = Obj()

You can register a hook and call the instance:
&gt;&gt;&gt; obj.register_hook(foo)
&gt;&gt;&gt; obj('yes', 'no')
instance called
foo called with yes and no

You can add hooks on top and call again to compare, here both hooks are triggered:
&gt;&gt;&gt; obj.register_hook(bar)
&gt;&gt;&gt; obj('yes', 'no')
instance called
foo called with yes and no
bar called with yes


Using hooks in PyTorch
There are two primary hooks in PyTorch: forward and backward. You also have pre- and post-hooks. Additionally there exists hooks on other actions such as load_state_dict...

To attach a hook on the forward process of a nn.Module, you should use register_forward_hook, the argument is a callback function that expects module, args, and output. This callback will be triggered on every forward execution.

For backward hooks, you should use register_full_backward_hook, the registered hook expects three arguments: module, grad_input, and grad_output. As of recent PyTorch versions, register_backward_hook has been deprecated and should not be used.


One side effect here is that you are registering the hook with self.get_attention and self.get_attention_gradient. The function passed to the register handler is not unbound to the class instance! In other words, on execution, these will be called without the self argument like:
self.get_attention(module, input, output)
self.get_attention_gradient(module, grad_input, grad_output)

This will fail. A simple way to fix this is to wrap the hook with a lambda when you register it:
module.register_forward_hook(
    lambda *args, **kwargs: Routine.get_attention(self, *args, **kwargs))

All in all, your class could look like this:
class Routine:
    def __init__(self, model, attention_layer_name):
        self.model = model

        for name, module in self.model.named_modules():
            if attention_layer_name in name:
                module.register_forward_hook(
                    lambda *args, **kwargs: Routine.get_attention(self, *args, **kwargs))
                module.register_full_backward_hook(
                    lambda *args, **kwargs: Routine.get_attention_gradient(self, *args, **kwargs))

        self.attentions = []
        self.attention_gradients = []

    def get_attention(self, module, input, output):
        self.attentions.append(output.cpu())

    def get_attention_gradient(self, module, grad_input, grad_output):
        self.attention_gradients.append(grad_input[0].cpu())

    def __call__(self, input_tensor):
        self.model.zero_grad()
        output = self.model(input_tensor)
        loss = output.mean()
        loss.backward()

When initialized with a single linear layer model:
routine = Routine(nn.Sequential(nn.Linear(10,10)), attention_layer_name='0')

You can call the instance, this will first trigger the forward hook with (because of self.model(input_tensor), and then the backward hook (because of loss.backward()).
&gt;&gt;&gt; routine(torch.rand(1,10, requires_grad=True))

Following your implementation, your forward hook is caching the output of the &quot;attention_layer_name&quot; layer in self.attentions.
&gt;&gt;&gt; routine.attentions
[tensor([[-0.3137, -0.2265, -0.2197,  0.2211, -0.6700, 
          -0.5034, -0.1878, -1.1334,  0.2025,  0.8679]], grad_fn=&lt;...&gt;)]

Similarly for the self.attention_gradients:
&gt;&gt;&gt; routine.attentions_gradients
[tensor([[-0.0501,  0.0393,  0.0353, -0.0257,  0.0083,  
           0.0426, -0.0004, -0.0095, -0.0759, -0.0213]])] 


It is important to note that the cached outputs and gradients will remain in self.attentions and self.attentions_gradients and get appended on every execution of Routine.__call__.
"
"I was under the impression that typing module in Python is mostly for increasing code readability and for code documentation purposes.
After playing around with it and reading about the module, I've managed to confuse myself with it.
Code below works even though those two variables are not initialized (as you would normally initialize them e.g. a = &quot;test&quot;).
I've only put a type hint on it and everything seems ok. That is, I did not get a NameError as I would get if I just had a in my code NameError: name 'a' is not defined
Is declaring variables in this manner (with type hints) an OK practice? Why does this work?
from typing import Any

test_var: int
a: Any

print('hi')

I expected test_var: int to return an error saying that test_var is not initiated and that I would have to do something like test_var: int = 0 (or any value at all). Does this get set to a default value because I added type hint to it?
","It is fairly straightforward, when you consider the namespaces involved. This is hinted at by the fact that you get a NameError, when you actually try and do anything with test_var, such as passing it to a function (like print). It tells you that the name you used is not known to the interpreter.
What does variable assignment do?
What happens, when you assign a value to a variable in the global namespace of a module for the first time, is it gets added to that module's globals dictionary with the key being the variable name as a string and the value being, well, its value. You can see this dictionary by calling the built-in globals function in that module:
(I will be using pprint in some of the following examples to make the output easier to read.)
from pprint import pprint

a = 1

pprint(globals())

The output looks something like this:
{'__annotations__': {},
 ...
 '__name__': '__main__',
 ...
 'a': 1,
 ...}

There are various other keys in the globals dictionary that we can ignore for this matter. But you can see that the key 'a' appears in it and its associated value is the 1 we assigned to the variable named a before. (In case this is not obvious, the order of statements matters; if you check the output of globals() before assigning a value to a, there will be no entry in that dictionary for it.)
What does annotation do?
When you look closer at that dictionary, you'll find another interesting key there, namely __annotations__. Right now, its value is an empty dictionary. But I bet you can already guess, what will happen, if we annotate our variable with a type:
from pprint import pprint

a: int = 1

pprint(globals())

The output:
{'__annotations__': {'a': &lt;class 'int'&gt;},
 ...
 'a': 1,
 ...}

When we add a type hint to (i.e. annotate) a variable, the interpreter adds that name and type to the relevant __annotations__ dictionary (see annotation assignment docs); in this case that of our module. By the way, since the __annotations__ dictionary is in our global namespace we can access it directly:
a: int = 1

print(&quot;a&quot; in globals())        # True
print(&quot;a&quot; in __annotations__)  # True

As you can see, __annotations__ is a variable like any other, except that it is present by default, without you having to manually assign anything to it. It gets updated any time a variable is annotated in its scope.

Side note
There is nothing forcing us to annotate a correctly. We can assign a wrong type to it and Python will gladly add that to the __annotations__ dictionary and this incorrect annotation will have no effect whatsoever on the value assignment:
a: str = 1

print(a)                # 1
print(type(a) is int)   # True
print(__annotations__)  # {'a': &lt;class 'str'&gt;}

In fact, the interpreter's laissez-faire policy regarding annotations goes so far that we can use literally anything for the annotation as long as it is a syntactically valid expression, even though it may make zero sense semantically. For example a complex number literal:
a: 2+1j = 1

print(a)                # 1
print(type(a) is int)   # True
print(__annotations__)  # {'a': (2+1j)}

You can even call arbitrary functions in the annotation itself:
a: bin(3) = 1

print(a)                # 1
print(type(a) is int)   # True
print(__annotations__)  # {'a': '0b11'}

But let us get back to the topic.

Can you annotate without assigning?
Finally, what happens, if we just annotate without assigning a value to a variable?
a: int

print(&quot;a&quot; in globals())        # False
print(&quot;a&quot; in __annotations__)  # True

And that is the explanation of why we get an error, if we try and e.g. print out a in this example, but otherwise don't get any error. The code merely told the interpreter (and any static type checker) about the annotation, but it assigned no value, thus not creating an entry in the global namespace dictionary.
It makes sense, if you think about it: What should be set as the value for a in that namespace? It has no value (not even None or NotImplemented or anything like that). To the interpreter the a: int line merely meant the creation of an entry in the __annotations__ of our module, which is perfectly valid.
Runtime meaning of annotations
I would also like to stress the fact that the annotation is not meaningless for the interpreter and thus runtime, as some people often claim. It is admittedly rarely used, but as we just saw in the example, you can absolutely work with annotations at runtime. Whether or not this is useful is obviously up to you. Some packages like Pydantic or the standard library's dataclasses actually rely heavily on annotations for their purposes.
The value set in the __annotations__ dictionary in our example is actually a reference to the int class. So we can absolutely work with it at runtime, if we want to:
a: int

a_type = __annotations__[&quot;a&quot;]
print(a_type is int)  # True
print(a_type(&quot;2&quot;))    # 2

You can play around with this concept in class namespaces as well (not just with the module namespace), but I'll leave this as an exercise for the reader.
So to wrap up, for a name to be added to any namespace, it must have a value assigned to it. Not assigning a value and just providing an annotation is totally fine to create an entry in that namespace's __annotations__.
"
"df.filter(pl.col(&quot;MyDate&quot;) &gt;= &quot;2020-01-01&quot;)

does not work like it does in pandas.
I found a workaround
df.filter(pl.col(&quot;MyDate&quot;) &gt;= pl.datetime(2020,1,1))

but this does not solve a problem if I need to use string variables.
","You can turn the string into a date type e.g. with .str.to_date()
Building on the example above:
import polars as pl
from datetime import datetime

df = pl.DataFrame({
    &quot;dates&quot;: [datetime(2021, 1, 1), datetime(2021, 1, 2), datetime(2021, 1, 3)],
    &quot;vals&quot;: range(3)
})

df.filter(pl.col('dates') &gt;= pl.lit(my_date_str).str.to_date())

shape: (2, 2)
┌─────────────────────┬──────┐
│ dates               ┆ vals │
│ ---                 ┆ ---  │
│ datetime[μs]        ┆ i64  │
╞═════════════════════╪══════╡
│ 2021-01-02 00:00:00 ┆ 1    │
│ 2021-01-03 00:00:00 ┆ 2    │
└─────────────────────┴──────┘

"
"I know how to pass fixed arguments in the launch.json, e.g. In Visual Studio Code, how to pass arguments in launch.json . What I really need is a prompt where I can give a value for an argument that changes.
In addition, my argument is a (data) directory for which there is a very ugly long absolute path. I'd really like to be able to set the working directory to a path which contains each of my individual data directories so I only need to provide a relative directory path, i.e. just the directory name.
I'm working with Python, on Windows (not my choice) using VS Code 1.55.2 (not my choice, either).
","You can use input variables
{
  &quot;version&quot;: &quot;0.2.0&quot;,
  &quot;configurations&quot;: [
    {
      &quot;name&quot;: &quot;Python: Current File with arguments&quot;,
      &quot;type&quot;: &quot;python&quot;,
      &quot;request&quot;: &quot;launch&quot;,
      &quot;program&quot;: &quot;${file}&quot;,
      &quot;args&quot;: [
        &quot;--dir&quot;,
        &quot;/some/fixed/dir/${input:enterDir}&quot;
      ]
    }
  ],
  &quot;inputs&quot;: [
    {
      &quot;id&quot;: &quot;enterDir&quot;,
      &quot;type&quot;: &quot;promptString&quot;,
      &quot;description&quot;: &quot;Subdirectory to process&quot;,
      &quot;default&quot;: &quot;data-0034&quot;
    }
  ]
}

You can place the ${input:enterDir} in any string in the task &quot;configurations&quot; like the &quot;cwd&quot; property.
If you like to pick a directory from a list because it is dynamic you can use the extension Command Variable that has the command pickFile
Command Variable v1.36.0 supports the fixed folder specification.
{
  &quot;version&quot;: &quot;0.2.0&quot;,
  &quot;configurations&quot;: [
    {
      &quot;name&quot;: &quot;Python: Current File with arguments&quot;,
      &quot;type&quot;: &quot;python&quot;,
      &quot;request&quot;: &quot;launch&quot;,
      &quot;program&quot;: &quot;${file}&quot;,
      &quot;args&quot;: [
        &quot;--dir&quot;,
        &quot;${input:pickDir}&quot;
      ]
    }
  ],
  &quot;inputs&quot;: [
    {
      &quot;id&quot;: &quot;pickDir&quot;,
      &quot;type&quot;: &quot;command&quot;,
      &quot;command&quot;: &quot;extension.commandvariable.file.pickFile&quot;,
      &quot;args&quot;: {
        &quot;include&quot;: &quot;**/*&quot;,
        &quot;display&quot;: &quot;fileName&quot;,
        &quot;description&quot;: &quot;Subdirectory to process&quot;,
        &quot;showDirs&quot;: true,
        &quot;fromFolder&quot;: { &quot;fixed&quot;: &quot;/some/fixed/dir&quot; }
      }
    }
  ]
}

On Unix like systems you can include the folder in the include glob pattern. On Windows you have to use the fromFolder to convert the directory path to a usable glob pattern. If you have multiple folders you can use the predefined property.
"
"I am trying to write a python script that will convert triangular-mesh objects to quad-mesh objects.

For example, image (a) will be my input (.obj/.stl) file and image (b) will be the output.
I am a noob with mesh-algorithms or how they work all together. So, far this is the script I have written:
import bpy

inp = 'mushroom-shelve-1-merged.obj'


# Load the triangle mesh OBJ file
bpy.ops.import_scene.obj(filepath=inp, 
                        use_smooth_groups=False,
                        use_image_search=False)

# Get the imported mesh
obj = bpy.context.selected_objects[0]

# Convert triangles to quads
# The `beauty` parameter can be set to False if desired
bpy.ops.object.mode_set(mode='EDIT')
bpy.ops.mesh.select_all(action='SELECT')
bpy.ops.mesh.tris_convert_to_quads(beauty=True)
bpy.ops.object.mode_set(mode='OBJECT')

# Export to OBJ with quads
bpy.ops.export_scene.obj(filepath='quad_mesh.obj')

This results in the following error:
Traceback (most recent call last):
  File &quot;/home/arrafi/mesh-convert-application/test.py&quot;, line 8, in &lt;module&gt;
    bpy.ops.import_scene.obj(filepath=inp, 
  File &quot;/home/arrafi/mesh-convert-application/venv/lib/python3.10/site-packages/bpy/4.0/scripts/modules/bpy/ops.py&quot;, line 109, in __call__
    ret = _op_call(self.idname_py(), kw)
AttributeError: Calling operator &quot;bpy.ops.import_scene.obj&quot; error, could not be found

Any help with what I am doing wrong here would be greatly appreciated.

Also please provide your suggestions for if you know any better way to convert triangular-mesh to quad-mesh with Python.
If you guys know of any API that I can call with python to do the conversion, that would work too.

","Turns out bpy.ops.import_scene.obj was removed at bpy==4 which is the latest blender-api for python, hence the error. In bpy&gt;4 you have to use bpy.ops.wm.obj_import(filepath='')
I just downgraded to bpy==3.60 to import object directly in the current scene.
pip install bpy==3.6.0

I also modified my script to take input of .obj files in triangular-mesh and then convert the mesh to quadrilateral, then export as both stl and obj. Here's my working script:
def convert_tris_to_quads(obj_path, export_folder):
    try:
        filename = os.path.basename(obj_path).split('.')[0]
        logging.info(f&quot;Importing {obj_path}&quot;)

        bpy.ops.object.select_all(action='DESELECT')
        bpy.ops.object.select_by_type(type='MESH')
        bpy.ops.object.delete()
    
        bpy.ops.import_scene.obj(filepath=obj_path)
        print(&quot;current objects in the scene: &quot;, [obj for obj in bpy.context.scene.objects])
        for obj in bpy.context.selected_objects:
            bpy.context.view_layer.objects.active = obj
            
        logging.info(&quot;Converting mesh&quot;)
        bpy.ops.object.mode_set(mode='EDIT')
        bpy.ops.mesh.select_all(action='SELECT')
        bpy.ops.mesh.tris_convert_to_quads()
        bpy.ops.object.mode_set(mode='OBJECT')

        # Export to OBJ
        obj_export_path = export_folder + filename + '_quad.obj'
        logging.info(f&quot;Exporting OBJ to {obj_export_path}&quot;)
        bpy.ops.export_scene.obj(filepath=obj_export_path, use_selection=True)

        # Export to STL
        stl_export_path = export_folder + filename + '_quad.stl'
        logging.info(f&quot;Exporting STL to {stl_export_path}&quot;)
        bpy.ops.export_mesh.stl(filepath=stl_export_path, use_selection=True)

    except Exception as e:
        logging.error(f&quot;Error processing {obj_path}: {e}&quot;)
        return False

This still might not be the best approach to this, so do let me know if anyone know any better approach.
"
"Consider the following code in Python, where multiplying a pre-transposed matrix yields faster execution time compared to multiplying a non-transposed matrix:
import numpy as np
import time

# Generate random matrix
matrix_size = 1000
matrix = np.random.rand(matrix_size, matrix_size)

# Transpose the matrix
transposed_matrix = np.transpose(matrix)

# Multiply non-transposed matrix
start = time.time()
result1 = np.matmul(matrix, matrix)
end = time.time()
execution_time1 = end - start

# Multiply pre-transposed matrix
start = time.time()
result2 = np.matmul(transposed_matrix, transposed_matrix)
end = time.time()
execution_time2 = end - start

print(&quot;Execution time (non-transposed):&quot;, execution_time1)
print(&quot;Execution time (pre-transposed):&quot;, execution_time2)

Surprisingly, multiplying the pre-transposed matrix is faster. One might assume that the order of multiplication should not affect the performance significantly, but there seems to be a difference.
Why does processing a pre-transposed matrix result in faster execution time compared to a non-transposed matrix? Is there any underlying reason or optimization that explains this behavior?
UPDATE
I've taken the comments about the cache into consideration and I'm generating new matrices on each loop:
import numpy as np
import time
import matplotlib.pyplot as plt

# Generate random matrices
matrix_size = 3000



# Variables to store execution times
execution_times1 = []
execution_times2 = []

# Perform matrix multiplication A @ B^T and measure execution time for 50 iterations
num_iterations = 50
for _ in range(num_iterations):
    matrix_a = np.random.rand(matrix_size, matrix_size)
    start = time.time()
    result1 = np.matmul(matrix_a, matrix_a)
    end = time.time()
    execution_times1.append(end - start)

# Perform matrix multiplication A @ B and measure execution time for 50 iterations
for _ in range(num_iterations):
    matrix_b = np.random.rand(matrix_size, matrix_size)
    start = time.time()
    result2 = np.matmul(matrix_b, matrix_b.T)
    end = time.time()
    execution_times2.append(end - start)

# Print average execution times
avg_execution_time1 = np.mean(execution_times1)
avg_execution_time2 = np.mean(execution_times2)
#print(&quot;Average execution time (A @ B^T):&quot;, avg_execution_time1)
#print(&quot;Average execution time (A @ B):&quot;, avg_execution_time2)

# Plot the execution times
plt.plot(range(num_iterations), execution_times1, label='A @ A')
plt.plot(range(num_iterations), execution_times2, label='B @ B.T')
plt.xlabel('Iteration')
plt.ylabel('Execution Time')
plt.title('Matrix Multiplication Execution Time Comparison')
plt.legend()
plt.show()

# Display BLAS configuration
np.show_config()

Results:

blas_mkl_info:
    libraries = ['mkl_rt']
    library_dirs = ['C:/Users/User/anaconda3\\Library\\lib']
    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
    include_dirs = ['C:/Users/User/anaconda3\\Library\\include']
blas_opt_info:
    libraries = ['mkl_rt']
    library_dirs = ['C:/Users/User/anaconda3\\Library\\lib']
    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
    include_dirs = ['C:/Users/User/anaconda3\\Library\\include']
lapack_mkl_info:
    libraries = ['mkl_rt']
    library_dirs = ['C:/Users/User/anaconda3\\Library\\lib']
    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
    include_dirs = ['C:/Users/User/anaconda3\\Library\\include']
lapack_opt_info:
    libraries = ['mkl_rt']
    library_dirs = ['C:/Users/User/anaconda3\\Library\\lib']
    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
    include_dirs = ['C:/Users/User/anaconda3\\Library\\include']
Supported SIMD extensions in this NumPy install:
    baseline = SSE,SSE2,SSE3
    found = SSSE3,SSE41,POPCNT,SSE42,AVX,F16C,FMA3,AVX2
    not found = AVX512F,AVX512CD,AVX512_SKX,AVX512_CLX,AVX512_CNL

","It doesn't seem really obvious on my machine.
On 1000 runs. I get these timings (x=non transposed, y=transposed).
There are more red dots (under the y=x axis) than blue dots. 685/315 to be more accurate. So, p-value wise, no doubt, that cannot be just random effect.
(1000 coins drawn, with 685 heads is a clear anomaly)

But timing-wise, it is not obvious. The cluster is mainly centered on the y=x axis.
Now I started this answer because I was pretty sure that this was a cache problem. When I was in engineer school (a very long time ago, when those considerations where even more important they are now, and taught by teachers who, themselves date back from a time when it was even more important), in HPC lessons, we were taught to be very careful when switching from Fortran to C, because of cache effect: when iterating an array, it is very important to interate it in the order it is in memory (which in numpy is still called either &quot;C&quot; order vs &quot;fortran&quot; order, proof that it is still an important consideration for people who care more that I do - I rarely need to care in my everyday job, hence the reason I invoke school memory and not job memory).
Because when dealing with the number that is right next to the one you've just processed before in memory, that number is probably already loaded in cache memory. While if the next number you process is 1 row under (in C order, so further in memory), then, it is more likely that it is not in cache. With nowadays cache size, it takes big matrix so that it makes a difference tho.
Since transpose doesn't move any data, and just adjust the strides, the effect of working on transposed matrix is that you change the order in memory of the processed data. So, if you consider the naive algorithm
for i in range(N):
    for j in range(N):
        res[i,j]=0
        for k in range(N):
            res[i,j] += A[i,k] * B[k,j]

if A and B are in C order, then iteration of matrix A is done in memory order (we iterate along a row, columns by columns, so adjacent number in memory one after the other), while B is not.
If that order is inversed, for example, because they have been transposed, then it is the reverse. It is B that is iterated in the order that  won't pose a cache problem and A that is not.
Well, no need to stay too long on this, since I tell all that to explain why I wanted to investigate possibility of a cache problem (my intend was to compare the same multiplication with a copy of a transposed matrix, so that it is the same matrix multiplication, with only order changing. And also to try to see if there is a threshold in matrix size under which the phenomenon is not visible, which would also validate cache problem, since, for this to matter, the whole matrix must not fit in cache)
But, the first step while doing so, is also to start to avoid bias, because first computation use data not yet in cache, while second use data already in cache (especially in the case where the whole matrix fits in cache).
So, here is the first thing I've tried: just inverted computation order. Compute fist on transposed_matrix, and then on matrix.

This time, shifting is in favor of blue dots (and, of course, I've changed only the computation order, not the meaning of axis. So x is still matrix@matrix timing, and y still transposed_matrix
The number of red dots this time is 318 vs 682. So, almost exactly the opposite as before.
So, conclusion (valid at least for my machine): this is indeed a cache problem. But a cache problem caused only by the fact that there is a bias in favor of transposed_matrix: it is already in cache (since the data are the same as the data of matrix), when you use it to compute.
Edit: about question update.
As I said in comment (but since the question was updated, and this answer, already quite upvoted, I think it is important that it also appears in that answer, for future readers), the update is something different.
Your first question was about A@A vs A.T@A.T. The second appeared to be faster. But it was only with 1 single operation. So the reason, as I've shown, was just due to the fact that when second operation is done, A is already in cache memory (which was not the case, when first operation was done). Because A.T is the same data as A (not a copy. But the same data, at same memory address).
My previous answer shows that if you reverse, and compute A.T@A.T fist, then A@A, then it is, on the contrary A.T@A.T that is slower, and in the exact same proportion.
Another way to show it is
import numpy as np
import timeit 

A=np.random.normal(0,1,(1000,1000))
B=A.copy()

A@A
print(timeit.timeit(lambda: A@A, number=20))
A.T@A.T
print(timeit.timeit(lambda: A.T@A.T, number=20))
B@B
print(timeit.timeit(lambda: B@B, number=20))

(The fact to perform A@A before timeit, is just to ensure that first of the 20 computations is not slower because of cache consideration)
On my computer, all those operation takes 1 second almost exactly (the number=20 was chosen so that it takes 1 second)
This time, no cache effect, because we run things 21 times each, not counting time at the 1st run. And no influence of .T
Now, for your question update, that is something else
A@A.T
print(timeit.timeit(lambda: A@A.T, number=20))
A.T@A
print(timeit.timeit(lambda: A.T@A, number=20))
A@B.T
print(timeit.timeit(lambda: A@B.T, number=20))

This times, the 1st two operations takes only 650 ms. And not because of cache: it is the same whatever the order of those operations.
That is because numpy is able to detect that A.T and A are the same matrix, but with one transposition operation.
(It is quite easy for it to detect so: it is the same data address, but strides and shape (well here shape is square anyway; but more importantly, strides is inverted) are inverted: A.strides → (8000,8), A.T.strides → (8, 8000).
So, it is easy for numpy to realize that this is a A@A.T situation. And therefore to apply an algorithm that computes that faster. As said in comment (and said before I did by others in comments, but also by others, days ago, who misread your first question... but were right to do so, since they answered in advance to what is now the update): A@A.T is symmetrical. So, there are some easy correction here.
Note that
timeit.timeit(lambda: A@B, number=20)
timeit.timeit(lambda: A@B.T, number=20)

are both 1 second (as A@A, and A.T@A.T are). So, it is easy to understand that A@B, A@A, A.T@A.T all just use one standard &quot;matrix multiplication&quot; algorithm. Which A@A.T, A.T@A use a faster one.
Since B is a copy of A, A@B.T has the same symmetrical result as A@A.T. But this times, because it is a copy, numpy cannot realize that it is a A@A.T situation, cannot realize that it is a symmetrical result (before the result is computed). So A@B.T has the same standard &quot;1 second&quot; timing as A@A. While A@A.T has not.
Which confirms that it does rely on the same address, inverted strides criteria. As long as it is either not the same address, or not the same strides inverted, standard algorithm, 1 second. If it is both the same address, but strides inverted, then, special algorithm, 650 ms.
"
"In GNU awk, there is a four argument version of split that can optionally keep all the separators from the split in a second array. This is useful if you want to reconstruct a select subset of columns from a file where the delimiter may be more complicated than just a single character.
Suppose I have the following file:
# sed makes the invisibles visible...
# âˆ™ is a space; \t is a literal tab; $ is line end
$ sed -E 's/\t/\\t/g; s/ /âˆ™/g; s/$/\$/' f.txt
a\tâˆ™âˆ™bâˆ™c\tdâˆ™_âˆ™e$
aâˆ™âˆ™âˆ™bâˆ™c\tdâˆ™_âˆ™e$
âˆ™âˆ™âˆ™aâˆ™âˆ™âˆ™bâˆ™c\tdâˆ™_âˆ™e$
aâˆ™âˆ™âˆ™b_c\tdâˆ™_âˆ™e\t$
abcd$

Here I have a field comprised of anything other than the delimiter character set, and
a delimiter of one or more characters of the set [\s_].
With gawk, you can do:
gawk '{
    printf &quot;[&quot;
    n=split($0, flds, /[[:space:]_]+/, seps)
    for(i=1; i&lt;=n; i++) 
           printf &quot;[\&quot;%s\&quot;, \&quot;%s\&quot;]%s&quot;, flds[i], seps[i], i&lt;n ? &quot;, &quot; : &quot;]&quot; ORS
    }
' f.txt

Prints (where the first element is the field, the second is the match to the delimiter regexp):
[[&quot;a&quot;, &quot;      &quot;], [&quot;b&quot;, &quot; &quot;], [&quot;c&quot;, &quot;   &quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, &quot;&quot;]]
[[&quot;a&quot;, &quot;   &quot;], [&quot;b&quot;, &quot; &quot;], [&quot;c&quot;, &quot;  &quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, &quot;&quot;]]
[[&quot;&quot;, &quot;   &quot;], [&quot;a&quot;, &quot;   &quot;], [&quot;b&quot;, &quot; &quot;], [&quot;c&quot;, &quot; &quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, &quot;&quot;]]
[[&quot;a&quot;, &quot;   &quot;], [&quot;b&quot;, &quot;_&quot;], [&quot;c&quot;, &quot;  &quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, &quot;   &quot;], [&quot;&quot;, &quot;&quot;]]
[[&quot;abcd&quot;, &quot;&quot;]]

Ruby's str.split, unfortunately, does not have the same functionality. (Neither does Python's or Perl's.)
What you can do is capture the match string from the delimiter regexp:
irb(main):053&gt; s=&quot;a   b c    d _ e&quot;
=&gt; &quot;a   b c    d _ e&quot;
irb(main):054&gt; s.split(/([\s_]+)/)
=&gt; [&quot;a&quot;, &quot;   &quot;, &quot;b&quot;, &quot; &quot;, &quot;c&quot;, &quot;    &quot;, &quot;d&quot;, &quot; _ &quot;, &quot;e&quot;]

Then use that result with .each_slice(2) and replace the nil's with '':
irb(main):055&gt; s.split(/([\s_]+)/).each_slice(2).map{|a,b| [a,b]}
=&gt; [[&quot;a&quot;, &quot;   &quot;], [&quot;b&quot;, &quot; &quot;], [&quot;c&quot;, &quot;    &quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, nil]]
irb(main):056&gt; s.split(/([\s_]+)/).each_slice(2).map{|a,b| [a,b]}.map{|sa| sa.map{|e| e.nil? ? &quot;&quot; : e} }
=&gt; [[&quot;a&quot;, &quot;   &quot;], [&quot;b&quot;, &quot; &quot;], [&quot;c&quot;, &quot;    &quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, &quot;&quot;]]

Which allows gawk's version of split to be replicated:
ruby -ne 'p $_.gsub(/\r?\n$/,&quot;&quot;).split(/([\s_]+)/).each_slice(2).
                map{|a,b| [a,b]}.map{|sa| sa.map{|e| e.nil? ? &quot;&quot; : e} }' f.txt

Prints:
[[&quot;a&quot;, &quot;\t  &quot;], [&quot;b&quot;, &quot; &quot;], [&quot;c&quot;, &quot;\t&quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, &quot;&quot;]]
[[&quot;a&quot;, &quot;   &quot;], [&quot;b&quot;, &quot; &quot;], [&quot;c&quot;, &quot;\t&quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, &quot;&quot;]]
[[&quot;&quot;, &quot;   &quot;], [&quot;a&quot;, &quot;   &quot;], [&quot;b&quot;, &quot; &quot;], [&quot;c&quot;, &quot;\t&quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, &quot;&quot;]]
[[&quot;a&quot;, &quot;   &quot;], [&quot;b&quot;, &quot;_&quot;], [&quot;c&quot;, &quot;\t&quot;], [&quot;d&quot;, &quot; _ &quot;], [&quot;e&quot;, &quot;\t&quot;]]
[[&quot;abcd&quot;, &quot;&quot;]]

So the same output (other than the line with trailing \t which gawk has as an empty field, delimiter combination.)
In Python, roughly the same method also works:
python3 -c '
import sys, re 
from itertools import zip_longest
with open(sys.argv[1]) as f:
    for line in f:
        lp=re.split(r&quot;([\s_]+)&quot;, line.rstrip(&quot;\r\n&quot;))
        print(list(zip_longest(*[iter(lp)]*2, fillvalue=&quot;&quot;)) )
' f.txt   

I am looking for a general algorithm to replicate the functionality of gawk's four argument split in Ruby/Python/Perl/etc. The Ruby and Python I have here works.
Most of solutions (other than for gawk) to I want to split on this delimiter and keep the delimiter? involve a unique regex more complex than simply matching the delimiter. Most seem to be either  scanning for a field, delimiter combination or use lookarounds. I am specifically trying to use a simple regexp that matches the delimiter only without lookarounds. With roughly the same regexp I would have used with GNU awk.
So stated generally:

Take a regexp matching the delimiter fields (without having to think much about the data fields) and put inside a capturing group;
Take the resulting array of [field1, delimiter1, field2, delimiter2, ...] and create array of [[field1, delimiter1], [field2, delimiter2], ...]

That method is easily used in Ruby (see above) and Python (see above) and Perl (I was too lazy to write that one...)
Is this the best way to do this?
","With splitting you always have one more field than the delimiters, which is why you have to fill in an empty string as the delimiter for the last field. A simpler way to achieve the filling would be to always append an empty string to the list returned by the split so that you can use the itertools.batched function (available since Python 3.12, or as a recipe beforehand) to produce easy pairings:
import re
from io import StringIO
from itertools import batched

file = StringIO('''a\t  b c\td _ e
a   b c\td _ e
   a   b c\td _ e
a   b_c\td _ e\t
abcd''')

for line in file:
    print(list(batched(re.split(r&quot;([\s_]+)&quot;, line.rstrip('\r\n')) + [''], 2)))

This outputs:
[('a', '\t  '), ('b', ' '), ('c', '\t'), ('d', ' _ '), ('e', '')]
[('a', '   '), ('b', ' '), ('c', '\t'), ('d', ' _ '), ('e', '')]
[('', '   '), ('a', '   '), ('b', ' '), ('c', '\t'), ('d', ' _ '), ('e', '')]
[('a', '   '), ('b', '_'), ('c', '\t'), ('d', ' _ '), ('e', '\t'), ('', '')]
[('abcd', '')]

Demo here
"
"I was surprised to read here that

The start and step arguments default to None

since it also says:

slice(start, stop, step=1)

Return a slice object representing the set of indices specified by range(start, stop, step).

So I expected the default argument value for the step parameter to be 1.
I know that slice(a, b, None) == slice(a, b, 1) returns False, but I am curious if slice(a, b, None) always returns the same slice as slice(a, b, 1), or if there is some example that I haven't been able to think of for which they will return different slices.
I couldn't find anything about this in the extensive post on slicing here
","Slice's step indeed defaults to None, but using step 1 and None should be equivalent for all practical purposes. That's because in the C code where the step is actually used, there are checks which transform None into 1 anyway:
int
PySlice_GetIndices(PyObject *_r, Py_ssize_t length,
                   Py_ssize_t *start, Py_ssize_t *stop, Py_ssize_t *step)
{
    PySliceObject *r = (PySliceObject*)_r;
    if (r-&gt;step == Py_None) {
        *step = 1;
    } ...    
}

And:
int
PySlice_Unpack(PyObject *_r,
               Py_ssize_t *start, Py_ssize_t *stop, Py_ssize_t *step)
{
    PySliceObject *r = (PySliceObject*)_r;
    ...
    if (r-&gt;step == Py_None) {
        *step = 1;
    }
    ...
}

If you're wondering why they don't just default to 1 instead, perhaps it's because users may still want to slice using None explicitly (e.g. L[1:2:None]), and/or to give 3rd-party types the opportunity to handle 1 and None differently in their __getitem__/__setitem__/__delitem__ implementation.
"
"I'm using Langchain 0.0.345. I cannot get a verbose output of what's going on under the hood using the LCEL approach to chain building.
I have this code:
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.globals import set_verbose

set_verbose(True)

prompt = ChatPromptTemplate.from_template(&quot;tell me a joke about {topic}&quot;)
model = ChatOpenAI()
output_parser = StrOutputParser()

chain = prompt | model | output_parser

chain.invoke({&quot;topic&quot;: &quot;ice cream&quot;})

According to the documentation using set_verbose is the way to have a verbose output showing intermediate steps, prompt builds etc. But the output of this script is just a string without any intermediate steps.
Actually, the module langchain.globals does not appear even mentioned in the API documentation.
I have also tried setting the verbose=True parameter in the model creation, but it also does not work. This used to work with the former approach building with classes and so.
How is the recommended and current approach to have the output logged so you can understand what's going on?
Thanks!
","You can add a callback handler to the invoke method's configuration.
Like this:
from langchain.callbacks.tracers import ConsoleCallbackHandler

# ...your code

chain.invoke({&quot;topic&quot;: &quot;ice cream&quot;}, config={'callbacks': [ConsoleCallbackHandler()]})

Code with change incorporated:
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.callbacks.tracers import ConsoleCallbackHandler

prompt = ChatPromptTemplate.from_template(&quot;tell me a joke about {topic}&quot;)
model = ChatOpenAI()
output_parser = StrOutputParser()

chain = prompt | model | output_parser

chain.invoke({&quot;topic&quot;: &quot;ice cream&quot;}, config={'callbacks': [ConsoleCallbackHandler()]})

The output isn't the same as the original &quot;verbose mode&quot;, but this is the closest alternative.
Alternatives
For more targeted output or less &quot;verbosity&quot;
Try attaching a callback handler to specific objects. For example:
ChatOpenAI().with_config({'callbacks': [ConsoleCallbackHandler()]})

You can learn more about customizing callbacks here
For high verbosity
Global debug still works with LCEL:
from langchain.globals import set_debug

set_debug(True)

# your code

For a GUI
you can use weights and biases or langsmith
"
"I have a file called main.py in which I put a POST call with only one input parameter (integer). Simplified code is given below:
from fastapi import FastAPI

app = FastAPI()

@app.post(&quot;/do_something/&quot;)
async def do_something(process_id: int):
    # some code
    return {&quot;process_id&quot;: process_id}

Now, if I run the code for the test, saved in the file test_main.py, that is:
from fastapi.testclient import TestClient
from main import app

client = TestClient(app)

def test_do_something():
    response = client.post(
        &quot;/do_something/&quot;,
        json={
            &quot;process_id&quot;: 16
        }
    )
    return response.json()

print(test_do_something())

I get:
{'detail': [{'loc': ['query', 'process_id'], 'msg': 'field required', 'type': 'value_error.missing'}]}

I can't figure out what the mistake is. It is necessary that it remains a POST call.
","The error, basically, says that the required query parameter process_id is missing. The reason for that error is that you send a POST request with request body, i.e., JSON payload; however, your endpoint expects a query parameter.
To receive the data in JSON format instead, one needs to create a Pydantic BaseModel—as shown below—and send the data from the client in the same way you already do.
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()


class Item(BaseModel):
    process_id: int


@app.post(&quot;/do_something&quot;)
async def do_something(item: Item):
    return item

Test the above as shown in your question:
def test_do_something():
    response = client.post(&quot;/do_something&quot;, json={&quot;process_id&quot;: 16})
    return response.json()

If, however, you had to pass a query parameter, then you would create an endpoint in the same way you did, that is:
@app.post(&quot;/do_something&quot;)
async def do_something(process_id: int):
    return {&quot;process_id&quot;: process_id}

but on client side, you would need to add the parameter to the URL itself, as described in the documentation (e.g., &quot;/do_something?process_id=16&quot;), or use the params attribute  and as shown below:
def test_do_something():
    response = client.post(&quot;/do_something&quot;, params={&quot;process_id&quot;: 16})
    return response.json()

Update
Alternatively, another way to pass JSON data when having a single body parameter is to use Body(..., embed=True), as shown below:
@app.post(&quot;/do_something&quot;)
def do_something(process_id: int = Body(..., embed=True)):
    return process_id

For more details and options on how to post JSON data in FastAPI, please have a look at this answer and this answer.
"
"I am trying to load this semantic segmentation model from HF using the following code:
from transformers import pipeline

model = pipeline(&quot;image-segmentation&quot;, model=&quot;Carve/u2net-universal&quot;, device=&quot;cpu&quot;)

But I get the following error:
OSError: tamnvcc/isnet-general-use does not appear to have a file named config.json. Checkout 'https://huggingface.co/tamnvcc/isnet-general-use/main' for available files.

Is it even possible to load models from HuggingFace without config.json file provided?
I also tried loading the model via:
id2label = {0: &quot;background&quot;, 1: &quot;target&quot;}
label2id = {&quot;background&quot;: 0, &quot;target&quot;: 1}
image_processor = AutoImageProcessor.from_pretrained(&quot;Carve/u2net-universal&quot;)
model = AutoModelForSemanticSegmentation(&quot;Carve/u2net-universal&quot;, id2label=id2label, label2id=label2id)

But got the same error.
","TL;DR
You will need to make a lot of assumption if you don't have the config.json and the model card doesn't have any documentation
After some guessing, possibly it's this:
from u2net import U2NET
import torch

model = U2NET()

model.load_state_dict(torch.load('full_weights.pth', map_location=torch.device('cpu')))


In Long
Looking at the files available in the model card, we see these files:

.gitattributes
README.md
full_weights.pth

A good guess would be that the .pth file is a PyTorch model binary. Given that, we can try:
import shutil
import requests

import torch


# Download the .pth file locally
url = &quot;https://huggingface.co/Carve/u2net-universal/resolve/main/full_weights.pth&quot;
response = requests.get(url, stream=True)
with open('full_weights.pth', 'wb') as out_file:
    shutil.copyfileobj(response.raw, out_file)

model = torch.load('full_weights.pth', map_location=torch.device('cpu'))

But what you end up with is NOT a usable model, it's just the model parameters/weights (aka checkpoint file), i.e.
type(model)

[out]:
collections.OrderedDict

Looking at the layer names, it looks like a rebnconvin model that points to the https://github.com/xuebinqin/U-2-Net code:
model.keys()

[out]:
odict_keys(['stage1.rebnconvin.conv_s1.weight', 'stage1.rebnconvin.conv_s1.bias', 'stage1.rebnconvin.bn_s1.weight', 'stage1.rebnconvin.bn_s1.bias', 'stage1.rebnconvin.bn_s1.running_mean', 'stage1.rebnconvin.bn_s1.running_var', 'stage1.rebnconv1.conv_s1.weight', 'stage1.rebnconv1.conv_s1.bias', 'stage1.rebnconv1.bn_s1.weight', 'stage1.rebnconv1.bn_s1.bias', 'stage1.rebnconv1.bn_s1.running_mean', 'stage1.rebnconv1.bn_s1.running_var', ...])

ASSUMING THAT YOU CAN TRUST THE CODE from the github, you can try installing it with:
! wget https://raw.githubusercontent.com/xuebinqin/U-2-Net/master/model/u2net.py

And guessing from the layer names and model name, it looks like a U2Net from https://arxiv.org/abs/2005.09007v3
So you can try:
from u2net import U2NET

model = U2NET()

model.load_state_dict(torch.load('full_weights.pth', map_location=torch.device('cpu')))

"
"I have a single row data-frame like below
Num     TP1(USD)    TP2(USD)    TP3(USD)    VReal1(USD)     VReal2(USD)     VReal3(USD)     TiV1 (EUR)  TiV2 (EUR)  TiV3 (EUR)  TR  TR-Tag
AA-24   0       700     2100    300     1159    2877    30       30     47      10  5

I want to get a dataframe like the one below
ID  Price   Net     Range
1   0       300     30
2   700     1159    30
3   2100    2877    47

The logic here is that
a. there will be 3 columns names that contain TP/VR/TV. So in the ID, we have 1, 2 &amp; 3 (these can be generated by extracting the value from the column names or just by using a range to fill)
b. TP1 value goes into first row of column 'Price',TP2 value goes into second row of column 'Price' &amp; so on
c. Same for VR &amp; TV. The values go into 'Net' &amp; 'Range columns
d. Columns 'Num', 'TR'  &amp; 'TR=Tag' are not relevant for the result.
I tried df.filter(regex='TP').stack(). I get all the 'TP' column &amp; I can access individual values be index ([0],[1],[2]). I could not get all of them into a column directly.
I also wondered if there may be a easier way of doing this.
","Assuming 'Num' is a unique identifier, you can use pandas.wide_to_long:
pd.wide_to_long(df, stubnames=['TP', 'VR', 'TV'], i='Num', j='ID')

or, for an output closer to yours:
out = (pd
 .wide_to_long(df, stubnames=['TP', 'VR', 'TV'], i='Num', j='ID')
 .reset_index('ID')
 .drop(columns=['TR', 'TR-Tag'])
 .rename(columns={'TP': 'Price', 'VR': 'Net', 'TV': 'Range'})
 )

output:
       ID  Price   Net  Range
Num                          
AA-24   1      0   300     30
AA-24   2    700  1159     30
AA-24   3   2100  2877     47

updated answer
out = (pd
 .wide_to_long(df.set_axis(df.columns.str.replace(r'\(USD\)$', '', regex=True),
                           axis=1),
               stubnames=['TP', 'VReal', 'TiV'], i='Num', j='ID')
 .reset_index('ID')
 .drop(columns=['TR', 'TR-Tag'])
 .rename(columns={'TP': 'Price', 'VReal': 'Net', 'TiV': 'Range'})
 )

output:
       ID  Price   Net  Range
Num                          
AA-24   1      0   300     30
AA-24   2    700  1159     30
AA-24   3   2100  2877     47

"
"Let's consider I have the following TypedDict:
class A(TypedDict):
    a: int
    b: int

What is the best practice for setting default values for this class?
I tried to add a constructor but it doesn't seem to work.
class A(TypedDict):
    a: int
    b: int
    def __init__(self):
        TypedDict.__init__(self)
        a = 0
        b = 1

EDIT:
I don't want to use dataclass because I need to serialize and deserialize to JSON files and dataclasses have some problem with it.
What do you think?
","TypedDict is only for specifying that a dict follows a certain layout, not an actual class. You can of course use a TypedDict to create an instance of that specific layout but it doesn't come with defaults.
One possible solution is to add a factory method to the class.
You could use this factory method instead to set defaults.
from typing import TypedDict

class A(TypedDict):
    a: int
    b: int

    @classmethod
    def create(cls, a: int = 0, b: int = 1) -&gt; A:
        return A(a=a, b=b)

a = A.create(a=4)
# {&quot;a&quot;: 4, &quot;b&quot;: 1}

If having a dict is not a strict requirement then @dataclass is good having small objects with defaults.
from dataclasses import dataclass

@dataclass
class A:
    a: int = 0
    b: int = 1

If you need to create a dictionary from them, you can use asdict
from dataclasses import asdict

a = A(a=4)
asdict(a)  # {&quot;a&quot;: 4, &quot;b&quot;: 1}

"
"I have a data set with three columns. Column A is to be checked for strings. If the string matches foo or spam, the values in the same row for the other two columns L and G should be changed to XX. For this I have tried the following.
df = pl.DataFrame(
    {
        &quot;A&quot;: [&quot;foo&quot;, &quot;ham&quot;, &quot;spam&quot;, &quot;egg&quot;,],
        &quot;L&quot;: [&quot;A54&quot;, &quot;A12&quot;, &quot;B84&quot;, &quot;C12&quot;],
        &quot;G&quot;: [&quot;X34&quot;, &quot;C84&quot;, &quot;G96&quot;, &quot;L6&quot;,],
    }
)
print(df)

shape: (4, 3)
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ A    â”† L   â”† G   â”‚
â”‚ ---  â”† --- â”† --- â”‚
â”‚ str  â”† str â”† str â”‚
â•žâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•¡
â”‚ foo  â”† A54 â”† X34 â”‚
â”‚ ham  â”† A12 â”† C84 â”‚
â”‚ spam â”† B84 â”† G96 â”‚
â”‚ egg  â”† C12 â”† L6  â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

expected outcome
shape: (4, 3)
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ A    â”† L   â”† G   â”‚
â”‚ ---  â”† --- â”† --- â”‚
â”‚ str  â”† str â”† str â”‚
â•žâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•¡
â”‚ foo  â”† XX  â”† XX  â”‚
â”‚ ham  â”† A12 â”† C84 â”‚
â”‚ spam â”† XX  â”† XX  â”‚
â”‚ egg  â”† C12 â”† L6  â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

I tried this
df = df.with_columns(
    pl.when((pl.col(&quot;A&quot;) == &quot;foo&quot;) | (pl.col(&quot;A&quot;) == &quot;spam&quot;))
    .then((pl.col(&quot;L&quot;)= &quot;XX&quot;) &amp; (pl.col( &quot;G&quot;)= &quot;XX&quot;))
    .otherwise((pl.col(&quot;L&quot;))&amp;(pl.col( &quot;G&quot;)))
)

However, this does not work. Can someone help me with this?
","For setting multiple columns to the same value you could use:
df.with_columns(
   pl.when(pl.col(&quot;A&quot;).is_in([&quot;foo&quot;, &quot;spam&quot;]))
     .then(pl.lit(&quot;XX&quot;))
     .otherwise(pl.col(&quot;L&quot;, &quot;G&quot;))
     .name.keep()
)

shape: (4, 3)
┌──────┬─────┬─────┐
│ A    ┆ L   ┆ G   │
│ ---  ┆ --- ┆ --- │
│ str  ┆ str ┆ str │
╞══════╪═════╪═════╡
│ foo  ┆ XX  ┆ XX  │
│ ham  ┆ A12 ┆ C84 │
│ spam ┆ XX  ┆ XX  │
│ egg  ┆ C12 ┆ L6  │
└──────┴─────┴─────┘


.is_in() can be used instead of multiple == x | == y chains

If you need different values, you can pack them into a struct and extract/unnest the fields.
df.with_columns(
   pl.when(pl.col(&quot;A&quot;).is_in([&quot;foo&quot;, &quot;spam&quot;]))
     .then(pl.struct(L = pl.lit(&quot;AAA&quot;), G = pl.lit(&quot;BBB&quot;)))
     .otherwise(pl.struct(&quot;L&quot;, &quot;G&quot;))
     .struct.field(&quot;L&quot;, &quot;G&quot;) # or .struct.unnest()
)

shape: (4, 3)
┌──────┬─────┬─────┐
│ A    ┆ L   ┆ G   │
│ ---  ┆ --- ┆ --- │
│ str  ┆ str ┆ str │
╞══════╪═════╪═════╡
│ foo  ┆ AAA ┆ BBB │
│ ham  ┆ A12 ┆ C84 │
│ spam ┆ AAA ┆ BBB │
│ egg  ┆ C12 ┆ L6  │
└──────┴─────┴─────┘

"
"I tried to implement a formula, from which a coefficients of Fourier Series could be calculated. (I used 3B1B's video about it: Video) and writing code for that, my first test subject was singular contour of batman logo, I first take a binary picture of batman logo and use marching squares algorithm to find contour of it. after that i rescale values and get this results:

And Here is Code for creating this points: (Contour_Classifier.py)
import numpy as np
import matplotlib.pyplot as plt
from skimage import measure, draw

def read_binary_image(file_path):
    # Open the file and read line by line
    with open(file_path, 'r') as file:
        lines = file.readlines()

    height, width = len(lines), len(lines[0])
    print(height, width)
    # Process lines into a 2D numpy array
    image_data = []

    for i in range(height + 2):
        arr = []
        for j in range(width + 2):
            arr.append(0)
        image_data.append(arr)

    for i in range(2, height + 1):
        for j in range(2, width + 1):
            if(lines[i - 2][j - 2] != '1'):
                image_data[i][j] = 0
            else:
                image_data[i][j] = 1

    # Convert list to numpy array for easier manipulation
    image_array = np.array(image_data)

    return image_array

def display_image(image_array):
    # Display the binary image using matplotlib
    plt.imshow(image_array, cmap=&quot;gray&quot;)
    plt.axis('off')  # Hide axes
    plt.show()

# Example usage
file_path = 'KOREKT\images\sbetmeni.txt'  # Replace with the path to your file
image_array = read_binary_image(file_path)
#display_image(image_array)

#----------------------------------------------------------------------------------------------------------
#-------------------------------------------Finding Contours-----------------------------------------------
#----------------------------------------------------------------------------------------------------------

contours = measure.find_contours(image_array, level=0.5, positive_orientation='high')

fixed_contours = []
for contour in contours:
    fixed_contour = np.column_stack((contour[:, 1], contour[:, 0]))  # Swap (row, column) to (column, row)
    fixed_contour[:, 1] = image_array.shape[0] - fixed_contour[:, 1]  # Invert the y-axis
    # Normalize coordinates between [0, 1]
    fixed_contour[:, 0] /= image_array.shape[1]  # Normalize x (width)
    fixed_contour[:, 1] /= image_array.shape[0]  # Normalize y (height)

    fixed_contour[:, 0] *= 250  # Normalize x (width)
    fixed_contour[:, 1] *= 250  # Normalize y (height)

    fixed_contours.append(fixed_contour)
contours = fixed_contours

print(fixed_contours[0])

def visualize_colored_contours(contours, title=&quot;Colored Contours&quot;):
    # Create a plot
    plt.figure(figsize=(8, 8))

    for i, contour in enumerate(contours):
        # Extract X and Y coordinates
        x, y = zip(*contour)
        # Plot the points with a unique color
        plt.plot(x, y, marker='o', label=f'Contour {i+1}')

    plt.title(title)
    plt.xlabel(&quot;X&quot;)
    plt.ylabel(&quot;Y&quot;)
    plt.legend()
    plt.grid(True)
    plt.axis(&quot;equal&quot;)
    plt.show()

# Visualize the normalized contours
visualize_colored_contours(contours)

Now we go to the main part, where we implement the fourier series algorithm. I divide the time interal (t) into the amount of points provided and i make assumtion that all of that points relative to t have same distances between eachother. I use approximation of integral as the sum of the points as provided into the formula.
And Here is code implementing it (Fourier_Coefficients.py):
import numpy as np

def calculate_Fourier(points, num_coefficients):
    complex_points = []
    for point in points:
        complex_points.append(point[0] + 1j * point[1])


    t = np.linspace(0, 1, len(complex_points), endpoint=False)

    c_k = np.zeros(num_coefficients, dtype=np.complex128)

    for i in range(num_coefficients):
        c_k[i] = np.sum(complex_points * np.exp(-2j * np.pi * i * t) * t[1])

    return c_k

(NOTE: For this code t1 is basically deltaT, because it equals to 1/len(complex_points)
And Now, in the next slide i animate whole process, where i also wrote additional code snippet for creating a gif. If my implementation were correct it shouldn't have anu difficulty creating a batman shape, but we can observe really weird phenomenons throught the gif.
this is code snippet for this part
import numpy as np
import matplotlib.pyplot as plt
import imageio
from Fourier_Coefficients import calculate_Fourier
from Countour_Classifier import contours



# List to store file names for GIF creation
png_files = []

# Generate plots iteratively
for i in range(len(contours[0])):


    contour_coefficients = []

    for contour in contours:
        contour_coefficients.append(calculate_Fourier(contour, i))

    # Fourier coefficients (complex numbers) and frequencies
    coefficients = contour_coefficients[0]  # First contour
    frequencies = np.arange(len(coefficients))

    # Time parameters
    t = np.linspace(0, 1, len(coefficients))  # One period
    curve = np.zeros(len(t), dtype=complex)

    # Use the first (i + 1) coefficients
    for j in range(len(coefficients)):
        c, f = coefficients[j], frequencies[j]
        curve += c * np.exp(1j * 2 * np.pi * f * t)

    # Plotting
    plt.figure(figsize=(8, 8))
    plt.plot(curve.real, curve.imag, label=&quot;Trajectory&quot;, color=&quot;blue&quot;)
    plt.scatter(0, 0, color=&quot;black&quot;, label=&quot;Origin&quot;)
    plt.axis(&quot;equal&quot;)
    plt.title(f&quot;Fourier Series with {i + 1} Coefficients&quot;)
    plt.xlabel(&quot;Real Part (X)&quot;)
    plt.ylabel(&quot;Imaginary Part (Y)&quot;)
    plt.legend()
    plt.text(-0.5, -0.5, f&quot;Using {i + 1} coefficients&quot;, fontsize=12, color=&quot;red&quot;)

    # Save the figure as a PNG file
    filename = f&quot;fourier_{i + 1}_coefficients.png&quot;
    plt.savefig(filename)
    plt.close()

    # Append the file name to the list
    png_files.append(filename)

# Create a GIF from the PNG files
gif_filename = &quot;fourier_series.gif&quot;
with imageio.get_writer(gif_filename, mode='I', duration=0.5) as writer:
    for filename in png_files:
        image = imageio.imread(filename)
        writer.append_data(image)

print(&quot;Plots saved as PNG files and GIF created as 'fourier_series.gif'.&quot;)

Now this is the result
GIF
Observation #1
when coefficients number is 0, 1, 2 or 3 it doesnt draw anything.
Observation #2
As coefficients number raises, we get the wobbly circular shape, where the lower part of the image is slightly more identical tot he original imagine, but messes up on its wings
Observation #3
As we get closer to the len(complex_numbers), the situacion changes and we get this weird shapes, different from circular
Observation #4
When we surpass the len(complex_number), it draws a random gibberish
Observation #5
When the number of the divisions inside the t value in animation.py code is altered we get completely different images.
EDIT 1
here is actual .txt data provided for further testing.
https://pastebin.com/Q51pT09E
After all of this information given, can you guys help me out whats wrong with my code
","In the definition of the Fourier series, you can see that n goes from negative infinity to positive infinity. The issue in your code is that you forgot to compute the coefficients associated with negative values of n.
Here is a simple example that shows how to compute the coefficients (from -50 to 50) associated with an ellipse,
and build a curve from them:
import numpy as np
import matplotlib.pyplot as plt

def get_ellipse():
    t = np.linspace(0, 1, 100)
    X = 2 * np.cos(2 * np.pi * t)
    Y = np.sin(2 * np.pi * t)
    return (X, Y)

def calculate_Fourier(X, Y, N):
    complex_points = [complex(x, y) for x, y in zip(X, Y)]
    t = np.linspace(0, 1, len(complex_points), endpoint=False)
    coefficients = np.zeros(2 * N + 1, dtype=complex)
    for i in range(len(coefficients)):
        n = i - N
        coefficients[i] = np.sum(complex_points * np.exp(-2j * np.pi * n * t) * t[1])
    return coefficients

def build_curve(coefficients, num_points):
    N = (len(coefficients) - 1) / 2
    t = np.linspace(0, 1, num_points)
    curve = np.zeros(num_points, dtype=complex)
    for i in range(len(coefficients)):
        c = coefficients[i]
        n = i - N
        curve += c * np.exp(2j * np.pi * n * t)
    return curve

X, Y = get_ellipse()
coefficients = calculate_Fourier(X, Y, 50)
curve = build_curve(coefficients, 50)

plt.plot(curve.real, curve.imag, color=&quot;blue&quot;)
plt.show()

Result:

Remark: if the number of coefficients is too high, you will get numerical instability.
"
"I need to do a lot of calculations on numpy arrays, with some of the calculations being repeated. I had the idea of caching the results, but observe that

In most cases, the cached version is slower than just carrying out all calculations.
Not only is the cached version slower, line profiling also indicates that the absolute time spent on numpy operations increase, even though there are fewer of them.

I can accept the first observation by some combined magic of numpy and the python interpreter, but the second observation makes no sense to me. I also see similar behavior when operating on scipy sparse matrices.
The full application is complex, but the behavior can be reproduced by the following:
import numpy as np
from time import time

def numpy_comparison(do_cache: bool, array_size: int, num_arrays: int, num_iter: int):
    # Create random arrays
    arrays: dict[int, np.ndarray] = {}
    for i in range(num_arrays):  
        arrays[i] = np.random.rand(array_size)

    if do_cache:  # Set up the cache if needed - I cannot use lru_cache or similar in practice
        cache: dict[tuple[int, int], np.ndarray] = {}

    for _ in range(num_iter):  # Loop over random pairs of array, add, store if relevant
        i, j = np.random.randint(num_arrays, size=2)

        if do_cache and (i, j) in cache:
            a = cache[(i, j)]  # a is not used further here, but would be in the real case
        else:
            a = arrays[i] + arrays[j]
            if do_cache:
                cache[(i, j)] = a

Now running (with no multithreading)
%timeit numpy_comparison(do_cache=False, array_size=10000, num_arrays=100, num_iter=num_iter)
%timeit numpy_comparison(do_cache=True, array_size=10000, num_arrays=100, num_iter=num_iter)

gives the following results



num_iter
No caching
With caching




100
10.3ms
13.7ms


1000
28.8ms
62.7ms


10000
225ms
392ms


100000
2.12s
1.62s



Varying the array size and number of arrays give similar behavior. When num_iter is sufficiently high, retrieving from cache is most efficient, but in the regime relevant for my application, num_iter=1000 when the average chance of hitting a cached value is about 5%. Line profiling indicates this is not caused by working on cache, but on the addition of the arrays being slow.
Can anyone give a hint of what is going on here?
","TL;DR: page faults explain why the cache-based version is significantly slower than the one without a cache when num_iter is small. This is a side effect of creating many new Numpy arrays and deleted only at the end. When num_iter is big, the cache becomes more effective (as explained in the JonSG's answer). Using another system allocator like TCMalloc can strongly reduce this overhead.

When you create many new temporary arrays, Numpy requests some memory to the system allocator which request relatively large buffers to the operating system (OS). The first touch to memory pages causes a page fault enabling the OS to actually setup the pages (actual page fault): the virtual pages are mapped to a physical one and the target pages are filled with zeros for security reasons (information should not leak from one process to another). When all arrays are deleted, Numpy free the memory space and the underlying memory allocator has a good chance to give the memory back to the OS (so other processes can use it).
Page faults are very expensive because the CPU needs to switch from the user-land to kernel one (with elevated privileges). The kernel needs to setup many data structures and call a lot of functions to do that.

Profiling &amp; Analysis
To prove page faults are the issue and how bad page faults are performance-wise, here is the low-level breakdown of the time when the cache is enabled (using perf):
  13,75%  _multiarray_umath.cpython-311-x86_64-linux-gnu.so  [.] DOUBLE_add_AVX2
   7,47%  [kernel]                                           [k] __irqentry_text_end
   6,94%  _mt19937.cpython-311-x86_64-linux-gnu.so           [.] __pyx_f_5numpy_6random_8_mt19937_mt19937_double
   3,63%  [kernel]                                           [k] clear_page_erms
   3,22%  [kernel]                                           [k] error_entry
   2,98%  [kernel]                                           [k] native_irq_return_iret
   2,88%  libpython3.11.so.1.0                               [.] _PyEval_EvalFrameDefault
   2,35%  [kernel]                                           [k] sync_regs
   2,28%  [kernel]                                           [k] __list_del_entry_valid_or_report
   2,27%  [kernel]                                           [k] unmap_page_range
   1,62%  [kernel]                                           [k] __handle_mm_fault
   1,45%  [kernel]                                           [k] __mod_memcg_lruvec_state
   1,43%  mtrand.cpython-311-x86_64-linux-gnu.so             [.] random_standard_uniform_fill
   1,10%  [kernel]                                           [k] do_anonymous_page
   1,10%  _mt19937.cpython-311-x86_64-linux-gnu.so           [.] mt19937_gen
   0,98%  [kernel]                                           [k] mas_walk
   0,93%  libpython3.11.so.1.0                               [.] PyObject_GenericGetAttr
   0,91%  [kernel]                                           [k] get_mem_cgroup_from_mm
   0,89%  [kernel]                                           [k] get_page_from_freelist
   0,79%  libpython3.11.so.1.0                               [.] _PyObject_Malloc
   0,77%  [kernel]                                           [k] lru_gen_add_folio
   0,72%  [nvidia]                                           [k] _nv039919rm
   0,65%  [kernel]                                           [k] lru_gen_del_folio.constprop.0
   0,63%  [kernel]                                           [k] blk_cgroup_congested
   0,62%  [kernel]                                           [k] handle_mm_fault
   0,59%  [kernel]                                           [k] __alloc_pages_noprof
   0,57%  [kernel]                                           [k] lru_add
   0,57%  [kernel]                                           [k] folio_batch_move_lru
   0,56%  [kernel]                                           [k] __rcu_read_lock
   0,52%  [kernel]                                           [k] do_user_addr_fault
[...] (many others functions taking &lt;0.52% each)

As we can see, there are a lot of [kernel] functions called and most of them are due to page faults. For example, __irqentry_text_end and native_irq_return_iret (taking ~10% of the time) is caused by CPU interrupts triggered when the CPython process access to pages for the first time. clear_page_erms is the function clearing a memory page during a first touch. Several functions are related to the virtual to physical memory mapping (e.g. AFAIK ones related to the LRU cache). Note that DOUBLE_add_AVX2 is the internal native Numpy function actually summing the two arrays. In comparison, here is the breakdown with the cache disabled:
  20,85%  _multiarray_umath.cpython-311-x86_64-linux-gnu.so  [.] DOUBLE_add_AVX2
  17,39%  _mt19937.cpython-311-x86_64-linux-gnu.so           [.] __pyx_f_5numpy_6random_8_mt19937_mt19937_double
   5,69%  libpython3.11.so.1.0                               [.] _PyEval_EvalFrameDefault
   3,35%  mtrand.cpython-311-x86_64-linux-gnu.so             [.] random_standard_uniform_fill
   2,46%  _mt19937.cpython-311-x86_64-linux-gnu.so           [.] mt19937_gen
   2,15%  libpython3.11.so.1.0                               [.] PyObject_GenericGetAttr
   1,76%  [kernel]                                           [k] __irqentry_text_end
   1,46%  libpython3.11.so.1.0                               [.] _PyObject_Malloc
   1,07%  libpython3.11.so.1.0                               [.] PyUnicode_FromFormatV
   1,03%  libc.so.6                                          [.] printf_positional
   0,93%  libpython3.11.so.1.0                               [.] _PyObject_Free
   0,88%  _multiarray_umath.cpython-311-x86_64-linux-gnu.so  [.] NpyIter_AdvancedNew
   0,79%  _multiarray_umath.cpython-311-x86_64-linux-gnu.so  [.] ufunc_generic_fastcall
   0,77%  [kernel]                                           [k] error_entry
   0,74%  _bounded_integers.cpython-311-x86_64-linux-gnu.so  [.] __pyx_f_5numpy_6random_17_bounded_integers__rand_int64
   0,72%  [nvidia]                                           [k] _nv039919rm
   0,69%  [kernel]                                           [k] native_irq_return_iret
   0,66%  [kernel]                                           [k] clear_page_erms
   0,55%  libpython3.11.so.1.0                               [.] PyType_IsSubtype
   0,55%  [kernel]                                           [k] sync_regs
   0,52%  mtrand.cpython-311-x86_64-linux-gnu.so             [.] __pyx_pw_5numpy_6random_6mtrand_11RandomState_31randint
   0,48%  [kernel]                                           [k] unmap_page_range
   0,46%  libpython3.11.so.1.0                               [.] PyDict_GetItemWithError
   0,43%  _multiarray_umath.cpython-311-x86_64-linux-gnu.so  [.] PyArray_NewFromDescr_int
   0,40%  libpython3.11.so.1.0                               [.] _PyFunction_Vectorcall
   0,38%  [kernel]                                           [k] __mod_memcg_lruvec_state
   0,38%  _multiarray_umath.cpython-311-x86_64-linux-gnu.so  [.] promote_and_get_ufuncimpl
   0,37%  libpython3.11.so.1.0                               [.] PyDict_SetItem
   0,36%  libpython3.11.so.1.0                               [.] PyObject_RichCompareBool
   0,36%  _multiarray_umath.cpython-311-x86_64-linux-gnu.so  [.] PyUFunc_GenericReduction
[...] (many others functions taking &lt;0.35% each)

There are clearly less kernel function called in the top 30 most expensive functions. We can still see the interrupt related functions, but note that this low-level profiling is global to my whole machine and so these interrupt-related function likely comes from other processes (e.g. perf itself, the graphical desktop environment, and Firefox running as I write this answer).
There is another reason proving page faults are the main culprit: during my tests, the system allocator suddenly changed its behavior because I allocated many arrays before running the same commands, and this results in only few kernel calls (in perf) as well as very close timings between the two analyzed variants (with/without cache):
# First execution:
In [3]: %timeit numpy_comparison(do_cache=False, array_size=10000, num_arrays=100, num_iter=num_iter)
   ...: %timeit numpy_comparison(do_cache=True, array_size=10000, num_arrays=100, num_iter=num_iter)
20.2 ms ± 63.7 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)
46.4 ms ± 81.9 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)

# Second execution:
In [4]: %timeit numpy_comparison(do_cache=False, array_size=10000, num_arrays=100, num_iter=num_iter)
   ...: %timeit numpy_comparison(do_cache=True, array_size=10000, num_arrays=100, num_iter=num_iter)
19.8 ms ± 43.4 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)
46.3 ms ± 155 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)

# After creating many Numpy arrays (not deleted since) with no change of `numpy_comparison`:
In [95]: %timeit numpy_comparison(do_cache=False, array_size=10000, num_arrays=100, num_iter=num_iter)
    ...: %timeit numpy_comparison(do_cache=True, array_size=10000, num_arrays=100, num_iter=num_iter)
18.4 ms ± 15.4 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)
19.9 ms ± 26.6 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)    &lt;-----

We can clearly see that the overhead of using a cache is now pretty small. This also means the performance could be very different in a real-world application (because of a different allocator state), or even on different platforms.

Solution to mitigate page faults
You can use alternative system allocators like TCMalloc which requests a big chunk of memory to the OS at startup time so not to often pay page faults. Here are results with it (using the command line LD_PRELOAD=libtcmalloc_minimal.so.4 ipython):
In [3]: %timeit numpy_comparison(do_cache=False, array_size=10000, num_arrays=100, num_iter=num_iter)
   ...: %timeit numpy_comparison(do_cache=True, array_size=10000, num_arrays=100, num_iter=num_iter)
16.9 ms ± 51.6 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)
19.5 ms ± 29.9 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)

The overhead related to the cache which was actually due the creation of many temporary arrays and more specifically page faults is now &gt;10 times smaller!
I think the remaining overhead is due to the larger memory working set as pointed out by NickODell in comments (this cause more cache misses due to cash trashing and more data to be loaded from the slow DRAM). Put it shortly, the cache version is simply less cache friendly.
Here are results with num_item=100_000 with/without TCMalloc:
# Default system allocator (glibc)
In [97]: %timeit numpy_comparison(do_cache=False, array_size=10000, num_arrays=100, num_iter=num_iter)
    ...: %timeit numpy_comparison(do_cache=True, array_size=10000, num_arrays=100, num_iter=num_iter)
1.31 s ± 4.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
859 ms ± 3.41 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

# TCMalloc allocator
In [3]: %timeit -n 1 numpy_comparison(do_cache=False, array_size=10000, num_arrays=100, num_iter=num_iter)
   ...: %timeit -n 1 numpy_comparison(do_cache=True, array_size=10000, num_arrays=100, num_iter=num_iter)
1.28 s ± 13.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
774 ms ± 83.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

This behavior is expected: the cache starts to be useful with more hits. Note that TCMalloc makes the overall execution faster in most case!
"
"import polars as pl

df = pl.DataFrame({
    &quot;Letter&quot;: [&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;C&quot;, &quot;C&quot;, &quot;D&quot;, &quot;D&quot;, &quot;E&quot;],
    &quot;Value': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
})

I want to group Letter and collect their corresponding Value in a List.
Related Pandas question: How to group dataframe rows into list in pandas groupby
I know pandas code will not work here:
df.group_by(&quot;a&quot;)[&quot;b&quot;].apply(list)


TypeError: 'GroupBy' object is not subscriptable

Output will be:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Letter â”† Value     â”‚
â”‚ ---    â”† ---       â”‚
â”‚ str    â”† list[i64] â”‚
â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ A      â”† [1, 2]    â”‚
â”‚ B      â”† [3, 4, 5] â”‚
â”‚ C      â”† [6, 7]    â”‚
â”‚ D      â”† [8, 9]    â”‚
â”‚ E      â”† [10]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

","You could do this.
import polars as pl

df = pl.DataFrame(
    {
        'Letter': ['A', 'A', 'B', 'B', 'B', 'C', 'C', 'D','D','E'],
        'Value': [1, 2, 3, 4, 5, 6, 7, 8, 9,10]
    }
)
g = df.group_by('Letter', maintain_order=True).agg(pl.col('Value'))
print(g)

This will print
┌────────┬───────────┐
│ Letter ┆ Value     │
│ ---    ┆ ---       │
│ str    ┆ list[i64] │
╞════════╪═══════════╡
│ A      ┆ [1, 2]    │
│ B      ┆ [3, 4, 5] │
│ C      ┆ [6, 7]    │
│ D      ┆ [8, 9]    │
│ E      ┆ [10]      │
└────────┴───────────┘

maintain_order=True is required if you want to order of the groups to be consistent with the input data.
"
"Is there a way to make the processes in concurrent.futures.ProcessPoolExecutor terminate if the parent process terminates for any reason?
Some details: I'm using ProcessPoolExecutor in a job that processes a lot of data. Sometimes I need to terminate the parent process with a kill command, but when I do that the processes from ProcessPoolExecutor keep running and I have to manually kill them too. My primary work loop looks like this:
with concurrent.futures.ProcessPoolExecutor(n_workers) as executor:
    result_list = [executor.submit(_do_work, data) for data in data_list]
    for id, future in enumerate(
            concurrent.futures.as_completed(result_list)):
        print(f'{id}: {future.result()}')

Is there anything I can add here or do differently to make the child processes in executor terminate if the parent dies?
","You can start a thread in each process to terminate when parent process dies:
def start_thread_to_terminate_when_parent_process_dies(ppid):
    pid = os.getpid()

    def f():
        while True:
            try:
                os.kill(ppid, 0)
            except OSError:
                os.kill(pid, signal.SIGTERM)
            time.sleep(1)

    thread = threading.Thread(target=f, daemon=True)
    thread.start()

Usage: pass initializer and initargs to ProcessPoolExecutor
with concurrent.futures.ProcessPoolExecutor(
        n_workers,
        initializer=start_thread_to_terminate_when_parent_process_dies,  # +
        initargs=(os.getpid(),),                                         # +
) as executor:

This works even if the parent process is SIGKILL/kill -9'ed.
"
"The following is an example of items rated by 1,2 or 3 stars.
I am trying to count all combinations of item ratings (stars) per month.
In the following example, item 10 was rated in month 1 and has two ratings equal 1, one rating equal 2 and one rating equal 3.
inp = pd.DataFrame({'month':[1,1,1,1,1,2,2,2], 
                    'item':[10,10,10,10,20,20,20,20], 
                    'star':[1,2,1,3,3,2,2,3]}
                  )

 month item star
0   1   10  1
1   1   10  2
2   1   10  1
3   1   10  3
4   1   20  3
5   2   20  2
6   2   20  2
7   2   20  3

For the given above input frame output should be:
   month    item    star_1_cnt  star_2_cnt  star_3_cnt
0   1       10      2           1           1
1   1       20      0           0           1
2   2       20      0           2           1

I am trying to solve the problem starting with the following code,
which result still needs to be converted to the desired format of the output frame and which gives the wrong answers:
1   20  3   (1, 1)
2   20  3   (1, 1)

Anyway, there should be a better way to create the output table, then finalizing this one:
months = [1,2]
items = [10,20]
stars = [1,2,3]

d = {'month': [], 'item': [], 'star': [], 'star_cnts': [] }

for month in months:
    for star in stars:
        for item in items:
            star_cnts=dict(inp[(inp['item']==item) &amp; (inp['star']==star)].value_counts()).values()
            d['month'].append(month)
            d['item'].append(item)
            d['star'].append(star)
            d['star_cnts'].append(star_cnts)
            
pd.DataFrame(d)

    month   item    star    star_cnts
0   1       10      1       (2)
1   1       20      1       ()
2   1       10      2       (1)
3   1       20      2       (2)
4   1       10      3       (1)
5   1       20      3       (1, 1)
6   2       10      1       (2)
7   2       20      1       ()
8   2       10      2       (1)
9   2       20      2       (2)
10  2       10      3       (1)
11  2       20      3       (1, 1)

â€‹
","This seems like a nice problem for pd.get_dummies:
new_df = (
    pd.concat([df, pd.get_dummies(df['star'])], axis=1)
    .groupby(['month', 'item'], as_index=False)
    [df['star'].unique()]
    .sum()
)

Output:
&gt;&gt;&gt; new_df
   month  item  1  2  3
0      1    10  2  1  1
1      1    20  0  0  1
2      2    20  0  2  1

Renaming, too:
u = df['star'].unique()
new_df = (
    pd.concat([df, pd.get_dummies(df['star'])], axis=1)
    .groupby(['month', 'item'], as_index=False)
    [u]
    .sum()
    .rename({k: f'star_{k}_cnt' for k in df['star'].unique()}, axis=1)
)

Output:
&gt;&gt;&gt; new_df
   month  item  star_1_cnt  star_2_cnt  star_3_cnt
0      1    10           2           1           1
1      1    20           0           0           1
2      2    20           0           2           1

Obligatory one- (or two-) liners:
# Renames the columns
u = df['star'].unique()
new_df = pd.concat([df, pd.get_dummies(df['star'])], axis=1).groupby(['month', 'item'], as_index=False)[u].sum().rename({k: f'star_{k}_cnt' for k in df['star'].unique()}, axis=1)

"
"I have a problem. So I have a task that runs every time when a user writes a chat message on my discord server - it's called on_message. So my bot has many things to do in this event, and I often get this kind of error:
Task was destroyed but it is pending!
task: &lt;Task pending name='pycord: on_message' coro=&lt;Client._run_event() done, defined at /Bots/gift-bot/discord/client.py:374&gt; wait_for=&lt;Future pending cb=[&lt;TaskWakeupMethWrapper object at 0x7f68a7bdfc10&gt;()]&gt;&gt;

So I think if I want to fix this, I need to speedup my code. But sadly, I don't have any clue how i can do it to fix this error.
Edit: I integrated timings and this is what I get printed:
Task was destroyed but it is pending!
task: &lt;Task pending name='pycord: on_message' coro=&lt;Client._run_event() done, defined at /Bots/gift-bot/discord/client.py:374&gt; wait_for=&lt;Future pending cb=[&lt;TaskWakeupMethWrapper object at 0x7f01063f98e0&gt;()]&gt;&gt;
2 if checks done - 7.867813110351562e-06
5 if checks done - 0.0061550140380859375
mysql checks done - 0.010785341262817383
task done - 0.13075661659240723
2 if checks done - 8.344650268554688e-06
5 if checks done - 0.011545896530151367
mysql checks done - 0.02138519287109375
task done - 0.11132025718688965
2 if checks done - 2.0503997802734375e-05
5 if checks done - 0.008122920989990234
mysql checks done - 0.012276411056518555
2 if checks done - 1.0728836059570312e-05
5 if checks done - 0.014346837997436523
mysql checks done - 0.040288448333740234
task done - 0.12520265579223633
2 if checks done - 1.0728836059570312e-05
5 if checks done - 0.0077972412109375
mysql checks done - 0.013320684432983398
task done - 0.1502058506011963
task done - 0.10663175582885742
2 if checks done - 9.775161743164062e-06
5 if checks done - 0.006486177444458008
mysql checks done - 0.011229515075683594
Task was destroyed but it is pending!
task: &lt;Task pending name='pycord: on_message' coro=&lt;Client._run_event() done, defined at /Bots/gift-bot/discord/client.py:374&gt; wait_for=&lt;Future pending cb=[&lt;TaskWakeupMethWrapper object at 0x7f010609a9d0&gt;()]&gt;&gt;
2 if checks done - 6.67572021484375e-06
5 if checks done - 0.0049741268157958984
mysql checks done - 0.008575677871704102
task done - 0.10633635520935059

And this is the code for the integrated timings:
    @commands.Cog.listener(&quot;on_message&quot;)
    async def on_message(self, message):
        start = time.time()

        # Check ob Nachricht gezÃ¤hlt werden kann


        if message.author.bot:
            return

        if message.type != discord.MessageType.default:
            return
            
        print(f&quot;2 if checks done - {time.time() - start}&quot;)

        if isinstance(message.channel, discord.channel.DMChannel):
            return await message.reply(f'Hey {message.author.name}!\nLeider bin ich der falsche Ansprechpartner, falls du Hilfe suchst.. &lt;:pepe_hands:705896495601287320&gt;\nBetrete den https://discord.gg/deutschland Bl4cklist-Discord und sende unserem Support-Bot &lt;@671421220566204446&gt; (`Bl4cklistðŸ”¥Support#7717`) eine Private-Nachricht, damit sich unser Support-Team um dein Problem so schnell es geht kÃ¼mmern kann. &lt;:pepe_love:759741232443949107&gt;')

        # ENTFERNEN AM 30. APRIL
        prefix_now = await get_prefix(message)
        if message.content.startswith(str(prefix_now)):
            try:
                await message.reply(&quot;â€º &lt;a:alarm:769215249261789185&gt; - **UMSTIEG AUF SLASH-COMMANDS:** Ab **jetzt** laufen alle Befehle dieses Bots auf `/` - um Leistung zu sparen und die Erfahrung zu verbessern. Nutze `/help` um eine Befehlsliste zu sehen.&quot;)
            except discord.Forbidden:
                pass
            return

        if self.client.user in message.mentions:

                response = choice([
                &quot;Mit mir kann man die coolsten Gewinnspiele starten! &lt;a:gift:843914342835421185&gt;&quot;,
                'Wird Zeit jemanden den Tag zu versÃ¼ÃŸen! &lt;:smile:774755282618286101&gt;',
                &quot;Wer nicht auf diesem Server ist, hat die Kontrolle Ã¼ber sein Leben verloren! &lt;a:lach_blue2:803693710490861608&gt;&quot;,
                &quot;Wann startet endlich ein neues Gewinnspiel? &lt;:whut:848347703217487912&gt;&quot;,
                &quot;Ich bin der BESTE Gewinnspiel-Bot - Wer was anderes sagt, lÃ¼gt! &lt;:wyldekatze:842157727169773608&gt;&quot;
                ])

                try:
                    await message.reply(f&quot;{response} (Mein PrÃ¤fix: `/`)&quot;, mention_author=False)
                except (discord.Forbidden, discord.HTTPException, discord.NotFound):
                    pass
                return
                
        print(f&quot;5 if checks done - {time.time() - start}&quot;)


        # Cooldown


        #self.member_cooldown_list = [i for i in self.member_cooldown_list if i[1] + self.cooldown_val &gt; int(time.time())]
        #member_index = next((i for i, v in enumerate(self.member_cooldown_list) if v[0] == message.author.id), None)
        #if member_index is not None:
        #    if self.member_cooldown_list[member_index][1] + self.cooldown_val &gt; int(time.time()):
        #        return

        #self.member_cooldown_list.append((message.author.id, int(time.time())))


        # Rollen-Check (Bonus/Ignore)


        count = 1
        mydb = await getConnection()
        mycursor = await mydb.cursor()
        await mycursor.execute(&quot;SELECT ignore_role_id, bonus_role_id FROM guild_role_settings WHERE guild_id = %s&quot;, (message.author.guild.id,))
        in_database = await mycursor.fetchone()
        if in_database:
            if in_database[0] is not None:
                role_list = in_database[0].split(&quot; &quot;)
                for roleid in role_list:
                    try:
                        int(roleid)
                    except ValueError:
                        continue

                    role = message.author.guild.get_role(int(roleid))
                    if role is None:
                        continue

                    if role in message.author.roles:
                        await mycursor.close()
                        mydb.close()
                        return

            if in_database[1] is not None:
                role_list = in_database[1].split(&quot; &quot;)
                for roleid in role_list:
                    try:
                        int(roleid)
                    except ValueError:
                        continue

                    role = message.author.guild.get_role(int(roleid))
                    if role is None:
                        continue

                    if role in message.author.roles:
                        count += 1


        # Kanal-Check (Bonus/Ignore)


        await mycursor.execute(&quot;SELECT ignore_channel_id FROM guild_channel_settings WHERE guild_id = %s&quot;, (message.author.guild.id,))
        in_database1 = await mycursor.fetchone()
        if in_database1:
            if in_database1[0] is not None:
                channel_list = in_database1[0].split(&quot; &quot;)
                for channelid in channel_list:

                    try:
                        int(channelid)
                    except ValueError:
                        continue

                    if int(message.channel.id) == int(channelid):
                        await mycursor.close()
                        mydb.close()
                        return
                        
        print(f&quot;mysql checks done - {time.time() - start}&quot;)


        # In Datenbank eintragen

        await mycursor.execute(&quot;SELECT * FROM guild_message_count WHERE guild_id = %s AND user_id = %s&quot;,
                               (message.author.guild.id, message.author.id))
        in_database2 = await mycursor.fetchone()
        if in_database2:
            await mycursor.execute(
                &quot;UPDATE guild_message_count SET user_id = %s, message_count = message_count + %s WHERE guild_id = %s AND user_id = %s&quot;,
                (message.author.id, count, message.author.guild.id, message.author.id))
        else:
            await mycursor.execute(
                &quot;INSERT INTO guild_message_count (user_id, message_count, guild_id) VALUES (%s, %s, %s)&quot;,
                (message.author.id, count, message.author.guild.id))

        await mydb.commit()
        await mycursor.close()
        mydb.close()
        
        print(f&quot;task done - {time.time() - start}&quot;)

If I try to start my bot with asyncio.run(client.start('token')) I'm getting this error multiple times:
Ignoring exception in on_guild_channel_delete
Traceback (most recent call last):
  File &quot;/Bots/gift-bot/discord/client.py&quot;, line 382, in _run_event
    await coro(*args, **kwargs)
  File &quot;/Bots/gift-bot/cogs/misc_events.py&quot;, line 738, in on_guild_channel_delete
    await self.client.wait_until_ready()
  File &quot;/Bots/gift-bot/discord/client.py&quot;, line 978, in wait_until_ready
    await self._ready.wait()
  File &quot;/usr/local/lib/python3.9/asyncio/locks.py&quot;, line 226, in wait
    await fut
RuntimeError: Task &lt;Task pending name='pycord: on_guild_channel_delete' coro=&lt;Client._run_event() running at /Bots/gift-bot/discord/client.py:382&gt;&gt; got Future &lt;Future pending&gt; attached to a different loop

I'm using Python3.9 on a Debian 10 vServer with pycord2.0.0b5.
","The await expression blocks the containing coroutine until the awaited awaitable returns. This hinders the progress of the coroutine. But await is necessary in a coroutine to yield control back to the event loop so that other coroutines can progress.
Too many awaits can be problematic, it just makes progress slow.
I've refactored on_message coroutine method by breaking it into sub tasks.
async def _check_channel(self, message, pool):
    async with pool.acquire() as conn:
        async with conn.cursor() as cursor:
            await cursor.execute(
                &quot;SELECT ignore_channel_id FROM guild_channel_settings WHERE guild_id = %s&quot;,
                (message.author.guild.id,),
            )
            in_database = await cursor.fetchone()

    if in_database and in_database[0] is not None:
        channel_list = in_database[0].split(&quot; &quot;)
        for channelid in channel_list:

            try:
                channel_id_int = int(channelid)
            except ValueError:
                continue

            if int(message.channel.id) == channel_id_int:
                return False


async def _get_role_count(self, message, pool):
    async with pool.acquire() as conn:
        async with conn.cursor() as cursor:
            await cursor.execute(
                &quot;SELECT ignore_role_id, bonus_role_id FROM guild_role_settings WHERE guild_id = %s&quot;,
                (message.author.guild.id,),
            )
            in_database = await cursor.fetchone()
    if in_database:
        first_item, second_item, *_ = in_database
        if first_item is not None:
            role_list = first_item.split(&quot; &quot;)
            for roleid in role_list:
                try:
                    roleid_int = int(roleid)
                except ValueError:
                    continue

                role = message.author.guild.get_role(roleid_int)
                if role is None:
                    continue
                if role in message.author.roles:
                    return False

        if second_item is not None:
            role_list = second_item.split(&quot; &quot;)
            count = 0
            for roleid in role_list:
                try:
                    roleid_int = int(roleid)
                except ValueError:
                    continue

                role = message.author.guild.get_role(roleid_int)
                if role is None:
                    continue
                if role in message.author.roles:
                    count += 1
            return count


@commands.Cog.listener(&quot;on_message&quot;)
async def on_message(self, message):
    if message.author.bot:
        return
    if message.type != discord.MessageType.default:
        return
    if isinstance(message.channel, discord.channel.DMChannel):
        return

    # Cooldown

    self.member_cooldown_list = [
        i
        for i in self.member_cooldown_list
        if i[1] + self.cooldown_val &gt; int(time.time())
    ]
    member_index = next(
        (
            i
            for i, v in enumerate(self.member_cooldown_list)
            if v[0] == message.author.id
        ),
        None,
    )
    if member_index is not None:
        if self.member_cooldown_list[member_index][1] + self.cooldown_val &gt; int(
            time.time()
        ):
            return

    self.member_cooldown_list.append((message.author.id, int(time.time())))

    loop = asyncio.get_running_loop()
    db_pool = await aiomysql.create_pool(
        minsize=3,
        host=&quot;&lt;host&gt;&quot;,
        port=3306,
        user=&quot;&lt;user&gt;&quot;,
        password=&quot;&lt;password&gt;&quot;,
        db=&quot;&lt;db_name&gt;&quot;,
        autocommit=False,
        loop=loop,
    )
    count = 1

    check_channel_task = asyncio.create_task(
        self._check_channel(self, message, db_pool)
    )
    role_count_task = asyncio.create_task(self._get_role_count(self, message, db_pool))

    # write to database

    mydb = await db_pool.acquire()
    mycursor = await mydb.cursor()
    await mycursor.execute(
        &quot;SELECT * FROM guild_message_count WHERE guild_id = %s AND user_id = %s&quot;,
        (message.author.guild.id, message.author.id),
    )
    in_database = await mycursor.fetchone()

    role_count = await role_count_task
    check_channel = await check_channel_task
    if False in (role_count, check_channel):
        await mycursor.close()
        db_pool.release(mydb)
        db_pool.close()
        await db_pool.wait_closed()
        return
    if role_count:
        count += role_count
    if in_database:
        await mycursor.execute(
            &quot;INSERT INTO guild_message_count (user_id, message_count, guild_id) VALUES (%s, %s, %s) ON DUPLICATE KEY UPDATE message_count = message_count + 1&quot;,
            (message.author.id, count, message.author.guild.id),
        )

    await mydb.commit()
    await mycursor.close()
    db_pool.release(mydb)
    db_pool.close()
    await db_pool.wait_closed()

I've created two private async methods with code from part of the on_message method to make progress concurrent. While on_message is blocked in an await, the refactored methods may progress independent of on_message method. To make this happen I create two tasks out of the two new coroutines. asyncio.create_tasks schedules tasks to be run negating the need for an await. These tasks may run as soon as on_message yields control back to event loop on any await following the tasks creation.
I didn't run the code. This is best effort. You have to try experimenting by moving the block which awaits the tasks around. And also run it with client.run to avoid got Future  attached to a different loop error.
"
"I have 107 images and I want to extract text from them, and I am using Gemini API, and this is my code till now:
# Gemini Model
model = genai.GenerativeModel('gemini-pro-vision', safety_settings=safety_settings)

# Code
images_to_process = [os.path.join(image_dir, image_name) for image_name in os.listdir(image_dir)] # list of 107 images 

prompt = &quot;&quot;&quot;Carefully scan this images: if it has text, extract all the text and return the text from it. If the image does not have text return '&lt;000&gt;'.&quot;&quot;&quot;

for image_path in tqdm(images_to_process):
    img = Image.open(image_path)
    output = model.generate_content([prompt, img])
    text = output.text

    print(text)

In this code, I am just taking one image at a time and extracting text from it using Gemini.
Problem -
I have 107 images and this code is taking ~10 minutes to run. I know that Gemini API can handle 60 requests per minute. How to send 60 images at the same time? How to do it in batch?
","2024-10 update: I've added a Cookbook Quickstart on asynchronous requests to show how this works. The advice below is still correct.

In synchronous Python you can use something like a ThreadPoolExecutor to make your requests in separate threads.
The Gemini Python SDK has an async API though, which can be a bit more natural:
$ python -m asyncio

&gt;&gt;&gt; import asyncio
&gt;&gt;&gt; import google.generativeai as genai
&gt;&gt;&gt; import PIL

&gt;&gt;&gt; model = genai.GenerativeModel('gemini-pro-vision')
&gt;&gt;&gt; imgs = ['/path/img.jpg', ...]
&gt;&gt;&gt; prompt = &quot;...&quot;

&gt;&gt;&gt; async def process_image(img: str) -&gt; str:
...   r = await model.generate_content_async([prompt, PIL.Image.open(img)])
...   # TODO: error handling
...   return r.text

&gt;&gt;&gt; jobs = asyncio.gather(*[process_image(img) for img in imgs])
&gt;&gt;&gt; results = await jobs  # or run_until_complete(jobs)
&gt;&gt;&gt; results
['text is here', ...]

This uses the implicit asyncio REPL event loop, in a real app you'll need to set up and use your own event loop.
See also TaskGroups.
"
"What I am after is Python code able to reverse the order of the values in each of the array anti-diagonals in a numpy array.
I have already tried various combinations of np.rot90, np.fliplr, np.transpose, np.flipud but none is able to give me the original shape of the 5x3 array with all the anti-diagonals reversed.
Any idea how to accomplish this?
Example:
[[ 1  2  4]
 [ 3  5  7]
 [ 6  8 10]
 [ 9 11 13]
 [12 14 15]]

Should become:
[[ 1  3  6]
 [ 2  5  9]
 [ 4  8 12]
 [ 7 11 14]
 [10 13 15]]

I suppose it must be easy, but somehow I have yet failed to find how to do it efficiently on arrays with millions of values.

Inspired by the already provided answers (status 2024-05-23 11:37 CET) and re-thinking what would be the most efficient way of getting the required transformation done it seems that giving a simple function taking two indices : iRow, jColumn of a value in an array and returning the required i,j indices to access the array as if it were flipped/reversed over the diagonals will provide fastest results. With such function for the over the diagonals flipped version of the array would be getting the right values without operating on the array as easy as in a trivial case of one-based and column/row based access to array values demonstrated below:
import numpy as np 
srcArr = np.array([[ 1,  2,  3,  4,  5,  6],
       [ 7,  8,  9, 10, 11, 12],
       [13, 14, 15, 16, 17, 18],
       [19, 20, 21, 22, 23, 24]])

def ijOfArrayValueGivenOneBasedColumnRowBasedIndices(i, j):
     return ( j - 1, i - 1 )
 
print( srcArr[
    ijOfArrayValueGivenOneBasedColumnRowBasedIndices(
        3,4)] ) # gives 21
print( srcArr[3,4] ) # gives 23

From this perspective the question comes down to providing a function
ijIndicesToSourceArray_gettingValueOfSourceArrayWithReversedRightLeftAntiDiagonalsAt(i,j,arrShapeRows,arrShapeColumns)
","This one seems fairly fast, especially for wide matrices like your width=n=1920, height=m=1080 :
def mirror(a):
    m, n = a.shape
    if m == n:
        return a.T.copy()
    if m &gt; n:
        return mirror(a.T).T

    # Shear
    v = a.flatten()
    w = v[:-m].reshape((m, n-1))
    
    # Flip the parallelogram
    w[:, m-1:] = w[::-1, m-1:]

    # Flip the triangles
    t = np.vstack((w[:, :m-1].reshape((m-1, m)), v[-m:]))
    t = t.T
    w[:, :m-1] = t[:-1].reshape((m, m-1))

    # Write flipped parts back and unshear
    v[:-m] = w.ravel()
    v[-m:] = t[-1]
    return v.reshape((m, n))

Attempt This Online!
The idea: Slice/reshape the m×n matrix to an m×(n-1) matrix so that the parallelogram (green part) becomes a rectangle so we can just flip it upside down. For example:

Now reshape the m×n matrix to an m×(n-1) matrix (omit the black last m cells), which moves the yellow cells to the front of the next row, shifting the next row as needed:

Now we can easily flip the green parallelogram part. For the top-left and bottom-right triangles, reshape/shear the left part of this back and put the omitted cells under it:

This can now simply be transposed and written back.
"
"Tqdm documentation shows an example of tqdm working on pandas apply using progress_apply. I adapted the following code from here https://tqdm.github.io/docs/tqdm/ on a process that regularly take several minutes to perform (func1 is a regex function).
from tqdm import tqdm
tqdm.pandas()
df.progress_apply(lambda x: func1(x.textbody), axis=1)

The resulting progress bar doesn't show any progress. It just jumps from 0 at the start of the loop to 100 when it is finished. I am currently running tqdm version 4.61.2
","Utilizing tqdm with pandas
Generally speaking, people tend to use lambdas when performing operations on a column or row. This can be done in a number of ways.

Please note: that if you are working in jupyter notebook you should use tqdm_notebook instead of tqdm.
Also I'm not sure what your code looks like but if you're simply following the example given in the tqdm docs, and you're only performing 100 interations, computers are fast and will blow through that before your progress bar has time to update. Perhaps it would be more instructive to use a larger dataset like I provided below.

Example 1:
from tqdm import tqdm # version 4.62.2
import pandas as pd # version 1.4.1
import numpy as np

tqdm.pandas(desc='My bar!') # lots of cool paramiters you can pass here. 
# the below line generates a very large dataset for us to work with. 
df = pd.DataFrame(np.random.randn(100000000, 4), columns=['a','b','c','d'])
# the below line will square the contents of each element in an column-wise 
# fashion 
df.progress_apply(lambda x: x**2)

Output:
Output
Example 2:
# you could apply a function within the lambda expression for more complex 
# operations. And keeping with the above example... 

tqdm.pandas(desc='My bar!') # lots of cool paramiters you can pass here. 
# the below line generates a very large dataset for us to work with. 
df = pd.DataFrame(np.random.randn(100000000, 4), columns=['a','b','c','d'])

def function(x):
    return x**2
     
df.progress_apply(lambda x: function(x))

"
"I want to send data from app.post() to app.get() using RedirectResponse.
@app.get('/', response_class=HTMLResponse, name='homepage')
async def get_main_data(request: Request,
                        msg: Optional[str] = None,
                        result: Optional[str] = None):
    if msg:
        response = templates.TemplateResponse('home.html', {'request': request, 'msg': msg})
    elif result:
        response = templates.TemplateResponse('home.html', {'request': request, 'result': result})
    else:
        response = templates.TemplateResponse('home.html', {'request': request})
    return response

@app.post('/', response_model=FormData, name='homepage_post')
async def post_main_data(request: Request,
                         file: FormData = Depends(FormData.as_form)):
       if condition:
        ......
        ......

        return RedirectResponse(request.url_for('homepage', **{'result': str(trans)}), status_code=status.HTTP_302_FOUND)

    return RedirectResponse(request.url_for('homepage', **{'msg': str(err)}), status_code=status.HTTP_302_FOUND)


How do I send result or msg via RedirectResponse, url_for() to app.get()?
Is there a way to hide the data in the URL either as path parameter or query parameter? How do I achieve this?

I am getting the error starlette.routing.NoMatchFound: No route exists for name &quot;homepage&quot; and params &quot;result&quot;. when trying this way.
Update:
I tried the below:
return RedirectResponse(app.url_path_for(name='homepage')
                                + '?result=' + str(trans),
                                status_code=status.HTTP_303_SEE_OTHER)

The above works, but it works by sending the param as query param, i.e., the URL looks like this localhost:8000/?result=hello. Is there any way to do the same thing but without showing it in the URL?
","In brief, as explained in this answer and this answer, as well as mentioned by @tiangolo here, when performing a RedirectResponse from a POST request route to a GET request route, the response status code has to change to 303 See Other. For instance (completet working example is given below):
return RedirectResponse(redirect_url, status_code=status.HTTP_303_SEE_OTHER) 

As for the reason for getting starlette.routing.NoMatchFound error, this is because request.url_for() receives path parameters, not query parameters. Your msg and result parameters are query ones; hence, the error.
A solution would be to use a CustomURLProcessor, as suggested in this and this answer, allowing you to pass both path (if need to) and query parameters to the url_for() function and obtain the URL. As for hiding the path and/or query parameters from the URL, you can use a similar approach to this answer that uses history.pushState() (or history.replaceState()) to replace the URL in the browser's address bar.
Working example can be found below (you can use your own TemplateResponse in the place of HTMLResponse).
Working Example
from fastapi import FastAPI, Request, status
from fastapi.responses import RedirectResponse, HTMLResponse
from typing import Optional
import urllib

app = FastAPI()

class CustomURLProcessor:
    def __init__(self):  
        self.path = &quot;&quot; 
        self.request = None

    def url_for(self, request: Request, name: str, **params: str):
        self.path = request.url_for(name, **params)
        self.request = request
        return self
    
    def include_query_params(self, **params: str):
        parsed = list(urllib.parse.urlparse(self.path))
        parsed[4] = urllib.parse.urlencode(params)
        return urllib.parse.urlunparse(parsed)
        

@app.get('/', response_class=HTMLResponse)
def event_msg(request: Request, msg: Optional[str] = None):
    if msg:
        html_content = &quot;&quot;&quot;
        &lt;html&gt;
           &lt;head&gt;
              &lt;script&gt;
                 window.history.pushState('', '', &quot;/&quot;);
              &lt;/script&gt;
           &lt;/head&gt;
           &lt;body&gt;
              &lt;h1&gt;&quot;&quot;&quot; + msg + &quot;&quot;&quot;&lt;/h1&gt;
           &lt;/body&gt;
        &lt;/html&gt;
        &quot;&quot;&quot;
        return HTMLResponse(content=html_content, status_code=200)
    else:
        html_content = &quot;&quot;&quot;
        &lt;html&gt;
           &lt;body&gt;
              &lt;h1&gt;Create an event&lt;/h1&gt;
              &lt;form method=&quot;POST&quot; action=&quot;/&quot;&gt;
                 &lt;input type=&quot;submit&quot; value=&quot;Create Event&quot;&gt;
              &lt;/form&gt;
           &lt;/body&gt;
        &lt;/html&gt;
        &quot;&quot;&quot;
        return HTMLResponse(content=html_content, status_code=200)

@app.post('/')
def event_create(request: Request):
    redirect_url = CustomURLProcessor().url_for(request, 'event_msg').include_query_params(msg=&quot;Succesfully created!&quot;)
    return RedirectResponse(redirect_url, status_code=status.HTTP_303_SEE_OTHER)

Update 1 - About including query parameters
Regarding adding query params to url_for() function, another solution would be using Starlette's starlette.datastructures.URL, which now provides a method to include_query_params. Example:
from starlette.datastructures import URL

redirect_url = URL(request.url_for('event_msg')).include_query_params(msg=&quot;Succesfully created!&quot;)

Update 2 - About including query parameters
The request.url_for() function now returns a starlette.datastructures.URL object. Hence, you could add query parameters as follows:
redirect_url = request.url_for('event_msg').include_query_params(msg=&quot;Succesfully created!&quot;)

"
"I have a dataframe that contains 1681 evenly distributed 2D grid points. Each data point has its x and y coordinates, a label representing its category (or phase), and a color for that category.
         x     y      label    color
0    -40.0 -30.0         Fe  #660066
1    -40.0 -29.0         Fe  #660066
2    -40.0 -28.0        FeS  #ff7f50
3    -40.0 -27.0        FeS  #ff7f50
4    -40.0 -26.0        FeS  #ff7f50
...    ...   ...        ...      ...
1676   0.0   6.0  Fe2(SO4)3  #8a2be2
1677   0.0   7.0  Fe2(SO4)3  #8a2be2
1678   0.0   8.0  Fe2(SO4)3  #8a2be2
1679   0.0   9.0  Fe2(SO4)3  #8a2be2
1680   0.0  10.0  Fe2(SO4)3  #8a2be2

[1681 rows x 4 columns]

I want to generate a polygon diagram that shows the linear boundary of each category (in my case also known as a &quot;phase diagram&quot;). Sor far I can only show this kind of diagram in a simple scatter plot like this:
import matplotlib.pyplot as plt
import pandas as pd

plt.figure(figsize=(8., 8.))
for color in df.color.unique():
    df_color = df[df.color==color]
    plt.scatter(
            x=df_color.x,
            y=df_color.y,
            c=color,
            s=100,
            label=df_color.label.iloc[0]
    )
plt.xlim([-40., 0.])
plt.ylim([-30., 10.])
plt.xlabel('Log pO2(g)')
plt.ylabel('Log pSO2(g)')
plt.legend(bbox_to_anchor=(1.05, 1.))
plt.show()


However, what I want is a phase diagram with clear linear boundaries that looks something like this:

Is there any way I can generate such phase diagram using matplotlib? Note that the boundary is not deterministic, especially when the grid points are not dense enough. Hence there needs to be some kind of heuristics, for example the boundary line should always lie in the middle of two neighboring points with different categories. I imagine there will be some sort of line fitting or interpolation needed, and matplotlib.patches.Polygon is probably useful here.
For easy testing, I attach a code snippet for generating the data, but the polygon information shown below are not supposed to be used for generating the phase diagram
import numpy as np
import pandas as pd
from shapely.geometry import Point, Polygon

labels = ['Fe', 'Fe3O4', 'FeS', 'Fe2O3', 'FeS2', 'FeSO4', 'Fe2(SO4)3']
colors = ['#660066', '#b6fcd5', '#ff7f50', '#ffb6c1', '#c6e2ff', '#d3ffce', '#8a2be2']
polygons = []
polygons.append(Polygon([(-26.7243,-14.7423), (-26.7243,-30.0000), (-40.0000,-30.0000), 
(-40.0000,-28.0181)]))
polygons.append(Polygon([(-18.1347,-0.4263), (-16.6048,1.6135), (-16.6048,-30.0000),
(-26.7243,-30.0000), (-26.7243,-14.7423), (-18.1347,-0.4263)]))
polygons.append(Polygon([(-18.1347,-0.4263), (-26.7243,-14.7423),
(-40.0000,-28.0181), (-40.0000,-22.2917), (-18.1347,-0.4263)]))
polygons.append(Polygon([(0.0000,-20.2615), (0.0000,-30.0000), (-16.6048,-30.0000),
(-16.6048,1.6135), (-16.5517,1.6865), (-6.0517,-0.9385), (0.0000,-3.9643)]))
polygons.append(Polygon([(-14.2390,10.0000), (-14.5829,7.5927), (-16.5517,1.6865),
(-16.6048,1.6135), (-18.1347,-0.4263), (-40.0000,-22.2917), (-40.0000,10.0000)]))
polygons.append(Polygon([(-6.0517,-0.9385), (-16.5517,1.6865), (-14.5829,7.5927),
(-6.0517,-0.9385)]))
polygons.append(Polygon([(0.0000,-3.9643), (-6.0517,-0.9385), (-14.5829,7.5927),
(-14.2390,10.0000), (0.0000,10.0000)]))

x_grid = np.arange(-40., 0.01, 1.)
y_grid = np.arange(-30., 10.01, 1.)
xy_grid = np.array(np.meshgrid(x_grid, y_grid)).T.reshape(-1, 2).tolist()
data = []
for coords in xy_grid:
    point = Point(coords)
    for i, poly in enumerate(polygons):
        if poly.buffer(1e-3).contains(point):
            data.append({
                'x': point.x,
                'y': point.y,
                'label': labels[i],
                'color': colors[i]
            })
            break
df = pd.DataFrame(data)

","I am not sure if you can easily get a representation with contiguous polygons, however you could easily get the bounding polygon from a set of points using shapely.convex_hull:
import shapely
import matplotlib.pyplot as plt

f, ax = plt.subplots(figsize=(8, 8))

for (name, color), coords in df.groupby(['label', 'color'])[['x', 'y']]:
    polygon = shapely.convex_hull(shapely.MultiPoint(coords.to_numpy()))
    ax.fill(*polygon.exterior.xy, color=color)
    ax.annotate(name, polygon.centroid.coords[0],
                ha='center', va='center')

If you want the shapely polygons:
polygons = {k: shapely.convex_hull(shapely.MultiPoint(g.to_numpy()))
            for k, g in df.groupby(['label', 'color'])[['x', 'y']]}

Output:

contiguous polygons
To have contiguous polygons you can use the same strategy after adding points with a greater density and assigning them to their closest counterpart with a KDTree:
from scipy.spatial import KDTree

# interpolate points on the initial polygons
polygons = {k: shapely.convex_hull(shapely.MultiPoint(g.to_numpy()))
            for k, g in df.groupby('label')[['x', 'y']]}

def interp_ext(shape):
    try:
        return np.c_[shape.xy].T
    except NotImplementedError:
        pass
    e = shape.exterior if hasattr(shape, 'exterior') else shape
    points = e.interpolate(np.linspace(0, e.length, 1000))
    return np.c_[Polygon(points).exterior.xy].T

df2 = (pd.DataFrame([(l, *interp_ext(p)) for l, p in polygons.items()],
                    columns=['label', 'x', 'y'])
         .merge(df[['label', 'color']], on='label') 
         .explode(['x', 'y'])
      )

# get bounding values
xmin, ymin, xmax, ymax = df[['x', 'y']].agg(['min', 'max']).values.ravel()

# create a grid with a higher density (here 10x)
Xs = np.arange(xmin, xmax, 0.1)
Ys = np.arange(ymin, ymax, 0.1)
Xs, Ys = (x.ravel() for x in np.meshgrid(Xs, Ys))
grid = np.c_[Xs, Ys]

# indentify closest reference point
_, idx = KDTree(df2[['x', 'y']]).query(grid)

# create new DataFrame with labels/colors
df3 = pd.DataFrame(np.c_[grid, df2[['label', 'color']].to_numpy()[idx]],
                   columns=['x', 'y', 'label', 'color']
                  )

# plot
f, ax = plt.subplots(figsize=(8, 8))

for (name, color), coords in df3.groupby(['label', 'color'])[['x', 'y']]:
    polygon = shapely.convex_hull(shapely.MultiPoint(coords.to_numpy()))
    ax.fill(*polygon.exterior.xy, color=color)
    ax.annotate(name, polygon.centroid.coords[0],
                ha='center', va='center')

Output:

Another, faster, option could be to use a Voronoi diagram based on the original shapes. I found a library (voronoi-diagram-for-polygons) that does this but requires GeoPandas:
import geopandas as gpd
from longsgis import voronoiDiagram4plg
from shapely import Polygon, convex_hull, coverage_union

# create the initial convex hulls
tmp = (df.groupby(['label', 'color'])
         .apply(lambda x: convex_hull(Polygon(x[['x', 'y']].to_numpy())))
         .reset_index(name='geometry')
      )

# convert to geodataframe
gdf = gpd.GeoDataFrame(tmp, geometry='geometry')

# Split using a Voronoi diagram
mask = Polygon([(xmin, ymin), (xmin, ymax), (xmax, ymax), (xmax, ymin)])
tmp = voronoiDiagram4plg(gdf, mask, densify=True)

# plot
tmp.plot(color=gdf['color'])

Output:

"
"I find the glimpse function very useful in R/dplyr. But as someone who is used to R and is working with Python now, I haven't found something as useful for Panda dataframes.
In Python, I've tried things like .describe() and .info() and .head() but none of these give me the useful snapshot which R's glimpse() gives us.
Nice features which I'm quite accustomed to having in glimpse() include:

All variables/column names as rows in the output
All variable/column data types
The first few observations of each column
Total number of observations
Total number of variables/columns

Here is some simple code you could work it with:
R
library(dplyr)

test &lt;- data.frame(column_one = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;),
           column_two = c(1:4))

glimpse(test)

# The output is as follows

Rows: 4
Columns: 2
$ column_one &lt;chr&gt; &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;
$ column_two &lt;int&gt; 1, 2, 3, 4

Python
import pandas as pd

test = pd.DataFrame({'column_one':['A', 'B', 'C', 'D'],
                     'column_two':[1, 2, 3, 4]})

Is there a single function for Python which mirrors these capabilities closely (not multiple and not partly)? If not, how would you create a function that does the job precisely?
","Here is one way to do it:
def glimpse(df):
    print(f&quot;Rows: {df.shape[0]}&quot;)
    print(f&quot;Columns: {df.shape[1]}&quot;)
    for col in df.columns:
        print(f&quot;$ {col} &lt;{df[col].dtype}&gt; {df[col].head().values}&quot;)

Then:
import pandas as pd

df = pd.DataFrame(
    {&quot;column_one&quot;: [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;], &quot;column_two&quot;: [1, 2, 3, 4]}
)

glimpse(df)

# Output
Rows: 4
Columns: 2
$ column_one &lt;object&gt; ['A' 'B' 'C' 'D']
$ column_two &lt;int64&gt; [1 2 3 4]

"
"Assume I have this dataframe
import polars as pl

df = pl.DataFrame({
    'item':         ['CASH', 'CHECK', 'DEBT', 'CHECK', 'CREDIT', 'CASH'],
    'quantity':     [100, -20, 0, 10, 0, 0],
    'value':        [99, 47, None, 90, None, 120],
    'value_other':  [97, 57, None, 91, None, 110],
    'value_other2': [94, 37, None, 93, None, 115],
})

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ item   â”† quantity â”† value â”† value_other â”† value_other2 â”‚
â”‚ ---    â”† ---      â”† ---   â”† ---         â”† ---          â”‚
â”‚ str    â”† i64      â”† i64   â”† i64         â”† i64          â”‚
â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ CASH   â”† 100      â”† 99    â”† 97          â”† 94           â”‚
â”‚ CHECK  â”† -20      â”† 47    â”† 57          â”† 37           â”‚
â”‚ DEBT   â”† 0        â”† null  â”† null        â”† null         â”‚
â”‚ CHECK  â”† 10       â”† 90    â”† 91          â”† 93           â”‚
â”‚ CREDIT â”† 0        â”† null  â”† null        â”† null         â”‚
â”‚ CASH   â”† 0        â”† 120   â”† 110         â”† 115          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Now I want to set all value columns to 0 for all rows where value is null and quantity == 0.
Right now I have this solution
cols = ['value', 'value_other', 'value_other2']
df   = df.with_columns([
    pl.when(pl.col('value').is_null() &amp; (pl.col('quantity') == 0))
    .then(0)
    .otherwise(pl.col(col))
    .alias(col)
    for col in cols
])

which correctly gives
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ item   â”† quantity â”† value â”† value_other â”† value_other2 â”‚
â”‚ ---    â”† ---      â”† ---   â”† ---         â”† ---          â”‚
â”‚ str    â”† i64      â”† i64   â”† i64         â”† i64          â”‚
â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ CASH   â”† 100      â”† 99    â”† 97          â”† 94           â”‚
â”‚ CHECK  â”† -20      â”† 47    â”† 57          â”† 37           â”‚
â”‚ DEBT   â”† 0        â”† 0     â”† 0           â”† 0            â”‚
â”‚ CHECK  â”† 10       â”† 90    â”† 91          â”† 93           â”‚
â”‚ CREDIT â”† 0        â”† 0     â”† 0           â”† 0            â”‚
â”‚ CASH   â”† 0        â”† 120   â”† 110         â”† 115          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

However, I feel this is very inefficient as my when condition is executed for every value column. Is there a way to achieve this using only polar internal functions &amp; without the native for-loop?
","You can pass list of column names into pl.col() and when\then\otherwise accepts Expr which can contain multiple columns.
cols = ['value', 'value_other', 'value_other2']

df.with_columns(
    pl
    .when((pl.col.quantity != 0) | pl.col.value.is_not_null())
    .then(pl.col(cols))
    .otherwise(0)
)

# or

df.with_columns(
    pl
    .when(pl.col.quantity != 0).then(pl.col(cols))
    .when(pl.col.value.is_not_null()).then(pl.col(cols))
    .otherwise(0)
)

shape: (6, 5)
┌────────┬──────────┬───────┬─────────────┬──────────────┐
│ item   ┆ quantity ┆ value ┆ value_other ┆ value_other2 │
│ ---    ┆ ---      ┆ ---   ┆ ---         ┆ ---          │
│ str    ┆ i64      ┆ i64   ┆ i64         ┆ i64          │
╞════════╪══════════╪═══════╪═════════════╪══════════════╡
│ CASH   ┆ 100      ┆ 99    ┆ 97          ┆ 94           │
│ CHECK  ┆ -20      ┆ 47    ┆ 57          ┆ 37           │
│ DEBT   ┆ 0        ┆ 0     ┆ 0           ┆ 0            │
│ CHECK  ┆ 10       ┆ 90    ┆ 91          ┆ 93           │
│ CREDIT ┆ 0        ┆ 0     ┆ 0           ┆ 0            │
│ CASH   ┆ 0        ┆ 120   ┆ 110         ┆ 115          │
└────────┴──────────┴───────┴─────────────┴──────────────┘

"
"I have a FastAPI application for which I enable Authentication by injecting a dependency function.
controller.py
router = APIRouter(
prefix=&quot;/v2/test&quot;,
tags=[&quot;helloWorld&quot;],
dependencies=[Depends(api_key)],
responses={404: {&quot;description&quot;: &quot;Not found&quot;}},

)
Authorzation.py
async def api_key(api_key_header: str = Security(api_key_header_auth)):
if api_key_header != API_KEY:
    raise HTTPException(
        status_code=401,
        detail=&quot;Invalid API Key&quot;,
    )

This works fine. However, I would like to disable the authentication based on environment. For instance, I would want to keep entering the authentication key in localhost environment.
","You could create a subclass of APIKeyHeader class and override the __call__() method to perform a check whether the request comes from a &quot;safe&quot; client, such as localhost or 127.0.0.1, using request.client.host, as explained here. If so, you could set the api_key to application's API_KEY value, which would be returned and used by the check_api_key() dependency function to validate the api_key. In case there were multiple API keys, one could perform a check on the client's hostname/IP in both the __call__() and the check_api_key() functions, and raise an exception only if the client's IP is not in the safe_clients list.
Example
from fastapi import FastAPI, Request, Depends, HTTPException
from starlette.status import HTTP_403_FORBIDDEN
from fastapi.security.api_key import APIKeyHeader
from fastapi import Security
from typing import Optional

API_KEY = 'some-api-key'
API_KEY_NAME = 'X-API-KEY'
safe_clients = ['127.0.0.1']


class MyAPIKeyHeader(APIKeyHeader):
    async def __call__(self, request: Request) -&gt; Optional[str]:
        if request.client.host in safe_clients:
            api_key = API_KEY
        else:
            api_key = request.headers.get(self.model.name)
            if not api_key:
                if self.auto_error:
                    raise HTTPException(
                        status_code=HTTP_403_FORBIDDEN, detail='Not authenticated'
                    )
                else:
                    return None

        return api_key


api_key_header_auth = MyAPIKeyHeader(name=API_KEY_NAME)


async def check_api_key(request: Request, api_key: str = Security(api_key_header_auth)):
    if api_key != API_KEY:
        raise HTTPException(status_code=401, detail='Invalid API Key')

 
app = FastAPI(dependencies=[Depends(check_api_key)])


@app.get('/')
def main(request: Request):
    return request.client.host

Example (UPDATED)
The previous example could be simplified to the one below, which does not require overriding the APIKeyHeader class. You could instead set the auto_error flag to False, which would prevent APIKeyHeader from raising the pre-defined error in case the api_key is missing from the request, but rather let you handle it on your own in the check_api_key() function.
from fastapi import FastAPI, Request, Security, Depends, HTTPException
from fastapi.security.api_key import APIKeyHeader


# List of valid API keys
API_KEYS = [
    'z77xQYZWROmI4fY4',
    'FXhO4i3bLA1WIsvR'
]
API_KEY_NAME = 'X-API-KEY'
safe_clients = ['127.0.0.1']
api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=False)


async def check_api_key(request: Request, api_key: str = Security(api_key_header)):
    if api_key not in API_KEYS and request.client.host not in safe_clients:
        raise HTTPException(status_code=401, detail='Invalid or missing API Key')

 
app = FastAPI(dependencies=[Depends(check_api_key)])


@app.get('/')
def main(request: Request):
    return request.client.host

How to remove/hide the Authorize button from Swagger UI
The example provided above  will work as expected, that is, users whose their IP address is included in the safe_clients list won't be asked to provide an API key in order to issue requests to the API, regardless of the Authorize button being present in Swagger UI page when accessing the autodocs at /docs. If you still, however, would like to remove the Authorize button from the UI for safe_clients, you could have a custom middleware, as demonstrated here, in order to remove the securitySchemes component from the OpenAPI schema (in /openapi.json)—Swagger UI is actually based on OpenAPI Specification. This approach was inspired by the link mentioned earlier, as well as here and here. Please make sure to add the middleware after initializing your app in the example above (i.e., after app = FastAPI(dependencies=...))
from fastapi import Response

# ... rest of the code is the same as above

app = FastAPI(dependencies=[Depends(check_api_key)])


@app.middleware(&quot;http&quot;)
async def remove_auth_btn(request: Request, call_next):
    response = await call_next(request)
    if request.url.path == '/openapi.json' and request.client.host in safe_clients:
        response_body = [section async for section in response.body_iterator]
        resp_str = response_body[0].decode()  # convert &quot;response_body&quot; bytes into string
        resp_dict = json.loads(resp_str)  # convert &quot;resp_str&quot; into dict
        del resp_dict['components']['securitySchemes']  # remove securitySchemes
        resp_str = json.dumps(resp_dict)  # convert &quot;resp_dict&quot; back to str
        return Response(content=resp_str, status_code=response.status_code, media_type=response.media_type)
    
    return response
    

"
"I have a large dataset (~5 Mio rows) with results from a Machine Learning training. Now I want to check to see if the results hit the &quot;target range&quot; or not. Lets say this range contains all values between -0.25 and +0.25. If it's inside this range, it's a Hit, if it's below Low and on the other side High.
I now would create this three columns Hit, Low, High and calculate for each row which condition applies and put a 1 into this col, the other two would become 0. After that I would group the values and sum them up. But I suspect there must be a better and faster way, such as calculate it directly while grouping.

Data
import pandas as pd

df = pd.DataFrame({&quot;Type&quot;:[&quot;RF&quot;, &quot;RF&quot;, &quot;RF&quot;, &quot;MLP&quot;, &quot;MLP&quot;, &quot;MLP&quot;], &quot;Value&quot;:[-1.5,-0.1,1.7,0.2,-0.7,-0.6]})

+----+--------+---------+
|    | Type   |   Value |
|----+--------+---------|
|  0 | RF     |    -1.5 | &lt;- Low
|  1 | RF     |    -0.1 | &lt;- Hit
|  2 | RF     |     1.7 | &lt;- High
|  3 | MLP    |     0.2 | &lt;- Hit
|  4 | MLP    |    -0.7 | &lt;- Low
|  5 | MLP    |    -0.6 | &lt;- Low
+----+--------+---------+


Expected Output
pd.DataFrame({&quot;Type&quot;:[&quot;RF&quot;, &quot;MLP&quot;], &quot;Low&quot;:[1,2], &quot;Hit&quot;:[1,1], &quot;High&quot;:[1,0]})

+----+--------+-------+-------+--------+
|    | Type   |   Low |   Hit |   High |
|----+--------+-------+-------+--------|
|  0 | RF     |     1 |     1 |      1 |
|  1 | MLP    |     2 |     1 |      0 |
+----+--------+-------+-------+--------+

","You could use cut to define the groups and pivot_table to reshape:
(df.assign(group=pd.cut(df['Value'],
                        [float('-inf'), -0.25, 0.25, float('inf')],
                        labels=['Low', 'Hit', 'High']))
   .pivot_table(index='Type', columns='group', values='Value', aggfunc='count')
   .reset_index()
   .rename_axis(None, axis=1)
)

Or crosstab:
(pd.crosstab(df['Type'],
             pd.cut(df['Value'],
                    [float('-inf'), -0.25, 0.25, float('inf')],
                    labels=['Low', 'Hit', 'High'])
             )
   .reset_index().rename_axis(None, axis=1)
 )

output:
  Type  Low  Hit  High
0  MLP    2    1     0
1   RF    1    1     1

"
"currently I'm working with FastAPI and pydantic as serializer.
Problem is, we're using snowflake id on the server side, which means we need to convert those ids to string before sending to client (javascript) because the id is larger than JS's MAX SAFE INTEGER.
So I tried to create a new class which extends python's int type and customize how it will be serialized and deserialized. Here's my code:
class SnowflakeId(int):
    @classmethod
    def __get_validators__(cls):
        yield cls.validate

    @classmethod
    def validate(cls, v: str):
        return int(v)

    @classmethod
    def __modify_schema__(cls, field_schema: dict) -&gt; None:
        field_schema['type'] = 'string'

And here is the model:
class BaseModel(pydantic.BaseModel):
    __abstract__ = True

    id: SnowflakeId

    class Config:
        orm_mode = True
        arbitrary_types_allowed = True
        json_encoders = {
            SnowflakeId: lambda v: str(v)
        }
        alias_generator = camelize
        allow_population_by_field_name = True

It works fine when deserializing from json string into int id, however, when it comes to the serialization, the output still is integer.
I want it to serialize the id into string also, is it possible?
","Yes it is!
json_encoders is a good try, however under the hood pydantic calls json.dumps. So for serializable types (like your SnowflakeId) it won't care about additional json_encoders.
What you can do is to override dumps method:
def my_dumps(v, *, default):
    for key, value in v.items():
        if isinstance(value, SnowflakeId):
            v[key] = str(value)
        else:
            v[key] = value
    return json.dumps(v)

class BaseModel(pydantic.BaseModel):
    id: SnowflakeId

    class Config:
        json_dumps = my_dumps

And let validate return SnowflakeId:
class SnowflakeId(int):
    ...

    @classmethod
    def validate(cls, v: str):
        return cls(v)

m = BaseModel(id=&quot;123&quot;)
print(m.json()) # {&quot;id&quot;: &quot;123&quot;}

"
"After updating Python package elasticsearch from 7.6.0 to 8.1.0, I started to receive an error at this line of code:
count = es.count(index=my_index, body={'query': query['query']} )[&quot;count&quot;]

receive following error message:

DeprecationWarning: The 'body' parameter is deprecated and will be
removed in a future version. Instead use individual parameters.
count = es.count(index=ums_index, body={'query': query['query']}
)[&quot;count&quot;]

I don't understand how to use the above-mentioned &quot;individual parameters&quot;.
Here is my query:
query = {
    &quot;bool&quot;: {
        &quot;must&quot;: 
        [
                {&quot;exists&quot; : { &quot;field&quot; : 'device'}},
                {&quot;exists&quot; : { &quot;field&quot; : 'app_version'}},                    
                {&quot;exists&quot; : { &quot;field&quot; : 'updatecheck'}},
                {&quot;exists&quot; : { &quot;field&quot; : 'updatecheck_status'}},
                {&quot;term&quot; : { &quot;updatecheck_status&quot; : 'ok'}},
                {&quot;term&quot; : { &quot;updatecheck&quot; : 1}},
                {
                    &quot;range&quot;: {
                    &quot;@timestamp&quot;: {
                        &quot;gte&quot;: from_date,
                        &quot;lte&quot;: to_date,
                        &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd&quot;
                        }
                    }
                }
        ],
        &quot;must_not&quot;:
        [
                {&quot;term&quot; : { &quot;device&quot; : &quot;&quot;}},
                {&quot;term&quot; : { &quot;updatecheck&quot; : &quot;&quot;}},
                {&quot;term&quot; : { &quot;updatecheck_status&quot; : &quot;&quot;}},
                {
                    &quot;terms&quot; : { 
                        &quot;app_version&quot; : ['2.2.1.1', '2.2.1.2', '2.2.1.3', '2.2.1.4', '2.2.1.5',
                                        '2.2.1.6', '2.2.1.7', '2.1.2.9', '2.1.3.2', '0.0.0.0', '']
                    }
                }
        ]
    }
}

In the official documentation, I can't find any chance to find examples of how to pass my query in new versions of Elasticsearch.
Possibly someone has a solution for this case other than reverting to previous versions of Elasticsearch?
","According to the documentation, this is now to be done as follows:
# ✅ New usage:
es.search(query={...})

# ❌ Deprecated usage:
es.search(body={&quot;query&quot;: {...}})

So the queries are done directly in the same line of code without &quot;body&quot;, substituting the api you need to use, in your case &quot;count&quot; for &quot;search&quot;.
You can try the following:
# ✅ New usage:
es.count(query={...})

# ❌ Deprecated usage:
es.count(body={&quot;query&quot;: {...}})
enter code here

You can find out more by clicking on the following link:
https://github.com/elastic/elasticsearch-py/issues/1698
For example, if the query would be:
GET index-00001/_count
{
    &quot;query&quot; : {
        &quot;match_all&quot;: {
        }
    }
}

Python client would be the next:
my_index = &quot;index-00001&quot;
query =  {
           &quot;match_all&quot;: {
            }
          }
hits = en.count(index=my_index, query=query)

or
hits = en.count(index=my_index, query={&quot;match_all&quot;: {}})

"
"I am working with very large (several GB) 2-dimensional square NumPy arrays. Given an input array a, for each element, I would like to find the direction of its largest adjacent neighbor. I am using the provided sliding window view to try to avoid creating unnecessary copies:
# a is an L x L array of type np.float32
swv = sliding_window_view(a, (3, 3)) # (L-2) x (L-2) x 3 x 3
directions = swv.reshape(L-2, L-2, 9)[:,:,1::2].argmax(axis = 2).astype(np.uint8)

However, calling reshape here creates a (L-2) x (L-2) x 9 copy instead of a view, which consumes an undesirably large chunk of memory. Is there a way to do this operation in a vectorized fashion, but with a smaller memory footprint?
EDIT: Many of the responses are geared towards NumPy, which uses CPU (since that's what I initially asked, to simplify the problem). Would the optimal strategy be different for using CuPy, which is NumPy for GPU? As far as I know, it makes using Numba much less straightforward.
","Since using sliding_window_view is not efficient for your use case, I will provide an alternative using Numba.
First, to simplify the implementation, define the following argmax alternative.
from numba import njit


@njit
def argmax(*values):
    &quot;&quot;&quot;argmax alternative that can take an arbitrary number of arguments.

    Usage: argmax(0, 1, 3, 2)  # 2
    &quot;&quot;&quot;
    max_arg = 0
    max_value = values[0]
    for i in range(1, len(values)):
        value = values[i]
        if value &gt; max_value:
            max_value = value
            max_arg = i
    return max_arg

This is a standard argmax function, except it takes multiple scalar arguments instead of a single numpy array.
Using this argmax alternative, your operation can be easily re-implemented.
@njit(cache=True)
def neighbor_argmax(a):
    height, width = a.shape[0] - 2, a.shape[1] - 2
    out = np.empty((height, width), dtype=np.uint8)
    for y in range(height):
        for x in range(width):
            # window: a[y:y + 3, x:x + 3]
            # center: a[y + 1, x + 1]
            out[y, x] = argmax(
                a[y, x + 1],  # up
                a[y + 1, x],  # left
                a[y + 1, x + 2],  # right
                a[y + 2, x + 1],  # down
            )
    return out

This function requires only a few variables to operate, excluding the input and output buffers.
So we don't need to worry about memory footprint.
Alternatively, you can use stencil, a sliding window utility for Numba.
With stencil, you only need to define the kernel. Numba will take care of the rest.
from numba import njit, stencil

@stencil
def kernel(window):
    # window: window[-1:2, -1:2]
    # center: window[0, 0]
    return np.uint8(  # Don't forget to cast to np.uint8.
        argmax(
            window[-1, 0],  # up
            window[0, -1],  # left
            window[0, 1],  # right
            window[1, 0],  # down
        )
    )


@njit(cache=True)
def neighbor_argmax_stencil(a):
    return kernel(a)[1:-1, 1:-1]  # Slicing is not mandatory.

It can also be inlined, if you like.
@njit(cache=True)
def neighbor_argmax_stencil_inlined(a):
    f = stencil(lambda w: np.uint8(argmax(w[-1, 0], w[0, -1], w[0, 1], w[1, 0])))
    return f(a)[1:-1, 1:-1]  # Slicing is not mandatory.

However, stencil is very limited in functionality and cannot completely replace sliding_window_view.
One difference is that there is no option to skip the edges.
It is always padded with a constant value (0 by default).
That is, if you put (L, L) matrix, you will get (L, L) output, not (L-2, L-2).
This is why I am slicing the output in the code above to match your implementation.
However, this may not be the desired behavior, as it breaks memory contiguity.
You can copy after slicing, but be aware that it will increase the peak memory usage.
In addition, it should be noted that these functions can also be easily adapted for multi-threading.
For details, please refer to the benchmark code below.
Here is the benchmark.
import math
import timeit

import numpy as np
from numba import njit, prange, stencil
from numpy.lib.stride_tricks import sliding_window_view


def baseline(a):
    L = a.shape[0]
    swv = sliding_window_view(a, (3, 3))  # (L-2) x (L-2) x 3 x 3
    directions = swv.reshape(L - 2, L - 2, 9)[:, :, 1::2].argmax(axis=2).astype(np.uint8)
    return directions


@njit
def argmax(*values):
    &quot;&quot;&quot;argmax alternative that can accept an arbitrary number of arguments.

    Usage: argmax(0, 1, 3, 2)  # 2
    &quot;&quot;&quot;
    max_arg = 0
    max_value = values[0]
    for i in range(1, len(values)):
        value = values[i]
        if value &gt; max_value:
            max_value = value
            max_arg = i
    return max_arg


@njit(cache=True)
def neighbor_argmax(a):
    height, width = a.shape[0] - 2, a.shape[1] - 2
    out = np.empty((height, width), dtype=np.uint8)
    for y in range(height):
        for x in range(width):
            # window: a[y:y + 3, x:x + 3]
            # center: a[y + 1, x + 1]
            out[y, x] = argmax(
                a[y, x + 1],  # up
                a[y + 1, x],  # left
                a[y + 1, x + 2],  # right
                a[y + 2, x + 1],  # down
            )
    return out


@njit(cache=True, parallel=True)  # Add parallel=True.
def neighbor_argmax_mt(a):
    height, width = a.shape[0] - 2, a.shape[1] - 2
    out = np.empty((height, width), dtype=np.uint8)
    for y in prange(height):  # Change this to prange.
        for x in range(width):
            # window: a[y:y + 3, x:x + 3]
            # center: a[y + 1, x + 1]
            out[y, x] = argmax(
                a[y, x + 1],  # up
                a[y + 1, x],  # left
                a[y + 1, x + 2],  # right
                a[y + 2, x + 1],  # down
            )
    return out


@stencil
def kernel(window):
    # window: window[-1:2, -1:2]
    # center: window[0, 0]
    return np.uint8(  # Don't forget to cast to np.uint8.
        argmax(
            window[-1, 0],  # up
            window[0, -1],  # left
            window[0, 1],  # right
            window[1, 0],  # down
        )
    )


@njit(cache=True)
def neighbor_argmax_stencil(a):
    return kernel(a)[1:-1, 1:-1]  # Slicing is not mandatory.


@njit(cache=True)
def neighbor_argmax_stencil_with_copy(a):
    return kernel(a)[1:-1, 1:-1].copy()  # Slicing is not mandatory.


@njit(cache=True, parallel=True)
def neighbor_argmax_stencil_mt(a):
    return kernel(a)[1:-1, 1:-1]  # Slicing is not mandatory.


@njit(cache=True)
def neighbor_argmax_stencil_inlined(a):
    f = stencil(lambda w: np.uint8(argmax(w[-1, 0], w[0, -1], w[0, 1], w[1, 0])))
    return f(a)[1:-1, 1:-1]  # Slicing is not mandatory.


def benchmark():
    size = 2000  # Total nbytes (in MB) for a.
    n = math.ceil(math.sqrt(size * (10 ** 6) / 4))
    rng = np.random.default_rng(0)
    a = rng.random(size=(n, n), dtype=np.float32)
    print(f&quot;{a.shape=}, {a.nbytes=:,}&quot;)

    expected = baseline(a)
    # expected = neighbor_argmax_mt(a)
    assert expected.shape == (n - 2, n - 2) and expected.dtype == np.uint8

    candidates = [
        baseline,
        neighbor_argmax,
        neighbor_argmax_mt,
        neighbor_argmax_stencil,
        neighbor_argmax_stencil_mt,
        neighbor_argmax_stencil_with_copy,
        neighbor_argmax_stencil_inlined,
    ]
    name_len = max(len(f.__name__) for f in candidates)
    for f in candidates:
        assert np.array_equal(expected, f(a)), f.__name__
        t = timeit.repeat(lambda: f(a), repeat=3, number=1)
        print(f&quot;{f.__name__:{name_len}} : {min(t)}&quot;)


if __name__ == &quot;__main__&quot;:
    benchmark()

Result:
a.shape=(22361, 22361), a.nbytes=2,000,057,284
baseline                          : 24.971996600041166
neighbor_argmax                   : 0.1917789001017809
neighbor_argmax_mt                : 0.11929619999136776
neighbor_argmax_stencil           : 0.2940085999434814
neighbor_argmax_stencil_mt        : 0.17756330000702292
neighbor_argmax_stencil_with_copy : 0.46573049994185567
neighbor_argmax_stencil_inlined   : 0.29338629997801036

I think these results are enough to make you consider giving Numba a try :)

The following section was added after this answer was accepted.
Here is the CUDA version. (I'm using numba 0.60.0)
from numba import cuda

@cuda.jit(device=True)
def argmax_cuda(values):  # cuda.jit cannot handle an arbitrary number of arguments.
    max_arg = 0
    max_value = values[0]
    for i in range(1, len(values)):
        value = values[i]
        if value &gt; max_value:
            max_value = value
            max_arg = i
    return max_arg


@cuda.jit
def neighbor_argmax_cuda_impl(a, out):
    y, x = cuda.grid(2)
    if y &lt; out.shape[0] and x &lt; out.shape[1]:
        out[y, x] = argmax_cuda(
            # Make sure to use a tuple, not a list.
            (
                a[y, x + 1],  # up
                a[y + 1, x],  # left
                a[y + 1, x + 2],  # right
                a[y + 2, x + 1],  # down
            )
        )


def neighbor_argmax_cuda(a, out):
    # If the input/output array is not on the GPU, you can transfer it like this.
    # However, note that this operation alone takes longer than neighbor_argmax_mt.
    # a = cuda.to_device(a)
    # out = cuda.to_device(out)

    # Block settings. I'm not sure if this is the optimal one.
    threadsperblock = (16, 16)
    blockspergrid_x = int(np.ceil(out.shape[1] / threadsperblock[1]))
    blockspergrid_y = int(np.ceil(out.shape[0] / threadsperblock[0]))
    blockspergrid = (blockspergrid_x, blockspergrid_y)

    neighbor_argmax_cuda_impl[blockspergrid, threadsperblock](a, out)

    # Back to CPU, if necessary.
    # out = out.copy_to_host()
    return out

As Jérôme explained in detail, the time taken to transfer the input/output arrays from the host to the device cannot be ignored.
a.shape=(22361, 22361), a.nbytes=2,000,057,284
neighbor_argmax                         : 0.47917880106251687
neighbor_argmax_mt                      : 0.08353979291860014
neighbor_argmax_cuda (with transfer)    : 0.5072600540006533
neighbor_argmax_cuda (without transfer) : 9.134004358202219e-05

(I had to use another machine to use CUDA. For that reason, the results for the CPU are different from the ones I put above.)
"
"I want to define a model that has a self-referential (or recursive) foreign key using SQLModel. (This relationship pattern is also sometimes referred to as an adjacency list.) The pure SQLAlchemy implementation is described here in their documentation.
Let's say I want to implement the basic tree structure as described in the SQLAlchemy example linked above, where I have a Node model and each instance has an id primary key, a data field (say of type str), and an optional reference (read foreign key) to another node that we call its parent node (field name parent_id).
Ideally, every Node object should have a parent attribute, which will be None, if the node has no parent node; otherwise it will contain (a pointer to) the parent Node object.
And even better, every Node object should have a children attribute, which will be a list of Node objects that reference it as their parent.
The question is twofold:

What is an elegant way to implement this with SQLModel?

How would I create such node instances and insert them into the database?


","The sqlmodel.Relationship function allows explicitly passing additional keyword-arguments to the sqlalchemy.orm.relationship constructor that is being called under the hood via the sa_relationship_kwargs parameter. This parameter expects a mapping of strings representing the SQLAlchemy parameter names to the values we want to pass through as arguments.
Since SQLAlchemy relationships provide the remote_side parameter for just such an occasion, we can leverage that directly to construct the self-referential pattern with minimal code. The documentation mentions this in passing, but crucially the remote_side value

may be passed as a Python-evaluable string when using Declarative.

This is exactly what we need. The only missing piece then is the proper use of the back_populates parameter and we can build the model like so:
from typing import Optional
from sqlmodel import Field, Relationship, Session, SQLModel, create_engine


class Node(SQLModel, table=True):
    __tablename__ = 'node'  # just to be explicit

    id: Optional[int] = Field(default=None, primary_key=True)
    data: str
    parent_id: Optional[int] = Field(
        foreign_key='node.id',  # notice the lowercase &quot;n&quot; to refer to the database table name
        default=None,
        nullable=True
    )
    parent: Optional['Node'] = Relationship(
        back_populates='children',
        sa_relationship_kwargs=dict(
            remote_side='Node.id'  # notice the uppercase &quot;N&quot; to refer to this table class
        )
    )
    children: list['Node'] = Relationship(back_populates='parent')

# more code below...

Side note: We define id as optional as is customary with SQLModel to avoid being nagged by our IDE when we want to create an instance, for which the id will only be known, after we have added it to the database. The parent_id and parent attributes are obviously defined as optional because not every node needs to have a parent in our model.
To test that everything works the way we expect it to:
def test() -&gt; None:
    # Initialize database &amp; session:
    sqlite_file_name = 'database.db'
    sqlite_uri = f'sqlite:///{sqlite_file_name}'
    engine = create_engine(sqlite_uri, echo=True)
    SQLModel.metadata.drop_all(engine)
    SQLModel.metadata.create_all(engine)
    session = Session(engine)

    # Initialize nodes:
    root_node = Node(data='I am root')

    # Set the children's `parent` attributes;
    # the parent nodes' `children` lists are then set automatically:
    node_a = Node(parent=root_node, data='a')
    node_b = Node(parent=root_node, data='b')
    node_aa = Node(parent=node_a, data='aa')
    node_ab = Node(parent=node_a, data='ab')

    # Add to the parent node's `children` list;
    # the child node's `parent` attribute is then set automatically:
    node_ba = Node(data='ba')
    node_b.children.append(node_ba)

    # Commit to DB:
    session.add(root_node)
    session.commit()

    # Do some checks:
    assert root_node.children == [node_a, node_b]
    assert node_aa.parent.parent.children[1].parent is root_node
    assert node_ba.parent.data == 'b'
    assert all(n.data.startswith('a') for n in node_ab.parent.children)
    assert (node_ba.parent.parent.id == node_ba.parent.parent_id == root_node.id) \
           and isinstance(root_node.id, int)


if __name__ == '__main__':
    test()

All the assertions are satisfied and the test runs without a hitch.
Also, by using the echo=True switch for the database engine, we can verify in our log output that the table is created as we expected:
CREATE TABLE node (
    id INTEGER, 
    data VARCHAR NOT NULL, 
    parent_id INTEGER, 
    PRIMARY KEY (id), 
    FOREIGN KEY(parent_id) REFERENCES node (id)
)

"
"I'm trying to update my code to pydantic v2 and having trouble finding a good way to replicate the custom types I had in version 1. I'll use my custom date type as an example. The original implementation and usage looked something like this:
from datetime import date
from pydantic import BaseModel


class CustomDate(date):
    # Override POTENTIAL_FORMATS and fill it with date format strings to match your data
    POTENTIAL_FORMATS = []
    
    @classmethod
    def __get_validators__(cls):
        yield cls.validate_date
        
    @classmethod
    def validate_date(cls, field_value, values, field, config) -&gt; date:
        if type(field_value) is date:
            return field_value
        return to_date(field.name, field_value, cls.POTENTIAL_FORMATS, return_str=False)

class ExampleModel(BaseModel):
    class MyDate(CustomDate):
        POTENTIAL_FORMATS = ['%Y-%m-%d', '%Y/%m/%d']
    dt: MyDate

I tried to follow the official docs and the examples laid out here below and it mostly worked, but the info parameter does not have the fields I need (data and field_name). Attempting to access them gives me an AttributeError.
info.field_name
*** AttributeError: No attribute named 'field_name'

Both the Annotated and __get_pydantic_core_schema__ approaches have this issue
from datetime import date
from typing import Annotated

from pydantic import BaseModel, BeforeValidator
from pydantic_core import core_schema  

class CustomDate:
    POTENTIAL_FORMATS = []

    @classmethod
    def validate(cls, field_value, info):
        if type(field_value) is date:
            return field_value
        return to_date(info.field_name, field_value, potential_formats, return_str=False)

    @classmethod
    def __get_pydantic_core_schema__(cls, source, handler) -&gt; core_schema.CoreSchema:
        return core_schema.general_plain_validator_function(cls.validate)


def custom_date(potential_formats):
    &quot;&quot;&quot;
    :param potential_formats: A list of datetime format strings
    &quot;&quot;&quot;
    def validate_date(field_value, info) -&gt; date:
        if type(field_value) is date:
            return field_value
        return to_date(info.field_name, field_value, potential_formats, return_str=False)
    CustomDate = Annotated[date, BeforeValidator(validate_date)]
    return CustomDate


class ExampleModel(BaseModel):
    class MyDate(CustomDate):
        POTENTIAL_FORMATS = ['%Y-%m-%d', '%Y/%m/%d']
    dt: MyDate
    dt2: custom_date(['%Y-%m-%d', '%Y/%m/%d'])

If I just include the validate_date function as a regular field_validator I get info with all the fields I need, it's only when using it with custom types that I see this issue. How do I write a custom type that has access to previously validated fields and the name of the field being validated?
","As of version 2.4 you can get the field_name and data together. See the updated docs here.
Now the first version of my custom data type looks like:
class CustomDate:
    POTENTIAL_FORMATS = []

    @classmethod
    def validate(cls, field_value, info):
        if type(field_value) is date:
            return field_value
        return to_date(info.field_name, field_value, cls.POTENTIAL_FORMATS, return_str=False)

    @classmethod
    def __get_pydantic_core_schema__(cls, source, handler) -&gt; core_schema.CoreSchema:
        return core_schema.with_info_before_validator_function(
            cls.validate, handler(date), field_name=handler.field_name
        )

Where all I needed to change was which core_schema validator function I was using. The second version of my custom data type (the one using Annotated) now works as is with no changes.
Before Pydantic 2.4
It looks like accessing info.data and info.field_name inside a custom type validator is not currently possible in v2 according to this feature request.
If all you need is info.data, then it looks like you can define your validator with core_schema.field_before_validator_function (I'd guess all the field_* validators work), although you will need to make up a field name:
from dataclasses import dataclass
from typing import Annotated, List, Any, Callable


from pydantic import ValidationError, BaseModel, Field, BeforeValidator, field_validator, GetCoreSchemaHandler
from pydantic_core import core_schema, CoreSchema


def fn(v: str, info: core_schema.ValidationInfo, *args, **kwargs) -&gt; str:
    try:
        print(f'Validating {info.field_name}')
        return info.data['use_this']
    except AttributeError as err:
        return 'No data'


class AsFieldB4Method(str):
    @classmethod
    def __get_pydantic_core_schema__(
        cls, source_type: Any, handler: GetCoreSchemaHandler, *args, **kwargs
    ) -&gt; CoreSchema:
        return core_schema.field_before_validator_function(fn, 'not_the_real_field_name', core_schema.str_schema())


class MyModel(BaseModel):
    use_this: str
    core_schema_field_b4_method: AsFieldB4Method  # Partially works

From the comments, it sounds like the pydantic team want to make it work with non-field validators and to make accessing info.field_name possible, so hopefully that happens. I'll update this answer when the change happens, but check that link in case I missed it.
"
"I think I'm missing something simple
I have a python poetry application:
name = &quot;my-first-api&quot;
version = &quot;0.1.0&quot;
description = &quot;&quot;
readme = &quot;README.md&quot;
packages = [{include = &quot;application&quot;}]

[tool.poetry.scripts]
start = &quot;main:start&quot;

[tool.poetry.dependencies]
python = &quot;&gt;=3.10,&lt;3.12&quot;
pip= &quot;23.0.1&quot;
setuptools=&quot;65.5.0&quot;
fastapi=&quot;0.89.1&quot;
uvicorn=&quot;0.20.0&quot;

[tool.poetry.group.dev.dependencies]
pyinstaller = &quot;^5.10.1&quot;
pytest = &quot;^7.3.1&quot;

[build-system]
requires = [&quot;poetry-core&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;

I can run this and build this using Poetry, however, I would like to be able to create the executable with a poetry script as well.
Now I build it like this:
poetry run pyinstaller main.py --collect-submodules application --onefile --name myapi
I would like something like
poetry package to automatically create this executable as well. How do I hook that up?
Btw. ths does not work :(
[tool.poetry.scripts]
start = &quot;main:start&quot;
builddist = &quot;poetry run pyinstaller main.py --collect-submodules application --onefile --name myapi&quot;

","I have found a solution using the pyinstaller API.
As you may know already, Poetry will only let you run 'scripts' if they are functions inside your package. Just like in your pyproject.toml, you map the start command to main:start, which is the start() function of your main.py module.
Similarly, you can create a function in a module that triggers Pyinstaller and map that to a command that you can run as poetry run &lt;commmand&gt;.

Assuming you have a module structure like this:
my_package
├── my_package
│   ├── __init__.py
│   ├── pyinstaller.py
│   └── main.py
└── pyproject.toml

1. Create a file pyinstaller.py to call the Pyinstaller API
The file should be inside your package structure, as shown in the diagram above. This is adapted from the Pyinstaller docs
import PyInstaller.__main__
from pathlib import Path

HERE = Path(__file__).parent.absolute()
path_to_main = str(HERE / &quot;main.py&quot;)

def install():
    PyInstaller.__main__.run([
        path_to_main,
        '--onefile',
        '--windowed',
        # other pyinstaller options... 
    ])

2. Map the build command in pyproject.toml
In the pyproject.toml file, add this
[tool.poetry.scripts]
build = &quot;my_package.pyinstaller:install&quot;

3. From the terminal, invoke the build command
You must do so under the virtual environment that poetry creates:
poetry run build

🎉 Profit
"
"I know it isn't a correct thing to do, but I would like to try to install package that requires Python 3.8, but my installed Python is 3.7.
Is it possible using pip? Or I must clone the repository and change the setup.py?
","You can use the --ignore-requires-python option.
pip install --help

  --ignore-requires-python    Ignore the Requires-Python information.

You can try it with your package, or also with this minimal setup.py:
from setuptools import setup, find_packages

setup(
    name=&quot;foobar&quot;,
    version=&quot;1.0&quot;,
    packages=find_packages(),
    python_requires=&quot;&lt;3.7&quot;
)

With Python 3.7,
$ pip install .
Processing /home/vvvvv/75726452
ERROR: Package 'foobar' requires a different Python: 3.7.11 not in '&lt;3.7'

$ pip install . --ignore-requires-python
Processing /home/vvvvv/75726452
Installing collected packages: foobar
    Running setup.py install for foobar ... done
Successfully installed foobar-1.0


However, as Mark Ransom suggested, there probably is a good reason the package you are trying to install does requires a version of Python different from 3.7.
"
"I have a list that contains very large integers and I want to cast it into a pandas column with a specific dtype. As an example, if the list contains 2**31, which is outside the limit of int32 dtype, casting it into dtype int32 throws an Overflow Error, which lets me know to use another dtype or handle the number in some other way beforehand.
import pandas as pd
pd.Series([2**31], dtype='int32')

# OverflowError: Python int too large to convert to C long

But if a number is large but inside the dtype limits (i.e. 2**31-1), and some number is added to it which results in a value that is outside the dtype limits, then instead of an OverflowError, the operation is executed without any errors, yet the value is now inverted, becoming a completely wrong number for the column.
pd.Series([2**31-1], dtype='int32') + 1

0   -2147483648
dtype: int32

Why is it happening? Why doesnâ€™t it raise an error like the first case?
PS. I'm using pandas 2.1.1 and numpy 1.26.0 on Python 3.12.0.
","
Why does an operation on a large integer silently overflow?

As a short answer, that's because of how numpy deals with overflows.

On my platform (with the same versions of Python/Packages as yours) :
from platform import *
import numpy as np; import pandas as pd

system(), version(), machine()
python_version(), pd.__version__, np.__version__

('Linux',
 '#34~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Sep  7 13:12:03 UTC 2',
 'x86_64')
('3.12.0', '2.1.1', '1.26.0')

I can reproduce your issue but with a larger integer than the one you choose as an example :
pd.Series([2**63], dtype=&quot;int32&quot;) raises this :

OverflowError: Python int too large to convert to C long

While pd.Series([2**31], dtype=&quot;int32&quot;) raises this :

ValueError: Values are too large to be losslessly converted to int32. To cast anyway, use pd.Series(values).astype(int32)

Details
Let's agree that you're using two different type of objects, which potentially means two different scenarios, i.e, 1) error raised or 2) no error raised :

pd.Series : the Series constructor
pd.Series.add : a method of the former

The construction : pd.Series([2**31], dtype=&quot;int32&quot;)
It is handled behind the scenes by sanitize_array which receives your input (the list [2**31], i.e [2147483648]) and call in this case maybe_cast_to_integer_array. The latter will make a classical NumPy construction using np.array :
casted = np.array([2147483648], dtype=&quot;int32&quot;)


DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 2147483648 to int32 will fail in the future.
For the old behavior, usually:
np.array(value).astype(dtype)
will give the desired result (the cast overflows).
np.array([2147483648], dtype='int32')

You may ask yourself why the warning above doesn't show off while constructing your Series, well that's because pandas silents it. Now, right after casting, pandas calls np.asarray without specifying a dtype to let NumPy infer a dtype (which is int64 here) : arr = np.asarray(arr). And since, casted.dtype &lt; arr.dtype, the ValueError is triggered.
The addition : pd.Series([2**31-1], dtype=&quot;int32&quot;) + 1
This one is delegated to _na_arithmetic_op that receives array([2147483647], dtype=int32) and 1 and try to add them together with the help of _evaluate_standard to make a classical operator.add operation that is equivalent to np.array([2147483647]) + 1 and since the fixed size of NumPy numeric types may cause overflow errors when a value requires more memory than available in the data type, the result is the array([-2147483648], dtype=int32) which is passed to sanitize_array to construct back a Series :
pd.Series([2**31-1], dtype=&quot;int32&quot;) + 1

0   -2147483648
dtype: int32

NB : When you go beyond the limit of the int32, NumPy wraps around to the minimum value :
a = np.array([2**31-1], dtype=&quot;int32&quot;); b = 1
a+b # this gives array([-2147483648], dtype=int32)

Here is some other examples :
def wrap_int32(i, N, l=2**31):
    return ((i+N) % l) - l

wrap_int32(2**31, 0) # -2147483648
wrap_int32(2**31, 1) # -2147483647
wrap_int32(2**31, 2) # -2147483646
wrap_int32(2**31, 3) # -2147483645
# ...



I have a list that contains very large integers and I want to cast it into a pandas column with a specific dtype. As an example, if the list contains 2**31, which is outside the limit of int32 dtype, casting it into dtype int32 throws an OverflowError, which lets me know to use another dtype or handle the number in some other way beforehand.

Maybe you should consider opening an issue so that the arithmetic operations made by pandas raise an error in case of an overflow. And as a workaround (or maybe a solution?) for your usecase, you can try catching upstream the integers that doesn't fall within the int32 range :
iint32 = np.iinfo(np.int32)

lst = [100, 1234567890000, -1e19, 2**31, 2**31-1, -350]

out = [i for i in lst if iint32.min &lt;= i and i &lt;= iint32.max]
# [100, 2147483647, -350]

"
"Say you want to wrap the dataclass decorator like so:
from dataclasses import dataclass

def something_else(klass):
    return klass

def my_dataclass(klass):
    return something_else(dataclass(klass))

How should my_dataclass and/or something_else be annotated to indicate that the return type is a dataclass?
See the following example on how the builtin @dataclass works but a custom @my_dataclass does not:

@dataclass
class TestA:
    a: int
    b: str

TestA(0, &quot;&quot;) # fine


@my_dataclass
class TestB:
    a: int
    b: str

TestB(0, &quot;&quot;) # error: Too many arguments for &quot;TestB&quot; (from mypy)

","There is no feasible way to do this prior to PEP 681.
A dataclass does not describe a type but a transformation. The actual effects of this cannot be expressed by Python's type system – @dataclass is handled by a MyPy Plugin which inspects the code, not just the types. This is triggered on specific decorators without understanding their implementation.
dataclass_makers: Final = {
    'dataclass',
    'dataclasses.dataclass',
}

While it is possible to provide custom MyPy plugins, this is generally out of scope for most projects. PEP 681 (Python 3.11) adds a generic &quot;this decorator behaves like @dataclass&quot;-marker that can be used for all transformers from annotations to fields.
PEP 681 is available to earlier Python versions via typing_extensions.
Enforcing dataclasses
For a pure typing alternative, define your custom decorator to take a dataclass and modify it. A dataclass can be identified by its __dataclass_fields__ field.
from typing import Protocol, Any, TypeVar, Type, ClassVar
from dataclasses import Field

class DataClass(Protocol):
    __dataclass_fields__: ClassVar[dict[str, Field[Any]]]

DC = TypeVar(&quot;DC&quot;, bound=DataClass)

def my_dataclass(klass: Type[DC]) -&gt; Type[DC]:
    ...

This allows the type checker to understand and verify that a dataclass class is needed.
@my_dataclass
@dataclass
class TestB:
    a: int
    b: str

TestB(0, &quot;&quot;)  # note: Revealed type is &quot;so_test.TestB&quot;

@my_dataclass
class TestC:  # error: Value of type variable &quot;DC&quot; of &quot;my_dataclass&quot; cannot be &quot;TestC&quot;
    a: int
    b: str

Custom dataclass-like decorators
The PEP 681 dataclass_transform decorator is a marker for other decorators to show that they act &quot;like&quot; @dataclass. In order to match the behaviour of @dataclass, one has to use field_specifiers  to indicate that fields are denoted the same way.
from typing import dataclass_transform, TypeVar, Type
import dataclasses

T = TypeVar(&quot;T&quot;)

@dataclass_transform(
    field_specifiers=(dataclasses.Field, dataclasses.field),
)
def my_dataclass(klass: Type[T]) -&gt; Type[T]:
    return something_else(dataclasses.dataclass(klass))

It is possible for the custom dataclass decorator to take all keywords as @dataclass. dataclass_transform can be used to mark their respective defaults, even when not accepted as keywords by the decorator itself.
"
"I have a build script to run a simple python app. I am trying to set it up that it will run for any user that has conda installed and in their PATH. No other prerequisites. I have that pretty much accomplished but would like to make it more efficient for returning users.
build_run.sh
conda init bash
conda env create --name RUN_ENV --file ../run_env.yml -q --force
conda activate RUN_ENV
python run_app.py
conda deactivate

I would like to make it that the script checks if RUN_ENV already exists and activates it instead of forcing its creation every time. I tried
ENVS=$(conda env list | awk '{print }' )
if [[ conda env list = *&quot;RUN_ENV&quot;* ]]; then
   conda activate RUN_ENV
else 
   conda env create --name RUN_ENV --file ../run_env.yml -q
   conda activate RUN_ENV
   exit
fi;
python run_app.py
conda deactivate

but it always came back as false and tried to create RUN_ENV
","update 2022
i've been receiving upvotes recently. so i'm going to bump up that this method overall is not natively &quot;conda&quot; and might not be the best approach. like i said originally, i do not use conda. take my advice at your discretion.
rather, please refer to @merv's comment in the question suggesting the use of the --prefix flag
additionally take a look at the documentation for further details
NOTE: you can always use a function within your bash script for repeated command invocations with very specific flags
e.g
function PREFIXED_CONDA(){
   action=${1};
   # copy $1 to $action;
   shift 1;
   # delete first argument and shift remaining indeces to the left
   conda ${action} --prefix /path/to/project ${@}
}


i am not sure how conda env list works (i don't use Anaconda); and your current if-tests are vague
but i'm going out on a limb and guessing this is what you're looking for
#!/usr/bin/env bash
# ...
find_in_conda_env(){
    conda env list | grep &quot;${@}&quot; &gt;/dev/null 2&gt;/dev/null
}

if find_in_conda_env &quot;.*RUN_ENV.*&quot; ; then
   conda activate RUN_ENV
else 
# ...

instead of bringing it out into a separate function, you could also do
# ...
if conda env list | grep &quot;.*RUN_ENV.*&quot; &gt;/dev/null 2&gt;&amp;1; then
# ...

bonus points for neatness and clarity if you use command grouping
# ...
if { conda env list | grep 'RUN_ENV'; } &gt;/dev/null 2&gt;&amp;1; then
# ...

if simply checks the exit code. and grep exits with 0 (success) as long as there's at least one match of the pattern provided; this evaluates to &quot;true&quot; in the if statement
(grep would match and succeed even if the pattern is just 'RUN_ENV' ;) )

the awk portion of ENVS=$(conda env list | awk '{print }' ) does virtually nothing. i would expect the output to be in tabular format, but {print } does no filtering, i believe you were looking for {print $n} where n is a column number or awk /PATTERN/ {print} where PATTERN is likely RUN_ENV and only lines which have PATTERN are printed.
but even so, storing a table in a string variable is going to be messing. you might want an array.
then coming to your if-condition, it's plain syntactically wrong.

the [[ construct is for comparing values: integer, string, regex
but here on the left of = we have a command conda env list

which i believe is also the contents of $ENVS


hence we can assume you meant [[ &quot;${ENVS}&quot; == *&quot;RUN_ENV&quot;* ]]

or alternately [[ $(conda env list) == *&quot;RUN_ENV&quot;* ]]


but still, regex matching against a table... not very intuitive imo
but it works... sort of
the proper clean syntax for regex matching is

[[ ${value} =~ /PATTERN/ ]]



"
"I'm trying to test my FastAPI endpoints by overriding the injected database using the officially recommended method in the FastAPI documentation.
The function I'm injecting the db with is a closure that allows me to build any desired database from a MongoClient by giving it the database name whilst (I assume) still working with FastAPI depends as it returns a closure function's signature. No error is thrown so I think this method is correct:
# app
def build_db(name: str):
    def close():
          return build_singleton_whatever(MongoClient, args....)
     return close

Adding it to the endpoint:
# endpoint
@app.post(&quot;/notification/feed&quot;)
async def route_receive_notifications(db: Database = Depends(build_db(&quot;someDB&quot;))):
   ...

And finally, attempting to override it in the tests:
# pytest
# test_endpoint.py
fastapi_app.dependency_overrides[app.build_db] = lambda x: lambda: x

However, the dependency doesn't seem to override at all and the test ends up creating a MongoClient with the IP of the production database as in normal execution.
So, any ideas on overriding FastAPI dependencies that are given parameters in their endpoints?
I have tried creating a mock closure function with no success:
def mock_closure(*args):
    def close():
        return args
    return close

app.dependency_overrides[app.build_db] = mock_closure('otherDB')

And I have also tried providing the same signature, including the parameter, with still no success:
app.dependency_overrides[app.build_db('someDB')] = mock_closure('otherDB')

Edit note I'm also aware I can create a separate function that creates my desired database and use that as the dependency, but I would much prefer to use this dynamic version as it's more scalable to using more databases in my apps and avoids me writing essentially repeated functions just so they can be cleanly injected.
","My case involved an HTTP client wrapper, instead of a DB. I think it could be applied to your case as well.
Context: I want to inject values for a FastAPI handler's dependency to test various scenarios.
We have a handler with its dependencies
@router.get(&quot;/{foo}&quot;)
async def get(foo, client = Depends(get_client)): # get_client is the key to override
  client = get_client()
  return await client.request(foo)

The function get_client is the dependency I want to override in my tests. It returns a Client object that takes a function that performs an HTTP request to an external service (this function actually wraps aiohttp, but that's not the important part). Here's its barebone definition:
class Client:
  def __init__(request):
    self._request = request
  
  async def request(self, params):
    return await self._request(params)

We want to test various responses from the external service, so we need to build the function that returns a function that returns a Client object (sorry for the tongue-twister), with its params:
def get_client_getter(response):
  async def request_mock(*args, **kwargs):
    return response

  def get_client():
    return Client(request=request_mock)

  return get_client()

Then in the various tests we have:
def test_1():
    app.dependency_overrides[get_client] = get_client_getter(1)
    ...

def test_true():
    app.dependency_overrides[get_client] = get_client_getter(True)
    ...

def test_none():
    app.dependency_overrides[get_client] = get_client_getter(None)
    ...

"
"I have a Pandas dataframe with the following structure:
A       B       C
a       b       1
a       b       2
a       b       3
c       d       7
c       d       8
c       d       5
c       d       6
c       d       3
e       b       4
e       b       3
e       b       2
e       b       1

And I will like to transform it into this:
A       B       C1      C2      C3      C4      C5
a       b       1       2       3       NAN     NAN
c       d       7       8       5       6       3
e       b       4       3       2       1       NAN

In other words, something like groupby A and B and expand C into different columns.
Knowing that the length of each group is different.
C is already ordered
Shorter groups can have NAN or NULL values (empty), it does not matter.
","Use GroupBy.cumcount and pandas.Series.add with 1, to start naming the new columns from 1 onwards, then pass this to DataFrame.pivot, and add DataFrame.add_prefix to rename the columns (C1, C2, C3, etc...). Finally use DataFrame.rename_axis to remove the indexes original name ('g') and transform the MultiIndex into columns by using DataFrame.reset_indexcolumns A,B:
df['g'] = df.groupby(['A','B']).cumcount().add(1)

df = df.pivot(['A','B'], 'g', 'C').add_prefix('C').rename_axis(columns=None).reset_index()
print (df)
   A  B   C1   C2   C3   C4   C5
0  a  b  1.0  2.0  3.0  NaN  NaN
1  c  d  7.0  8.0  5.0  6.0  3.0
2  e  b  4.0  3.0  2.0  1.0  NaN

Because NaN is by default of type float, if you need the columns dtype to be integers add DataFrame.astype with Int64:
df['g'] = df.groupby(['A','B']).cumcount().add(1)

df = (df.pivot(['A','B'], 'g', 'C')
        .add_prefix('C')
        .astype('Int64')
        .rename_axis(columns=None)
        .reset_index())
print (df)
   A  B  C1  C2  C3    C4    C5
0  a  b   1   2   3  &lt;NA&gt;  &lt;NA&gt;
1  c  d   7   8   5     6     3
2  e  b   4   3   2     1  &lt;NA&gt;

EDIT: If there's a maximum N new columns to be added, it means that A,B are duplicated. Therefore, it will beneeded to add helper groups g1, g2 with integer and modulo division, adding a new level in index:
N = 4
g  = df.groupby(['A','B']).cumcount()
df['g1'], df['g2'] = g // N, (g % N) + 1
df = (df.pivot(['A','B','g1'], 'g2', 'C')
        .add_prefix('C')
        .droplevel(-1)
        .rename_axis(columns=None)
        .reset_index())
print (df)
   A  B   C1   C2   C3   C4
0  a  b  1.0  2.0  3.0  NaN
1  c  d  7.0  8.0  5.0  6.0
2  c  d  3.0  NaN  NaN  NaN
3  e  b  4.0  3.0  2.0  1.0 

"
"I'm trying to write an endpoint that just accepts an image and attempts to convert it into another format, by running a command on the system. Then I return the converted file. It's slow and oh-so-simple, and I don't have to store files anywhere, except temporarily.
I'd like all the file-writing to happen in a temporary directory, so it gets cleaned up.
The route works fine if the output file is not in the temporary directory. But if I try to put the output file in the temporary directory, the FileResponse can't find it, and requests fail.
RuntimeError: File at path /tmp/tmpp5x_p4n9/out.jpg does not exist.
Is there something going on related to the asynchronous nature of FastApi that FileResponse can't wait for the subprocess to create the file its making?  Can I make it wait? (removing async from the route does not help).
@app.post(&quot;/heic&quot;)
async def heic(img: UploadFile):
    with TemporaryDirectory() as dir:
        inname = os.path.join(dir, &quot;img.heic&quot;)
        f = open(inname,&quot;wb&quot;)
        f.write(img.file.read())
        f.flush()

        # setting outname in the temp dir fails!
        # outname = os.path.join(dir, 'out.jpg')

        outname = os.path.join('out.jpg')

        cmd = f&quot;oiiotool {f.name} -o {outname}&quot;
        process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
        process.wait()
        return FileResponse(outname, headers={'Content-Disposition':'attachment; filename=response.csv'})


Thank you for any insights!
","According to the documentation of TemporaryDirectory()

This class securely creates a temporary directory using the same rules
as mkdtemp(). The resulting object can be used as a context manager
(see Examples). On completion of the context or destruction of the
temporary directory object, the newly created temporary directory and
all its contents are removed from the filesystem.

It seems that the directory and contents are already being released before the FastAPI request is returned. However you can use Dependency injection in FastAPI, so why no inject a temporary directory?
First define the dependency:
async def get_temp_dir():
    dir = TemporaryDirectory()
    try:
        yield dir.name
    finally:
        del dir

And add the dependency to your endpoint:
@app.post(&quot;/heic&quot;)
async def heic(imgfile: UploadFile = File(...), dir=Depends(get_temp_dir)):
    inname = os.path.join(dir, &quot;img.heic&quot;)
    f = open(inname,&quot;wb&quot;)
    f.write(imgfile.file.read())
    f.flush()

    outname = os.path.join(dir, 'out.jpg')
    cmd = f&quot;oiiotool {f.name} -o {outname}&quot;
    process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
    process.wait()
    return FileResponse(inname, headers={'Content-Disposition':'attachment; filename=response.csv'})

I've tested it with returning the incoming file and that works.
"
"I am trying to remove null values across a list of selected columns. But it seems that I might have got the with_columns operation not right. What's the right approach if you want to operate the removing only on selected columns?
df = pl.DataFrame(
    {
        &quot;id&quot;: [&quot;NY&quot;, &quot;TK&quot;, &quot;FD&quot;], 
        &quot;eat2000&quot;: [1, None, 3], 
        &quot;eat2001&quot;: [-2, None, 4],
        &quot;eat2002&quot;: [None, None, None],
        &quot;eat2003&quot;: [-9, None, 8],
        &quot;eat2004&quot;: [None, None, 8]
    }
); df

â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ id  â”† eat2000 â”† eat2001 â”† eat2002 â”† eat2003 â”† eat2004 â”‚
â”‚ --- â”† ---     â”† ---     â”† ---     â”† ---     â”† ---     â”‚
â”‚ str â”† i64     â”† i64     â”† f64     â”† i64     â”† i64     â”‚
â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•¡
â”‚ NY  â”† 1       â”† -2      â”† null    â”† -9      â”† null    â”‚
â”‚ TK  â”† null    â”† null    â”† null    â”† null    â”† null    â”‚
â”‚ FD  â”† 3       â”† 4       â”† null    â”† 8       â”† 8       â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

col_list = [word for word in df.columns if word.startswith((&quot;eat&quot;))]
(
    df
    .with_columns(
        pl.col(col_list).filter(~pl.fold(True, lambda acc, s: acc &amp; s.is_null(), pl.all()))
    )
)

# InvalidOperationError: dtype String not supported in 'not' operation

Expected output:
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ id  â”† eat2000 â”† eat2001 â”† eat2002 â”† eat2003 â”† eat2004 â”‚
â”‚ --- â”† ---     â”† ---     â”† ---     â”† ---     â”† ---     â”‚
â”‚ str â”† i64     â”† i64     â”† f64     â”† i64     â”† i64     â”‚
â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•¡
â”‚ NY  â”† 1       â”† -2      â”† null    â”† -9      â”† null    â”‚
â”‚ FD  â”† 3       â”† 4       â”† null    â”† 8       â”† 8       â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

","
polars.col also accepts Regular expressions which is one way to select all columns that start with a specific string.
polars.all_horizontal combines all results horizontally (i.e., row-wise) to give a single True/False value per row.

df.select(
    ~pl.all_horizontal(pl.col(r'^eat.*$').is_null())
)

shape: (3, 1)
┌───────┐
│ all   │
│ ---   │
│ bool  │
╞═══════╡
│ true  │
│ false │
│ true  │
└───────┘

DataFrame.filter can be used to keep only the true rows:
df.filter(
    ~pl.all_horizontal(pl.col(r'^eat.*$').is_null())
)

shape: (2, 6)
┌─────┬─────────┬─────────┬─────────┬─────────┬─────────┐
│ id  ┆ eat2000 ┆ eat2001 ┆ eat2002 ┆ eat2003 ┆ eat2004 │
│ --- ┆ ---     ┆ ---     ┆ ---     ┆ ---     ┆ ---     │
│ str ┆ i64     ┆ i64     ┆ f32     ┆ i64     ┆ i64     │
╞═════╪═════════╪═════════╪═════════╪═════════╪═════════╡
│ NY  ┆ 1       ┆ -2      ┆ null    ┆ -9      ┆ null    │
│ FD  ┆ 3       ┆ 4       ┆ null    ┆ 8       ┆ 8       │
└─────┴─────────┴─────────┴─────────┴─────────┴─────────┘

The ~ in front of the pl.all_horizontal stands for negation.  Notice that we didn't need the col_list.
One caution: the regex expression in the pl.col must start with ^ and end with $.  These cannot be omitted, even if the resulting regex expression is otherwise valid.
Alternately, if you don't like the ~ operator, there is .not_()
df.filter(
    pl.all_horizontal(pl.col(r'^eat.*$').is_null()).not_()
)

Or, we can check if there are any non-null values instead:
df.filter(
    pl.any_horizontal(pl.col(r'^eat.*$').is_not_null())
)

Other Notes
As an aside, Polars has other dedicated horizontal functions e.g. min_horizontal, max_horizontal, sum_horizontal
Edit - using fold
FYI, here's how to use the fold method, if that is what you'd prefer. Note the use of pl.col with a regex expression.
df.filter(
    ~pl.fold(True, lambda acc, s: acc &amp; s.is_null(), exprs=pl.col(r'^eat.*$'))
)

shape: (2, 6)
┌─────┬─────────┬─────────┬─────────┬─────────┬─────────┐
│ id  ┆ eat2000 ┆ eat2001 ┆ eat2002 ┆ eat2003 ┆ eat2004 │
│ --- ┆ ---     ┆ ---     ┆ ---     ┆ ---     ┆ ---     │
│ str ┆ i64     ┆ i64     ┆ null    ┆ i64     ┆ i64     │
╞═════╪═════════╪═════════╪═════════╪═════════╪═════════╡
│ NY  ┆ 1       ┆ -2      ┆ null    ┆ -9      ┆ null    │
│ FD  ┆ 3       ┆ 4       ┆ null    ┆ 8       ┆ 8       │
└─────┴─────────┴─────────┴─────────┴─────────┴─────────┘

"
"Hopefully the title isn't too misleading, I'm not sure the best way to phrase my question.
I'm trying to create a (X, Y) coordinate data type in Python. Is there a way to create a &quot;custom data type&quot; so that I have an object with a value, but also some supporting attributes?
So far I've made this simple class:
class Point:
    def __init__(self, x, y):
        self.x = x
        self.y = y
        self.tuple = (x, y)

Ideally, I'd like to be able to do something like this:
&gt;&gt;&gt; p = Point(4, 5)
&gt;&gt;&gt;
&gt;&gt;&gt; my_x = p.x    # can access the `x` attribute with &quot;dot syntax&quot;
&gt;&gt;&gt;
&gt;&gt;&gt; my_tuple = p  # or can access the tuple value directly
                  # without needing to do `.tuple`, as if the `tuple`
                  # attribute is the &quot;default&quot; attribute for the object

NOTE I'm not trying to simply display the tuple, I know I can do that with the __repr__ method
In a way, I'm trying to create a very simplified numpy.ndarray, because the ndarrays are a datatype that have their own attributes. I tried looking thru the numpy source to see how this is done, but it was way over my head, haha.
Any tips would be appreciated!
","I am not sure what you want to do with the tuple. p will always be an instance of Point. What you intend to do there won't work.
If you just don't want to use the dot notation, you could use a namedtuple or a dataclass instead of a class. Then cast their instances to a tuple using tuple() and astuple().

Using a namedtuple and tuple():
from collections import namedtuple

Point = namedtuple(&quot;Point&quot;, [&quot;x&quot;, &quot;y&quot;])

p = Point(4, 5)

x = p.x
y = p.y

xy = p  # xy = tuple(p) not necessary since namedtuple is already a tuple

Note: namedtuple is immutable, i.e. you can't change x and y.

Using a dataclasses.dataclass and dataclasses.astuple():
from dataclasses import dataclass, astuple

@dataclass
class Point:
    x: int
    y: int
    
p = Point(4, 5)

x = p.x
y = p.y

xy = astuple(p)

"
"I have a numpy array that maps x-y-coordinates to the appropriate z-coordinates. For this I use a 2D array that represents x and y as its axes and contains the corresponding z values:
import numpy as np
x_size = 2000
y_size = 2500
z_size = 400
rng = np.random.default_rng(123)
z_coordinates = np.linspace(0, z_size, y_size) + rng.laplace(0, 1, (x_size, y_size))

So each of the 2000*2500 x-y-points is assigned a z-value (float between 0 and 400). Now I want to look up for each integer z and integer x which is the closest y-value, essentially creating a map that is of shape (x_size, z_size) and holds the best y-values.
The simplest approach is creating an empty array of target shape and iterating over each z value:
y_coordinates = np.empty((x_size, z_size), dtype=np.uint16)
for i in range(z_size):
    y_coordinates[:, i] = np.argmin(
        np.abs(z_coordinates - i),
        axis=1,
    )

however this takes about 11 s on my machine, which unfortunately is way to slow.
Surely using a more vectorised approach would be faster, such as:
y_coordinates = np.argmin(
    np.abs(
        z_coordinates[..., np.newaxis] - np.arange(z_size)
    ),
    axis=1,
)

Surprisingly this runs about 60% slower than the version above (tested at 1/10th size, since at full size this uses excessive memory).
Also wrapping the code blocks in functions and decorating them with numba's @jit(nopython=True) doesn't help.
How can I speed up the calculation?
","This answer provide an algorithm with an optimal complexity: O(x_size * (y_size + z_size)). This algorithm is the fastest one proposed so far (by a large margin). It is implemented in Numba using multiple threads.

Explanation of the approach
The idea is that there is no need to iterate over all Z values : we can iterate over z_coordinates line by line, and for each line of z_coordinates, we fill an array used to find the nearest value for each possible z. The best candidate for the value z is stored in arr[z].
In practice, there are tricky corner cases making things a more complicated. For example, due to rounding, I decided to fill the neighbours of arr[z] (i.e. arr[z-1] and arr[z+1]) so to make the algorithm simpler. Moreover, when there are not enough values so arr cannot be fully filled by all the values in a line of z_coordinates, we need to fill the holes in the arr. In some more complicated cases (combining rounding issue while kind of holes in arr), we need to correct the values in arr (or operate on more distant neighbours which is not efficient). The number of step in the correction function should always be a small constant, certainly &lt;= 3 (it nerver reached 3 in practice in my tests). Note that, in practice, no corner case happens on the specific input dataset provided.
Each line is computed in parallel using multiple threads. I assume the array is not too small (to avoid to deal with more corner cases in the code and make it simpler) which should not be an issue. I also assume there are no special values like NaN in z_coordinates.

Resulting code
Here is the final code:
import numba as nb
import numpy as np

# Fill the missing values in the value-array if there is not enough values (e.g. pretty large z_size)
# (untested)
@nb.njit('(float64[::1], uint16[::1], int64)')
def fill_missing_values(all_val, all_pos, z_size):
    i = 0
    while i &lt; z_size:
        # If there is a missing value
        if all_pos[i] == 0xFFFF:
            j = i
            while j &lt; z_size and all_pos[j] == 0xFFFF:
                j += 1
            if i == 0:
                # Fill the hole based on 1 value (lower bound)
                assert j+1 &lt; z_size and all_pos[j] == 0xFFFF and all_pos[j] != 0xFFFF
                for i2 in range(i, j):
                    all_val[i2] = all_val[j+1]
                    all_pos[i2] = all_pos[j+1]
            elif j == z_size:
                # Fill the hole based on 1 value (upper bound)
                assert i-1 &gt;= 0 and all_pos[i-1] != 0xFFFF and all_pos[i] == 0xFFFF
                for i2 in range(i, j):
                    all_val[i2] = all_val[i-1]
                    all_pos[i2] = all_pos[i-1]
            else:
                assert i-1 &gt;= 0 and j &lt; z_size and all_pos[i-1] != 0xFFFF and all_pos[j] != 0xFFFF
                lower_val = all_val[i-1]
                lower_pos = all_pos[i-1]
                upper_val = all_val[j]
                upper_pos = all_pos[j]
                # Fill the hole based on 2 values
                for i2 in range(i, j):
                    if np.abs(lower_val - i2) &lt; np.abs(upper_val - i2):
                        all_val[i2] = lower_val
                        all_pos[i2] = lower_pos
                    else:
                        all_val[i2] = upper_val
                        all_pos[i2] = upper_pos
            i = j
        i += 1

# Correct values in very pathological cases where z_size is big so there are not enough 
# values added to the value-array causing some values of the value-array to be incorrect.
# The number of `while` iteration should be always &lt;= 3 in practice
@nb.njit('(float64[::1], uint16[::1], int64)')
def correct_values(all_val, all_pos, z_size):
    while True:
        stop = True
        for i in range(0, z_size-1):
            current = np.abs(all_val[i] - i)
            if np.abs(all_val[i+1] - i) &lt; current:
                all_val[i] = all_val[i+1]
                all_pos[i] = all_pos[i+1]
                stop = False
        for i in range(1, z_size):
            current = np.abs(all_val[i] - i)
            if np.abs(all_val[i-1] - i) &lt; current:
                all_val[i] = all_val[i-1]
                all_pos[i] = all_pos[i-1]
                stop = False
        if stop:
            break

@nb.njit('(float64[:,::1], int64)', parallel=True)
def compute_fastest(z_coordinates, z_size):
    x_size, y_size = z_coordinates.shape
    assert y_size &gt;= 2 and z_size &gt;= 2
    y_coordinates = np.empty((x_size, z_size), dtype=np.uint16)
    for x in nb.prange(x_size):
        all_pos = np.full(z_size, 0xFFFF, dtype=np.uint16)
        all_val = np.full(z_size, np.inf, dtype=np.float64)
        for y in range(0, y_size):
            val = z_coordinates[x, y]
            #assert not np.isnan(val)
            if val &lt; 0: # Lower bound
                i = 0
                if np.abs(val - i) &lt; np.abs(all_val[i] - i):
                    all_val[i] = val
                    all_pos[i] = y
            elif val &gt;= z_size: # Upper bound
                i = z_size - 1
                if np.abs(val - i) &lt; np.abs(all_val[i] - i):
                    all_val[i] = val
                    all_pos[i] = y
            else: # Inside the array of values
                offset = np.int32(val)
                for i in range(max(offset-1, 0), min(offset+2, z_size)):
                    if np.abs(val - i) &lt; np.abs(all_val[i] - i):
                        all_val[i] = val
                        all_pos[i] = y
        fill_missing_values(all_val, all_pos, z_size)
        correct_values(all_val, all_pos, z_size)
        for i in range(0, z_size):
            y_coordinates[x, i] = all_pos[i]
    return y_coordinates


Performance results
Here are performance results on my machine with a i5-9600KF CPU (6 cores), Numpy 1.24.3, Numba 58.1, on Windows, for the provided input:
Naive fully vectorized code in the question:   113000 ms  (slow due to swapping)
Naive loop in the question:                      8460 ms
ZLi's implementation:                            1964 ms
Naive Numba parallel code with loops:             402 ms
PaulS' implementation:                            262 ms
This Numba code:                                   12 ms  &lt;----------

Note the fully-vectorized code in the question use so much memory it cause memory swapping. It completely saturate my 32 GiB of RAM (about 24 GiB was available in practice) which is clearly not reasonable!
Note the PaulS' implementation is about equally fast with 32-bit and 64-bit on my machine. This is probably because the operation is compute-bound on my machine (dependent of the speed of the RAM).
This Numba implementation is 705 times faster than the fastest implementation in the question. It is also 22 times faster than the best answer so far! It also use a tiny amount of additional RAM for the computation (&lt;1 MiB).
"
"I just want to get class data in my python script like: person, car, truck, dog  but my output more than this. Also I can not use results as a string.
Python script:
from ultralytics import YOLO

model = YOLO(&quot;yolov8n.pt&quot;) 
results = model.predict(source=&quot;0&quot;)

Output:
0: 480x640 1 person, 1 car, 7.1ms
0: 480x640 1 person, 1 car, 7.2ms
0: 480x640 1 person, 1 car, 7.1ms
0: 480x640 1 person, 1 car, 7.1ms
0: 480x640 1 person, 1 car, 7.1ms
0: 480x640 1 person, 7.9ms
0: 480x640 1 person, 7.1ms
0: 480x640 1 person, 1 car, 7.1ms
0: 480x640 1 person, 1 car, 7.1ms

","You can pass each class to the model's name dict like this:
from ultralytics.yolo.engine.model import YOLO
  
model = YOLO(&quot;yolov8n.pt&quot;)
results = model.predict(stream=True, imgsz=512) # source already setup
names = model.names

for r in results:
    for c in r.boxes.cls:
        print(names[int(c)])

output:
YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs
bus
person
person
person
person
image 1/2 /home/xyz/ultralytics/ultralytics/assets/bus.jpg: 512x384 4 persons, 1 bus, 35.7ms
person
person
person
tie
tie
image 2/2 /home/xyz/ultralytics/ultralytics/assets/zidane.jpg: 288x512 3 persons, 2 ties, 199.0ms
Speed: 3.9ms pre-process, 117.4ms inference, 27.9ms postprocess per image at shape (1, 3, 512, 512)

"
"I would like to generate a list of combinations. I will try to simplify my problem to make it understandable.
We have 3 variables :

x : number of letters
k : number of groups
n : number of letters per group

I would like to generate using python a list of every possible combinations, without any duplicate knowing that : i don't care about the order of the groups and the order of the letters within a group.
As an example, with x = 4, k = 2, n = 2 :
# we start with 4 letters, we want to make 2 groups of 2 letters
letters = ['A','B','C','D']

# here would be a code that generate the list

# Here is the result that is very simple, only 3 combinations exist.
combos = [ ['AB', 'CD'], ['AC', 'BD'], ['AD', 'BC'] ]

Since I don't care about the order of or within the groups, and letters within a group, ['AB', 'CD'] and ['DC', 'BA'] is a duplicate.
This is a simplification of my real problem, which has those values : x = 12, k = 4, n = 3. I tried to use some functions from itertools, but with that many letters my computer freezes because it's too many combinations.
Another way of seeing the problem : you have 12 players, you want to make 4 teams of 3 players. What are all the possibilities ?
Could anyone help me to find an optimized solution to generate this list?
","There will certainly be more sophisticated/efficient ways of doing this, but here's an approach that works in a reasonable amount of time for your example and should be easy enough to adapt for other cases.
It generates unique teams and unique combinations thereof, as per your specifications.
from itertools import combinations

# this assumes that team_size * team_num == len(players) is a given
team_size = 3
team_num = 4
players = list('ABCDEFGHIJKL')
unique_teams = [set(c) for c in combinations(players, team_size)]

def duplicate_player(combo):
    &quot;&quot;&quot;Returns True if a player occurs in more than one team&quot;&quot;&quot;
    return len(set.union(*combo)) &lt; len(players)
    
result = (combo for combo in combinations(unique_teams, team_num) if not duplicate_player(combo))

result is a generator that can be iterated or turned into a list with list(result). On kaggle.com, it takes a minute or so to generate the whole list of all possible combinations (a total of 15400, in line with the computations by @beaker and @John Coleman in the comments). The teams are tuples of sets that look like this:
[({'A', 'B', 'C'}, {'D', 'E', 'F'}, {'G', 'H', 'I'}, {'J', 'K', 'L'}),
 ({'A', 'B', 'C'}, {'D', 'E', 'F'}, {'G', 'H', 'J'}, {'I', 'K', 'L'}),
 ({'A', 'B', 'C'}, {'D', 'E', 'F'}, {'G', 'H', 'K'}, {'I', 'J', 'L'}),
 ...
]

If you want, you can cast them into strings by calling ''.join() on each of them.
"
"I need to check if object is descendant of typing.Literal, I have annotation like this:
GameState: Literal['start', 'stop']

And I need to check GameState annotation type:
def parse_values(ann)
   if isinstance(ann, str):
       # do sth
   if isinstance(ann, int):
       # do sth
   if isinstance(ann, Literal):
       # do sth

But it causes error, so I swapped the last one to:
if type(ann) == Literal:
   # do sth

But it never returns True, so anyone knows a workaround for this?
","typing.get_origin(tp) is the proper way
It was implemented in Python 3.8 (Same as typing.Literal)
The docstring is thoroughly instructive:
def get_origin(tp):
    &quot;&quot;&quot;Get the unsubscripted version of a type.

    This supports generic types, Callable, Tuple, Union, Literal, Final, ClassVar
    and Annotated. Return None for unsupported types. Examples::

        get_origin(Literal[42]) is Literal
        get_origin(int) is None
        get_origin(ClassVar[int]) is ClassVar
        get_origin(Generic) is Generic
        get_origin(Generic[T]) is Generic
        get_origin(Union[T, int]) is Union
        get_origin(List[Tuple[T, T]][int]) == list
    &quot;&quot;&quot;

In your use case it would be:
from typing import Literal, get_origin

def parse_values(ann):
    if isinstance(ann, str):
        return &quot;str&quot;
    elif isinstance(ann, int):
        return &quot;int&quot;
    elif get_origin(ann) is Literal:
        return &quot;Literal&quot;

assert parse_values(&quot;foo&quot;) == &quot;str&quot;
assert parse_values(5) == &quot;int&quot;
assert parse_values(Literal[&quot;bar&quot;, 6]) == &quot;Literal&quot;

"
"I am working with a Polars DataFrame and need to perform computations on each row using values from other rows. Currently, I am using the map_elements method, but it is not efficient.
In the following example, I add two new columns to a DataFrame:

sum_lower: The sum of all elements that are smaller than the current element.
max_other: The maximum value from the DataFrame, excluding the current element.

Here is my current implementation:
import polars as pl

COL_VALUE = &quot;value&quot;

def fun_sum_lower(current_row, df):
    tmp_df = df.filter(pl.col(COL_VALUE) &lt; current_row[COL_VALUE])
    sum_lower = tmp_df.select(pl.sum(COL_VALUE)).item()
    return sum_lower

def fun_max_other(current_row, df):
    tmp_df = df.filter(pl.col(COL_VALUE) != current_row[COL_VALUE])
    max_other = tmp_df.select(pl.col(COL_VALUE)).max().item()
    return max_other

if __name__ == '__main__':
    df = pl.DataFrame({COL_VALUE: [3, 7, 1, 9, 4]})

    df = df.with_columns(
        pl.struct([COL_VALUE])
        .map_elements(lambda row: fun_sum_lower(row, df), return_dtype=pl.Int64)
        .alias(&quot;sum_lower&quot;)
    )

    df = df.with_columns(
        pl.struct([COL_VALUE])
        .map_elements(lambda row: fun_max_other(row, df), return_dtype=pl.Int64)
        .alias(&quot;max_other&quot;)
    )

    print(df)

The output of the above code is:
shape: (5, 3)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ value â”† sum_lower â”† max_other â”‚
â”‚ ---   â”† ---       â”† ---       â”‚
â”‚ i64   â”† i64       â”† i64       â”‚
â•žâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ 3     â”† 1         â”† 9         â”‚
â”‚ 7     â”† 8         â”† 9         â”‚
â”‚ 1     â”† 0         â”† 9         â”‚
â”‚ 9     â”† 15        â”† 7         â”‚
â”‚ 4     â”† 4         â”† 9         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

While this code works, it is not efficient due to the use of lambdas and row-wise operations.
Is there a more efficient way to achieve this in Polars, without using lambdas, iterating over rows, or running Python code?
I also tried using Polars methods: cum_sum, group_by_dynamic, and rolling, but I don't think those can be used for this task.
","For your specific use case you don't really need join, you can calculate values with window functions.

pl.Expr.shift() to exclude current row.
pl.Expr.cum_sum() to calculate sum of all elements up to the current row.
pl.Expr.max() to calculate max.
pl.Expr.bottom_k() to calculate 2 largest elements so then we can take pl.Expr.min() as second largest.

(
    df
    .sort(&quot;value&quot;)
    .with_columns(
        sum_lower = pl.col.value.shift(1).cum_sum().fill_null(0),
        max_other =
        pl.when(pl.col.value.max() != pl.col.value)
        .then(pl.col.value.max())
        .otherwise(pl.col.value.bottom_k(2).min()) 
    )
)

shape: (5, 3)
┌───────┬───────────┬───────────┐
│ value ┆ sum_lower ┆ max_other │
│ ---   ┆ ---       ┆ ---       │
│ i64   ┆ i64       ┆ i64       │
╞═══════╪═══════════╪═══════════╡
│ 1     ┆ 0         ┆ 9         │
│ 3     ┆ 1         ┆ 9         │
│ 4     ┆ 4         ┆ 9         │
│ 7     ┆ 8         ┆ 9         │
│ 9     ┆ 15        ┆ 7         │
└───────┴───────────┴───────────┘

You can also use pl.DataFrame.with_row_index() to keep current order so you can revert to it at the end with pl.DataFrame.sort().
(
    df.with_row_index()
    .sort(&quot;value&quot;)
    .with_columns(
        sum_lower = pl.col.value.shift(1).cum_sum().fill_null(0),
        max_other =
        pl.when(pl.col.value.max() != pl.col.value)
        .then(pl.col.value.max())
        .otherwise(pl.col.value.bottom_k(2).min()) 
    )
    .sort(&quot;index&quot;)
    .drop(&quot;index&quot;)
)

Another possible solution would be to use DuckDB integration with Polars.
Using window functions, getting advantage of excellent DuckDB windows framing options.

max(arg, n) to calculate top 2 largest elements.

import duckdb

duckdb.sql(&quot;&quot;&quot;
    select
        d.value,
        coalesce(sum(d.value) over(
            order by d.value
            rows unbounded preceding
            exclude current row
        ), 0) as sum_lower,
        max(d.value) over(
            rows between unbounded preceding and unbounded following
            exclude current row
        ) as max_other
    from df as d
&quot;&quot;&quot;).pl()

shape: (5, 3)
┌───────┬───────────────┬───────────┐
│ value ┆ sum_lower     ┆ max_other │
│ ---   ┆ ---           ┆ ---       │
│ i64   ┆ decimal[38,0] ┆ i64       │
╞═══════╪═══════════════╪═══════════╡
│ 1     ┆ 0             ┆ 9         │
│ 3     ┆ 1             ┆ 9         │
│ 4     ┆ 4             ┆ 9         │
│ 7     ┆ 8             ┆ 9         │
│ 9     ┆ 15            ┆ 7         │
└───────┴───────────────┴───────────┘

Or using lateral join:
import duckdb

duckdb.sql(&quot;&quot;&quot;
    select
        d.value,
        coalesce(s.value, 0) as sum_lower,
        m.value as max_other
    from df as d,
        lateral (select sum(t.value) as value from df as t where t.value &lt; d.value) as s,
        lateral (select max(t.value) as value from df as t where t.value != d.value) as m
&quot;&quot;&quot;).pl()

shape: (5, 3)
┌───────┬───────────┬───────────┐
│ value ┆ sum_lower ┆ max_other │
│ ---   ┆ ---       ┆ ---       │
│ i64   ┆ i64       ┆ i64       │
╞═══════╪═══════════╪═══════════╡
│ 3     ┆ 1         ┆ 9         │
│ 7     ┆ 8         ┆ 9         │
│ 1     ┆ 0         ┆ 9         │
│ 9     ┆ 15        ┆ 7         │
│ 4     ┆ 4         ┆ 9         │
└───────┴───────────┴───────────┘

duplicate values
pure polars solution above works well if there're no duplicate values, but if there are, you can also work around it.
Here're 2 examples depending on whether you want to keep original order or not:
# not keeping original order
(
    df
    .select(pl.col.value.value_counts()).unnest(&quot;value&quot;)
    .sort(&quot;value&quot;)
    .with_columns(
        sum_lower = pl.col.value.shift(1).cum_sum().fill_null(0),
        max_other =
        pl.when(pl.col.value.max() != pl.col.value)
        .then(pl.col.value.max())
        .otherwise(pl.col.value.bottom_k(2).min()),
        value = pl.col.value.repeat_by(&quot;count&quot;)
    ).drop(&quot;count&quot;).explode(&quot;value&quot;)
)

# keeping original order
(
    df.with_row_index()
    .group_by(&quot;value&quot;).agg(&quot;index&quot;)
    .sort(&quot;value&quot;)
    .with_columns(
        sum_lower = pl.col.value.shift(1).cum_sum().fill_null(0),
        max_other =
        pl.when(pl.col.value.max() != pl.col.value)
        .then(pl.col.value.max())
        .otherwise(pl.col.value.bottom_k(2).min()) 
    )
    .explode(&quot;index&quot;)
    .sort(&quot;index&quot;)
    .drop(&quot;index&quot;)
)

"
"I'm trying to find the algorithm efficiently solving this problem:

Given an unsorted array of numbers, you need to divide it into several subarrays of length from a to b, so that the sum of differences between the minimum and maximum numbers in each of the subarrays is the greatest. The order of the numbers must be preserved.
Examples:
a = 3, b = 7
input: [5, 8, 4, 5, 1, 3, 5, 1, 3, 1]
answer: [[5, 8, 4], [5, 1, 3], [5, 1, 3, 1]] (diff sum is 12)

a = 3, b = 4
input: [1, 6, 2, 2, 5, 2, 8, 1, 5, 6]
answer: [[1, 6, 2], [2, 5, 2, 8], [1, 5, 6]] (diff sum is 16)

a = 4, b = 5
input: [5, 8, 4, 5, 1, 3, 5, 1, 3, 1, 2]
answer: splitting is impossible


The only solution I've come up with so far is trying all of the possible subarray combinations.
from collections import deque

def partition_array(numbers, min_len, max_len):
  max_diff_subarray = None

  queue = deque()

  for end in range(min_len - 1, max_len):
    if end &lt; len(numbers):
      diff = max(numbers[0:end + 1]) - min(numbers[0:end + 1])
      queue.append(Subarray(previous=None, start=0, end=end, diff_sum=diff))

  while queue:
    subarray = queue.popleft()

    if subarray.end == len(numbers) - 1:
      if max_diff_subarray is None:
        max_diff_subarray = subarray
      elif max_diff_subarray.diff_sum &lt; subarray.diff_sum:
        max_diff_subarray = subarray
      continue

    start = subarray.end + 1

    for end in range(start + min_len - 1, start + max_len):
      if end &lt; len(numbers):
        diff = max(numbers[start:end + 1]) - min(numbers[start:end + 1])
        queue.append(Subarray(previous=subarray, start=start, end=end, diff_sum=subarray.diff_sum + diff))
      else:
        break

  return max_diff_subarray

class Subarray:
  def __init__(self, previous=None, start=0, end=0, diff_sum=0):
    self.previous = previous
    self.start = start
    self.end = end
    self.diff_sum = diff_sum

numbers = [5, 8, 4, 5, 1, 3, 5, 1, 3, 1]
a = 3
b = 7
result = partition_array(numbers, a, b)
print(result.diff_sum)

Are there any more time efficient solutions?
","First let's solve a simpler problem. Let's run through an array, and give mins and maxes for all windows of fixed size.
def window_mins_maxes (size, array):
    min_values = deque()
    min_positions = deque()
    max_values = deque()
    max_positions = deque()

    for i, value in enumerate(array):
        if size &lt;= i:
            yield (i, min_values[0], max_values[0])
            if min_positions[0] &lt;= i - size:
                min_values.popleft()
                min_positions.popleft()

            if max_positions[0] &lt;= i - size:
                max_values.popleft()
                max_positions.popleft()

        while 0 &lt; len(min_values) and value &lt;= min_values[-1]:
            min_values.pop()
            min_positions.pop()
        min_values.append(value)
        min_positions.append(i)

        while 0 &lt; len(max_values) and max_values[-1] &lt;= value:
            max_values.pop()
            max_positions.pop()
        max_values.append(value)
        max_positions.append(i)

    yield (len(array), min_values[0], max_values[0])

This clearly takes memory O(size). What's less obvious is that it takes time O(n) to process an array of length n. But we can see that with amortized analysis. To each element we'll attribute the cost of checking the possible value that is smaller than it, the cost of some later element checking that it should be removed, and the cost of being added. That accounts for all operations (though this isn't the order that they happen) and is a fixed amount of work per element.
Also note that the memory needed for this part of the solution fits within O(n).
So far I'd consider this a well-known dynamic programming problem. Now let's make it more challenging.
We will tackle the partition problem as a traditional dynamic programming problem. We'll build up an array best_weight of the best partition to that point, and prev_index of the start of the previous partition ending just before that point.
To build it up, we'll use the above algorithm to take a previous partition and add one of min_len to it. If it is better than the previous, we'll save its information in those arrays. We'll then scan forward from that partition and do that up to max_len. Then we move on to the next possible start of a partition.
When we're done we'll find the answer from that code.
Here is what that looks like:
def partition_array(numbers, min_len, max_len):
    if max_len &lt; min_len or len(numbers) &lt; min_len:
        return (None, None)

    best_weight = [None for _ in numbers]
    prev_index = [None for _ in numbers]

    # Need an extra entry for off of the end of the array.
    best_weight.append(None)
    prev_index.append(None)

    best_weight[0] = 0

    for i, min_value, max_value in window_mins_maxes(min_len, numbers):
        window_start_weight = best_weight[i - min_len]
        if window_start_weight is not None:
            j = i
            while j - i &lt; max_len - min_len and j &lt; len(numbers):
                new_weight = window_start_weight + max_value - min_value
                if best_weight[j] is None or best_weight[j] &lt; new_weight:
                    best_weight[j] = new_weight
                    prev_index[j] = i - min_len

                if numbers[j] &lt; min_value:
                    min_value = numbers[j]
                if max_value &lt; numbers[j]:
                    max_value = numbers[j]
                j += 1

            # And fill in the longest value.
            new_weight = window_start_weight + max_value - min_value
            if best_weight[j] is None or best_weight[j] &lt; new_weight:
                best_weight[j] = new_weight
                prev_index[j] = i - min_len

    if best_weight[-1] is None:
        return (None, None)
    else:
        path = [len(numbers)]
        while prev_index[path[-1]] is not None:
            path.append(prev_index[path[-1]])
        path = list(reversed(path))
        partitioned = [numbers[path[i]:path[i+1]] for i in range(len(path)-1)]
        return (best_weight[-1], partitioned)

Note that we do O(1) work for each possible start and length. And so that is time O((max_len + 1 - min_len)*n). And the data structures we used are all bounded above by O(n) in size. Giving the overall efficiency that I promised in the comments.
Now let's test it.
print(partition_array([5, 8, 4, 5, 1, 3, 5, 1, 3, 1], 3, 7))
print(partition_array([1, 6, 2, 2, 5, 2, 8, 1, 5, 6], 3, 4))
print(partition_array([5, 8, 4, 5, 1, 3, 5, 1, 3, 1, 2], 4, 5))

And the output is:
(12, [[5, 8, 4], [5, 1, 3], [5, 1, 3, 1]])
(16, [[1, 6, 2], [2, 5, 2, 8], [1, 5, 6]])
(None, None)

"
"I need to use pipeline in order to get the tokenization and inference from the distilbert-base-uncased-finetuned-sst-2-english model over my dataset.
My data is a list of sentences, for recreation purposes we can assume it is:
texts = [&quot;this is the first sentence&quot;, &quot;of my data.&quot;, &quot;In fact, thats not true,&quot;, &quot;but we are going to assume it&quot;, &quot;is&quot;]
Before using pipeline, I was getting the logits from the model outputs like this:
with torch.no_grad():
     logits = model(**tokenized_test).logits

Now I have to use pipeline, so this is the way I'm getting the model's output:
 selected_model = &quot;distilbert-base-uncased-finetuned-sst-2-english&quot;
 tokenizer = AutoTokenizer.from_pretrained(selected_model)
 model = AutoModelForSequenceClassification.from_pretrained(selected_model, num_labels=2)
 classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)
 print(classifier(text))

which gives me:
[{'label': 'POSITIVE', 'score': 0.9746173024177551}, {'label': 'NEGATIVE', 'score': 0.5020197629928589}, {'label': 'NEGATIVE', 'score': 0.9995120763778687}, {'label': 'NEGATIVE', 'score': 0.9802979826927185}, {'label': 'POSITIVE', 'score': 0.9274746775627136}]
And I cant get the 'logits' field anymore.
Is there a way to get the logits instead of the label and score? Would a custom pipeline be the best and/or easiest way to do it?
","When you use the default pipeline, the postprocess function will usually take the softmax, e.g.
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')
model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')


text = ['hello this is a test',
 'that transforms a list of sentences',
 'into a list of list of sentences',
 'in order to emulate, in this case, two batches of the same lenght',
 'to be tokenized by the hf tokenizer for the defined model']

classifier(text, batch_size=2, truncation=&quot;only_first&quot;)

[out]:
[{'label': 'NEGATIVE', 'score': 0.9379090666770935},
 {'label': 'POSITIVE', 'score': 0.9990271329879761},
 {'label': 'NEGATIVE', 'score': 0.9726701378822327},
 {'label': 'NEGATIVE', 'score': 0.9965035915374756},
 {'label': 'NEGATIVE', 'score': 0.9913086891174316}]

So what you want is to overload the postprocess logic by inheriting from the pipeline.
To check which pipeline the classifier inherits do this:
classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)
type(classifier)

[out]:
transformers.pipelines.text_classification.TextClassificationPipeline

Now that you know the parent class of the task pipeline you want to use, now you can do this and still enjoy the perks of the precoded batching from TextClassificationPipeline:
from transformers import TextClassificationPipeline

class MarioThePlumber(TextClassificationPipeline):
    def postprocess(self, model_outputs):
        best_class = model_outputs[&quot;logits&quot;]
        return best_class

pipe = MarioThePlumber(model=model, tokenizer=tokenizer)

pipe(text, batch_size=2, truncation=&quot;only_first&quot;)

[out]:
[tensor([[ 1.5094, -1.2056]]),
 tensor([[-3.4114,  3.5229]]),
 tensor([[ 1.8835, -1.6886]]),
 tensor([[ 3.0780, -2.5745]]),
 tensor([[ 2.5383, -2.1984]])]

"
"I have found very easy and useful to load world map from geopandas datasets, as probably many others, for example:
import geopandas as gpd
world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))
However, this gives a FutureWarning that dataset module is deprecated and will be removed in the future. There are maps available for download, for example from https://www.naturalearthdata.com/downloads/110m-cultural-vectors/ but the files are zipped and it does not seem like a convinient workflow to either get and process files from there or neither include processed files with the source.
Is there an alternative? What is the best way to do this, especially if I want my code to work with future versions of Geopandas?
","You can read it from Nacis :
import geopandas as gpd

url = &quot;https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_0_countries.zip&quot;

gdf = gpd.read_file(url)


old answer:
The simplest solution would be to download/store the shapefile somewhere.
That being said, if (for some reason), you need to read it from the source, you can do it this way :
import fsspec

url = &quot;https://www.naturalearthdata.com/http//www.naturalearthdata.com/&quot; \
      &quot;download/110m/cultural/ne_110m_admin_0_countries.zip&quot;
    
with fsspec.open(f&quot;simplecache::{url}&quot;) as file:
    gdf = gpd.read_file(file)

Output :
          featurecla  scalerank  ...     FCLASS_UA                            geometry
0    Admin-0 country          1  ...          None  MULTIPOLYGON (((180.00000 -16.0...
1    Admin-0 country          1  ...          None  POLYGON ((33.90371 -0.95000, 34...
2    Admin-0 country          1  ...          None  POLYGON ((-8.66559 27.65643, -8...
..               ...        ...  ...           ...                                 ...
174  Admin-0 country          1  ...  Unrecognized  POLYGON ((20.59025 41.85541, 20...
175  Admin-0 country          1  ...          None  POLYGON ((-61.68000 10.76000, -...
176  Admin-0 country          1  ...          None  POLYGON ((30.83385 3.50917, 29....

[177 rows x 169 columns]


"
"Consider a FastAPI using the lifespan parameter like this:
def lifespan(app):
    print('lifespan start')
    yield
    print('lifespan end')


app = FastAPI(lifespan=lifespan)

Now I want to register a sub app with its own lifecycle functions:
app.mount(mount_path, sub_app)

How can I register startup/shutdown handlers for the sub app?
All solutions I could find either require control over the lifespan generator (which I don't have) or involve deprecated methods like add_event_handler (which doesn't work when lifespan is set).

Update Minimal reproducible example:
from fastapi import FastAPI

# --- main app ---

def lifespan(_):
    print(&quot;startup&quot;)
    yield
    print(&quot;shutdown&quot;)

app = FastAPI(lifespan=lifespan)

@app.get(&quot;/&quot;)
async def root():
    return {&quot;message&quot;: &quot;Hello World&quot;}

# --- sub app ---

sub_app = FastAPI()

@sub_app.get(&quot;/&quot;)
async def sub_root():
    return {&quot;message&quot;: &quot;Hello Sub World&quot;}

app.mount(&quot;/sub&quot;, sub_app)
app.on_event(&quot;startup&quot;)(lambda: print(&quot;sub startup&quot;))    # doesn't work
app.on_event(&quot;shutdown&quot;)(lambda: print(&quot;sub shutdown&quot;))  # doesn't work

Run with: uvicorn my_app:app --port 8000
","I found a solution, but I'm not sure if I like it... It accesses the existing lifespan generator via app.router.lifespan_context and wraps it with additional startup/shutdown commands:
from contextlib import asynccontextmanager

...

main_app_lifespan = app.router.lifespan_context

@asynccontextmanager
async def lifespan_wrapper(app):
    print(&quot;sub startup&quot;)
    async with main_app_lifespan(app) as maybe_state:
        yield maybe_state
    print(&quot;sub shutdown&quot;)

app.router.lifespan_context = lifespan_wrapper

Output:
INFO:     Waiting for application startup.
sub startup
startup
INFO:     Application startup complete.
...
INFO:     Shutting down
INFO:     Waiting for application shutdown.
shutdown
sub shutdown
INFO:     Application shutdown complete.

"
"Im trying to accept data from an API and then validate the response structure with a Pydantic base model. However, I have the case where sometimes some fields will not come included in the response, while sometimes they do. The problem is, when I try to validate the structure, Pydantic starts complaining about those fields being &quot;missing&quot; even though they can be missing sometimes. I really don't understand how to define a field as &quot;missible&quot;. The docs mention that a field that is just defined as a name and a type is considered this way, but I haven't had any luck
This is a simple example of what I'm trying to accomplish
# Response: {a: 1, b: &quot;abc&quot;, c: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;]}
response: dict = json.loads(request_response)

# Pydantic Base Model
from pydantic import BaseModel
class Model(BaseModel):
   a: int
   b: str
   c: List[str]
   d: float

# Validating 
Model(**response)

# Return: ValidationError - Missing &quot;d&quot; field


How do I make it so that &quot;d&quot; doesnt cause the validation to throw an error? I have tried to switch &quot;d&quot; to d: Optional[float] and d: Optional[float] = 0.0, but nothing works.
Thanks!
","Pydantic v2
Either a model has a field or it does not. In a sense, a field is always required to have a value on a fully initialized model instance. It is just that a field may have a default value that will be assigned to it, if no value was explicitly provided during initialization. (see Basic Model Usage in the docs)
The question for you is ultimately: What value should be assigned to field d, if it is not set explicitly? Should it be None or should it be some default float value (like e.g. 0.) or something else? Whatever you choose, you must specify that default value in the model definition and remember to annotate the field with the correct type.
If you choose a default float like 0. for instance, your type remains the same and you just define the field as d: float = 0.. If you want the default to be of a different type like None, you will need to change the definition to d: float | None = None.
For the sake of completeness, you may also define a default factory instead of a static value to have the actual value be calculated during initialization.
Here is a short demo:
from pydantic import BaseModel, Field, ValidationError


class Model(BaseModel):
    a: int
    b: str
    c: list[str]
    d: float | None = None  # equivalent: `d: typing.Optional[float] = None`
    e: float = 0.
    f: float = Field(default_factory=lambda: 420.69)


if __name__ == '__main__':
    instance = Model.model_validate({
        &quot;a&quot;: 1,
        &quot;b&quot;: &quot;abc&quot;,
        &quot;c&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],
    })
    print(instance.model_dump_json(indent=4))

    try:
        Model.model_validate({
            &quot;a&quot;: 1,
            &quot;b&quot;: &quot;abc&quot;,
            &quot;c&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],
            &quot;d&quot;: None,  # fine
            &quot;e&quot;: None,  # error
            &quot;f&quot;: None,  # error
        })
    except ValidationError as e:
        print(e.json(indent=4))

Output:
{
    &quot;a&quot;: 1,
    &quot;b&quot;: &quot;abc&quot;,
    &quot;c&quot;: [
        &quot;a&quot;,
        &quot;b&quot;,
        &quot;c&quot;
    ],
    &quot;d&quot;: null,
    &quot;e&quot;: 0.0,
    &quot;f&quot;: 420.69
}

[
    {
        &quot;type&quot;: &quot;float_type&quot;,
        &quot;loc&quot;: [
            &quot;e&quot;
        ],
        &quot;msg&quot;: &quot;Input should be a valid number&quot;,
        &quot;input&quot;: null,
        &quot;url&quot;: &quot;https://errors.pydantic.dev/2.7/v/float_type&quot;
    },
    {
        &quot;type&quot;: &quot;float_type&quot;,
        &quot;loc&quot;: [
            &quot;f&quot;
        ],
        &quot;msg&quot;: &quot;Input should be a valid number&quot;,
        &quot;input&quot;: null,
        &quot;url&quot;: &quot;https://errors.pydantic.dev/2.7/v/float_type&quot;
    }
]


[Old answer] Pydantic v1
As @python_user said, both your suggestions work.
Admittedly, the behavior of typing.Optional for Pydantic fields is poorly documented. Perhaps because it is assumed to be obvious. I personally don't find it obvious because Optional[T] is just equivalent to Union[T, None] (or T | None in the new notation).
Annotating a field with any other union of types, while omitting a default value will result in the field being required. But if you annotate with a union that includes None, the field automatically receives the None default value. Kind of inconsistent, but that is the way it is.

Update (2024-04-20): As @Michael mentioned in the comments, with the release of Pydantic v2, the maintainers addressed this exact inconsistency (and arguably &quot;fixed&quot; it). The change is explained in the documentation section on Required Fields. Quote: (emphasis mine)

In Pydantic V1, fields annotated with Optional or Any would be given an implicit default of None even if no default was explicitly specified. This behavior has changed in Pydantic V2, and there are no longer any type annotations that will result in a field having an implicit default value.

See my Pydantic v2 answer above for an example.

However, the question is ultimately what you want your model fields to be. What value should be assigned to field d, if it is not set explicitly? Should it be None or should it be some default float value? (like e.g. 0.) If None is fine, then you can use Optional[float] or float | None, and you don't need to specify the default value. (Specifying Optional[float] = None is equivalent.) If you want any other default value, you'll need to specify it accordingly, e.g. d: float = 0.0; but in that case None would be an invalid value for that field.
from pydantic import BaseModel, ValidationError


class Model(BaseModel):
    a: int
    b: str
    c: list[str]
    d: float | None  # or typing.Optional[float]
    e: float = 0.


if __name__ == '__main__':
    print(Model.parse_obj({
        &quot;a&quot;: 1,
        &quot;b&quot;: &quot;abc&quot;,
        &quot;c&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],
    }), &quot;\n&quot;)
    try:
        Model.parse_obj({
            &quot;a&quot;: 1,
            &quot;b&quot;: &quot;abc&quot;,
            &quot;c&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],
            &quot;e&quot;: None,
        })
    except ValidationError as e:
        print(e)

Output:

a=1 b='abc' c=['a', 'b', 'c'] d=None e=0.0 

1 validation error for Model
e
  none is not an allowed value (type=type_error.none.not_allowed)

"
"I am learning FastAPI and I have this example.
from fastapi import FastAPI

app = FastAPI()

@app.get(&quot;/&quot;)
async def root():
    return {&quot;message&quot;: &quot;Hello World&quot;}

I saved the script as main.ipynb
The tutorial says to run this line of code in the command line: uvicorn main:app --reload
I am getting this error:
(venv) PS C:\Users\xxx\xxxx&gt; uvicorn main:app --reload
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [21304] using WatchFiles
ERROR:    Error loadinimport module &quot;main&quot;.INFO:     Stopping reloader process [21304]

The reason is because I am using .ipynb as opposed to .py.
How can i fix this error while using .ipynb.
Thanks so much
","If you attempted to start the server as usual inside Jupyter, for example:
import uvicorn

if __name__ == &quot;__main__&quot;:
    uvicorn.run(app)

you would get the following error:
RuntimeError: asyncio.run() cannot be called from a running event loop

This is due to Jupyter already running an event loop, and once Uvicorn calls asyncio.run() internally, the above error is raised.
As per asyncio.run() documentation:

This function cannot be called when another asyncio event loop is
running in the same thread (see relevant asyncio implementation, where the error is raised).
[...]
This function always creates a new event loop and closes it at the
end. It should be used as a main entry point for asyncio programs, and
should ideally only be called once.

Solution 1
If you wouldd like to run uvicorn from an already running async environment, use uvicorn.Server.serve() instead (you could add the below to a new code cell in your Jupyter notebook, and then run it):
import asyncio
import uvicorn

if __name__ == &quot;__main__&quot;:
    config = uvicorn.Config(app)
    server = uvicorn.Server(config)
    await server.serve()

or, get the current (running) event loop, using asyncio.get_running_loop(), and then call loop.create_task() for creating a task to run inside the event loop for the current thread:
import asyncio
import uvicorn

if __name__ == &quot;__main__&quot;:
    config = uvicorn.Config(app)
    server = uvicorn.Server(config)
    loop = asyncio.get_running_loop()
    loop.create_task(server.serve())

Solution 2
Alternatively, you can use nest_asyncio, which allows nested use of asyncio.run() and loop.run_until_complete():
import nest_asyncio
import uvicorn

if __name__ == &quot;__main__&quot;:
    nest_asyncio.apply()
    uvicorn.run(app)

"
"I am currently working on adding type hints to a project and can't figure out how to get this right. I have a list of lists, with the nested list containing two elements of type int and float. The first element of the nested list is always an int and the second is always a float.
my_list = [[1000, 5.5], [1432, 2.2], [1234, 0.3]]

I would like to type annotate it so that unpacking the inner list in for loops or loop comprehensions keeps the type information. I could change the inner lists to tuples and would get what I'm looking for:
def some_function(list_arg: list[tuple[int, float]]): pass


However, I need the inner lists to be mutable. Is there a nice way to do this for lists? I know that abstract classes like Sequence and Collection do not support multiple types.
","I think the question highlights a fundamental difference between statically typed Python and dynamically typed Python. For someone who is used to dynamically typed Python (or Perl or JavaScript or any number of other scripting languages), it's perfectly normal to have diverse data types in a list. It's convenient, flexible, and doesn't require you to define custom data types. However, when you introduce static typing, you step into a tighter box that requires more rigorous design.
As several others have already pointed out, type annotations for lists require all elements of the list to be the same type, and don't allow you to specify a length. Rather than viewing this as a shortcoming of the type system, you should consider that the flaw is in your own design. What you are really looking for is a class with two data members. The first data member is named 0, and has type int, and the second is named 1, and has type float. As your friend, I would recommend that you define a proper class, with meaningful names for these data members. As I'm not sure what your data type represents, I'll make up names, for illustration.
class Sample:
    def __init__(self, atomCount: int, atomicMass: float):
        self.atomCount = atomCount
        self.atomicMass = atomicMass

This not only solves the typing problem, but also gives a major boost to readability. Your code would now look more like this:
my_list = [Sample(1000, 5.5), Sample(1432, 2.2), Sample(1234, 0.3)]

def some_function(list_arg: list[Sample]): pass

I do think it's worth highlighting Stef's comment, which points to this question. The answers given highlight two useful features related to this.
First, as of Python 3.7, you can mark a class as a data class, which will automatically generate methods like __init__(). The Sample class would look like this, using the @dataclass decorator:
from dataclasses import dataclass

@dataclass
class Sample:
    atomCount: int
    atomicMass: float

Another answer to that question mentions a PyPi package called recordclass, which it says is basically a mutable namedtuple. The typed version is called RecordClass
from recordclass import RecordClass

class Sample(RecordClass):
    atomCount: int
    atomicMass: float

"
"I was updated my Pandas from I think it was 1.5.1 to 2.0.1. Any how I started getting an error on some code that works just fine before.
df = df.groupby(df['date'].dt.date)['Lake', 'Canyon'].mean().reset_index()


Traceback (most recent call last):   File &quot;f:...\My_python_file.py&quot;, line 37, in

df = df.groupby(df['date'].dt.date)['Lake', 'Canyon'].mean().reset_index()   File
&quot;C:\Users...\Local\Programs\Python\Python310\lib\site-packages\pandas\core\groupby\generic.py&quot;,
line 1767, in getitem
raise ValueError( ValueError: Cannot subset columns with a tuple with more than one element. Use a list instead.

","Versions before Pandas &lt; 2.0.0 raises a FutureWarning if you don't use double brackets to select multiple columns

FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead

From Pandas &gt;= 2.0.0, it raises a ValueError:

ValueError: Cannot subset columns with a tuple with more than one element. Use a list instead.

For example:
# Pandas &lt; 2.0.0
#             Missing [[ ... ]] --v              --v
&gt;&gt;&gt; df.groupby(df['date'].dt.date)['Lake', 'Canyon'].mean().reset_index()
...
FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.
  df.groupby(df['date'].dt.date)['Lake', 'Canyon'].mean().reset_index()

# Pandas &gt;= 2.0.0
&gt;&gt;&gt; df.groupby(df['date'].dt.date)['Lake', 'Canyon'].mean().reset_index()
...
ValueError: Cannot subset columns with a tuple with more than one element. Use a list instead.

Fix this using [[col1, col2, ...]]:
&gt;&gt;&gt; df.groupby(df['date'].dt.date)[['Lake', 'Canyon']].mean().reset_index()
         date  Lake  Canyon
0  2023-05-02   1.5     3.5

Minimal Reproducible Example:
import pandas as pd

df = pd.DataFrame({'date': ['2023-05-02 12:34:56', '2023-05-02 12:32:12'], 
                   'Lake': [1, 2], 'Canyon': [3, 4]})
df['date'] = pd.to_datetime(df['date'])
print(df)

# Output
                 date  Lake  Canyon
0 2023-05-02 12:34:56     1       3
1 2023-05-02 12:32:12     2       4

"
"Why doesn't FastAPI return the cookie to my frontend, which is a React app?
Here is my code:
@router.post(&quot;/login&quot;)
def user_login(response: Response,username :str = Form(),password :str = Form(),db: Session = Depends(get_db)):
    user = db.query(models.User).filter(models.User.mobile_number==username).first()
    if not user:
        raise HTTPException(400, detail='wrong phone number or password')
    if not verify_password(password, user.password):
        raise HTTPException(400, detail='wrong phone number or password')
    
   
    access_token = create_access_token(data={&quot;sub&quot;: user.mobile_number})
    response.set_cookie(key=&quot;fakesession&quot;, value=&quot;fake-cookie-session-value&quot;) #here I am set cookie 
    return {&quot;status&quot;:&quot;success&quot;}  

When I login from Swagger UI autodocs, I can see the cookie in the response headers using DevTools on Chrome browser. However, when I login from my React app, no cookie is returned. I am using axios to send the request like this:
await axios.post(login_url, formdata)
","First, create the cookie, as shown in the example below, and make sure there is no error returned when performing the Axios POST request, and that you get a 'status': 'success' response with 200 status code. You may want to have a look at this answer as well, which provides explains how to use the max_age and expires flags too.
from fastapi import FastAPI, Response

app = FastAPI()

@app.get('/')
def main(response: Response):
    response.set_cookie(key='token', value='some-token-value', httponly=True) 
    return {'status': 'success'}

Second, as you mentioned that you are using React in the frontend—which needs to be listening on a different port from the one used for the FastAPI backend, meaning that you are performing CORS requests—you need to set the withCredentials property to true (by default this is set to false), in order to allow receiving/sending credentials, such as cookies and HTTP authentication headers, from/to other origins.
Thus, to accept cookies sent by the server, you need to use withCredentials: true in your Axios request; otherwise, the cookies will be ignored in the response (which is the default behaviour, when withCredentials is set to false; hence, preventing different domains from setting cookies for their own domain). The same withCredentials: true property has to be included in  every subsequent request to your API, if you would like the cookie to be sent to the server, so that the user can be authenticated and provided with access to protected routes.
Hence, an Axios request that includes credentials should look like this:
await axios.post(url, data, {withCredentials: true}))

The equivalent in a fetch() request (i.e., using Fetch API) is credentials: 'include'. The default value for credentials is same-origin. Using credentials: 'include' will cause the browser to include credentials in both same-origin and cross-origin requests, as well as set any cookies sent back by cross-origin responses. For instance:
fetch('https://example.com', {
  credentials: 'include'
});

Note that two servers with same domain and protocol, but different port numbers , e.g., http://localhost:8000 and http://localhost:3000 are considered different origins (see FastAPI documentation on CORS and details later on how to enable CORS in your FastAPI backend in such cases). You may also have a look at this answer, which provides details around HTTP cookies in general, as well as solutions for setting cross-site cookies—which you don't actually need in your case, as the domain name is the same for both the backend and frontend (regardless of the port numbers that differ, as cookies don't provide isolation by port), and hence, setting the cookie as usual should work just fine.
As you may have already understood, there is a distinction between same-site/cross-site and same-origin/cross-origin requests. In summary, two URLs are considered to be same-site, if they have the same scheme (e.g., http or https) and the same domain (e.g., localhost, 127.0.0.1 or example.com). They don't need to have the same port or subdomain. On the other hand, two URLs are considered to be same-origin, if they have the same scheme, domain, subdomain and port number as well.
Note that if you are accessing your React frontend by typing http://localhost:3000 in the address bar of your browser, then your Axios requests to FastAPI backend should use the localhost domain in the URL, for instance:
axios.post('http://localhost:8000',...

and not 127.0.0.1, such as:
axios.post('http://127.0.0.1:8000',...

as localhost and 127.0.0.1 are two different domains, and hence, the cookie would otherwise fail to be created for the localhost domain, as it would be created for 127.0.0.1, i.e., the domain used in the axios request (and then, that would be a case for cross-site cookies, as described in the linked answer above, which again, in your case, would not be needed). Similarly, if you are accessing the frontend page in your browser at http://127.0.0.1:3000, your JS requests should use 127.0.0.1; for instance, axios.post('http://127.0.0.1:8000',....
Enabling CORS in FastAPI
Since you are performing a cross-origin request, you would need to explicitly specify the allowed origins, as described in this answer (behind the scenes, that is setting the Access-Control-Allow-Origin response header). For instance:
origins = ['http://localhost:3000', 'http://127.0.0.1:3000',
           'https://localhost:3000', 'https://127.0.0.1:3000'] 

Using the * wildcard instead would mean that all origins are allowed; however, that would also only allow certain types of communication, excluding everything that involves credentials, such as cookies, authorization headers, etc.—hence, you should not use the * wildcard, but instead specify the origins, as shown above.
Also, you would need to make sure to set allow_credentials=True when using the CORSMiddleware (which sets the Access-Control-Allow-Credentials response header to true). Example (see this for more details):
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=[&quot;*&quot;],
    allow_headers=[&quot;*&quot;],
)

"
"I am a little confused about the method pyspark.sql.Window.rowsBetween that accepts Window.unboundedPreceding, Window.unboundedFollowing, and Window.currentRow objects as start and end arguments. Could you please explain how the function works and how to use Window objects correctly, with some examples? Thank you!
","Rows between/Range between as the name suggests help with limiting the number of rows considered inside a window.
Let us take a simple example.
Starting with data:
dfw = (
    spark
    .createDataFrame(
        [
            (&quot;abc&quot;, 1, 100),
            (&quot;abc&quot;, 2, 200),
            (&quot;abc&quot;, 3, 300),
            (&quot;abc&quot;, 4, 200),
            (&quot;abc&quot;, 5, 100),
        ],
        &quot;name string,id int,price int&quot;,
    )
)
# output
+----+---+-----+
|name| id|price|
+----+---+-----+
| abc|  1|  100|
| abc|  2|  200|
| abc|  3|  300|
| abc|  4|  200|
| abc|  5|  100|
+----+---+-----+

Now over this data let's try to find of running max i.e max for each row:
(
    dfw
    .withColumn(
        &quot;rm&quot;,
        F.max(&quot;price&quot;).over(Window.partitionBy(&quot;name&quot;).orderBy(&quot;id&quot;))
    )
    .show()
)

#output
+----+---+-----+---+
|name| id|price| rm|
+----+---+-----+---+
| abc|  1|  100|100|
| abc|  2|  200|200|
| abc|  3|  300|300|
| abc|  4|  200|300|
| abc|  5|  100|300|
+----+---+-----+---+

So as expected it looked at each price from top to bottom one by one and populated the max value it got this behaviour is known as start = Window.unboundedPreceding to end = Window.currentRow
Now changing rows between values to start = Window.unboundedPreceding to end = Window.unboundedFollowing we will get as below:
(
    dfw
    .withColumn(
        &quot;rm&quot;,
        F.max(&quot;price&quot;).over(
            Window
            .partitionBy(&quot;name&quot;)
            .orderBy(&quot;id&quot;)
            .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)
        )
    )
    .show()
)

#output
+----+---+-----+---+
|name| id|price| rm|
+----+---+-----+---+
| abc|  1|  100|300|
| abc|  2|  200|300|
| abc|  3|  300|300|
| abc|  4|  200|300|
| abc|  5|  100|300|
+----+---+-----+---+

Now as you can see in the same window it's looking downwards in all values for a max instead of limiting it to the current row.
Now third will be start = Window.currentRow and end = Window.unboundedFollowing
(
    dfw
    .withColumn(
        &quot;rm&quot;,
        F.max(&quot;price&quot;).over(
            Window
            .partitionBy(&quot;name&quot;)
            .orderBy(&quot;id&quot;)
            .rowsBetween(Window.currentRow, Window.unboundedFollowing)
        )
    )
    .show()
)

#output
+----+---+-----+---+
|name| id|price| rm|
+----+---+-----+---+
| abc|  1|  100|300|
| abc|  2|  200|300|
| abc|  3|  300|300|
| abc|  4|  200|200|
| abc|  5|  100|100|
+----+---+-----+---+

Now it's looking down only for a max starting its row from the current one.
Also, it's not limited to just these 3 to use as is you can even start = Window.currentRow-1 and end = Window.currentRow+1 so instead of looking for all values above or below it will only look at 1 row above and 1 row below.
like this:
(
    dfw
    .withColumn(
        &quot;rm&quot;,
        F.max(&quot;price&quot;).over(
            Window
            .partitionBy(&quot;name&quot;)
            .orderBy(&quot;id&quot;)
            .rowsBetween(Window.currentRow-1, Window.currentRow+1)
        )
    )
    .show()
)

# output
+----+---+-----+---+
|name| id|price| rm|
+----+---+-----+---+
| abc|  1|  100|200|
| abc|  2|  200|300|
| abc|  3|  300|300|
| abc|  4|  200|300|
| abc|  5|  100|200|
+----+---+-----+---+

So you can imagine it a window inside the window which works around the current row it's processing.
"
"New to ARIMA and attempting to model a dataset in Python using auto ARIMA.
I'm using auto-ARIMA as I believe it will be better at defining the values of p, d and q however the results are poor and I need some guidance.
Please see my reproducible attempts below
Attempt as follows:
    # DEPENDENCIES
    import pandas as pd 
    import numpy as np 
    import matplotlib.pyplot as plt
    import pmdarima as pm 
    from pmdarima.model_selection import train_test_split 
    from statsmodels.tsa.stattools import adfuller
    from pmdarima.arima import ADFTest
    from pmdarima import auto_arima
    from sklearn.metrics import r2_score 

# CREATE DATA
data_plot = pd.DataFrame(data removed)

# SET INDEX
data_plot['date_index'] = pd.to_datetime(data_plot['date']
data_plot.set_index('date_index', inplace=True)

# CREATE ARIMA DATASET
arima_data = data_plot[['value']]
arima_data

# PLOT DATA
arima_data['value'].plot(figsize=(7,4))

The above steps result in a dataset that should look like this.

# Dicky Fuller test for stationarity 
adf_test = ADFTest(alpha = 0.05)
adf_test.should_diff(arima_data)

Result = 0.9867 indicating non-stationary data which should be handled by appropriate over of differencing later in auto arima process.
# Assign training and test subsets - 80:20 split 

print('Dataset dimensions;', arima_data.shape)
train_data = arima_data[:-24]
test_data = arima_data[-24:]
print('Training data dimension:', train_data.shape, round((len(train_data)/len(arima_data)*100),2),'% of dataset')
print('Test data dimension:', test_data.shape, round((len(train_data)/len(arima_data)*100),2),'% of dataset')

#Â Plot training &amp; test data
plt.plot(train_data)
plt.plot(test_data)


 # Run auto arima
    arima_model = auto_arima(train_data, start_p=0, d=1, start_q=0,
    max_p=5, max_d=5, max_q=5,
    start_P=0, D=1, start_Q=0, max_P=5, max_D=5,
    max_Q=5, m=12, seasonal=True,
    stationary=False,
    error_action='warn', trace=True,
    suppress_warnings=True, stepwise=True,
    random_state=20, n_fits=50)
        
    print(arima_model.aic())

Output suggests best model is 'ARIMA(1,1,1)(0,1,0)[12]' with AIC 1725.35484
#Store predicted values and view resultant df

prediction = pd.DataFrame(arima_model.predict(n_periods=25), index=test_data.index)
prediction.columns = ['predicted_value']
prediction

# Plot prediction against test and training trends 

plt.figure(figsize=(7,4))
plt.plot(train_data, label=&quot;Training&quot;)
plt.plot(test_data, label=&quot;Test&quot;)
plt.plot(prediction, label=&quot;Predicted&quot;)
plt.legend(loc='upper right')
plt.show()


# Finding r2 model score
    test_data['predicted_value'] = prediction 
    r2_score(test_data['value'], test_data['predicted_value'])

Result: -6.985
","Is auto_arima a method done by you? It depends how you differentiate and what you do there. Did you check the autocorrelation and partial autocorrelation to know which repeating time lags you have there?
Also, it seems you have some seasonality patterns every year, you could try a SARIMA model if you are not doing it already.
To try a SARIMA model you have to:

Stationarized the data, in this case by differentiation you can convert the moving mean a stationary one.

data_stationarized = train_data.diff()[1:]


Check the autocorrelation and partial autocorrelation to check the seasonality.
You can use the library statsmodels for this.

import statsmodels.api as sm
sm.graphics.tsa.plot_acf(data_stationarized);


You can see that the most prominent flag is the twelfth flag, so as the granularity of the data is by month, that means there is prominent seasonality pattern every 12 months.
We can check the partial autocorrelation to confirm it too:
sm.graphics.tsa.plot_pacf(data_stationarized);


Again the most prominent flag is the twelfth one.

Fit the model with a seasonality order of 12. There are more parameters to explain which can be adjusted to have better results, but then this post will be very long.

model = sm.tsa.SARIMAX(endog=train_data, order=(2,0,0), seasonal_order=(2,0,0,12))
model_fit = model.fit()


Evaluate the results

from sklearn.metrics import mean_squared_error

y_pred = model_fit.forecast(steps=24)

# when squared=False then is equals to RMSE
mean_squared_error(y_true=test_data.values, y_pred=y_pred, squared=False)

This outputs 12063.88, which you can use to compare different results more rigorously.
For a graphical check:
prediction = pd.DataFrame(model_fit.forecast(steps=25), index=test_data.index)
prediction.columns = ['predicted_value']
prediction

# Plot prediction against test and training trends

plt.figure(figsize=(7,4))
plt.plot(train_data, label=&quot;Training&quot;)
plt.plot(test_data, label=&quot;Test&quot;)
plt.plot(prediction, label=&quot;Predicted&quot;)
plt.legend(loc='upper right')
plt.xticks([])
plt.yticks([])
plt.show();


Now you can see that the predictions get closer to the expected values.
You could continue fine tuning the order and seasonal order to get even better results, I will advice to check the docs of statsmodel.
Another advice it's to analyze the autocorrelation and partial autocorrelation of the residuals to check if your model is capturing all of the patterns. You have them in the model_fit object.
"
"I'm developing a FastAPI application organized with the following module structure.
...
â”‚   â”œâ”€â”€ modules
â”‚   â”‚   â”œâ”€â”€ box
â”‚   â”‚   â”‚   â”œâ”€â”€ routes.py
â”‚   â”‚   â”‚   â”œâ”€â”€ services.py
â”‚   â”‚   â”‚   â”œâ”€â”€ models.py # the sqlalchemy classes
â”‚   â”‚   â”‚   â”œâ”€â”€ schemas.py # the pydantic schemas
â”‚   â”‚   â”œâ”€â”€ toy
â”‚   â”‚   â”‚   â”œâ”€â”€ routes.py
â”‚   â”‚   â”‚   â”œâ”€â”€ services.py
â”‚   â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â”‚   â”œâ”€â”€ schemas.py

Each module contains SQLAlchemy models, Pydantic models (also called schemas), FastAPI routes, and services that handle the business logic.
In this example, I am using two modules that represent boxes and toys. Each toy is stored in one box, and each box contains multiple toys, following a classic 1 x N relationship.
With SQLAlchemy everything goes well, defining relationships is straightforward by using TYPE_CHECKING to handle circular dependencies:
# my_app.modules.box.models.py

from sqlalchemy.orm import Mapped, mapped_column, relationship
if TYPE_CHECKING:
    from my_app.modules.toy.models import Toy

class Box(Base):
    __tablename__ = &quot;box&quot;
    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)

    toys: Mapped[list[&quot;Toy&quot;]] = relationship(back_populates=&quot;box&quot;)


# my_app.modules.toy.models.py

from sqlalchemy.orm import Mapped, mapped_column, relationship
if TYPE_CHECKING:
    from my_app.modules.box.models import Box

class Toy(Base):
    __tablename__ = &quot;toy&quot;
    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)
    box: Mapped[&quot;Box&quot;] = relationship(back_populates=&quot;toys&quot;)


This setup works perfectly without raising any circular import errors. However, I encounter issues when defining the same relationships between Pydantic schemas. If I import directly the modules on my schemas.py,
# my_app.modules.box.schemas.py
from my_app.modules.toy.schemas import ToyBase

class BoxBase(BaseModel):
    id: int

class BoxResponse(BoxBase):
    toys: list[ToyBase]

# my_app.modules.toy.schemas.py
from my_app.modules.box.schemas import BoxBase

class ToyBase(BaseModel):
    id: int
    
class ToyResponse(ToyBase):
    box: BoxBase

I recieve the circular import error:
ImportError: cannot import name 'ToyBase' from partially initialized module 'my_app.modules.toy.schemas' (most likely due to a circular import)...

I also try the SQLAlchemy approach of TYPE_CHECKING and string declaration:
# my_app.modules.box.schemas.py
if TYPE_CHECKING:
    from my_app.modules.toy.schemas import ToyBase

class BoxBase(BaseModel):
    id: int

class BoxResponse(BoxBase):
    toys: list[&quot;ToyBase&quot;]

# my_app.modules.toy.schemas.py
if TYPE_CHECKING:
    from my_app.modules.box.schemas import BoxBase

class ToyBase(BaseModel):
    id: int
    
class ToyResponse(ToyBase):
    box: &quot;BoxBase&quot;

But apparently, pydantic doesn't support this:
raise PydanticUndefinedAnnotation.from_name_error(e) from e
pydantic.errors.PydanticUndefinedAnnotation: name 'ToyBase' is not defined

(Some answers) suggest that the issue comes from a poor module organization. (Others) suggest, too complex and hard to understand solutions.
Maybe I'm wrong but I consider the relationship between Box and Toy something trivial and fundamental that should be manageable in any moderately complex project. For example, a straightforward use case would be to request a toy along with its containing box and vice versa, a box with all its toys. Aren't they legitimate requests?
So, my question
How can I define interrelated Pydantic schemas (BoxResponse and ToyResponse) that reference each other without encountering circular import errors? I'm looking for an clear and maintainable solution that preserves the independence of the box and toy modules, similar to how relationships are handled in SQLAlchemy models. Any suggestions or at least an explanation of why this is so difficult to achieve?
","I had this same issue and spent hours trying to figure it out, in the end i ended up just not type annotating the specific circular imports and i've lived happily ever after(so far). Maybe you could benefit from doing this same ;)
That being said, there are multiple ways of fixing circular imports. As highlighted here
What you've tried so far is:

Normal typing; doesnt work when a child import a parent.
String literals such as toys:[&quot;ToyResponse&quot;], this method still causes circular import errors because you are still importing the class to resolve the type.
Conditionally importing using TYPE_CHECK. This method seems promising and i believe you've almost got it but were missing one small detail, the TYPE_CHECK boolean must be checked at every place where the circular import types are being used see below:

As per the example you provided, you conditionally import your classes but you dont conditionally do the type checks on the class attributes which results in an undefined error when accessing the type.
As highlighted in the mypy docs:

The typing module defines a TYPE_CHECKING constant that is False at
runtime but treated as True while type checking.
Since code inside if TYPE_CHECKING: is not executed at runtime, it
provides a convenient way to tell mypy something without the code
being evaluated at runtime. This is most useful for resolving import
cycles.

# my_app.modules.box.schemas.py
from pydantic import BaseModel
from my_app.modules.toy.schemas import ToyResponse

class BoxResponse(BaseModel):
    id: int
    toys: list[&quot;ToyResponse&quot;] # Type check not required here since this is the parent class

# my_app.modules.toy.schemas.py
from typing import TYPE_CHECKING
from pydantic import BaseModel

if TYPE_CHECKING:
    from my_app.modules.box.schemas import BoxResponse

class ToyResponse(BaseModel):
    id: int
    if TYPE_CHECKING:
        box: &quot;BoxResponse&quot;
    else:
        box 

Personally the above seems hackish.
If you have Python 3.7 and up you could also use
__future__ import annotations. This will take type hints and treat them as string literals during the initial import. Which should prevent the circular import error.
"
"import polars as pl

df = pl.DataFrame(
    {&quot;name&quot;: list(&quot;abcdef&quot;), &quot;age&quot;: [21, 31, 32, 53, 45, 26], &quot;country&quot;: list(&quot;AABBBC&quot;)}
)

df.group_by(&quot;country&quot;).agg(
    pl.col(&quot;name&quot;).sort_by(&quot;age&quot;).first().alias(&quot;age_sort_1&quot;),
    pl.col(&quot;name&quot;).sort_by(&quot;age&quot;).get(2).alias(&quot;age_sort_2&quot;),  # OutOfBoundsError: index out of bounds
    # pl.col(&quot;name&quot;).sort_by(&quot;age&quot;).arr.get(2, null_on_oob=True).alias(&quot;age_2&quot;),
    # SchemaError: invalid series dtype: expected `FixedSizeList`, got `str`
    pl.col(&quot;name&quot;).sort_by(&quot;age&quot;).last().alias(&quot;age_sort_-1&quot;)
)

As shown in the code above, I want to get the name in each country whose age is in a specific order.
However, Expr.get does not provide the null_on_oob parameter. How to automatically fill in null when an out-of-bounds situation occurs?
In addition, the .arr.get method provides the null_on_oob parameter, but reports an error SchemaError: invalid series dtype: expected &quot;FixedSizeList&quot;, got &quot;str&quot;. I donâ€™t know what this error refers to and how to solve it.
ps: The above code uses the repeated code pl.col(&quot;name&quot;).sort_by(&quot;age&quot;) many times. Is there a more concise method?
","There's an open issue related to your question now - polars.Expr.take returns null if ComputeError: index out of bounds.
For now you can either use shift() (maintain_order = True just to make it more readable):
exp = pl.col.name.sort_by(&quot;age&quot;)

(
    df
    .group_by(&quot;country&quot;, maintain_order = True).agg(
        exp.first().alias(&quot;age_sort_1&quot;),
        exp.shift(-2).first().alias(&quot;age_sort_2&quot;),
        exp.last().alias(&quot;age_sort_-1&quot;),
    )
)

┌─────────┬────────────┬────────────┬─────────────┐
│ country ┆ age_sort_1 ┆ age_sort_2 ┆ age_sort_-1 │
│ ---     ┆ ---        ┆ ---        ┆ ---         │
│ str     ┆ str        ┆ str        ┆ str         │
╞═════════╪════════════╪════════════╪═════════════╡
│ A       ┆ a          ┆ null       ┆ b           │
│ B       ┆ c          ┆ d          ┆ d           │
│ C       ┆ f          ┆ null       ┆ f           │
└─────────┴────────────┴────────────┴─────────────┘

Or, just aggregate your data into list, and then use .list.get() which allows you to use null_on_oob parameter:
(
    df
    .group_by(&quot;country&quot;).agg(
        pl.col.name.sort_by(&quot;age&quot;)
    )
    .with_columns(
        pl.col.name
        .list.get(i, null_on_oob = True).alias(f&quot;age_sort_{c}&quot;)
        for i, c in [(0, 1), (2, 2), (-1, -1)]
    ).drop(&quot;name&quot;)
)

Alternatively you can use list.gather() to get the list of 3 elements you need, convert it to struct via list.to_struct() method and then unnest() it to columns:
(
    df
    .group_by(&quot;country&quot;).agg(
        pl.col.name.sort_by(&quot;age&quot;)
    )
    .with_columns(
        pl.col.name
        .list.gather([0,2,-1], null_on_oob = True)
        .list.to_struct(fields=[&quot;age_sort_1&quot;,&quot;age_sort_2&quot;,&quot;age_sort_-1&quot;])
    ).unnest(&quot;name&quot;)
)

"
"I'm studying the process of distributing artificial intelligence modules through FastAPI.
I created a FastAPI app that answers questions using a pre-learned Machine Learning model.
In this case, it is not a problem for one user to use it, but when multiple users use it at the same time, the response may be too slow.
Hence, when multiple users enter a question, is there any way to copy the model and load it in at once?
class sentencebert_ai():
    def __init__(self) -&gt; None:
        super().__init__()

 def ask_query(self,query, topN):
        startt = time.time()

        ask_result = []
        score = []
        result_value = []  
        embedder = torch.load(model_path)
        corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)
        query_embedding = embedder.encode(query, convert_to_tensor=True)
        cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0] #torch.Size([121])121ê°œì˜ ë§ë­‰ì¹˜ì— ëŒ€í•œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê°’ì´ë‹¤.
        cos_scores = cos_scores.cpu()

        top_results = np.argpartition(-cos_scores, range(topN))[0:topN]

        for idx in top_results[0:topN]:        
            ask_result.append(corpusid[idx].item())
            #.item()ìœ¼ë¡œ ì ‘ê·¼í•˜ëŠ” ì´ìœ ëŠ” tensor(5)ì—ì„œ í•´ë‹¹ ìˆ«ìžì— ì ‘ê·¼í•˜ê¸° ìœ„í•œ ë°©ì‹ì´ë‹¤.
            score.append(round(cos_scores[idx].item(),3))

        #ì„œë²„ì— json array í˜•íƒœë¡œ ë‚´ë³´ë‚´ê¸° ìœ„í•œ ìž‘ì—…
        for i,e in zip(ask_result,score):
            result_value.append({&quot;pred_id&quot;:i,&quot;pred_weight&quot;:e})
        endd = time.time()
        print('ì‹œê°„ì²´í¬',endd-startt)
        return result_value
        # return ','.join(str(e) for e in ask_result),','.join(str(e) for e in score)



class Item_inference(BaseModel):
    text : str
    topN : Optional[int] = 1

@app.post(&quot;/retrieval&quot;, tags=[&quot;knowledge recommendation&quot;])
async def Knowledge_recommendation(item: Item_inference):
  
    # db.append(item.dict())
    item.dict()
    results = _ai.ask_query(item.text, item.topN)

    return results


if __name__ == &quot;__main__&quot;:
    parser = argparse.ArgumentParser()
    parser.add_argument(&quot;--port&quot;, default='9003', type=int)
    # parser.add_argument(&quot;--mode&quot;, default='cpu', type=str, help='cpu for CPU mode, gpu for GPU mode')
    args = parser.parse_args()

    _ai = sentencebert_ai()
    uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=args.port,workers=4)

corrected version
@app.post(&quot;/aaa&quot;) def your_endpoint(request: Request, item:Item_inference): start = time.time() model = request.app.state.model item.dict() #ì»¤ë„ ì‹¤í–‰ì‹œ í•„ìš” _ai = sentencebert_ai() results = _ai.ask_query(item.text, item.topN,model) end = time.time() print(end-start) return results ``` 

","First, you should rather not load your model every time a request arrives, but rahter have it loaded once at startup (you could use the startup event for this) and store it on the app instance—using the generic app.state attribute (see implementation of State too)—which you can later retrieve, as described here and here
Update: startup event has recently been deprecated, and since then, the recommended way to handle startup and shutdown events is using the lifespan handler, as demonstrated in this answer. You might still find the references provided earlier useful, as they provide information on additional concepts in FastAPI. For now, you could keep using the startup event, but it is recommended not to, as it might be completely removed from future FastAPI/Starlette versions.
For instance:
from fastapi import Request

@app.on_event(&quot;startup&quot;)
async def startup_event():
    app.state.model = torch.load('&lt;model_path&gt;')

Second, if you do not have any async def functions inside your endpoint that you have to await, you could define your endpoint with normal def instead. In this way, FastAPI will run requests to that def endpoint in a separate thread from an external threadpool, which will then be awaited (so that the blocking operations inside won't block the event loop); whereas, async def endpoints run directly in the event loop, and thus any synchronous blocking operations inside would block the event loop. Please have a look at the answers here and here, as well as all the references included in them, in order to understand the concept of async/await, as well as the difference between using def and async def in FastAPI. Example with normal def endpoint:
@app.post('/')
def your_endpoint(request: Request):
    model = request.app.state.model
    # run your synchronous ask_query() function here

Alternatively, as described here, you could use an async def endpoint and have your CPU-bound task run in a separate process (which is more suited than using a thread), using ProcessPoolExecutor, and integrate it with asyncio, in order to await for it to complete and return the result(s). Beware that it is important to protect the main loop of code to avoid recursive spawning of subprocesses, etc.; that is, your code must be under if __name__ == '__main__'.
Note that in the example below a new ProcessPool is created every time a request arrives at / endpoint, but a more suitable approach would be to have a reusable ProcessPoolExecutor created at application startup instead, which you could add to app.state or request.state, as demonstrated in this answer. Also, as explained earlier, startup event is now deprecated, and you should rather use a lifepsan event, as demonstrated in the linked answer provided earlier at the beginning of this answer, as well as the one provided just above.
Example
from fastapi import FastAPI, Request
import concurrent.futures
import asyncio
import uvicorn


class MyAIClass():
    def __init__(self) -&gt; None:
        super().__init__()

    def ask_query(self, model, query, topN):
        # ...
 

ai = MyAIClass()
app = FastAPI()


@app.on_event(&quot;startup&quot;)
async def startup_event():
    app.state.model = torch.load('&lt;model_path&gt;')


@app.post('/')
async def your_endpoint(request: Request):
    model = request.app.state.model

    loop = asyncio.get_running_loop()
    with concurrent.futures.ProcessPoolExecutor() as pool:
        res = await loop.run_in_executor(pool, ai.ask_query, model, item.text, item.topN)


if __name__ == '__main__':
    uvicorn.run(app)

Using multiple workers
Note that if you plan on having several workers active at the same time, each worker has its own memory—in other words, workers do not share the same memory—and hence, each worker will load their own instance of the ML model into memory (RAM). If, for instance, you are using four workers for your app, the model will result in being loaded four times into RAM. Thus, if the model, as well as other variables in your code, are consuming a large amount of memory, each process/worker will consume an equivalent amount of memory. If you would like to avoid that, you may have a look at how to share objects across multiple workers, as well as—if you are using Gunicorn as a process manager with Uvicorn workers—you can use Gunicorn's --preload flag. As per the documentation:

Command line: --preload
Default: False
Load application code before the worker processes are forked.
By preloading an application you can save some RAM resources as well
as speed up server boot times. Although, if you defer application
loading to each worker process, you can reload your application code
easily by restarting workers.

Example:
gunicorn --workers 4 --preload --worker-class=uvicorn.workers.UvicornWorker app:app

Note that you cannot combine Gunicorn's --preload with --reload flag, as when the code is preloaded into the master process, the new worker processes—which will automatically be created, if your application code has changed—will still have the old code in memory, due to how fork() works.
"
"I'm trying to send HTTPS requests as quickly as possible. I know this would have to be concurrent requests due to my goal being 150 to 500+ requests a second. I've searched everywhere, but get no Python 3.11+ answer or one that doesn't give me errors. I'm trying to avoid AIOHTTP as the rigmarole of setting it up was a pain, which didn't even work.
The input should be an array or URLs and the output an array of the html string.
","This works, getting around 250+ requests a second.
This solution does work on Windows 10. You may have to pip install for concurrent and requests.
import time
import requests
import concurrent.futures

start = int(time.time()) # get time before the requests are sent

urls = [] # input URLs/IPs array
responses = [] # output content of each request as string in an array

# create an list of 5000 sites to test with
for y in range(5000):urls.append(&quot;https://example.com&quot;)

def send(url):responses.append(requests.get(url).content)

with concurrent.futures.ThreadPoolExecutor(max_workers=10000) as executor:
    futures = []
    for url in urls:futures.append(executor.submit(send, url))
        
end = int(time.time()) # get time after stuff finishes
print(str(round(len(urls)/(end - start),0))+&quot;/sec&quot;) # get average requests per second

Output:
286.0/sec
Note: If your code requires something extremely time dependent, replace the middle part with this:
with concurrent.futures.ThreadPoolExecutor(max_workers=10000) as executor:
    futures = []
    for url in urls:
        futures.append(executor.submit(send, url))
    for future in concurrent.futures.as_completed(futures):
        responses.append(future.result())

This is a modified version of what this site showed in an example.
The secret sauce is the max_workers=10000. Otherwise, it would average about 80/sec. Although, when setting it to beyond 1000, there wasn't any boost in speed.
"
"I've noticed, to my surprise, that in a function call, I could unpack a dict with strings that weren't even valid python identifiers.
It's surprising to me since argument names must be identifiers, so allowing a function call to unpack a **kwargs that has non-identifiers, with no run time error, doesn't seem healthy (since it could bury problems deeper that where they actually occur).
Unless there's an actual use to being able to do this, in which case my question becomes &quot;what would that use be?&quot;.
Example code
Consider this function:
def foo(**kwargs):
    first_key, first_val = next(iter(kwargs.items()))
    print(f&quot;{first_key=}, {first_val=}&quot;)
    return kwargs

This shows that, within a function call, you can't unpack a dict that has has integer keys, which is EXPECTED.
&gt;&gt;&gt; t = foo(**{1: 2, 3: 4})
TypeError                                 Traceback (most recent call last)
...
TypeError: foo() keywords must be strings

What is really not expected, and surprising, is that you can, on the other hand, unpack a dict with string keys, even if these are not valid python identifiers:
&gt;&gt;&gt; t = foo(**{'not an identifier': 1, '12': 12, ',(*&amp;$)': 100})
first_key='not an identifier', first_val=1
&gt;&gt;&gt; t
{'not an identifier': 1, '12': 12, ',(*&amp;$)': 100}

","Looks like this is more of a kwargs issue than an unpacking issue. For example, one wouldn't run into the same issue with foo:
def foo(a, b):
    print(a + b)

foo(**{&quot;a&quot;: 3, &quot;b&quot;: 2})
# 5

foo(**{&quot;a&quot;: 3, &quot;b&quot;: 2, &quot;c&quot;: 4})
# TypeError: foo() got an unexpected keyword argument 'c'

foo(**{&quot;a&quot;: 3, &quot;b&quot;: 2, &quot;not valid&quot;: 4})
# TypeError: foo() got an unexpected keyword argument 'not valid'

But when kwargs is used, that flexibility comes with a price. It looks like the function first attempts to pop out and map all the named arguments and then passes the remaining items in a dict called kwargs. Since all keywords are strings (but all strings are not valid keywords), the first check is easy - keywords must be strings. Beyond that, it's up to the author to figure out what to do with remaining items in kwargs.
def bar(a, **kwargs):
    print(locals())
    
bar(a=2)
# {'a': 2, 'kwargs': {}}

bar(**{&quot;a&quot;: 3, &quot;b&quot;: 2})
# {'a': 3, 'kwargs': {'b': 2}}

bar(**{&quot;a&quot;: 3, &quot;b&quot;: 2, &quot;c&quot;: 4})
# {'a': 3, 'kwargs': {'b': 2, 'c': 4}}

bar(**{1: 3, 3: 4})
# TypeError: keywords must be strings

Having said all that, there definitely is inconsistency but not a flaw. Some related discussions:

Supporting (or not) invalid identifiers in **kwargs 
feature: **kwargs allowing improperly named variables

"
"I am having some issues inserting into MongoDB via FastAPI.
The below code works as expected. Notice how the response variable has not been used in response_to_mongo().
The model is an sklearn ElasticNet model.
app = FastAPI()


def response_to_mongo(r: dict):
    client = pymongo.MongoClient(&quot;mongodb://mongo:27017&quot;)
    db = client[&quot;models&quot;]
    model_collection = db[&quot;example-model&quot;]
    model_collection.insert_one(r)


@app.post(&quot;/predict&quot;)
async def predict_model(features: List[float]):

    prediction = model.predict(
        pd.DataFrame(
            [features],
            columns=model.feature_names_in_,
        )
    )

    response = {&quot;predictions&quot;: prediction.tolist()}
    response_to_mongo(
        {&quot;predictions&quot;: prediction.tolist()},
    )
    return response

However when I write predict_model() like this and pass the response variable to response_to_mongo():
@app.post(&quot;/predict&quot;)
async def predict_model(features: List[float]):

    prediction = model.predict(
        pd.DataFrame(
            [features],
            columns=model.feature_names_in_,
        )
    )

    response = {&quot;predictions&quot;: prediction.tolist()}
    response_to_mongo(
        response,
    )
    return response

I get an error stating that:
TypeError: 'ObjectId' object is not iterable

From my reading, it seems that this is due to BSON/JSON issues between FastAPI and Mongo. However, why does it work in the first case when I do not use a variable? Is this due to the asynchronous nature of FastAPI?
","As per the documentation:

When a document is inserted a special key, &quot;_id&quot;, is automatically
added if the document doesn’t already contain an &quot;_id&quot; key. The value
of &quot;_id&quot; must be unique across the collection. insert_one() returns an
instance of InsertOneResult. For more information on &quot;_id&quot;, see the
documentation on _id.

Thus, in the second case of the example you provided, when you pass the dictionary to the insert_one() function, Pymongo will add to your dictionary the unique identifier (i.e., ObjectId) necessary to retrieve the data from the database; and hence, when returning the response from the endpoint, the ObjectId fails getting serialized—since, as described in this answer in detail, FastAPI, by default, will automatically convert that return value into JSON-compatible data using the jsonable_encoder (to ensure that objects that are not serializable are converted to a str), and then return a JSONResponse, which uses the standard json library to serialize the data.
Solution 1
Use the approach demonstrated here, by having the ObjectId converted to str by default, and hence, you can return the response as usual inside your endpoint.
# place these at the top of your .py file
import pydantic
from bson import ObjectId
pydantic.json.ENCODERS_BY_TYPE[ObjectId]=str

return response # as usual

Solution 2
Dump the loaded BSON to valid JSON string and then reload it as dict, as described here and here.
from bson import json_util
import json

response = json.loads(json_util.dumps(response))
return response

Solution 3
Define a custom JSONEncoder, as described here, to convert the ObjectId into str:
import json
from bson import ObjectId

class JSONEncoder(json.JSONEncoder):
    def default(self, o):
        if isinstance(o, ObjectId):
            return str(o)
        return json.JSONEncoder.default(self, o)


response = JSONEncoder().encode(response)
return response

Solution 4
You can have a separate output model without the 'ObjectId' (_id) field, as described in the documentation. You can declare the model used for the response with the parameter response_model in the decorator of your endpoint. Example:
from pydantic import BaseModel

class ResponseBody(BaseModel):
    name: str
    age: int


@app.get('/', response_model=ResponseBody)
def main():
    # response sample
    response = {'_id': ObjectId('53ad61aa06998f07cee687c3'), 'name': 'John', 'age': '25'}
    return response

Solution 5
Remove the &quot;_id&quot; entry from the response dictionary before returning it (see here on how to remove a key from a dict):
response.pop('_id', None)
return response

"
"I want to try out polars in Python so what I want to do is concatenate several dataframes that are read from jsons. When I change the index to date and have a look at lala1.head() I see that the column date is gone, so I basically lose the index. Is there a better solution or do I need to sort by date, which basically does the same as setting the index to date?
import polars as pl

quarterly_balance_df = pl.read_json('../AAPL/single_statements/1985-09-30-quarterly_balance.json')


q1 = quarterly_balance_df.lazy().with_columns(pl.col(&quot;date&quot;).str.to_date())
quarterly_balance_df = q1.collect()
q2 = quarterly_balance_df.lazy().with_columns(pl.col(&quot;fillingDate&quot;).str.to_date())
quarterly_balance_df = q2.collect()
q3 = quarterly_balance_df.lazy().with_columns(pl.col(&quot;acceptedDate&quot;).str.to_date())
quarterly_balance_df = q3.collect()

quarterly_balance_df2 = pl.read_json('../AAPL/single_statements/1986-09-30-quarterly_balance.json')

q1 = quarterly_balance_df2.lazy().with_columns(pl.col(&quot;date&quot;).str.to_date())
quarterly_balance_df2 = q1.collect()
q2 = quarterly_balance_df2.lazy().with_columns(pl.col(&quot;fillingDate&quot;).str.to_date())
quarterly_balance_df2 = q2.collect()
q3 = quarterly_balance_df2.lazy().with_columns(pl.col(&quot;acceptedDate&quot;).str.to_date())
quarterly_balance_df2 = q3.collect()

lala1 = pl.from_pandas(quarterly_balance_df.to_pandas().set_index('date'))
lala2 = pl.from_pandas(quarterly_balance_df.to_pandas().set_index('date'))

test = pl.concat([lala1,lala2])

","Polars intentionally eliminates the concept of an index.
From the &quot;Coming from Pandas&quot; section in the User Guide:

Polars aims to have predictable results and readable queries, as such we think an index does not help us reach that objective.

Indeed, the from_pandas method ignores any index.  For example, if we start with this data:
import polars as pl

df = pl.DataFrame(
    {
        &quot;key&quot;: [1, 2],
        &quot;var1&quot;: [&quot;a&quot;, &quot;b&quot;],
        &quot;var2&quot;: [&quot;r&quot;, &quot;s&quot;],
    }
)
print(df)

shape: (2, 3)
┌─────┬──────┬──────┐
│ key ┆ var1 ┆ var2 │
│ --- ┆ ---  ┆ ---  │
│ i64 ┆ str  ┆ str  │
╞═════╪══════╪══════╡
│ 1   ┆ a    ┆ r    │
│ 2   ┆ b    ┆ s    │
└─────┴──────┴──────┘

Now, if we export this Polars dataset to Pandas, set key as the index in Pandas, and then re-import to Polars, you'll see the 'key' column disappear.
pl.from_pandas(df.to_pandas().set_index(&quot;key&quot;))

shape: (2, 2)
┌──────┬──────┐
│ var1 ┆ var2 │
│ ---  ┆ ---  │
│ str  ┆ str  │
╞══════╪══════╡
│ a    ┆ r    │
│ b    ┆ s    │
└──────┴──────┘

This is why your Date column disappeared.
In Polars, you can sort, summarize, or join by any set of columns in a DataFrame.  No need to declare an index.
I recommend looking through the Polars User Guide.  It's a great place to start.  And there's a section for those coming from Pandas.
"
"In polars, what is the way to make a copy of a dataframe?  In pandas it would be:
df_copy = df.copy()

But what is the syntax for polars?
","That would be clone :
df = pl.DataFrame(
    {
        &quot;a&quot;: [1, 2, 3, 4],
        &quot;b&quot;: [0.5, 4, 10, 13],
        &quot;c&quot;: [True, True, False, True],
    }
)

df_copy = df.clone() #cheap deepcopy/clone

Output :
​
print(df_copy)
​
shape: (4, 3)
┌─────┬──────┬───────┐
│ a   ┆ b    ┆ c     │
│ --- ┆ ---  ┆ ---   │
│ i64 ┆ f64  ┆ bool  │
╞═════╪══════╪═══════╡
│ 1   ┆ 0.5  ┆ true  │
│ 2   ┆ 4.0  ┆ true  │
│ 3   ┆ 10.0 ┆ false │
│ 4   ┆ 13.0 ┆ true  │
└─────┴──────┴───────┘

"
"I have two subplots sharing x-axis, but it only shows the y-value of one subplot not both. I want the hover-display to show y values from both subplots.
Here is what is showing right now:

But I want it to show y values from the bottom chart as well even if I am hovering my mouse on the top chart and vice versa.
Here's my code:
title = 'Price over time'
err = 'Price'


fig = make_subplots(rows=2, cols=1,
                    vertical_spacing = 0.05,
                    shared_xaxes=True,
                    subplot_titles=(title,&quot;&quot;))

# A
fig.add_trace(go.Scatter(x= A_error['CloseDate'], 
                         y = A_error[err], 
                         line_color = 'green',
                         marker_color = 'green',
                         mode = 'lines+markers',
                         showlegend = True,
                         name = &quot;A&quot;,
                         stackgroup = 'one'),
              row = 1,
              col = 1,
              secondary_y = False)

# B
fig.add_trace(go.Scatter(x= B_error['CloseDate'], 
                         y = B_error[err], 
                         line_color = 'blue',
                         mode = 'lines+markers',
                         showlegend = True,
                         name = &quot;B&quot;,
                         stackgroup = 'one'),
              row = 2,
              col = 1,
              secondary_y = False)

fig.update_yaxes(tickprefix = '$')
fig.add_hline(y=0, line_width=3, line_dash=&quot;dash&quot;, line_color=&quot;black&quot;)

fig.update_layout(#height=600, width=1400, 
                  hovermode = &quot;x unified&quot;,
                  legend_traceorder=&quot;normal&quot;)

","Edit: At this time, I don't think a Unified hovermode across the subplots will be provided. I got the rationale for this from here. It does affect some features, but this can be applied to work around it. In your example, the horizontal line does not appear on both graphs.
So, I have added two horizontal lines in line mode for scatter plots to accommodate this.
With the two stock prices, you have set a threshold value for each. Your objective is the same threshold value, so please modify that.
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import yfinance as yf

df = yf.download(&quot;AAPL MSFT&quot;, start=&quot;2022-01-01&quot;, end=&quot;2022-07-01&quot;, group_by='ticker')
df.reset_index(inplace=True)

import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

title = 'Price over time'
err = 'Price'

fig = make_subplots(rows=2, cols=1,
                    vertical_spacing = 0.05,
                    shared_xaxes=True,
                    subplot_titles=(title,&quot;&quot;))

# AAPL
fig.add_trace(go.Scatter(x = df['Date'], 
                         y = df[('AAPL', 'Close')], 
                         line_color = 'green',
                         marker_color = 'green',
                         mode = 'lines+markers',
                         showlegend = True,
                         name = &quot;AAPL&quot;,
                         stackgroup = 'one'),
              row = 1,
              col = 1,
              secondary_y = False)
# APPL $150 horizontal line
fig.add_trace(go.Scatter(x=df['Date'],
                         y=[125]*len(df['Date']),
                         mode='lines',
                         line_width=3,
                         line_color='black',
                         line_dash='dash',
                         showlegend=False,
                         name='APPL'
                        ),
              row=1,
              col=1,
              secondary_y=False)
                                   

# MSFT
fig.add_trace(go.Scatter(x= df['Date'], 
                         y = df[('MSFT', 'Close')], 
                         line_color = 'blue',
                         mode = 'lines+markers',
                         showlegend = True,
                         name = &quot;MSFT&quot;,
                         stackgroup = 'one'),
              row = 2,
              col = 1,
              secondary_y = False)
# MSFT $150 horizontal line
fig.add_trace(go.Scatter(x=df['Date'],
                         y=[150]*len(df['Date']),
                         mode='lines',
                         line_width=3,
                         line_color='black',
                         line_dash='dash',
                         showlegend=False,
                         name='MSFT'
                        ),
              row=2,
              col=1,
              secondary_y=False)


fig.update_yaxes(tickprefix = '$')
fig.update_xaxes(type='date', range=[df['Date'].min(),df['Date'].max()])

#fig.add_hline(y=0, line_width=3, line_dash=&quot;dash&quot;, line_color=&quot;black&quot;)
fig.update_layout(#height=600, width=1400,
    hovermode = &quot;x unified&quot;,
    legend_traceorder=&quot;normal&quot;)
fig.update_traces(xaxis='x2')

fig.show()

enter code here


"
"As you probably know, the order of alphabet in some (maybe most) languages is different than their order in Unicode. That's why we may want to use icu.Collator to sort, like this Python example:
from icu import Collator, Locale
collator = Collator.createInstance(Locale(&quot;fa_IR.UTF-8&quot;))
mylist.sort(key=collator.getSortKey)

This works perfectly for Persian strings. But it also sorts all Persian strings before all ASCII / English strings (which is the opposite of Unicode sort).
What if we want to sort ASCII before this given locale?
Or ideally, I want to sort by 2 or multiple locales. (For example give multiple Locale arguments to Collator.createInstance)
If we could tell collator.getSortKey to return empty bytes for other locales, then I could create a tuple of 2 collator.getSortKey() results, for example:
from icu import Collator, Locale

collator1 = Collator.createInstance(Locale(&quot;en_US.UTF-8&quot;))
collator2 = Collator.createInstance(Locale(&quot;fa_IR.UTF-8&quot;))

def sortKey(s):
    return collator1.getSortKey(s), collator2.getSortKey(s)

mylist.sort(key=sortKey)

But looks like getSortKey always returns non-empty bytes.
","A bit late to answer the question, but here it is for future reference.
ICU collation uses the CLDR Collation Algorithm, which is a tailoring of the Unicode Collation Algorithm. The default collation is referred to as the root collation. Don't think in terms of Locales having a set of collation rules, think more in terms of locales specify any differences between the collation rules that the locale needs and the root collation. CLDR takes a minimalist approach, you only need to include the minimal set of differences needed based on the root collation.
English uses the root locale. No tailorings. Persian on the other hand has a few rules needed to override certain aspects of the root collation.
As the question indicates, the Persian collation rules order Arabic characters before Latin characters. In the collation rule set for Persian there is a rule [reorder Arab]. This rule is what you need to override.
There are a few ways to do this:

Use icu.RuleBasedCollator with a coustom set fo rules for Persian.
Create a standard Persian collation, retrieve the rules, strip out the reorder directive and then use modified rules with icu.RuleBasedCollator.
Create collator instance using a BCP-47 language tag, instead of a Locale identifier

There are other approaches as well, but the third is the simplest:
loc = Locale.forLanguageTag(&quot;fa-u-kr-latn-arab&quot;)
collator = Collator.createInstance(loc)
sorted(mylist, key=collator.getSortKey)

This will reorder the Persian collation rules, placing Latin script before Arabic script, then everything else afterwards.
Update 2024-06-27
The reordering directive above reorders Latin first, then Arabic script, then everything else based on its default ordering.
This works well for bilingual data in Persian and languages using the Latin script, but may not be as suitable for multiscript data.
There is a special ISO 15924 code Zzzz representing Unknown  script, as a ICU reorder code, it is used to represent all scripts not specifically specified in the reorder. So fa-u-kr-latn-arab would be the same as fa-u-kr-latn-arab-Zzzz, but if we use fa-u-kr-Zzzz without mentioning other codes, the collator will order scripts as per Root collation order. This would give us Persian specific sorting combined with the default script order of the Root collation:
import icu
data = [&quot;Salâm&quot;, &quot;سلام&quot;, &quot;тасли́м&quot;, &quot;Persian&quot;, &quot;فارسی&quot;, &quot;Персидский язык&quot;]

# Persian (Farsi) locale based collator
loc_fa = loc = icu.Locale('fa')
collator_fa = icu.Collator.createInstance(loc_fa)
sorted(data, key=collator_fa.getSortKey)
# ['سلام', 'فارسی', 'Persian', 'Salâm', 'Персидский язык', 'тасли́м']


# Persian (Farsi) locale based collator with reordering: Latin, Arabic, then other scripts
loc_alt = icu.Locale.forLanguageTag(&quot;fa-u-kr-latn-arab&quot;)
collator_alt = icu.Collator.createInstance(loc_alt)
sorted(data, key=collator_alt.getSortKey)
# ['Persian', 'Salâm', 'سلام', 'فارسی', 'Персидский язык', 'тасли́м']

# Persian (Farsi) locale based collator with reordering:  Other (Zzzz - Unknown script)
# Sets order to default CLDR order
loc = icu.Locale.forLanguageTag(&quot;fa-u-kr-Zzzz&quot;)
collator = icu.Collator.createInstance(loc)
sorted(data, key=collator.getSortKey)
# ['Persian', 'Salâm', 'Персидский язык', 'тасли́м', 'سلام', 'فارسی'

"
"I need to sort uint64 arrays of length 1e8-1e9, which is one of the performance bottlenecks in my current project. I have just recently updated numpy v2.0 version, in which the sorting algorithm is significantly optimized. Testing it on my hardware, its about 5x faster than numpy v1.26 version. But currently numpy's sorting algorithm cannot utilize multi-core CPUs even though it uses SIMD.
I tried to parallelize it and sort multiple np.array at the same time. One possible approach is to use numba prange, but numba has always had poor support for numpy sorting. numba.jit even has a slowdown effect on np.sort, and numba v0.60.0 fails to follow up on numpy v2.0's optimizations for sorting (https://github.com/numba/numba/issues/9611). The alternative is cython prange, but cython does not allow the creation of Python objects at nogil. Is there a way to sort numpy.array in parallel using cython or otherwise? If using cpp's parallel sorting libraries, are they faster than numpy's own sorting, taking into account the overhead of data type conversions?
arr=np.random.randint(0,2**64,int(3e8),dtype='uint64')  

sorted_arr=np.sort(arr)  # single thread np.sort takes 4 seconds (numpy v2.0.0)

","This answer show why a pure-Python, Numba or Cython implementation certainly cannot be used to write a (reasonably-simple) efficient implementation (this summaries the comments). It provides a C++ version which can be called from CPython. The provided version is fast independently of the Numpy version used (so Numpy 2.0 is not required).

Why it is certainly not possible directly with Numba/Cython/pure-Python
I do no think it is possible to call sort of Numpy in parallel with Cython/Numba because of the GIL and many additional issues.
Regarding Numba, parallel loops need the GIL to be release and no object can be manipulated inside it. The Numba sorting function does not actually call Numpy functions, but its own implementation which does not use the GIL nor create any Python object (which require the GIL to be enabled). The Numba sequential implementation is inefficient anyway. While one can try to re-implement a parallel sort from scratch, the parallel features are too limited for the resulting implementation to be really fast or reasonable simple (or both). Indeed, it is limited to a simple parallel for loop called prange (no atomics, critical sections, barriers, TLS storage, etc.).
Regarding Cython, prange of Cython requires the GIL to be disabled so creating Python object is not possible in the parallel loop preventing np.sort to be called... Cython provides more parallel features than Numba so re-implementing a parallel sort from scratch seems possible at first glance. However, in practice, it is really complicated (if even possible) to write a fast implementation because of many issues and opened/unknown bugs. Here are the issues I found out while trying to write such a code:

OpenMP barriers are not yet available and there is no sane (portable) replacement;
critical sections are also not yet available so one need to use manual locks (instead of just #pragma omp critical;
arrays must be allocated and freed manually using malloc and free in parallel sections (bug prone and resulting in a more complex code);
It is not possible to create views in parallel sections (only outside);
Cython does not seems to support well Numpy 2.0 yet causing many compilation errors and also runtime ones (see this post which seems related to this);
the documentation of OpenMP functions is rather limited (parts are simply missing);
variables of a prange-based loop cannot be reused in a range-based loop outside the prange-loop

I also tried to use a ThreadPoolExecutor so to call some optimized Cython/Numba functions and circumvent the aforementioned limitations of the two but it resulted in a very slow implementation (slower than just calling np.sort) mainly because of the GIL (nearly no speed up) and Numpy overhead (mainly temporary arrays and more precisely page-faults).

Efficient parallel C++ solution
We can write an efficient parallel C++ code performing the following steps:

split the input array in N slices
perform a bucket sort on each part in parallel so we get M buckets for each slice
merge the resulting buckets so to get M buckets from the M x N buckets
sort the M buckets in parallel using a SIMD-optimized sort -- this can be done with the x86simdsort C++ library (used internally by Numpy) though it only works on x86-64 CPUs
merge the M buckets so to get the final array

We need to write a BucketList data structure so to add numbers in a variable-size container. This is basically a linked list of chunks. Note a growing std::vector is not efficient because each resize put too much pressure on memory (and std::deque operations are so slow that is is even slower).
Here is the resulting C++ code:
// File: wrapper.cpp
// Assume x86-simd-sort has been cloned in the same directory and built
#include &quot;x86-simd-sort/lib/x86simdsort.h&quot;
#include &lt;cstdlib&gt;
#include &lt;cstring&gt;
#include &lt;forward_list&gt;
#include &lt;mutex&gt;
#include &lt;omp.h&gt;


template &lt;typename T, size_t bucketMaxSize&gt;
struct BucketList
{
    using Bucket = std::array&lt;T, bucketMaxSize&gt;;

    std::forward_list&lt;Bucket&gt; buckets;
    uint32_t bucketCount;
    uint32_t lastBucketSize;

    BucketList() : bucketCount(1), lastBucketSize(0)
    {
        buckets.emplace_front();
    }

    void push_back(const T&amp; value)
    {
        if (lastBucketSize == bucketMaxSize)
        {
            buckets.emplace_front();
            lastBucketSize = 0;
            bucketCount++;
        }

        Bucket* lastBucket = &amp;*buckets.begin();
        (*lastBucket)[lastBucketSize] = value;
        lastBucketSize++;
    }

    size_t size() const
    {
        return (size_t(bucketCount) - 1lu) * bucketMaxSize + lastBucketSize;
    }

    size_t bucketSize(size_t idx) const
    {
        return idx == 0 ? lastBucketSize : bucketMaxSize;
    }
};


extern &quot;C&quot; void parallel_sort(int64_t* arr, size_t size)
{
    static const size_t bucketSize = 2048;
    static const size_t radixBits = 11;
    static const size_t bucketCount = 1 &lt;&lt; radixBits;

    struct alignas(64) Slice
    {
        int64_t* data = nullptr;
        size_t size = 0;
        size_t global_offset = 0;
        size_t local_offset = 0;
        std::mutex mutex;
    };

    std::array&lt;Slice, bucketCount&gt; slices;

    #pragma omp parallel
    {
        std::array&lt;BucketList&lt;int64_t, bucketSize&gt;, bucketCount&gt; tlsBuckets;

        #pragma omp for nowait
        for (size_t i = 0; i &lt; size; ++i)
        {
            constexpr uint64_t signBit = uint64_t(1) &lt;&lt; uint64_t(63);
            const uint64_t idx = (uint64_t(arr[i]) ^ signBit) &gt;&gt; (64 - radixBits);
            tlsBuckets[idx].push_back(arr[i]);
        }

        #pragma omp critical
        for (size_t i = 0; i &lt; bucketCount; ++i)
            slices[i].size += tlsBuckets[i].size();

        #pragma omp barrier

        #pragma omp single
        {
            size_t offset = 0;

            for (size_t i = 0; i &lt; bucketCount; ++i)
            {
                Slice&amp; slice = slices[i];
                slice.data = &amp;arr[offset];
                slice.global_offset = offset;
                offset += slice.size;
            }
        }

        for (size_t i = 0; i &lt; bucketCount; ++i)
        {
            Slice&amp; slice = slices[i];
            size_t local_offset;
            size_t local_offset_end;

            {
                std::scoped_lock lock(slice.mutex);
                local_offset = slice.local_offset;
                slice.local_offset += tlsBuckets[i].size();
                local_offset_end = slice.local_offset;
            }

            uint32_t bucketListId = 0;

            for(const auto&amp; kv : tlsBuckets[i].buckets)
            {
                const size_t actualBucketSize = tlsBuckets[i].bucketSize(bucketListId);
                memcpy(&amp;slice.data[local_offset], &amp;kv[0], sizeof(int64_t) * actualBucketSize);
                local_offset += actualBucketSize;
                bucketListId++;
            }
        }

        #pragma omp barrier

        #pragma omp for schedule(dynamic)
        for (size_t i = 0; i &lt; bucketCount; ++i)
            x86simdsort::qsort(&amp;slices[i].data[0], slices[i].size);
    }
}

A simple header can be written if you want to call this implementation from Cython (though it can be complicated due to the aforementioned Cython/Numpy-2.0 compatibility issue). Here is an example:
// File: wrapper.h
#include &lt;stdlib.h&gt;
#include &lt;stdint.h&gt;
void parallel_sort(int64_t* arr, size_t size)

You can compile the code with Clang using the following command lines on Linux:
clang++ -O3 -fopenmp -c wrapper.cpp -fPIC -g
clang wrapper.o -o wrapper.so -fopenmp --shared -Lx86-simd-sort/build -lx86simdsortcpp

The following one may also be needed to find the x86-simd-sort library at runtime once cloned and built:
export LD_LIBRARY_PATH=x86-simd-sort/build:$LD_LIBRARY_PATH

You can finally use the fast sorting function from a Python code. I personally use ctypes because it worked directly with no issues (except when the code is compiled with GCC for unknown strange reasons). Here is an example:
import numpy as np

import ctypes
lib = ctypes.CDLL('./wrapper.so')
parallel_sort = lib.parallel_sort
parallel_sort.argtypes = [ctypes.c_voidp, ctypes.c_size_t]
parallel_sort.restype = None

fullCheck = False

print('Generating...')
a = np.random.randint(0, (1&lt;&lt;63) - 1, 1024*1024**2)
if fullCheck: b = a.copy()

print('Benchmark...')
#%time a.sort()
%time parallel_sort(a.ctypes.data, a.size)

print('Full check...' if fullCheck else 'Check...')
if fullCheck:
    b.sort()
    assert np.array_equal(b, a)
else:
    assert np.all(np.diff(a) &gt;= 0)


Notes and performance results
Note this require a lot of memory to do the test, especially if fullCheck is set to true.
Note that the C++ code is optimized for sorting huge arrays (with &gt;1e8 items). The memory consumption will be significant for smaller arrays compared to their size. The current code will even be slow for small arrays (&lt;1e5). You can tune constants/parameters regarding your needs. For tiny arrays, you can directly call the x86-simd-sort library. Once tuned properly, it should be faster than np.sort for all arrays (whatever their size). I strongly advise you to tune the parameters regarding your specific input and target CPU, especially radixBits. The current code/parameters are optimized for mainstream Intel CPUs (not recent big-little Intel ones nor AMD ones) and positive numbers. If you know there are only positive numbers in the input, you can skip the most-significantly bit (sign bit).
Here is the resulting timings on my 6-core i5-9600KF CPU (with Numpy 2.0):
np.sort:             19.3 s
Proposed C++ code:    4.3 s

The C++ parallel implementation is 4.5 times faster than the sequential optimized Numpy one.

Note I did not massively test the code but basic checks like the one proposed in the provided Python script reported no error so far (even on negative numbers apparently).
Note that this sort is efficient if the highest bits of the sorted numbers are different set (this is the downside of bucket/radix sorts). Ideally, numbers should be uniformly distributed and use all the highest bits. If this is not the case, then the buckets will be unbalanced resulting in a lower scalability. In the worst case, only 1 bucket is used resulting in a serial implementation. You can track the highest bit set so to mitigate this issue. More complex approaches are required when there are some rare big outliers (eg. remapping preserving the ordering).
"
"I am trying to apply a function to a Dataframe column (series) that retrieves the day of the week based on the timestamps in the column. However, I am being thrown the following exception, even though the Polars docs include documentation for polars.Expr.apply.
AttributeError: 'Expr' object has no attribute 'apply'.

My goal is to create a new column of day names using the following code where the alertTime column is of dtype datetime64:
def get_day(dt_obj):
    days_of_week = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']
    return days_of_week[dt_obj.weekday()]
    
# Get the day of the week from the timestamp
df = df.with_columns(
  pl.col('alertTime').apply(get_day, return_dtype=pl.Utf8).alias('day_of_week')
)

Could anyone help with where I might be going wrong?
","apply was renamed to .map_elements() some time ago.
Previous versions printed a deprecation warning, but it was eventually removed after a grace period.
You're likely looking at the docs for an older version of Polars, but there is a &quot;version switcher&quot; on the docs site:



As for the actual task, you can also do it natively using .dt.to_string()
import datetime
import polars as pl

pl.select(
   pl.lit(str(datetime.datetime.now()))
     .str.to_datetime()
     .dt.to_string(&quot;%A&quot;)
)      

shape: (1, 1)
┌─────────┐
│ literal │
│ ---     │
│ str     │
╞═════════╡
│ Tuesday │
└─────────┘

"
"I need to convert a markdown table into a pandas DataFrame. I've managed to do this using the pd.read_csv function with '|' as the separator, but it seems like there's some additional cleanup required. Specifically, I need to remove the row containing '-----', which is used for table separation, and I also want to get rid of the last column.
Here's a simplified example of what I'm doing:
import pandas as pd
from io import StringIO

# The text containing the table
text = &quot;&quot;&quot;
| Some Title | Some Description             | Some Number |
|------------|------------------------------|-------------|
| Dark Souls | This is a fun game           | 5           |
| Bloodborne | This one is even better      | 2           |
| Sekiro     | This one is also pretty good | 110101      |
&quot;&quot;&quot;

# Use StringIO to create a file-like object from the text
text_file = StringIO(text)

# Read the table using pandas read_csv with '|' as the separator
df = pd.read_csv(text_file, sep='|', skipinitialspace=True)

# Remove leading/trailing whitespace from column names
df.columns = df.columns.str.strip()

# Remove the index column
df = df.iloc[:, 1:]

Is there a more elegant and efficient way to convert a markdown table into a DataFrame without needing to perform these additional cleanup steps? I'd appreciate any suggestions or insights on improving this process.
","Like this
import re
import pandas as pd

text = &quot;&quot;&quot;
| Some Title | Some Description             | Some Number |
|------------|------------------------------|-------------|  
| Dark Souls | This is a fun game           | 5           |
| Bloodborne | This one is even better      | 2           |
| Sekiro     | This one is also pretty good | 110101      |
&quot;&quot;&quot;

pattern = r&quot;\| ([\w\s]+) \| ([\w\s]+) \| ([\w\s]+) \|&quot;

# Use the findall function to extract all rows that match the pattern
matches = re.findall(pattern, text)

# Extract the header and data rows
header = matches[0]
data = matches[1:]

# Create a pandas DataFrame using the extracted header and data rows
df = pd.DataFrame(data, columns=header)

# Optionally, convert numerical columns to appropriate types
df['Some Number'] = df['Some Number'].astype(int)

print(df)

"
"I was trying to come up with a use case for the new @enum.nonmember decorator in Python 3.11. The docs clearly mention it is a decorator meant to be applied to members.
However, when I tried literally decorating a member directly:
import enum


class MyClass(enum.Enum):
    A = 1
    B = 2

    @enum.nonmember
    C = 3

this results in an error as:
Traceback (most recent call last):
  File &quot;C:\Program Files\Python311\Lib\code.py&quot;, line 63, in runsource
    code = self.compile(source, filename, symbol)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Program Files\Python311\Lib\codeop.py&quot;, line 153, in __call__
    return _maybe_compile(self.compiler, source, filename, symbol)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Program Files\Python311\Lib\codeop.py&quot;, line 73, in _maybe_compile
    return compiler(source, filename, symbol)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Program Files\Python311\Lib\codeop.py&quot;, line 118, in __call__
    codeob = compile(source, filename, symbol, self.flags, True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;&lt;input&gt;&quot;, line 9
    C = 3
    ^
SyntaxError: invalid syntax

However, if I had declared an atribute as a property or a descriptor it also wouldn't become an Enum member... So how, when and why do you use @enum.nonmember?
","You would use it like so:
import enum


class MyClass(enum.Enum):
    A = 1
    B = 2

    C = enum.nonmember(3)

As far as I can tell, the only reason why it is called a decorator, is because of nested classes.
Currently,
class MyClass(enum.Enum):
    A = 1
    B = 2

    class MyNestedClass:
        pass

makes MyClass.MyNestedClass into one of the members of MyClass. This will change in 3.13. So if you want the new behaviour now, you can use:
class MyClass(enum.Enum):
    A = 1
    B = 2

    @enum.nonmember
    class MyNestedClass:
        pass

In 3.13, if you want the current behaviour of making nested classes members, you can use
class MyClass(enum.Enum):
    A = 1
    B = 2

    @enum.member
    class MyNestedClass:
        pass

There is no reason to use enum.nonmember as a decorator on a method, since methods are already excluded from being members, but I think you could use enum.member on one to be able to define a method member if you wanted. Not sure why you would, though.
"
"I would like to know how to fill a column of a polars dataframe with random values.
The idea is that I have a dataframe with a given number of columns, and I want to add a column to this dataframe which is filled with different random values (obtained from a random.random() function for example).
This is what I tried for now:
df = df.with_columns(
    pl.when((pl.col('Q') &gt; 0)).then(random.random()).otherwise(pl.lit(1)).alias('Prob')
)

With this method, the result that I obtain is a column filled with one random value i.e. all the rows have the same value.
Is there a way to fill the column with different random values ?
Thanks by advance.
","You need a &quot;column&quot; of random numbers the same height as your dataframe?
np.random.rand could be useful for this:
df = pl.DataFrame({&quot;foo&quot;: [1, 2, 3]})

df.with_columns(pl.lit(np.random.rand(df.height)).alias(&quot;prob&quot;))

shape: (3, 2)
┌─────┬──────────┐
│ foo ┆ prob     │
│ --- ┆ ---      │
│ i64 ┆ f64      │
╞═════╪══════════╡
│ 1   ┆ 0.657389 │
│ 2   ┆ 0.616265 │
│ 3   ┆ 0.142611 │
└─────┴──────────┘

df.with_columns(
   pl.when(pl.col(&quot;foo&quot;) &gt; 2).then(pl.lit(np.random.rand(df.height)))
     .alias(&quot;prob&quot;)
)

shape: (3, 2)
┌─────┬──────────┐
│ foo ┆ prob     │
│ --- ┆ ---      │
│ i64 ┆ f64      │
╞═════╪══════════╡
│ 1   ┆ null     │
│ 2   ┆ null     │
│ 3   ┆ 0.686551 │
└─────┴──────────┘

It may also be possible to do similar with expressions?
e.g. with .int_range() and .sample()
df.with_columns(
   (pl.int_range(1000).sample(pl.len(), with_replacement=True) / 1000)
      .alias(&quot;prob&quot;)
)

shape: (3, 2)
┌─────┬───────┐
│ foo ┆ prob  │
│ --- ┆ ---   │
│ i64 ┆ f64   │
╞═════╪═══════╡
│ 1   ┆ 0.288 │
│ 2   ┆ 0.962 │
│ 3   ┆ 0.734 │
└─────┴───────┘

"
"I have created a simple API using FastAPI, and I am trying to pass a URL to a FastAPI route as an arbitrary path parameter.
from fastapi import FastAPI
app = FastAPI()
@app.post(&quot;/{path}&quot;)
def pred_image(path:str):
    print(&quot;path&quot;,path)
    return {'path':path}

When I test it, it doesn't work and throws an error. I am testing it this way:
http://127.0.0.1:8000/https://raw.githubusercontent.com/ultralytics/yolov5/master/data/images/zidane.jpg

","Option 1
You could simply use Starlette's path convertor to capture arbitrary paths. As per Starlette documentation, path returns the rest of the path, including any additional / characters. However, if your URL includes query parameters as well, you should append the query string to the path, as shown below:
from fastapi import FastAPI, Request

app = FastAPI()


@app.get('/{_:path}')
async def pred_image(request: Request):
    url = request.url.path[1:] if not request.url.query else request.url.path[1:] + &quot;?&quot; + request.url.query
    return {'url': url}

or:
@app.get('/{full_path:path}')
async def pred_image(full_path: str, request: Request):
    url = full_path if not request.url.query else full_path + &quot;?&quot; + request.url.query
    return {'url': url}

or, using split() to get the whole URL after the 3rd occurence of / character:
@app.get('/{_:path}')
async def pred_image(request: Request):
    url = request.url._url.split('/', 3)[-1]
    return {'url': url}

Test using the link below:
http://127.0.0.1:8000/https://www.google.com/search?q=my+query

Please note that the URL above will automatically be encoded by the web browser (as URLs can only contain ASCII characters), meaning that before sending the request, any special characters will be converted to other reserved ones, using the % sign followed by a hexadecimal pair. Hence, behind the scenes the request URL would look like this:
http://127.0.0.1:8000/https%3A%2F%2Fwww.google.com%2Fsearch%3Fq%3Dmy%2Bquery

If, for instance, you would like to test the endpoint through other clients, such as Python requests lib, you should then encode the URL yourself before sending the request. You could do that using Python's urllib.parse.quote() function, as shown below. Note that the quote() function considers / characters safe by default, meaning that any / characters won't be encoded. Hence, in this case, you should set the safe parameter to '' (i.e., empty string), in order for the / characters to be encoded as well.
Test using Python requests:
import requests
from urllib.parse import quote 

base_url = 'http://127.0.0.1:8000/'
path_param = 'https://www.google.com/search?q=my+query'
url = base_url + quote(path_param, safe='')
r = requests.get(url)
print(r.json())

Output:
{'url': 'https://www.google.com/search?q=my+query'}

Test using HTML &lt;form&gt;:
If you would like to test the above by passing the URL through an HTML &lt;form&gt;, instead of manually typing it after the base URL on your own, please have a look at Option 3 of this answer, which demonstrates how to convert a form &lt;input&gt; element into a path parameter on &lt;form&gt; submission. The encoding of the URL takes place behind the scences when the &lt;form&gt; is submitted, likely using functions such as encodeURIComponent() or encodeURI(); hence, there is no need for you to apply any techniques, in order to quote the URL beforehand, such as in Python requests earlier.
Option 2
As @luk2302 mentioned in the comments section, your client (i.e., end user, javascript, etc.) needs to encode the URL. The encoded URL, however, as provided by @luk2302 does not seem to work, leading to a &quot;detail&quot;: &quot;Not Found&quot; error. The reason is simply because when FastAPI decodes the complete request URL (i.e., request.url), any %2F characters are converted back to /, and hence, it recognizes those forward slashes as path separators and attempts to find a matching API route. Thus, you would need to encode/decode the URL twice, in order for this approach to work.
On server side, you can decode the URL (twice) as follows:
from urllib.parse import unquote 

@app.get('/{path}')
async def pred_image(path: str):
    return {'url': unquote(unquote(path))}  

Test using the link below:
http://127.0.0.1:8000/https%253A%252F%252Fwww.google.com%252Fsearch%253Fq%253Dmy%252Bquery

Test using Python requests:
import requests
from urllib.parse import quote 

base_url = 'http://127.0.0.1:8000/'
path_param = 'https://www.google.com/search?q=my+query'
url = base_url + quote(quote(path_param, safe=''), safe='')
r = requests.get(url)
print(r.json())

Option 3
Use a query parameter instead, as shown below:
@app.get('/')
async def pred_image(url: str):
    return {'url': url}

Test using the link below:
http://127.0.0.1:8000/?url=https://www.google.com/search?q=my+query

If, again, you would like to use Python requests lib to test the endpoint above, have a look at the example below. Note that since the image URL is now being sent as part of the query string (i.e., as a query parameter), requests will take care of the URL encoding; hence, there is no need for using the quote() function this time.
Test using Python requests:
import requests

base_url = 'http://127.0.0.1:8000/'
params = {'url': 'https://www.google.com/search?q=my+query'}
r = requests.get(base_url, params=params)
print(r.json())

Option 4
Since your endpoint seems to accept POST requests, you might consider having the client send the image URL in the body of the request, instead of passing it as a path parameter. Please have a look at the answers here, here and here, as well as FastAPI's documentation, on how to do that.

Note
If you are testing this by typing the aforementioned URLs into the address bar of a web browser, then keep using @app.get() routes, as when you type a URL in the address bar of your web browser, it performs a GET request. If, however, you need this to work with POST requests, you will have to change the endpoint's decorator to @app.post() (as shown in your question); otherwise, you will come across 405 &quot;Method Not Allowed&quot; error (see here for more details on such errors).
Finally, the endpoints in the examples above have been defined with async def, but depending on the operations that may take place inside the endpoints of your application, you may need to define them with normal def, or use other solutions, when blocking operations need to be performed. Hence, I would suggest having a look at this answer to better understand these concepts and when to use async def/def.
"
"I am trying to sift through a big database that is compressed in a .zst. I am aware that I can simply just decompress it and then work on the resulting file, but that uses up a lot of space on my ssd and takes 2+ hours so I would like to avoid that if possible.
Often when I work with large files I would stream it line by line with code like
with open(filename) as f:
    for line in f.readlines():
        do_something(line)

I know gzip has this
with gzip.open(filename,'rt') as f:
    for line in f:
        do_something(line)

but it doesn't seem to work with .zsf, so I am wondering if there're any libraries that can decompress and stream the decompressed data in a similar way. For example:
with zstlib.open(filename) as f:
    for line in f.zstreadlines():
        do_something(line)

","Knowing which package to use and what the corresponding docs are can be a bit confusing, as there appears to be several Python bindings to the actual Zstandard library.
Below, I am referring to the library by Gregory Szorc, that I installed from condas default channel with:
conda install zstd

# check:

conda list zstd
# # Name                    Version                   Build  Channel
# zstd                      1.5.5                hc292b87_0  

(even though the docs say to install with pip, which I don't unless there is no other way, as I like my conda environments to remain usable).
I am only inferring that this version is the one from G. Szorc, based on the comments in the __init__.py file:
# Copyright (c) 2017-present, Gregory Szorc
# All rights reserved.
#
# This software may be modified and distributed under the terms
# of the BSD license. See the LICENSE file for details.

&quot;&quot;&quot;Python interface to the Zstandard (zstd) compression library.&quot;&quot;&quot;

from __future__ import absolute_import, unicode_literals

# This module serves 2 roles:
#
# 1) Export the C or CFFI &quot;backend&quot; through a central module.
# 2) Implement additional functionality built on top of C or CFFI backend.

Thus, I think that the corresponding documentation is here.
In any case, quick test after install:
import zstandard as zstd

with zstd.open('test.zstd', 'w') as f:
    for i in range(10_000):
        f.write(f'foo {i} bar\n')

with zstd.open('test.zstd', 'r') as f:
    for i, line in enumerate(f):
        if i % 1000 == 0:
            print(f'line {i:4d}: {line}', end='')

Produces:
line    0: foo 0 bar
line 1000: foo 1000 bar
line 2000: foo 2000 bar
line 3000: foo 3000 bar
line 4000: foo 4000 bar
line 5000: foo 5000 bar
line 6000: foo 6000 bar
line 7000: foo 7000 bar
line 8000: foo 8000 bar
line 9000: foo 9000 bar

Notes:

if the file was written in binary (not text), then use mode='rb', same as a regular file. The underlying file is always written in binary mode, but if we use text mode for open, then according to open's doc, &quot;(...) an io.TextIOWrapper if opened for reading or writing in text mode&quot;.
notice that I use the iterator of f, not readlines(). From the inline docstring, they make it sound like readlines() returns a list of lines from the file, i.e. the whole thing is slurped in memory. With the iterator, it is more likely that only portions of the file are in memory at any moment (in zstd's buffer).
Reading this part of the docs however, I am less sure of the above. Stay tuned... (Edit: tested empirically, it holds, see below).

Addendum
ABout notes 2 and 3 above: I tested empirically, by changing the number of lines to 100 millions and compared the memory usage of two versions (using htop):
Streaming version
with zstd.open('test.zstd', 'r') as f:
    for i, line in enumerate(f):
        if i % 10_000_000 == 0:
            print(f'line {i:8d}: {line}', end='')

--no bump in memory usage.
Readlines version
with zstd.open('test.zstd', 'r') as f:
    for i, line in enumerate(f.readlines()):
        if i % 10_000_000 == 0:
            print(f'line {i:8d}: {line}', end='')

--bump in memory usage by a few GBs.
This may be specific to the version installed (1.5.5).
"
"Consider a Python protocol attribute which is also annotated with a protocol. I found in that case, both mypy and Pyright report an error even when my custom datatype follows the nested protocol. For example in the code below Outer follows the HasHasA protocol in that it has hasa: HasA because Inner follows HasA protocol.
from dataclasses import dataclass
from typing import Protocol

class HasA(Protocol):
    a: int

class HasHasA(Protocol):
    hasa: HasA

@dataclass
class Inner:
    a: int

@dataclass
class Outer:
    hasa: Inner

def func(b: HasHasA): ...

o = Outer(Inner(0))
func(o)

However, mypy shows the following error.
nested_protocol.py:22: error: Argument 1 to &quot;func&quot; has incompatible type &quot;Outer&quot;; expected &quot;HasHasA&quot;  [arg-type]
nested_protocol.py:22: note: Following member(s) of &quot;Outer&quot; have conflicts:
nested_protocol.py:22: note:     hasa: expected &quot;HasA&quot;, got &quot;Inner&quot;

What's wrong with my code?
","There's an issue on GitHub which is almost exactly the same as your example. I think the motivating case on the mypy docs explains quite well why this is illegal. Bringing a structural analogy to your example, let's fill in an implementation for func and tweak Inner slightly:
def func(b: HasHasA) -&gt; None:
    b.hasa.a += 100 - 100

@dataclass
class Inner:
    a: bool

o = Outer(Inner(bool(0)))
func(o)
if o.hasa.a is False:
    print(&quot;Oh no! This is still False!&quot;)
else:
    print(&quot;This is true now!&quot;)

This is of course a contrived example, but it shows that if the type-checker didn't warn you against this, the inner protocol can type-widen the inner type and perform value mutation, and you may silently perform type-unsafe operations.
As suggested by the mypy documentation, the solution is to make the outer protocol's variable read-only:
class HasHasA(Protocol):
    @property
    def hasa(self) -&gt; HasA:
        ...

"
"I am trying to convert detr model to tensor flow using onnx. I converted the model using torch.onnx.export with opset_version=12.(which produces a detr.onnx file)
Then I tried to convert the onnx file to tensorflow model using this example. I added onnx.check_model line to make sure model is loaded correctly.
import math
from PIL import Image
import requests
import matplotlib.pyplot as plt
import torch
from torch import nn
from torchvision.models import resnet50
import onnx
from onnx_tf.backend import prepare
import torchvision.transforms as T

torch.set_grad_enabled(False)
model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
im = Image.open(requests.get(url, stream=True).raw)
transform = T.Compose([
    T.Resize(800),
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
img = transform(im).unsqueeze(0)

torch.onnx.export(model, img, 'detr.onnx', opset_version = 12)
    
onnx_model = onnx.load('./detr.onnx')
    
result = onnx.checker.check_model(onnx_model)
    
tf_rep = prepare(onnx_model)
tf_rep.export_graph('./model.pb')

This code raises an exception when it reaches    tf_rep.export_graph('./model.pb') line.
onnx version = 1.13.0 , torch version = 1.13.0+cu117 , onnx_tf = 1.10.0
message of exception :
KeyError                                  Traceback (most recent call last)
Cell In[19], line 26
     23 result = onnx.checker.check_model(onnx_model)
     25 tf_rep = prepare(onnx_model)
---&gt; 26 tf_rep.export_graph('./model.pb')

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\onnx_tf\backend_rep.py:143, in TensorflowRep.export_graph(self, path)
    129 &quot;&quot;&quot;Export backend representation to a Tensorflow proto file.
    130 
    131 This function obtains the graph proto corresponding to the ONNX
   (...)
    137 :returns: none.
    138 &quot;&quot;&quot;
    139 self.tf_module.is_export = True
    140 tf.saved_model.save(
    141     self.tf_module,
    142     path,
--&gt; 143     signatures=self.tf_module.__call__.get_concrete_function(
    144         **self.signatures))
    145 self.tf_module.is_export = False

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\eager\def_function.py:1239, in Function.get_concrete_function(self, *args, **kwargs)
   1237 def get_concrete_function(self, *args, **kwargs):
   1238   # Implements GenericFunction.get_concrete_function.
-&gt; 1239   concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
   1240   concrete._garbage_collector.release()  # pylint: disable=protected-access
   1241   return concrete

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\eager\def_function.py:1219, in Function._get_concrete_function_garbage_collected(self, *args, **kwargs)
   1217   if self._stateful_fn is None:
   1218     initializers = []
-&gt; 1219     self._initialize(args, kwargs, add_initializers_to=initializers)
   1220     self._initialize_uninitialized_variables(initializers)
   1222 if self._created_variables:
   1223   # In this case we have created variables on the first call, so we run the
   1224   # defunned version which is guaranteed to never create variables.

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\eager\def_function.py:785, in Function._initialize(self, args, kwds, add_initializers_to)
    782 self._lifted_initializer_graph = lifted_initializer_graph
    783 self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)
    784 self._concrete_stateful_fn = (
--&gt; 785     self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
    786         *args, **kwds))
    788 def invalid_creator_scope(*unused_args, **unused_kwds):
    789   &quot;&quot;&quot;Disables variable creation.&quot;&quot;&quot;

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\eager\function.py:2523, in Function._get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2521   args, kwargs = None, None
   2522 with self._lock:
-&gt; 2523   graph_function, _ = self._maybe_define_function(args, kwargs)
   2524 return graph_function

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\eager\function.py:2760, in Function._maybe_define_function(self, args, kwargs)
   2758   # Only get placeholders for arguments, not captures
   2759   args, kwargs = placeholder_dict[&quot;args&quot;]
-&gt; 2760 graph_function = self._create_graph_function(args, kwargs)
   2762 graph_capture_container = graph_function.graph._capture_func_lib  # pylint: disable=protected-access
   2763 # Maintain the list of all captures

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\eager\function.py:2670, in Function._create_graph_function(self, args, kwargs)
   2665 missing_arg_names = [
   2666     &quot;%s_%d&quot; % (arg, i) for i, arg in enumerate(missing_arg_names)
   2667 ]
   2668 arg_names = base_arg_names + missing_arg_names
   2669 graph_function = ConcreteFunction(
-&gt; 2670     func_graph_module.func_graph_from_py_func(
   2671         self._name,
   2672         self._python_function,
   2673         args,
   2674         kwargs,
   2675         self.input_signature,
   2676         autograph=self._autograph,
   2677         autograph_options=self._autograph_options,
   2678         arg_names=arg_names,
   2679         capture_by_value=self._capture_by_value),
   2680     self._function_attributes,
   2681     spec=self.function_spec,
   2682     # Tell the ConcreteFunction to clean up its graph once it goes out of
   2683     # scope. This is not the default behavior since it gets used in some
   2684     # places (like Keras) where the FuncGraph lives longer than the
   2685     # ConcreteFunction.
   2686     shared_func_graph=False)
   2687 return graph_function

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\framework\func_graph.py:1247, in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)
   1244 else:
   1245   _, original_func = tf_decorator.unwrap(python_func)
-&gt; 1247 func_outputs = python_func(*func_args, **func_kwargs)
   1249 # invariant: `func_outputs` contains only Tensors, CompositeTensors,
   1250 # TensorArrays and `None`s.
   1251 func_outputs = nest.map_structure(
   1252     convert, func_outputs, expand_composites=True)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\eager\def_function.py:677, in Function._defun_with_scope.&lt;locals&gt;.wrapped_fn(*args, **kwds)
    673 with default_graph._variable_creator_scope(scope, priority=50):  # pylint: disable=protected-access
    674   # __wrapped__ allows AutoGraph to swap in a converted function. We give
    675   # the function a weak reference to itself to avoid a reference cycle.
    676   with OptionalXlaContext(compile_with_xla):
--&gt; 677     out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    678   return out

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\eager\function.py:3317, in class_method_to_instance_method.&lt;locals&gt;.bound_method_wrapper(*args, **kwargs)
   3312   return wrapped_fn(weak_instance(), *args, **kwargs)
   3314 # If __wrapped__ was replaced, then it is always an unbound function.
   3315 # However, the replacer is still responsible for attaching self properly.
   3316 # TODO(mdan): Is it possible to do it here instead?
-&gt; 3317 return wrapped_fn(*args, **kwargs)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\framework\func_graph.py:1233, in func_graph_from_py_func.&lt;locals&gt;.autograph_handler(*args, **kwargs)
   1231 except Exception as e:  # pylint:disable=broad-except
   1232   if hasattr(e, &quot;ag_error_metadata&quot;):
-&gt; 1233     raise e.ag_error_metadata.to_exception(e)
   1234   else:
   1235     raise

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\framework\func_graph.py:1222, in func_graph_from_py_func.&lt;locals&gt;.autograph_handler(*args, **kwargs)
   1220 # TODO(mdan): Push this block higher in tf.function's call stack.
   1221 try:
-&gt; 1222   return autograph.converted_call(
   1223       original_func,
   1224       args,
   1225       kwargs,
   1226       options=autograph.ConversionOptions(
   1227           recursive=True,
   1228           optional_features=autograph_options,
   1229           user_requested=True,
   1230       ))
   1231 except Exception as e:  # pylint:disable=broad-except
   1232   if hasattr(e, &quot;ag_error_metadata&quot;):

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\impl\api.py:439, in converted_call(f, args, kwargs, caller_fn_scope, options)
    437 try:
    438   if kwargs is not None:
--&gt; 439     result = converted_f(*effective_args, **kwargs)
    440   else:
    441     result = converted_f(*effective_args)

File ~\AppData\Local\Temp\__autograph_generated_fileq0h7j9t_.py:30, in outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.tf____call__(self, **kwargs)
     28 node = ag__.Undefined('node')
     29 onnx_node = ag__.Undefined('onnx_node')
---&gt; 30 ag__.for_stmt(ag__.ld(self).graph_def.node, None, loop_body, get_state, set_state, (), {'iterate_names': 'node'})
     31 outputs = ag__.converted_call(ag__.ld(dict), (), None, fscope)
     33 def get_state_4():

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py:463, in for_stmt(iter_, extra_test, body, get_state, set_state, symbol_names, opts)
    459   _tf_distributed_iterable_for_stmt(
    460       iter_, extra_test, body, get_state, set_state, symbol_names, opts)
    462 else:
--&gt; 463   _py_for_stmt(iter_, extra_test, body, None, None)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py:512, in _py_for_stmt(***failed resolving arguments***)
    510 else:
    511   for target in iter_:
--&gt; 512     body(target)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py:478, in _py_for_stmt.&lt;locals&gt;.protected_body(protected_iter)
    477 def protected_body(protected_iter):
--&gt; 478   original_body(protected_iter)
    479   after_iteration()
    480   before_iteration()

File ~\AppData\Local\Temp\__autograph_generated_fileq0h7j9t_.py:23, in outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.tf____call__.&lt;locals&gt;.loop_body(itr)
     21 node = itr
     22 onnx_node = ag__.converted_call(ag__.ld(OnnxNode), (ag__.ld(node),), None, fscope)
---&gt; 23 output_ops = ag__.converted_call(ag__.ld(self).backend._onnx_node_to_tensorflow_op, (ag__.ld(onnx_node), ag__.ld(tensor_dict), ag__.ld(self).handlers), dict(opset=ag__.ld(self).opset, strict=ag__.ld(self).strict), fscope)
     24 curr_node_output_map = ag__.converted_call(ag__.ld(dict), (ag__.converted_call(ag__.ld(zip), (ag__.ld(onnx_node).outputs, ag__.ld(output_ops)), None, fscope),), None, fscope)
     25 ag__.converted_call(ag__.ld(tensor_dict).update, (ag__.ld(curr_node_output_map),), None, fscope)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\impl\api.py:439, in converted_call(f, args, kwargs, caller_fn_scope, options)
    437 try:
    438   if kwargs is not None:
--&gt; 439     result = converted_f(*effective_args, **kwargs)
    440   else:
    441     result = converted_f(*effective_args)

File ~\AppData\Local\Temp\__autograph_generated_filetsq4l59p.py:62, in outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.tf___onnx_node_to_tensorflow_op(cls, node, tensor_dict, handlers, opset, strict)
     60     pass
     61 handler = ag__.Undefined('handler')
---&gt; 62 ag__.if_stmt(ag__.ld(handlers), if_body_1, else_body_1, get_state_1, set_state_1, ('do_return', 'retval_'), 2)
     64 def get_state_2():
     65     return ()

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py:1363, in if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)
   1361   _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)
   1362 else:
-&gt; 1363   _py_if_stmt(cond, body, orelse)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py:1416, in _py_if_stmt(cond, body, orelse)
   1414 def _py_if_stmt(cond, body, orelse):
   1415   &quot;&quot;&quot;Overload of if_stmt that executes a Python if statement.&quot;&quot;&quot;
-&gt; 1416   return body() if cond else orelse()

File ~\AppData\Local\Temp\__autograph_generated_filetsq4l59p.py:56, in outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.tf___onnx_node_to_tensorflow_op.&lt;locals&gt;.if_body_1()
     54     nonlocal retval_, do_return
     55     pass
---&gt; 56 ag__.if_stmt(ag__.ld(handler), if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py:1363, in if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)
   1361   _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)
   1362 else:
-&gt; 1363   _py_if_stmt(cond, body, orelse)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py:1416, in _py_if_stmt(cond, body, orelse)
   1414 def _py_if_stmt(cond, body, orelse):
   1415   &quot;&quot;&quot;Overload of if_stmt that executes a Python if statement.&quot;&quot;&quot;
-&gt; 1416   return body() if cond else orelse()

File ~\AppData\Local\Temp\__autograph_generated_filetsq4l59p.py:48, in outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.tf___onnx_node_to_tensorflow_op.&lt;locals&gt;.if_body_1.&lt;locals&gt;.if_body()
     46 try:
     47     do_return = True
---&gt; 48     retval_ = ag__.converted_call(ag__.ld(handler).handle, (ag__.ld(node),), dict(tensor_dict=ag__.ld(tensor_dict), strict=ag__.ld(strict)), fscope)
     49 except:
     50     do_return = False

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\impl\api.py:439, in converted_call(f, args, kwargs, caller_fn_scope, options)
    437 try:
    438   if kwargs is not None:
--&gt; 439     result = converted_f(*effective_args, **kwargs)
    440   else:
    441     result = converted_f(*effective_args)

File ~\AppData\Local\Temp\__autograph_generated_filec7_esoft.py:41, in outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.tf__handle(cls, node, **kwargs)
     39     nonlocal retval_, do_return
     40     raise ag__.converted_call(ag__.ld(BackendIsNotSupposedToImplementIt), (ag__.converted_call('{} version {} is not implemented.'.format, (ag__.ld(node).op_type, ag__.ld(cls).SINCE_VERSION), None, fscope),), None, fscope)
---&gt; 41 ag__.if_stmt(ag__.ld(ver_handle), if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)
     42 return fscope.ret(retval_, do_return)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py:1363, in if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)
   1361   _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)
   1362 else:
-&gt; 1363   _py_if_stmt(cond, body, orelse)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py:1416, in _py_if_stmt(cond, body, orelse)
   1414 def _py_if_stmt(cond, body, orelse):
   1415   &quot;&quot;&quot;Overload of if_stmt that executes a Python if statement.&quot;&quot;&quot;
-&gt; 1416   return body() if cond else orelse()

File ~\AppData\Local\Temp\__autograph_generated_filec7_esoft.py:33, in outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.tf__handle.&lt;locals&gt;.if_body()
     31 try:
     32     do_return = True
---&gt; 33     retval_ = ag__.converted_call(ag__.ld(ver_handle), (ag__.ld(node),), dict(**ag__.ld(kwargs)), fscope)
     34 except:
     35     do_return = False

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\impl\api.py:439, in converted_call(f, args, kwargs, caller_fn_scope, options)
    437 try:
    438   if kwargs is not None:
--&gt; 439     result = converted_f(*effective_args, **kwargs)
    440   else:
    441     result = converted_f(*effective_args)

File ~\AppData\Local\Temp\__autograph_generated_filevddqx9qt.py:12, in outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.tf__version(cls, node, **kwargs)
     10 try:
     11     do_return = True
---&gt; 12     retval_ = ag__.converted_call(ag__.ld(cls)._common, (ag__.ld(node),), dict(**ag__.ld(kwargs)), fscope)
     13 except:
     14     do_return = False

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\impl\api.py:439, in converted_call(f, args, kwargs, caller_fn_scope, options)
    437 try:
    438   if kwargs is not None:
--&gt; 439     result = converted_f(*effective_args, **kwargs)
    440   else:
    441     result = converted_f(*effective_args)

File ~\AppData\Local\Temp\__autograph_generated_filedezd6jrz.py:122, in outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.tf___common(cls, node, **kwargs)
    120 paddings = ag__.Undefined('paddings')
    121 constant_values = ag__.Undefined('constant_values')
--&gt; 122 ag__.if_stmt(ag__.ld(cls).SINCE_VERSION &lt; 11, if_body_1, else_body_1, get_state_1, set_state_1, ('constant_values', 'paddings'), 2)
    123 cond = ag__.converted_call(ag__.ld(tf).cond, (ag__.converted_call(ag__.ld(check_positive), (ag__.ld(paddings),), None, fscope), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(process_pos_pads), (ag__.ld(x), ag__.ld(paddings), ag__.ld(constant_values)), None, fscope)), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(process_neg_pads), (ag__.ld(x), ag__.ld(paddings), ag__.ld(constant_values)), None, fscope))), None, fscope)
    124 try:

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py:1363, in if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)
   1361   _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)
   1362 else:
-&gt; 1363   _py_if_stmt(cond, body, orelse)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py:1416, in _py_if_stmt(cond, body, orelse)
   1414 def _py_if_stmt(cond, body, orelse):
   1415   &quot;&quot;&quot;Overload of if_stmt that executes a Python if statement.&quot;&quot;&quot;
-&gt; 1416   return body() if cond else orelse()

File ~\AppData\Local\Temp\__autograph_generated_filedezd6jrz.py:119, in outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.tf___common.&lt;locals&gt;.else_body_1()
    117 nonlocal paddings, constant_values
    118 paddings = ag__.ld(tensor_dict)[ag__.ld(node).inputs[1]]
--&gt; 119 constant_values = ag__.if_exp(ag__.converted_call(ag__.ld(len), (ag__.ld(node).inputs,), None, fscope) == 3, lambda : ag__.ld(tensor_dict)[ag__.ld(node).inputs[2]], lambda : 0, 'ag__.converted_call(len, (node.inputs,), None, fscope) == 3')

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\conditional_expressions.py:27, in if_exp(cond, if_true, if_false, expr_repr)
     25   return _tf_if_exp(cond, if_true, if_false, expr_repr)
     26 else:
---&gt; 27   return _py_if_exp(cond, if_true, if_false)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\conditional_expressions.py:52, in _py_if_exp(cond, if_true, if_false)
     51 def _py_if_exp(cond, if_true, if_false):
---&gt; 52   return if_true() if cond else if_false()

File ~\AppData\Local\Temp\__autograph_generated_filedezd6jrz.py:119, in outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.tf___common.&lt;locals&gt;.else_body_1.&lt;locals&gt;.&lt;lambda&gt;()
    117 nonlocal paddings, constant_values
    118 paddings = ag__.ld(tensor_dict)[ag__.ld(node).inputs[1]]
--&gt; 119 constant_values = ag__.if_exp(ag__.converted_call(ag__.ld(len), (ag__.ld(node).inputs,), None, fscope) == 3, lambda : ag__.ld(tensor_dict)[ag__.ld(node).inputs[2]], lambda : 0, 'ag__.converted_call(len, (node.inputs,), None, fscope) == 3')

KeyError: in user code:

    File &quot;C:\Users\alihe\AppData\Local\Programs\Python\Python39\lib\site-packages\onnx_tf\backend_tf_module.py&quot;, line 99, in __call__  *
        output_ops = self.backend._onnx_node_to_tensorflow_op(onnx_node,
    File &quot;C:\Users\alihe\AppData\Local\Programs\Python\Python39\lib\site-packages\onnx_tf\backend.py&quot;, line 347, in _onnx_node_to_tensorflow_op  *
        return handler.handle(node, tensor_dict=tensor_dict, strict=strict)
    File &quot;C:\Users\alihe\AppData\Local\Programs\Python\Python39\lib\site-packages\onnx_tf\handlers\handler.py&quot;, line 59, in handle  *
        return ver_handle(node, **kwargs)
    File &quot;C:\Users\alihe\AppData\Local\Programs\Python\Python39\lib\site-packages\onnx_tf\handlers\backend\pad.py&quot;, line 91, in version_11  *
        return cls._common(node, **kwargs)
    File &quot;C:\Users\alihe\AppData\Local\Programs\Python\Python39\lib\site-packages\onnx_tf\handlers\backend\pad.py&quot;, line 73, in _common  *
        constant_values = tensor_dict[node.inputs[2]] if len(

    KeyError: ''

","The problem that you are facing is due to the use of dynamic padding instead of static pad shape at source of the model. This is exposed when you lower the onnx opset version during export.
import warnings
warnings.filterwarnings(&quot;ignore&quot;)

#import onnxruntime
import math
from PIL import Image
import requests
import matplotlib.pyplot as plt
import torch
from torch import nn
from torchvision.models import resnet50
import torchvision.transforms as T
import onnx
from onnx_tf.backend import prepare
#from onnxsim import simplify

torch.set_grad_enabled(False)
model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
im = Image.open(requests.get(url, stream=True).raw)
transform = T.Compose([
    T.Resize(800),
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
img = transform(im).unsqueeze(0)

model.eval()
torch.onnx.export(model, img, 'detr.onnx', opset_version = 10)
    
onnx_model = onnx.load('./detr.onnx')

#onnx_model, _ = simplify(model)
    
result = onnx.checker.check_model(onnx_model)
    
tf_rep = prepare(onnx_model)
tf_rep.export_graph('./model.pb')

Which throws the following output:
SymbolicValueError                        Traceback (most recent call last)
c:\Anaconda3\envs\workenv\lib\site-packages\torch\onnx\symbolic_opset9.py in _convert_padding_node(input)
   1821         try:
-&gt; 1822             padding = [
   1823                 symbolic_helper._get_const(v, &quot;i&quot;, &quot;padding&quot;) for v in input_list

c:\Anaconda3\envs\workenv\lib\site-packages\torch\onnx\symbolic_opset9.py in &lt;listcomp&gt;(.0)
   1822             padding = [
-&gt; 1823                 symbolic_helper._get_const(v, &quot;i&quot;, &quot;padding&quot;) for v in input_list
   1824             ]

c:\Anaconda3\envs\workenv\lib\site-packages\torch\onnx\symbolic_helper.py in _get_const(value, desc, arg_name)
    169     if not _is_constant(value):
--&gt; 170         raise errors.SymbolicValueError(
    171             f&quot;ONNX symbolic expected a constant value of the '{arg_name}' argument, &quot;

SymbolicValueError: ONNX symbolic expected a constant value of the 'padding' argument, got '509 defined in (%509 : Long(requires_grad=0, device=cpu) = onnx::Sub(%max_size_i, %498), scope: models.detr.DETR:: # C:\Users\Anurag/.cache\torch\hub\facebookresearch_detr_main\util\misc.py:349:0
)'  [Caused by the value '509 defined in (%509 : Long(requires_grad=0, device=cpu) = onnx::Sub(%max_size_i, %498), scope: models.detr.DETR:: # C:\Users\Anurag/.cache\torch\hub\facebookresearch_detr_main\util\misc.py:349:0
)' (type 'Tensor') in the TorchScript graph. The containing node has kind 'onnx::Sub'.] 
    (node defined in C:\Users\Anurag/.cache\torch\hub\facebookresearch_detr_main\util\misc.py(349): &lt;listcomp&gt;
C:\Users\Anurag/.cache\torch\hub\facebookresearch_detr_main\util\misc.py(349): _onnx_nested_tensor_from_tensor_list
C:\Users\Anurag/.cache\torch\hub\facebookresearch_detr_main\util\misc.py(313): nested_tensor_from_tensor_list
C:\Users\Anurag/.cache\torch\hub\facebookresearch_detr_main\models\detr.py(60): forward
c:\Anaconda3\envs\workenv\lib\site-packages\torch\nn\modules\module.py(1182): _slow_forward
...
        #5: 507 defined in (%507 : Long(requires_grad=0, device=cpu) = onnx::Sub(%478, %466), scope: models.detr.DETR:: # C:\Users\Anurag/.cache\torch\hub\facebookresearch_detr_main\util\misc.py:349:0
    )  (type 'Tensor')
    Outputs:
        #0: 510 defined in (%510 : int[] = prim::ListConstruct(%459, %509, %459, %508, %459, %507), scope: models.detr.DETR::
    )  (type 'List[int]')

My suggestion would be to pick up the model from another source.
For reference have a look at: ONNX symbolic expected a constant value of the padding argument
"
"In dplyr package of R, there's the option .keep = &quot;unused&quot; when creating new columns with the function mutate() (which is their equivalent of assign).
An example, for those who haven't used it:
&gt; head(iris)
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
4          4.6         3.1          1.5         0.2  setosa
5          5.0         3.6          1.4         0.2  setosa
6          5.4         3.9          1.7         0.4  setosa

# any column used in creating `new_col` is dropped afterwards automatically
&gt; mutate(.data = head(iris), new_col = Sepal.Length + Petal.Length * Petal.Width, .keep = &quot;unused&quot;)
  Sepal.Width Species new_col
1         3.5  setosa    5.38
2         3.0  setosa    5.18
3         3.2  setosa    4.96
4         3.1  setosa    4.90
5         3.6  setosa    5.28
6         3.9  setosa    6.08

I say they are equivalent, but there doesn't appear to be the option for doing this with assign in the Pandas documentation so I assume it doesn't exist. I was curious about creating a way of doing something similar then.
One way I can think of to do this is to create a list of names beforehand, and drop them afterwards, like this:
from sklearn import datasets
import pandas as pd

used_columns = ['sepal length (cm)', 'petal length (cm)', 'petal width (cm)']

iris = pd.DataFrame(datasets.load_iris().data, columns=datasets.load_iris().feature_names)

iris.assign(new_col = lambda x: x['sepal length (cm)'] + x['petal length (cm)'] * x['petal width (cm)']).drop(used_columns, axis=1)

or
iris.assign(new_col = lambda x: x[used_columns[0]] + x[used_columns[1]] * x[used_columns[2]]).drop(used_columns, axis=1)

Which seems  ~fine~, but requires a separate list, and with the first one, keeping two things updated, and with the second, the cognitive load of keeping track of what the nth list item is in my head.
So I was curious if there's another way I'm not aware of of doing this, that would be easier to maintain? Both of the ones above seem not very Pythonic?
Research I've done: I did a bunch of googling around this, with no luck. It seems there's plenty of ways of dropping columns, but none I've found seem particularly well-suited to this type of situation. Any help you could provide would be much appreciated! Answers which use other Python packages (e.g. janitor) are okay too.
","I've never used R but based on the definition of unused and AFIK, to simulate the same behaviour in pandas, you will need to pop each column from a copy of the original DataFrame :

&quot;unused&quot; retains only the columns not used in ... to create new columns. This is useful if you generate new columns, but no longer need the columns used to generate them.
DataFrame.pop(item) returns item and drops from frame. Raises KeyError if not found.


(
    iris.copy().assign(
        new_col= lambda x: x.pop('sepal length (cm)')
        + x.pop('petal length (cm)') * x.pop('petal width (cm)'))
)

Output :
     sepal width (cm)  new_col
0                 3.5     5.38
1                 3.0     5.18
2                 3.2     4.96
3                 3.1     4.90
4                 3.6     5.28
..                ...      ...
145               3.0    18.66
146               2.5    15.80
147               3.0    16.90
148               3.4    18.62
149               3.0    15.08

[150 rows x 2 columns]

"
"The documentation for np.typing.NDArray says that it is &quot;a generic version of np.ndarray[Any, np.dtype[+ScalarType]]&quot;. Where is the generalization in &quot;generic&quot; happening?
And in the documentation for numpy.ndarray.__class_getitem__ we have this example np.ndarray[Any, np.dtype[Any]] with no explanation as to what the two arguments are.
And why can I do np.ndarray[float], ie just use one argument? What does that mean?
","Note from the future: as of NumPy 2.0 the docs is more explicit to say

A np.ndarray[Any, np.dtype[+ScalarType]] type alias generic w.r.t. its dtype.type.

and as of 2.2 (dev docs currently) the type alias is changed to NDArray = np.ndarray[tuple[int, ...], np.dtype[+ScalarType]]. This now makes it clearer what the type alias represents, and what I concluded in my original answer below.

&quot;Generic&quot; in this context means &quot;generic type&quot; (see also the Glossary), typing-related objects that can be subscripted to generate more specific type &quot;instances&quot; (apologies for the sloppy jargon, I'm not well-versed in typing talk). Think typing.List that lets you use List[int] to denote a homogeneous list of ints.
As of Python 3.9 most standard-library collections have been upgraded to be compatible with typing as generic types themselves. Since tuple[foo] used to be invalid until 3.9, it was safe to allow tuple[int, int] to mean the same thing that typing.Tuple[int, int] used to mean: a tuple of two integers.
So as of 3.9 NumPy also allows using the np.ndarray type as a generic, this is what np.ndarray[Any, np.dtype[Any]] does. This &quot;signature&quot; matches the actual signature of np.ndarray.__init__() (__new__() if we want to be correct):
class numpy.ndarray(shape, dtype=float, ...)

So what np.ndarray[foo, bar] does is create a type for type hinting that means &quot;a NumPy array of shape type foo and dtype bar&quot;. People normally don't call np.ndarray() directly anyway (rather using helpers such as np.array() or np.full_like() and the like), so this is doubly fine in NumPy.
Now, since most code runs with arrays of more than one possible number of dimensions, it would be a pain to have to specify an arbitrary number of lengths for the shape tuple (the first &quot;argument&quot; of np.ndarray as a generic type). I assume this was the motivation to define a type alias that is still a generic in the second &quot;argument&quot;. This is np.typing.NDArray.
It lets you easily type hint something as an array of a given type without having to say anything about the shape, covering a vast subset of use cases (which would otherwise use np.ndarray[typing.Any, ...]). And this is still a generic, since you can parameterise it with a dtype. To quote the docs:
&gt;&gt;&gt; print(npt.NDArray)
numpy.ndarray[typing.Any, numpy.dtype[+ScalarType]]

&gt;&gt;&gt; print(npt.NDArray[np.float64])
numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]]

As usual with generics, you're allowed to specify an argument to the generic type, but you're not required to. ScalarType is derived from np.generic, a base class that covers most (maybe all) NumPy scalar types. And the library code that defines NDArray is here, and is fairly transparent to the point of calling the helper _GenericAlias for older Python (a backport of typing.GenericAlias). What you have at the end is a type alias that is still generic in one variable.

To address the last part of your question:

And why can I do np.ndarray[float], ie just use one argument? What does that mean?

I think the anticlimactic explanation is that we again need to look at the signature of np.ndarray():
class numpy.ndarray(shape, dtype=float, buffer=None, offset=0, strides=None, order=None)

There's one mandatory parameter (shape), all the others are optional. So I believe that np.ndarray[float] specifies that it corresponds to arrays whose shape is of type float (i.e. nonsense). There's an explicit check to only allow 1 or 2 parameters in the generic type:
    args_len = PyTuple_Check(args) ? PyTuple_Size(args) : 1;
    if ((args_len &gt; 2) || (args_len == 0)) {
        return PyErr_Format(PyExc_TypeError,
                            &quot;Too %s arguments for %s&quot;,
                            args_len &gt; 2 ? &quot;many&quot; : &quot;few&quot;,
                            ((PyTypeObject *)cls)-&gt;tp_name);
    }
    generic_alias = Py_GenericAlias(cls, args);

This snippet checks that two arguments were passed to __class_getitem__, raises otherwise, and in the valid cases defers to the C API version of typing.GenericAlias.
I'm pretty sure that there's no technical reason to exclude the other parameters of the ndarray constructor from the generic type, but there was a semantic reason that the third parameter, buffer makes no sense to be included typing (or there was just a general push to reduce the generality of the generic type to most common use cases).
All that being said, I haven't been able to construct a small example in which a type passed for the shape of the generic type leads to type checking errors in mypy. From several attempts it seems as if the shape was always checked as if it were typing.Any rather than whatever was passed as the first parameter of np.ndarray[...]. For instance, consider the following example:
import numpy as np

first: np.ndarray[tuple[int], np.dtype[np.int64]] = np.arange(3)  # OK

second: np.ndarray[tuple[int], np.dtype[np.int64]] = np.arange(3.0)  # error due to dtype mismatch

third: np.ndarray[tuple[int, int, int], np.dtype[np.int64]] = np.arange(3)  # no error even though shape mismatch

Running mypy 0.991 on Python 3.9 on this gives
foo.py:5: error: Incompatible types in assignment (expression has type &quot;ndarray[Any, dtype[floating[Any]]]&quot;, variable has type &quot;ndarray[Tuple[int], dtype[signedinteger[_64Bit]]]&quot;)  [assignment]
Found 1 error in 1 file (checked 1 source file)

Only the dtype mismatch is found, but not the shape mismatch. And I see the same thing if I use np.ndarray((3,), dtype=...) instead of np.arange(), so it's not just due to weird typing of the np.arange() helper (although I used it as an example because this is one function that's guaranteed to return a 1d array). Since I can't explain this behaviour I can't be certain that my understanding is correct, but I have no better model.
To come back to a question you asked in a comment:

Right, so then am I right in understanding that np.ndarray[int] is like np.ndarray[Any, int]?

No, at least we can exlude this (and what we see here is consistent with the first parameter only affecting the shape to whatever extent it does affect it):
from typing import Any

import numpy as np

first: np.ndarray[Any, np.dtype[np.int_]] = np.arange(3)  # OK because dtype matches

second: np.ndarray[np.dtype[np.int_]] = np.arange(3)  # OK because shape check doesn't actually work, and dtype is left as &quot;any scalar&quot;

third: np.ndarray[Any, np.dtype[np.int_]] = np.arange(3.0)  # error due to dtype mismatch

fourth: np.ndarray[np.dtype[np.int_]] = np.arange(3.0)  # no error, so this can't be the same as the third option

The result from mypy:
foo.py:7: error: &quot;ndarray&quot; expects 2 type arguments, but 1 given  [type-arg]
foo.py:9: error: Incompatible types in assignment (expression has type &quot;ndarray[Any, dtype[floating[Any]]]&quot;, variable has type &quot;ndarray[Any, dtype[signedinteger[Any]]]&quot;)  [assignment]
foo.py:11: error: &quot;ndarray&quot; expects 2 type arguments, but 1 given  [type-arg]
Found 3 errors in 1 file (checked 1 source file)

The four cases:

first: explicitly typed as &quot;int array with any shape&quot;, no error on type correct assignment
second: typed as &quot;array with an int-typed shape and any dtype&quot;, should fail because that's nonsense but doesn't (see the earlier musing about the impotence of shape type checks)
third: explicitly typed as another &quot;int array with any shape&quot;, being assigned a double array, leading to an error
fourth: typed as &quot;array with an int-typed shape and any dtype&quot;, leading to no error (see second). Since the third case leads to an error and the fourth doesn't, they can't be aliases of one another.

Also notable that mypy complains about the two lines where np.ndarray[np.dtype[np.int_]] is present:

foo.py:7: error: &quot;ndarray&quot; expects 2 type arguments, but 1 given  [type-arg]

This sounds like a single-parameter use of the generic is forbidden as far as mypy is concerned. I'm not sure why this is the case, but this would certainly simplify the situation.
"
"
CentOS 7 (strict requirement)
Python 3.11 (strict requirement)

I had to upgrage a software and it requires now Python 3.11.
I followed instructions from Internet (https://linuxstans.com/how-to-install-python-centos/), and now Python 3.11 is installed, but cannot download anything, so all the programs that have something to do with Internet, including PIP, do not work because SSL package is not installed.
The normal way to install a Python-package is to use PIP, which doesn't work because the SSL package I'm going to install is not installed.
I tried all the advices in internet, but they are all outdated and not working any more, because they are either not for the 3.11 version of Python or not for CentOS 7.
The error I'm getting when running the application software:

ModuleNotFoundError: No module named '_ssl'

When I try to install ssl with pip:
# pip install --trusted-host pypi.org ssl
WARNING: pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.
WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(&quot;Can't connect to HTTPS URL because the SSL module is not available.&quot;)': /simple/ssl/
WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(&quot;Can't connect to HTTPS URL because the SSL module is not available.&quot;)': /simple/ssl/
WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(&quot;Can't connect to HTTPS URL because the SSL module is not available.&quot;)': /simple/ssl/
WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(&quot;Can't connect to HTTPS URL because the SSL module is not available.&quot;)': /simple/ssl/
WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(&quot;Can't connect to HTTPS URL because the SSL module is not available.&quot;)': /simple/ssl/
Could not fetch URL https://pypi.org/simple/ssl/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/ssl/ (Caused by SSLError(&quot;Can't connect to HTTPS URL because the SSL module is not available.&quot;)) - skipping
ERROR: Could not find a version that satisfies the requirement ssl (from versions: none)
ERROR: No matching distribution found for ssl
WARNING: pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.
Could not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError(&quot;Can't connect to HTTPS URL because the SSL module is not available.&quot;)) - skipping

I downloaded GZip files from https://pypi.org/simple/ssl/, unpacked them locally and tried to install them from local source, but PIP insists on HTTPS connection ... stupid tool.
What to do?
","How to get PIP and other HTTPS-based Python programs to work after upgrading to Python 3.11:
First of all: you don't necessarily need any magical tools like pyenv. May be pyenv would do these steps, but I'd like to understand what is happening. (Ok, I admit that make is also a &quot;magic&quot; tool)
Briefly describing: during compilation of Python from source code there is an option to inject OpenSSL support directly into it.
In CentOS 7 Python 2.7.5 is installed by default and couldn't be updated to the later ones using built-in package manager. Python 3.6.8 is the latest version available in the CentOS 7 repos. 3.6 also couldn't be updated to the later ones using the package manager.
So the only possible solution is to compile Python from source code.

Update your yum packages, reboot, install all the packages neccesssary to run OpenSSL and Python.
Download the latest OpenSSL source code, unpack and compile.
Download the latest Python source code, unpack, configure to use the compiled OpenSSL and compile with altinstall parameter. Do not remove previous Python versions! You will have more problems than benefits. I had to revert virtual machine to the latest snapshot several times, because I destroyed something completely.

Update and install yum packages
&gt; yum update
&gt; yum install openssl-devel bzip2-devel libffi-devel

An article suggests also to install some &quot;Development Tools&quot;
&gt; yum groupinstall &quot;Development Tools&quot;

but this step failed for me and I was able to finish the installation without it.
Download the latest OpenSSL source code, unpack and compile
I've choosen /usr/src directory to do the manipulations with source code.
Download
&gt; cd /usr/src
&gt; wget https://ftp.openssl.org/source/openssl-1.1.1q.tar.gz --no-check-certificate

Unpack
&gt; tar -xzvf openssl-1.1.1q.tar.gz
&gt; cd openssl-1.1.1q

Compile
&gt; ./config --prefix=/usr --openssldir=/etc/ssl --libdir=lib no-shared zlib-dynamic
&gt; make

Run tests for the compiled OpenSSL
&gt; make test

Install
&gt; make install

Check that OpenSSL is installed
&gt; openssl version
OpenSSL 1.1.1q  5 Jul 2022
&gt; which openssl
/usr/bin/openssl

Download and compile Python
Download
&gt; cd /usr/src
&gt; wget https://www.python.org/ftp/python/3.11.0/Python-3.11.0a4.tgz

Unpack
&gt; tar -xzf Python-3.11.0a4.tgz
&gt; cd Python-3.11.0a4

Configure
&gt; ./configure --enable-optimizations --with-openssl=/usr

It is important that the --with-openssl option has the same value as the --prefix option when you configured OpenSSL above!!!
Compile and install (It's time for a cup of coffee - it takes time)
&gt; make altinstall

Checking that Python 3.11 is installed:
&gt; python3.11 -V
Python 3.11.0a4

If you have set symbolic links, then Python 3.11 should be callable by &quot;python3&quot; and/or &quot;python&quot; aliases
&gt; python3 -V
Python 3.11.0a4
&gt; python -V
Python 3.11.0a4

Also check that PIP is working and that symlink-aliases for it are there.
Now it's time to check that your Python-based programs are working. Some of them should be installed again by PIP, because they were installed in subdirectories of previous Python versions.
After doing these manipulations I also got SSL certificates error:

&lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:998)&gt;

After running
&gt; pip3 install certifi

the problem is gone.
"
"Given a Polars dataframe like below, how can I call explode() on both columns while expanding the null entry to the correct length to match up with its row?
shape: (3, 2)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ x         â”† y                   â”‚
â”‚ ---       â”† ---                 â”‚
â”‚ list[i64] â”† list[bool]          â”‚
â•žâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ [1]       â”† [true]              â”‚
â”‚ [1, 2]    â”† null                â”‚
â”‚ [1, 2, 3] â”† [true, false, true] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Currently calling df.explode([&quot;x&quot;, &quot;y&quot;]) will result in this error.
polars.exceptions.ShapeError: exploded columns must have matching element counts

I'm assuming there's not a built-in way. But I can't find/think of a way to convert that null into a list of correct length, such that the explode will work. Here, the required length is not known statically upfront.
I looked into passing list.len() expressions into repeat_by(), but repeat_by() doesn't support null.
","You were on the right track, trying to fill the missing values with a list of null values of correct length.
To make pl.Expr.repeat_by work with null, we need to ensure that the base expression is of a non-null type. This can be achieved by setting the dtype argument of pl.lit explicity.
Then, the list column of (lists of) nulls can be used to fill the null values in y. From there, exploding x and y simultaneously works as usually.
(
    df
    .with_columns(
        pl.col(&quot;y&quot;).fill_null(
            pl.lit(None, dtype=pl.Boolean).repeat_by(pl.col(&quot;x&quot;).list.len())
        )
    )
)

shape: (3, 2)
┌───────────┬─────────────────────┐
│ x         ┆ y                   │
│ ---       ┆ ---                 │
│ list[i64] ┆ list[bool]          │
╞═══════════╪═════════════════════╡
│ [1]       ┆ [true]              │
│ [1, 2]    ┆ [null, null]        │
│ [1, 2, 3] ┆ [true, false, true] │
└───────────┴─────────────────────┘

From here, df.explode(&quot;x&quot;, &quot;y&quot;) should work as expected.
Note. If there are more than two columns, which all might contain null values, one can combine the answer above with this answer to have a valid solution.
Note.
"
"In the following code:
a = [[&quot;2022&quot;], [&quot;2023&quot;]]
b = [[&quot;blue&quot;, &quot;red&quot;], [&quot;green&quot;, &quot;yellow&quot;]]
c = [[&quot;1&quot;, &quot;2&quot;, &quot;3&quot;], [&quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;], [&quot;8&quot;, &quot;9&quot;, &quot;10&quot;, &quot;11&quot;], [&quot;12&quot;, &quot;13&quot;]]

I would like a function that outputs this, but for any number of variables:
[
    [&quot;2022&quot;, &quot;blue&quot;, &quot;1&quot;],
    [&quot;2022&quot;, &quot;blue&quot;, &quot;2&quot;],
    [&quot;2022&quot;, &quot;blue&quot;, &quot;3&quot;],
    [&quot;2022&quot;, &quot;red&quot;, &quot;4&quot;],
    [&quot;2022&quot;, &quot;red&quot;, &quot;5&quot;],
    [&quot;2022&quot;, &quot;red&quot;, &quot;6&quot;],
    [&quot;2022&quot;, &quot;red&quot;, &quot;7&quot;],
    [&quot;2023&quot;, &quot;green&quot;, &quot;8&quot;],
    [&quot;2023&quot;, &quot;green&quot;, &quot;9&quot;],
    [&quot;2023&quot;, &quot;green&quot;, &quot;10&quot;],
    [&quot;2023&quot;, &quot;green&quot;, &quot;11&quot;],
    [&quot;2023&quot;, &quot;yellow&quot;, &quot;12&quot;],
    [&quot;2023&quot;, &quot;yellow&quot;, &quot;13&quot;],
]

I have searched for a function to do this with itertools or zip, but haven't found anything yet.
To clarify, my use case for this was to iterate through values of a nested/multi-level dropdown menu (the first dropdown returns options, and each option returns a different dropdown, and so on).
","First, you join the first argument, to a list of lists with only one element each.
Then for each sublist and its index i in the next argument, you pick the i-th list of the previous iteration res[i] and add to aux len(sublist) lists each of one is the res[i] with one item from sublist.
from itertools import chain

def f(*args):
    res = list(chain.from_iterable([[item] for item in l] for l in args[0]))
    for arg in args[1:]:
        aux = []
        for i, sublist in enumerate(arg):
            aux += [res[i] + [opt] for opt in sublist]
        res = aux
    return res

In addition if you want to verify that the arguments passed to the function are correct, you can use this:
def check(*args):
    size = sum(len(l) for l in args[0])
    for arg in args[1:]:
        if len(arg) != size:
            return False
        size = sum(len(l) for l in arg)
    return True

"
"I have a dataframe like:
data = {
    &quot;a&quot;: [[1], [2], [3, 4], [5, 6, 7]],
    &quot;b&quot;: [[], [8], [9, 10], [11, 12]],
}
df = pl.DataFrame(data)
&quot;&quot;&quot;
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ a         â”† b         â”‚
â”‚ ---       â”† ---       â”‚
â”‚ list[i64] â”† list[i64] â”‚
â•žâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ [1]       â”† []        â”‚
â”‚ [2]       â”† [8]       â”‚
â”‚ [3, 4]    â”† [9, 10]   â”‚
â”‚ [5, 6, 7] â”† [11, 12]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&quot;&quot;&quot;

Each pair of lists may not have the same length, and I want to &quot;truncate&quot; the explode to the shortest of both lists:
&quot;&quot;&quot;
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ a   â”† b   â”‚
â”‚ --- â”† --- â”‚
â”‚ i64 â”† i64 â”‚
â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•¡
â”‚ 2   â”† 8   â”‚
â”‚ 3   â”† 9   â”‚
â”‚ 4   â”† 10  â”‚
â”‚ 5   â”† 11  â”‚
â”‚ 6   â”† 12  â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
&quot;&quot;&quot;

I was thinking that maybe I'd have to fill the shortest of both lists with None to match both lengths, and then drop_nulls. But I was wondering if there was a more direct approach to this?
","Here's one approach:
min_length = pl.min_horizontal(pl.col('a', 'b').list.len())

out = (df.filter(min_length != 0)
       .with_columns(
           pl.col('a', 'b').list.head(min_length)
           )
       .explode('a', 'b')
       )

Output:
shape: (5, 2)
┌─────┬─────┐
│ a   ┆ b   │
│ --- ┆ --- │
│ i64 ┆ i64 │
╞═════╪═════╡
│ 2   ┆ 8   │
│ 3   ┆ 9   │
│ 4   ┆ 10  │
│ 5   ┆ 11  │
│ 6   ┆ 12  │
└─────┴─────┘

Explanation

Get the length for the lists in both columns with Expr.list.len and get the shortest for each row with pl.min_horizontal.
Now, filter out the rows where min_length == 0 (df.filter) and inside df.with_columns select the first n values of each list with Expr.list.head.
Finally, apply df.explode.

"
"I have the following line of code
 end_df['Soma Internet'] = end_df.iloc[:,end_df.columns.get_level_values(1) == 'Internet'].drop('site',axis=1).sum(axis=1)

It basically, filts my multi index df by a specific level 1 column. Drops a few not wanted columns. And does the sum, of all the other ones.
I took a glance, at a few of the documentation and other asked questions. But i didnt quite understood what causes the warning, and i also would love to rewrite this code, so i get rid of it.
","Let's try with an example (without data for simplicity):
import pandas as pd

# Column MultiIndex.
idx = pd.MultiIndex(levels=[['Col1', 'Col2', 'Col3'], ['subcol1', 'subcol2']], 
                    codes=[[2, 1, 0], [0, 1, 1]])

df = pd.DataFrame(columns=range(len(idx)))
df.columns = idx
print(df)

    Col3    Col2    Col1
subcol1 subcol2 subcol2

Clearly, the column MultiIndex is not sorted. We can check it with:
print(df.columns.is_monotonic_increasing)

False

This matters because Pandas performs index lookup and other operations much faster if the index is sorted, because it can use operations that assume the sorted order and are faster. Indeed, if we try to drop a column:
df.drop('Col1', axis=1)

PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.
  df.drop('Col1', axis=1)

Instead, if we sort the index before dropping, the warning disappears:
print(df.sort_index(axis=1))

# Index is now sorted in lexicographical order.
    Col1    Col2    Col3
subcol2 subcol2 subcol1

# No warning here.
df.sort_index(axis=1).drop('Col1', axis=1)

EDIT (see comments): As the warning suggests, this happens when we do not specify the level from which we want to drop the column. This is because to drop the column, pandas has to traverse the whole index (happens here). By specifying it we do not need such traversal:
# Also no warning.
df.drop('Col1', axis=1, level=0)

However, in general this problem relates more on row indices, as usually column multi-indices are way smaller. But definitely to keep it in mind for larger indices and dataframes. In fact, this is in particular relevant for slicing by index and for lookups. In those cases, you want your index to be sorted for better performance.
"
"Pandas 2.0 introduces the option to use PyArrow as the backend rather than NumPy. As of version 2.0, using it seems to require either calling one of the pd.read_xxx() methods with type_backend='pyarrow', or else constructing a DataFrame that's NumPy-backed and then calling .convert_dtypes on it.
Is there a more direct way to construct a PyArrow-backed DataFrame?
","If your data are known to be all of a specific type (say, int64[pyarrow]), this is straightforward:
import pandas as pd
data = {'col_1': [3, 2, 1, 0], 'col_2': [1, 2, 3, 4]}
df = pd.DataFrame(
    data,
    dtype='int64[pyarrow]',
    # ...
)


If your data are known to be all of the same type but the type is not known, then I don't know of a way to use the constructor. I tried dtype=pd.ArrowDtype, which does not work, and dtype=pd.ArrowDtype(), which needs an argument that I think would have to be a specific dtype.

One option for possibly-mixed and unknown data types is to make a pa.Table (using one of its methods) and then send it to pandas with the types_mapper kwarg. For example, using a dict:
import pyarrow as pa

data = {'col_1': [3, 2, 1, 0], 'col_2': ['a', 'b', 'c', 'd']}

pa_table = pa.Table.from_pydict(data)
df = pa_table.to_pandas(types_mapper=pd.ArrowDtype)

The last line is exactly what pd.read_parquet with dtype_backend='pyarrow' does under the hood, after reading parquet into a pa.Table. I thought it was worth highlighting the approach since it wouldn't have occurred to me otherwise.
The method pa.Table.from_pydict() will infer the data types. If the data are of mixed type, but known, and speed is very important, see https://stackoverflow.com/a/57939649 for how to make a predefined schema to pass to the pa.Table constructor.

The above method loses most of the flexibility of the DataFrame constructor (specifying an index, accepting various container types as input, etc.). You might be able to code around this and encapsulate it in a function.
Another workaround, as mentioned in the question, is to just construct a NumPy-backed DataFrame and call .convert_dtypes on it:
import pandas as pd

data = {'col_1': [3, 2, 1, 0], 'col_2': ['a', 'b', 'c', 'd']}
df = pd.DataFrame(
    data,
    index=[4, 5, 6, 7],
    # ...
).convert_dtypes(dtype_backend='pyarrow')

"
"I have this code that generates a toy DataFrame (production df is much complex):
import polars as pl
import numpy as np
import pandas as pd

def create_timeseries_df(num_rows):
    date_rng = pd.date_range(start='1/1/2020', end='1/01/2021', freq='T')
    data = {
        'date': np.random.choice(date_rng, num_rows),
        'category': np.random.choice(['A', 'B', 'C', 'D'], num_rows),
        'subcategory': np.random.choice(['X', 'Y', 'Z'], num_rows),
        'value': np.random.rand(num_rows) * 100
    }
    df = pd.DataFrame(data)
    df = df.sort_values('date')
    df.set_index('date', inplace=True, drop=False)
    df.index = pd.to_datetime(df.index)

    return df

num_rows = 1000000  # for example
df = create_timeseries_df(num_rows)

Then perform this transformations with Pandas.
df_pd = df.copy()
df_pd = df_pd.groupby(['category', 'subcategory'])
df_pd = df_pd.resample('W-MON')
df_pd.agg({
    'value': ['sum', 'mean', 'max', 'min']
}).reset_index()

But, obviously it is quite slow with Pandas (at least in production). Thus, I'd like to use Polars to speed up time. This is what I have so far:
#Convert to Polars DataFrame
df_pl = pl.from_pandas(df)

#Groupby, resample and aggregate
df_pl = df_pl.group_by('category', 'subcategory')
df_pl = df_pl.group_by_dynamic('date', every='1w', closed='right')
df_pl.agg(
   pl.col('value').sum().alias('value_sum'),
   pl.col('value').mean().alias('value_mean'),
   pl.col('value').max().alias('value_max'),
   pl.col('value').min().alias('value_min')
)

But I get AttributeError: 'GroupBy' object has no attribute 'group_by_dynamic'. Any ideas on how to use groupby followed by resample in Polars?
","You can pass additional columns to group by in a call to group_by_dynamic by passing a list with the named argument group_by=:
df_pl = df_pl.group_by_dynamic(
    &quot;date&quot;, every=&quot;1w&quot;, closed=&quot;right&quot;, group_by=[&quot;category&quot;, &quot;subcategory&quot;]
)

With this, I get a dataframe that looks similar to the one your pandas code produces:
shape: (636, 7)
┌──────────┬─────────────┬─────────────────────┬──────────────┬───────────┬───────────┬──────────┐
│ category ┆ subcategory ┆ date                ┆ sum          ┆ mean      ┆ max       ┆ min      │
│ ---      ┆ ---         ┆ ---                 ┆ ---          ┆ ---       ┆ ---       ┆ ---      │
│ str      ┆ str         ┆ datetime[ns]        ┆ f64          ┆ f64       ┆ f64       ┆ f64      │
╞══════════╪═════════════╪═════════════════════╪══════════════╪═══════════╪═══════════╪══════════╡
│ D        ┆ Z           ┆ 2019-12-30 00:00:00 ┆ 55741.652346 ┆ 50.399324 ┆ 99.946595 ┆ 0.008139 │
│ D        ┆ Z           ┆ 2020-01-06 00:00:00 ┆ 76161.42206  ┆ 50.139185 ┆ 99.96917  ┆ 0.138366 │
│ D        ┆ Z           ┆ 2020-01-13 00:00:00 ┆ 80222.894298 ┆ 49.581517 ┆ 99.937069 ┆ 0.117216 │
│ D        ┆ Z           ┆ 2020-01-20 00:00:00 ┆ 82042.968995 ┆ 50.456931 ┆ 99.981101 ┆ 0.009077 │
│ D        ┆ Z           ┆ 2020-01-27 00:00:00 ┆ 82408.144078 ┆ 49.494381 ┆ 99.954734 ┆ 0.023769 │
│ …        ┆ …           ┆ …                   ┆ …            ┆ …         ┆ …         ┆ …        │
│ B        ┆ Z           ┆ 2020-11-30 00:00:00 ┆ 79530.963748 ┆ 49.737939 ┆ 99.973554 ┆ 0.007446 │
│ B        ┆ Z           ┆ 2020-12-07 00:00:00 ┆ 80050.524653 ┆ 49.566888 ┆ 99.975546 ┆ 0.003066 │
│ B        ┆ Z           ┆ 2020-12-14 00:00:00 ┆ 77896.578291 ┆ 50.029915 ┆ 99.969098 ┆ 0.033222 │
│ B        ┆ Z           ┆ 2020-12-21 00:00:00 ┆ 76490.507942 ┆ 49.636929 ┆ 99.953563 ┆ 0.021683 │
│ B        ┆ Z           ┆ 2020-12-28 00:00:00 ┆ 46964.533378 ┆ 50.553857 ┆ 99.653981 ┆ 0.042546 │
└──────────┴─────────────┴─────────────────────┴──────────────┴───────────┴───────────┴──────────┘

"
"I'm migrating from v1 to v2 of Pydantic and I'm attempting to replace all uses of the deprecated @validator with @field_validator.
However, I was previously using the pre validator argument and after moving to @field_validator, I'm receiving the following error:
TypeError: field_validator() got an unexpected keyword argument 'pre'

Has the use of pre also been deprecated in V2? It seems it's still referenced in the V2 validator documentation though with the top-of-page warning:

This page still needs to be updated for v2.0.

Hoping somebody else has already worked through this and can suggest the best route forward. Thanks!
","
It seems it's still referenced in the V2 validator documentation [...]

Not really. At least nowhere in the field validators section. (There is just an outdated mention of it in the &quot;model validators&quot; section as of today.)
However, there is also a link to the detailed API reference for the field_validator decorator showing exactly what arguments you can pass to it.
The closest thing to the v1 pre=True argument would now be the mode=&quot;before&quot; argument.
Say you have the following code in Pydantic v1:
from pydantic import BaseModel, validator


class Foo(BaseModel):
    x: int

    @validator(&quot;x&quot;, pre=True)
    def do_stuff(cls, v: object) -&gt; object:
        if v is None:
            return 0
        return v


print(Foo(x=None))  # x=0

You would have to rewrite it like this in v2:
from pydantic import BaseModel, field_validator


class Foo(BaseModel):
    x: int

    @field_validator(&quot;x&quot;, mode=&quot;before&quot;)
    def do_stuff(cls, v: object) -&gt; object:
        if v is None:
            return 0
        return v


print(Foo(x=None))  # x=0

I expect the maintainers will gradually rewrite and expand the docs to cover more details.
"
"Using the python inspect module, in a function, I would like to get the source code of the line that called that function.
So in the following situation:
def fct1():
    # Retrieve the line that called me and extract 'a'
    return an object containing name='a'

a = fct1()

I would like to retrieve the string &quot;a = fct1()&quot; in fct1
All I can do so far is to retrieve the code of the whole module with :
code = inspect.getsource(sys._getframe().f_back)

Please note that fct1() can be called many times in the main module.
Eventually, what I want is to retrieve the variable name &quot;a&quot; which is easy if I can get s = &quot;a = fct1()&quot; in fct1() :
a_name = s.split(&quot;=&quot;)[0].strip()

","A really dumb solution would be to capture a stack trace and take the 2nd line:
import traceback

def fct1():
    stack = traceback.extract_stack(limit=2)
    print(traceback.format_list(stack)[0].split('\n')[1].strip())  # prints &quot;a = fct1()&quot;
    return None

a = fct1()

@jtlz2 asked for it in a decorator
import traceback

def add_caller(func):
    def wrapper(*args, **kwargs):
        stack = traceback.extract_stack(limit=2)
        func(*args, caller=traceback.format_list(stack)[0].split('\n')[1].strip(), **kwargs)
    return wrapper

@add_caller
def fct1(caller):
    print(caller)

fct1()

And it does work.
UPDATE Here's a functional version (which needed limit=3 for the caller of the caller):
import traceback

def source_line_of_caller():
    &quot;&quot;&quot;Return the Python source code line that called your function.&quot;&quot;&quot;
    stack = traceback.extract_stack(limit=3)
    return traceback.format_list(stack)[0].split('\n')[1].strip()

def _tester():
    assert &quot;_tester() # whoa, comments too&quot; == source_line_of_caller()

_tester() # whoa, comments too

"
"I need to drop rows that have a nan value in any column. As for null values with drop_nulls()
df.drop_nulls()

but for nans. I have found that the method drop_nans exist for Series but not for DataFrames
df['A'].drop_nans()

Pandas code that I'm using:
df = pd.DataFrame(
    {
        'A': [0, 0, 0, 1,None, 1],
        'B': [1, 2, 2, 1,1, np.nan]
    }
)
df.dropna()

","Another definition is: to keep rows where all values are not NaN
For that, we can use:

.is_not_nan() to test for &quot;not nan&quot;
pl.col(pl.Float32, pl.Float64) to select only float columns
.all_horizontal() to compute a row-wise True/False comparison
DataFrame.filter to keep only the &quot;True&quot; rows

df = pl.from_repr(&quot;&quot;&quot;
┌─────┬─────┬─────┐
│ A   ┆ B   ┆ C   │
│ --- ┆ --- ┆ --- │
│ f64 ┆ f64 ┆ str │
╞═════╪═════╪═════╡
│ 0.0 ┆ 1.0 ┆ a   │
│ 0.0 ┆ 2.0 ┆ b   │
│ 0.0 ┆ 2.0 ┆ c   │
│ 1.0 ┆ 1.0 ┆ d   │
│ NaN ┆ 1.0 ┆ e   │
│ 1.0 ┆ NaN ┆ g   │
└─────┴─────┴─────┘
&quot;&quot;&quot;)

df.filter(
   pl.all_horizontal(pl.col(pl.Float32, pl.Float64).is_not_nan())
)

shape: (4, 3)
┌─────┬─────┬─────┐
│ A   ┆ B   ┆ C   │
│ --- ┆ --- ┆ --- │
│ f64 ┆ f64 ┆ str │
╞═════╪═════╪═════╡
│ 0.0 ┆ 1.0 ┆ a   │
│ 0.0 ┆ 2.0 ┆ b   │
│ 0.0 ┆ 2.0 ┆ c   │
│ 1.0 ┆ 1.0 ┆ d   │
└─────┴─────┴─────┘

polars.selectors has also since been added which provides cs.float()
df.filter(
   pl.all_horizontal(cs.float().is_not_nan())
)

"
"what would be a recommended way to install your Python's package dependencies with poetry for Azure Pipelines? I see people only downloading poetry through pip which is a big no-no.
- script: |
    python -m pip install -U pip
    pip install poetry
    poetry install
  displayName: Install dependencies

I can use curl to download poetry.
  - script: |
      curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python -
      export PATH=$PATH:$HOME/.poetry/bin
      poetry install --no-root
    displayName: 'Install dependencies'

But then in each subsequent step I have to add poetry to PATH again ...
  - script: |
      curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python -
      export PATH=$PATH:$HOME/.poetry/bin
      poetry install --no-root
    displayName: 'Install dependencies'

  - script: |
      # export PATH=$PATH:$HOME/.poetry/bin
      poetry run flake8 src
    displayName: 'Linter'

  - script: |
      # export PATH=$PATH:$HOME/.poetry/bin
      poetry add pytest-azurepipelines
      poetry run pytest src
    displayName: 'Tests'

Is there any right way to use poetry in Azure Pipelines?
","Consulted this issue with a collegue. He recommended doing separate step to add Poetry to the PATH.
  - task: UsePythonVersion@0
    inputs:
      versionSpec: '3.8'
    displayName: 'Use Python 3.8'

  - script: |
      curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python -
      export PATH=$PATH:$HOME/.poetry/bin
      poetry install --no-root
    displayName: 'Install dependencies'

  - script: echo &quot;##vso[task.prependpath]$HOME/.poetry/bin&quot;
    displayName: Add poetry to PATH

  - script: |
      poetry run flake8 src
    displayName: 'Linter'

  - script: |
      poetry add pytest-azurepipelines
      poetry run pytest src
    displayName: 'Tests'

"
"I have a pandas dataframe filled with time-stamped data. It is out of order; and I am trying to sort by date, hours and minutes. The pandas dataframe will organize by date, but not by hours and minutes.
My dataframe is loaded in ('df'), and the column 'dttime' was changed it into a dateframe from integer numbers.
df['dttime'] = pd.to_datetime(df['dttime'], format='%y%m%d%H%M%S')

I resort it with:
df.sort_values(by='dttime')    

but that does not seem to have the right ordering of the hour minutes and seconds.
","I tried with some dummy data and it doesn't look like an issue to me. Please check the below code.
import pandas as pd
data = ['221011141200', '221011031200', '221011191200', '221011131600']

df = pd.DataFrame(data, columns=['dttime'])
df['dttime'] = pd.to_datetime(df['dttime'], format='%y%m%d%H%M%S')

# Before sorting
print(df)

# After sorting
df = df.sort_values(by='dttime')
print(df)

Output is as follows:
               dttime
0 2022-10-11 14:12:00
1 2022-10-11 03:12:00
2 2022-10-11 19:12:00
3 2022-10-11 13:16:00

               dttime
1 2022-10-11 03:12:00
3 2022-10-11 13:16:00
0 2022-10-11 14:12:00
2 2022-10-11 19:12:00

"
"As a toy example, let's use the Fibonacci sequence:
def fib(n: int) -&gt; int:
  if n &lt; 2:
    return 1
  return fib(n - 2) + fib(n - 1)

Of course, this will hang the computer if we try to:
print(fib(100))

So we decide to add memoization. To keep the logic of fib clear, we decide not to change fib and instead add memoization via a decorator:
from typing import Callable
from functools import wraps


def remember(f: Callable[[int], int]) -&gt; Callable[[int], int]:
    @wraps(f)
    def wrapper(n: int) -&gt; int:
        if n not in wrapper.memory:
            wrapper.memory[n] = f(n)
        return wrapper.memory[n]

    wrapper.memory = dict[int, int]()
    return wrapper


@remember
def fib(n: int) -&gt; int:
    if n &lt; 2:
        return 1
    return fib(n - 2) + fib(n - 1)


Now there is no problem if we:
print(fib(100))

573147844013817084101

However, mypy complains that &quot;Callable[[int], int]&quot; has no attribute &quot;memory&quot;, which makes sense, and usually I would want this complaint if I tried to access a property that is not part of the declared type...
So, how should we use typing to indicate that wrapper, while a Callable, also has the property memory?
","To describe something as &quot;a callable with a memory attribute&quot;, you could define a protocol (Python 3.8+, or earlier versions with typing_extensions):
from typing import Protocol


class Wrapper(Protocol):
    memory: dict[int, int]
    def __call__(self, n: int) -&gt; int: ...

In use, the type checker knows that a Wrapper is valid as a Callable[[int], int] and allows return wrapper as well as the assignment to wrapper.memory:
from functools import wraps
from typing import Callable, cast


def remember(f: Callable[[int], int]) -&gt; Callable[[int], int]:
    @wraps(f)
    def _wrapper(n: int) -&gt; int:
        if n not in wrapper.memory:
            wrapper.memory[n] = f(n)
        return wrapper.memory[n]
    wrapper = cast(Wrapper, _wrapper)
    wrapper.memory = dict()
    return wrapper

Playground
Unfortunately this requires wrapper = cast(Wrapper, _wrapper), which is not type safe - wrapper = cast(Wrapper, &quot;foo&quot;) would also check just fine.
"
"Python's standard library is vast, and my intuition tells that there must be a way in it to accomplish this, but I just can't figure it out. This is purely for curiosity and learning purposes:
I have two simple functions:
def increment(x):
    return x + 1

def double(x):
    return x * 2

and I want to compose them into a new function double_and_increment. I could of course simply do that as such:
double_and_increment = lambda x: increment(double(x))

but I could also do it in a more convoluted but perhaps more &quot;ergonomically scalable&quot; way:
import functools

double_and_increment = functools.partial(functools.reduce, lambda acc, f: f(acc), [double, increment])

Both of the above work fine:
&gt;&gt;&gt; double_and_increment(1)
3

Now, the question is, is there tooling in the standard library that would allow achieving the composition without any user-defined lambdas, regular functions, or classes.
The first intuition is to replace the lambda acc, f: f(acc) definition in the functools.reduce call with operator.call, but that unfortunately takes the arguments in the reverse order:
&gt;&gt;&gt; (lambda acc, f: f(acc))(1, str)  # What we want to replace.
&gt;&gt;&gt; '1'
&gt;&gt;&gt; import operator
&gt;&gt;&gt; operator.call(str, 1)  # Incorrect argument order.
&gt;&gt;&gt; '1'

I have a hunch that using functools.reduce is still the way to accomplish the composition, but for the life of me I can't figure out a way to get rid of the user-defined lambda.
Few out-of-the-box methods that got me close:
import functools, operator

# Curried form, can't figure out how to uncurry.
functools.partial(operator.methodcaller, '__call__')(1)(str)

# The arguments needs to be in the middle of the expression, which does not work.
operator.call(*reversed(operator.attrgetter('args')(functools.partial(functools.partial, operator.call)(1, str))))

Have looked through all the existing questions, but they are completely different and rely on using user-defined functions and/or lambdas.
","As mentioned in the other answer of mine I don't agree that the test suite discovered by @AKX should be considered as part of the standard library per the OP's rules.
As it turns out, while researching for an existing function to modify for my other answer, I found that there is this helper function _int_to_enum in the signal module that perfectly implements operator.call for a callable with a single argument, but with parameters reversed, exactly how the OP wants it, and is available since Python 3.5:
def _int_to_enum(value, enum_klass):
    &quot;&quot;&quot;Convert a numeric value to an IntEnum member.
    If it's not a known member, return the numeric value itself.
    &quot;&quot;&quot;
    try:
        return enum_klass(value)
    except ValueError:
        return value

So we can simply repurpose/abuse it:
from signal import _int_to_enum as rcall
from functools import reduce, partial

def increment(x):
    return x + 1

def double(x):
    return x * 2

double_and_increment = partial(reduce, rcall, [double, increment])
print(double_and_increment(1))

This outputs:
3

Demo: here
"
"Consider this simple Python script:
$ cat test_utc.py
from datetime import datetime

for i in range(10_000_000):
    first = datetime.utcnow()
    second = datetime.utcnow()

    assert first &lt;= second, f&quot;{first=} {second=} {i=}&quot;

When I run it from the shell like python test_utc.py it finishes w/o errors, just as expected. However, when I run it in a Docker container the assertion fails:
$ docker run -it --rm -v &quot;$PWD&quot;:/code -w /code python:3.10.4 python test_utc.py
Traceback (most recent call last):
  File &quot;/code/test_utc.py&quot;, line 7, in &lt;module&gt;
    assert first &lt;= second, f&quot;{first=} {second=} {i=}&quot;
AssertionError: first=datetime.datetime(2022, 5, 24, 19, 5, 1, 861308) second=datetime.datetime(2022, 5, 24, 19, 5, 1, 818270) i=1818860

How is it possible?
P.S. a colleague has reported that increasing the range parameter to 100_000_000 makes it fail in the shell on their mac as well (but not for me).
","utcnow refers to now refers to today refers to fromtimestamp refers to time, which says:

While this function normally returns non-decreasing values, it can return a lower value than a previous call if the system clock has been set back between the two calls.

The utcnow code also shows its usage of time:
def utcnow(cls):
    &quot;Construct a UTC datetime from time.time().&quot;
    t = _time.time()
    return cls.utcfromtimestamp(t)

Such system clock updates are also why monotonic exists, which says:

Return the value (in fractional seconds) of a monotonic clock, i.e. a clock that cannot go backwards. The clock is not affected by system clock updates.

And utcnow has no such guarantee.
Your computer doesn't have a perfect clock, every now and then it synchronizes via the internet with more accurate clocks, possibly adjusting it backwards. See for example answers here.
And looks like Docker makes it worse, see for example Addressing Time Drift in Docker Desktop for Mac from the Docker blog. Excerpt:

macOS doesn’t have native container support. The helper VM has its own internal clock, separate from the host’s clock. When the two clocks drift apart then suddenly commands which rely on the time, or on file timestamps, may start to behave differently

Lastly, you can increase your chance to catch a backwards update when one occurs. If one occurs not between getting first and second but between second and the next first, you'll miss it! Below code fixes that issue and is also micro-optimized (including removing the utcnow middle man) so it checks faster / more frequently:
import time
from itertools import repeat

def function():
    n = 10_000_000
    reps = repeat(1, n)
    now = time.time
    first = now()
    for _ in reps:
        second = now()
        assert first &lt;= second, f&quot;{first=} {second=} i={n - sum(reps)}&quot;
        first = second
function()

"
"I'm working on a project with the following directory structure:
project/
    package1/
        module1.py
        module2.py
    package2/
        module1.py
        module2.py
    main1.py
    main2.py
    main3.py
    ...
    mainN.py

where each mainX.py file is an executable Python script that imports modules from either package1, package2, or both. package1 and package2 are subpackages meant to be distributed along with the rest of the project (not independently).
The standard thing to do is to put your entry point in the top-level directory. I have N entry points, so I put them all in the top-level directory. The trouble is that N keeps growing, so my top-level directory is getting flooded with entry points.
I could move the mainX.py files to a sub-directory (say, project/run), but then all of the package1 and package2 imports would break. I could extract package1 and package2 to a separate repository and just expect it to be installed on the system (i.e., in the system / user python path), but that would complicate installation. I could modify the Python path as a precondition or during runtime, but that's messy and could introduce unintended consequences. I could write a single main.py entry point script with argument subparsers respectively pointing to run/main1.py, ..., run/mainN.py, but that would introduce coupling between main.py and each of the run/mainX.py files.
What's the standard, &quot;Pythonic&quot; solution to this issue?
","The standard solution is to use console_scripts packaging for your entry points - read about the entry-points specification here. This feature can be used to generate script wrappers like main1.py ... mainN.py at installation time.
Since these script wrappers are generated code, they do not exist in the project source directory at all, so that problem of clutter (&quot;top-level directory is getting flooded with entry points&quot;) goes away.
The actual code for the scripts will be defined somewhere within the package, and the places where the main*.py scripts will actually hook into code within the package is defined in the package metadata. You can hook a console script entry-point up to any callable within the package, provided it can be called without arguments (optional arguments, i.e. args with default values, are fine).
project
├── package1
│   ├── __init__.py
│   ├── module1.py
│   └── module2.py
├── package2
│   ├── __init__.py
│   ├── module1.py
│   └── module2.py
├── pyproject.toml
└── scripts
    └── __init__.py

This is the new directory structure. Note the addition of __init__.py files, which indicates that package1 and package2 are packages and not just subdirectories.
For the new files added, here's the scripts/__init__.py:
# these imports should work
#   from package1 import ...
#   from package2.module1 import ...

def myscript1():
    # put whatever main1.py did here
    print(&quot;hello&quot;)

def myscript2():
    # put whatever main2.py did here
    print(&quot;world&quot;)

These don't need to be all in the same file, and you can put them wherever you want within the package actually, as long as you update the hooks in the [project.scripts] section of the packaging definition.
And here's that packaging definition:
[build-system]
requires = [&quot;setuptools&quot;]
build-backend = &quot;setuptools.build_meta&quot;

[project]
name = &quot;mypackage&quot;
version = &quot;0.0.1&quot;

[project.scripts]
&quot;main1.py&quot; = &quot;scripts:myscript1&quot;
&quot;main2.py&quot; = &quot;scripts:myscript2&quot;

[tool.setuptools]
packages = [&quot;package1&quot;, &quot;package2&quot;, &quot;scripts&quot;]

Now when the package is installed, the console scripts are generated:
$ pip install --editable .
...
Successfully installed mypackage-0.0.1
$ main1.py
hello
$ main2.py
world

As mentioned, those executables do not live in the project directory, but within the site's scripts directory, which will be present on $PATH. The scripts are generated by pip, using vendored code from distlib's ScriptMaker. If you peek at the generated script files you'll see that they're simple wrappers, they'll just import the callable from within the package and then call it. Any argument parsing, logging configuration, etc must all still be handled within the package code.
$ ls
mypackage.egg-info  package1  package2  pyproject.toml  scripts
$ which main2.py
/tmp/project/.venv/bin/main2.py

The exact location of the scripts directory depends on your platform, but it can be checked like this in Python:
&gt;&gt;&gt; import sysconfig
&gt;&gt;&gt; sysconfig.get_path(&quot;scripts&quot;)
'/tmp/project/.venv/bin'

"
"I have a validator class with a method that performs multiple checks and may raise different exceptions:
class Validator:
    def validate(something) -&gt; None:
        if a:
            raise ErrorA()
        if b:
            raise ErrorB()
        if c:
            raise ErrorC()

There's a place in the outside (caller) code where I want to customize its behaviour and prevent ErrorB from being raised, without preventing ErrorC. Something like resumption semantics would be useful here. Hovewer, I haven't found a good way to achieve this.
To clarify: I have the control over Validator source code, but prefer to preserve its existing interface as much as possible.
Some possible solutions that I've considered:

The obvious
try:
    validator.validate(something)
except ErrorB:
    ...

is no good because it also suppresses ErrorC in cases where both ErrorB and ErrorC should be raised.

Copy-paste the method and remove the check:
# In the caller module

class CustomValidator(Validator):
    def validate(something) -&gt; None:
        if a:
            raise ErrorA()
        if c:
            raise ErrorC()

Duplicating the logic for a and c is a bad idea
and will lead to bugs if Validator changes.

Split the method into separate checks:
class Validator:
    def validate(something) -&gt; None:
        self.validate_a(something)
        self.validate_b(something)
        self.validate_c(something)

    def validate_a(something) -&gt; None:
        if a:
            raise ErrorA()

    def validate_b(something) -&gt; None:
        if b:
            raise ErrorB()

    def validate_c(something) -&gt; None:
        if c:
            raise ErrorC()

# In the caller module

class CustomValidator(Validator):
    def validate(something) -&gt; None:
        super().validate_a(something)
        super().validate_c(something)

This is just a slightly better copy-paste.
If some validate_d() is added later, we have a bug in CustomValidator.

Add some suppression logic by hand:
class Validator:
    def validate(something, *, suppress: list[Type[Exception]] = []) -&gt; None:
        if a:
            self._raise(ErrorA(), suppress)
        if b:
            self._raise(ErrorB(), suppress)
        if c:
            self._raise(ErrorC(), suppress)

    def _raise(self, e: Exception, suppress: list[Type[Exception]]) -&gt; None:
        with contextlib.suppress(*suppress):
            raise e

This is what I'm leaning towards at the moment.
There's a new optional parameter and the raise syntax becomes kinda ugly,
but this is an acceptable cost.

Add flags that disable some checks:
class Validator:
    def validate(something, *, check_a: bool = True,
                 check_b: bool = True, check_c: bool = True) -&gt; None:
        if check_a and a:
            raise ErrorA()
        if check_b and b:
            raise ErrorB()       
        if check_c and c:
            raise ErrorC()

This is good, because it allows to granually control different checks even
if they raise the same exception.
However, it feels verbose and will require additional maintainance
as Validator changes. I actually have more than three checks there.

Yield exceptions by value:
class Validator:
    def validate(something) -&gt; Iterator[Exception]:
        if a:
            yield ErrorA()
        if b:
            yield ErrorB()
        if c:
            yield ErrorC()

This is bad, because it's a breaking change for existing callers
and it makes propagating the exception (the typical use) way more verbose:
# Instead of
# validator.validate(something)

e = next(validator.validate(something), None)
if e is not None:
    raise e

Even if we keep everything backwards-compatible
class Validator:
    def validate(something) -&gt; None:
        e = next(self.iter_errors(something), None)
        if e is not None:
            raise e

    def iter_errors(something) -&gt; Iterator[Exception]:
        if a:
            yield ErrorA()
        if b:
            yield ErrorB()
        if c:
            yield ErrorC()

The new suppressing caller still needs to write all this code:
exceptions = validator.iter_errors(something)
e = next(exceptions, None)
if isinstance(e, ErrorB):
    # Skip ErrorB, don't raise it.
    e = next(exceptions, None)
if e is not None:
    raise e

Compared to the previous two options:
validator.validate(something, suppress=[ErrorB])

validator.validate(something, check_b=False)



","With bare exceptions you are looking at the wrong tool for the job. In Python, to raise an exception means that execution hits an exceptional case in which resuming is not possible. Terminating the broken execution is an express purpose of exceptions.

Execution Model: 4.3. Exceptions
Python uses the “termination” model of error handling: an exception handler can find out what happened and continue execution at an outer level, but it cannot repair the cause of the error and retry the failing operation (except by re-entering the offending piece of code from the top).

To get resumption semantics for exception handling, you can look at the generic tools for either resumption or for handling.

Resumption: Coroutines
Python's resumption model are coroutines: yield coroutine-generators or async coroutines both allow to pause and explicitly resume execution.
def validate(something) -&gt; Iterator[Exception]:
    if a:
        yield ErrorA()
    if b:
        yield ErrorB()
    if c:
        yield ErrorC()

It is important to distinguish between send-style &quot;proper&quot; coroutines and iterator-style &quot;generator&quot; coroutines. As long as no value must be sent into the coroutine, it is functionally equivalent to an iterator. Python has good inbuilt support for working with iterators:
for e in validator.iter_errors(something):
    if isinstance(e, ErrorB):
        continue  # continue even if ErrorB happens
    raise e

Similarly, one could filter the iterator or use comprehensions. Iterators easily compose and gracefully terminate, making them suitable for iterating exception cases.

Effect Handling
Exception handling is just the common use case for the more generic effect handling. While Python has no builtin effect handling support, simple handlers that address only the origin or sink of an effect can be modelled just as functions:
def default_handler(failure: BaseException):
    raise failure

def validate(something, failure_handler = default_handler) -&gt; None:
    if a:
        failure_handler(ErrorA())
    if b:
        failure_handler(ErrorB())
    if c:
        failure_handler(ErrorC())

This allows the caller to change the effect handling by supplying a different handler.
def ignore_b_handler(failure: BaseException):
    if not isinstance(failure, ErrorB):
        raise failure

validate(..., ignore_b_handler)

This might seem familiar to dependency inversion and is in fact related to it.
There are various stages of buying into effect handling, and it is possible to reproduce much if not all features via classes. Aside from technical functionality, one can implement ambient effect handlers (similar to how try &quot;connects&quot; to raise automatically) via thread local or context-local variables.
"
"When grouping a Polars dataframe in Python, how do you concatenate string values from a single column across rows within each group?
For example, given the following DataFrame:
import polars as pl

df = pl.DataFrame(
    {
        &quot;col1&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;],
        &quot;col2&quot;: [&quot;val1&quot;, &quot;val2&quot;, &quot;val1&quot;, &quot;val3&quot;, &quot;val3&quot;]
    }
)

Original df:
shape: (5, 2)
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ col1 â”† col2 â”‚
â”‚ ---  â”† ---  â”‚
â”‚ str  â”† str  â”‚
â•žâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•¡
â”‚ a    â”† val1 â”‚
â”‚ b    â”† val2 â”‚
â”‚ a    â”† val1 â”‚
â”‚ b    â”† val3 â”‚
â”‚ c    â”† val3 â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜

I want to run a group_by operation, like:

df.group_by('col1').agg(
    col2_g = pl.col('col2').some_function_like_join(',')
)

The expected output is:
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ col1 â”† col2_g    â”‚
â”‚ ---  â”† ---       â”‚
â”‚ str  â”† str       â”‚
â•žâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ a    â”† val1,val1 â”‚
â”‚ b    â”† val2,val3 â”‚
â”‚ c    â”† val3      â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

What is the name of the some_function_like_join function?
I have tried the following methods, and none work:
df.group_by('col1').agg(pl.col('col2').list.concat(','))
df.group_by('col1').agg(pl.col('col2').join(','))
df.group_by('col1').agg(pl.col('col2').list.join(','))

","If you want to concatenate them, I assume you want the result as a string with your specified delimiter:
out = df.group_by(&quot;col1&quot;).agg(
    pl.col(&quot;col2&quot;).str.join(&quot;,&quot;)
)

Result:
shape: (3, 2)
┌──────┬───────────┐
│ col1 ┆ col2      │
│ ---  ┆ ---       │
│ str  ┆ str       │
╞══════╪═══════════╡
│ a    ┆ val1,val1 │
│ b    ┆ val2,val3 │
│ c    ┆ val3      │
└──────┴───────────┘

If you want them within a List, you simply do:
out = df.groupby(&quot;col1&quot;).agg(
    pl.col(&quot;col2&quot;)
)

Result:
shape: (3, 2)
┌──────┬──────────────────┐
│ col1 ┆ col2             │
│ ---  ┆ ---              │
│ str  ┆ list[str]        │
╞══════╪══════════════════╡
│ a    ┆ [&quot;val1&quot;, &quot;val1&quot;] │
│ c    ┆ [&quot;val3&quot;]         │
│ b    ┆ [&quot;val2&quot;, &quot;val3&quot;] │
└──────┴──────────────────┘

"
"What's the alternative of pandas :
data['ColumnA'].str[:2]

in python polars?
pl.col('ColumnA').str[:3]

throws TypeError: 'ExprStringNameSpace' object is not subscriptable

error.
","You can use str.slice, It takes two arguments offset and length. offset specifies the start index and the length specifies the length of slice. If set to None (default), the slice is taken to the end of the string.
&gt;&gt;&gt; import polars as pl
&gt;&gt;&gt; 
&gt;&gt;&gt; df = pl.DataFrame({&quot;animal&quot;: [&quot;Crab&quot;, &quot;cat and dog&quot;, &quot;rab$bit&quot;, None]})
&gt;&gt;&gt; df
shape: (4, 1)
┌─────────────┐
│ animal      │
│ ---         │
│ str         │
╞═════════════╡
│ Crab        │
│ cat and dog │
│ rab$bit     │
│ null        │
└─────────────┘
&gt;&gt;&gt; df.with_columns(pl.col(&quot;animal&quot;).str.slice(0, 3).alias(&quot;sub_string&quot;))
shape: (4, 2)
┌─────────────┬────────────┐
│ animal      ┆ sub_string │
│ ---         ┆ ---        │
│ str         ┆ str        │
╞═════════════╪════════════╡
│ Crab        ┆ Cra        │
│ cat and dog ┆ cat        │
│ rab$bit     ┆ rab        │
│ null        ┆ null       │
└─────────────┴────────────┘

"
"As a minimum example, let's say we have next polars.DataFrame:
df = pl.DataFrame({&quot;sub_id&quot;: [1,2,3], &quot;engagement&quot;: [&quot;one:one,two:two&quot;, &quot;one:two,two:one&quot;, &quot;one:one&quot;], &quot;total_duration&quot;: [123, 456, 789]})



sub_id
engagement
total_duration




1
one:one,two:two
123


2
one:two,two:one
456


3
one:one
789



then, we explode &quot;engagement&quot; column
df = df.with_columns(pl.col(&quot;engagement&quot;).str.split(&quot;,&quot;)).explode(&quot;engagement&quot;)
and receive:



sub_id
engagement
total_duration




1
one:one
123


1
two:two
123


2
one:two
456


2
two:one
456


3
one:one
789



For visualization I use Plotly, and code would be following:
import plotly.express as px
fig = px.bar(df, x=&quot;sub_id&quot;, y=&quot;total_duration&quot;, color=&quot;engagement&quot;)
fig.show()

Resulting plot:

Now it basically means that subscribers 1 and 2 have their total_duration (total watched time) doubled.
How could I remain total_duration per sub, but leaving engagement groups as shown on the plot legend?
","An option to handle this in polars would be to split total_duration equally between engagement rows within sub_id. For this, we simply divide total_duration by the number of rows of the given sub_id.
(
    df
    .with_columns(
        pl.col(&quot;engagement&quot;).str.split(&quot;,&quot;)
    )
    .explode(&quot;engagement&quot;)
    .with_columns(
        pl.col(&quot;total_duration&quot;) / pl.len().over(&quot;sub_id&quot;)
    )
)


shape: (5, 3)
┌────────┬────────────┬────────────────┐
│ sub_id ┆ engagement ┆ total_duration │
│ ---    ┆ ---        ┆ ---            │
│ i64    ┆ str        ┆ f64            │
╞════════╪════════════╪════════════════╡
│ 1      ┆ one:one    ┆ 61.5           │
│ 1      ┆ two:two    ┆ 61.5           │
│ 2      ┆ one:two    ┆ 228.0          │
│ 2      ┆ two:one    ┆ 228.0          │
│ 3      ┆ one:one    ┆ 789.0          │
└────────┴────────────┴────────────────┘

"
"When selecting data with xarray at x,y locations, I get data for any pair of x,y. I would like to have a 1-D array not a 2-D array from the selection. Is there an efficient way to do this? (For now I am doing it with a for-loop...)
x = [x1,x2,x3,x4]
y = [y1,y2,y3,y4]
DS = 2-D array
subset = Dataset.sel(longitude=x, latitude=y, method='nearest')

To rephrase, I would like to have the dataset at [x1,y1],[x2,y2],[x3,y3],[x4,y4] not at other location i.e. [x1,y2].
","A list of points can be selected along multiple indices if the indexers are DataArrays with a common dimension. This will result in the array being reindexed along the indexers' common dimension.
Straight from the docs on More Advanced Indexing:
In [78]: da = xr.DataArray(np.arange(56).reshape((7, 8)), dims=['x', 'y'])

In [79]: da
Out[79]: 
&lt;xarray.DataArray (x: 7, y: 8)&gt;
array([[ 0,  1,  2,  3,  4,  5,  6,  7],
       [ 8,  9, 10, 11, 12, 13, 14, 15],
       [16, 17, 18, 19, 20, 21, 22, 23],
       [24, 25, 26, 27, 28, 29, 30, 31],
       [32, 33, 34, 35, 36, 37, 38, 39],
       [40, 41, 42, 43, 44, 45, 46, 47],
       [48, 49, 50, 51, 52, 53, 54, 55]])
Dimensions without coordinates: x, y

In [80]: da.isel(x=xr.DataArray([0, 1, 6], dims='z'),
   ....:         y=xr.DataArray([0, 1, 0], dims='z'))
   ....: 
Out[80]: 
&lt;xarray.DataArray (z: 3)&gt;
array([ 0,  9, 48])
Dimensions without coordinates: z

The indexing array can also be easily pulled out of a pandas DataFrame, with something like da.sel(longitude=df.longitude.to_xarray(), latitude=df.latitude.to_xarray()), which will result in the DataArray being reindexed by the DataFrame's index.
So in your case, rather than selecting with the lists or arrays x, y, turn them into DataArrays with a common dim - let's call it location:
x = xr.DataArray([x1,x2,x3,x4], dims=['location'])
y = xr.DataArray([y1,y2,y3,y4], dims=['location'])

Now your selection will work as you hope:
ds.sel(longitude=x, latitude=y, method='nearest')

"
"I tried to use Llama 3.1 without relying on external programs, but I was not successful. I downloaded the Meta-Llama-3.1-8B-Instruct model, which includes only the files consolidated.00.pth, params.json, and tokenizer.model.
The params.json file contains the following configuration:
{
  &quot;dim&quot;: 4096,
  &quot;n_layers&quot;: 32,
  &quot;n_heads&quot;: 32,
  &quot;n_kv_heads&quot;: 8,
  &quot;vocab_size&quot;: 128256,
  &quot;ffn_dim_multiplier&quot;: 1.3,
  &quot;multiple_of&quot;: 1024,
  &quot;norm_eps&quot;: 1e-05,
  &quot;rope_theta&quot;: 500000.0,
  &quot;use_scaled_rope&quot;: true
}

Can you guide me on how to use this model?
I have tried the following code:
import torch
from transformers import LlamaTokenizer, LlamaForCausalLM, LlamaConfig

model_path = 'Meta-Llama-3.1-8B-Instruct'
tokenizer_path = f'{model_path}/tokenizer.model'

# Load tokenizer
tokenizer = LlamaTokenizer.from_pretrained(tokenizer_path)

# Configure the model
model_config = LlamaConfig(
    hidden_size=4096,
    num_hidden_layers=32,
    num_attention_heads=32,
    intermediate_size=5324.8,  # This value is calculated as 4096 * 1.3
    vocab_size=128256,
    use_scaled_rope=True
)

# Load the model
model = LlamaForCausalLM(config=model_config)
model.load_state_dict(torch.load(f'{model_path}/consolidated.00.pth'))

model.eval()

# Tokenize and generate output
input_text = &quot;Hello, how are you?&quot;
inputs = tokenizer(input_text, return_tensors='pt')
outputs = model.generate(inputs['input_ids'])

# Decode and print the output
decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(decoded_output)

However, I got the following error:
(venv) PS C:\Users\Main\Desktop\mygguf&gt; python app.py
C:\Users\Main\Desktop\mygguf\venv\Lib\site-packages\transformers\tokenization_utils_base.py:2165: FutureWarning: Calling LlamaTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.
  warnings.warn(
You are using the default legacy behaviour of the &lt;class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'&gt;. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Traceback (most recent call last):
  File &quot;C:\Users\Main\Desktop\mygguf\app.py&quot;, line 9, in &lt;module&gt;
    tokenizer = LlamaTokenizer.from_pretrained(tokenizer_path)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Main\Desktop\mygguf\venv\Lib\site-packages\transformers\tokenization_utils_base.py&quot;, line 2271, in from_pretrained
    return cls._from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Main\Desktop\mygguf\venv\Lib\site-packages\transformers\tokenization_utils_base.py&quot;, line 2505, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Main\Desktop\mygguf\venv\Lib\site-packages\transformers\models\llama\tokenization_llama.py&quot;, line 171, in __init__
    self.sp_model = self.get_spm_processor(kwargs.pop(&quot;from_slow&quot;, False))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Main\Desktop\mygguf\venv\Lib\site-packages\transformers\models\llama\tokenization_llama.py&quot;, line 198, in get_spm_processor
    tokenizer.Load(self.vocab_file)
  File &quot;C:\Users\Main\Desktop\mygguf\venv\Lib\site-packages\sentencepiece\__init__.py&quot;, line 961, in Load
    return self.LoadFromFile(model_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\Main\Desktop\mygguf\venv\Lib\site-packages\sentencepiece\__init__.py&quot;, line 316, in LoadFromFile
    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Internal: could not parse ModelProto from Meta-Llama-3.1-8B-Instruct/tokenizer.model

","The way you should think about using llm model is that you have to pass it information systematically.
Since you are using a publicly available model they come with things like weights, cfg etc... so you don't need to declare yours.
All you need do is to start by declaring the file-paths of your model(i.e where you downloaded it).
Also there is tokenism (tokens are simply vectors which models understand they usually map it with the given words you ask it).
If the output is not the desired, You can use different tokenizers
You can look up the process of using different tokens or tokenizers such as BERT, All-net etc here is a link to a blog
You should also spend sometime on Huggingface website here is the link
hugging_face
Here is a snippet of how to use the model, I have provided comments on what each line does. I hope it helps you!
import torch
from transformers import AutoTokenizer, AutoModel
from transformers import LlamaTokenizer, LlamaForCausalLM, LlamaConfig

model_path = 'Meta-Llama-3.1-8B-Instruct'


# Load the tokenizer directly from the model path
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Load model configuration from params.json
config = LlamaConfig.from_json_file(f'{model_path}/params.json')

# load the model with the specific configs. 
model = LlamaForCausalLM(config=config)

# Load the weights of the model
state_dict = torch.load(f'{model_path}/consolidated.00.pth', map_location=torch.device('cpu'))
model.load_state_dict(state_dict)

model.eval()

# generate tokens and generate output
input_text = &quot;Hello, how are you?&quot;
inputs = tokenizer(input_text, return_tensors='pt')
outputs = model.generate(inputs['input_ids'])

# print the output you asked it 
output = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(output)

"
"Is there a correct way to have two walrus operators in 1 if statement?
if (three:= i%3==0) and (five:= i%5 ==0):
    arr.append(&quot;FizzBuzz&quot;)
elif three:
    arr.append(&quot;Fizz&quot;)
elif five:
    arr.append(&quot;Buzz&quot;)
else:
    arr.append(str(i-1))

This example works for three but five will be &quot;not defined&quot;.
","The logical operator and evaluates its second operand only conditionally. There is no correct way to have a conditional assignment that is unconditionally needed.
Instead use the &quot;binary&quot; operator &amp;, which evaluates its second operand unconditionally.
arr = []
for i in range(1, 25):
    #                        v force evaluation of both operands
    if (three := i % 3 == 0) &amp; (five := i % 5 == 0):
        arr.append(&quot;FizzBuzz&quot;)
    elif three:
        arr.append(&quot;Fizz&quot;)
    elif five:
        arr.append(&quot;Buzz&quot;)
    else:
        arr.append(str(i))

print(arr)
# ['1', '2', 'Fizz', '4', 'Buzz', 'Fizz', '7', '8', 'Fizz', 'Buzz', '11', ...]

Correspondingly, one can use | as an unconditional variant of or. In addition, the &quot;xor&quot; operator ^ has no equivalent with conditional evaluation at all.
Notably, the binary operators evaluate booleans as purely boolean  - for example, False | True is True not 1 – but may work differently for other types. To evaluate arbitrary values such as lists in a boolean context with binary operators, convert them to bool after assignment:
#  |~~~ force list to boolean ~~| | force evaluation of both operands
#  v    v~ walrus-assign list ~vv v
if bool(lines := list(some_file)) &amp; ((today := datetime.today()) == 0):
   ...

Since assignment expressions require parentheses for proper precedence, the common problem of different precedence between logical (and, or) and binary (&amp;, |, ^) operators is irrelevant here.
"
"I would like to ask how I can unnest a list of list and turn it into different columns of a dataframe. Specifically, I have the following dataframe where the Route_set column is a list of lists:
   Generation                              Route_set
0           0  [[20. 19. 47. 56.] [21. 34. 78. 34.]]

The desired output is the following dataframe:
   route1  route2
0      20      21
1      19      34
2      47      78
3      56      34

Any ideas how I can do it? Thank you in advance!
","You can try using df.explode and df.apply:
import pandas as pd

df = pd.DataFrame(data= {'Generation': 0, 'Route_set':[[[20., 19., 47., 56.], [21., 34., 78., 34.]]]})
df['route1']=df['Route_set'].apply(lambda x: x[0])
df['route2']=df['Route_set'].apply(lambda x: x[1])
df = df.explode(['route1', 'route2'], ignore_index=True)
df2 = df[df.columns.difference(['Route_set', 'Generation'])]

|    |   route1 |   route2 |
|---:|---------:|---------:|
|  0 |       20 |       21 |
|  1 |       19 |       34 |
|  2 |       47 |       78 |
|  3 |       56 |       34 |

Or you can just create a new dataframe with the values like this:
import pandas as pd

df = pd.DataFrame(data= {'Generation': 0, 'Route_set':[[[20., 19., 47., 56.], [21., 34., 78., 34.]]]})
df1 = pd.DataFrame.from_dict(dict(zip(['route1', 'route2'], df.Route_set.to_numpy()[0])), orient='index').transpose()

|    |   route1 |   route2 |
|---:|---------:|---------:|
|  0 |       20 |       21 |
|  1 |       19 |       34 |
|  2 |       47 |       78 |
|  3 |       56 |       34 |

Update 1:
import pandas as pd

df = pd.DataFrame(data= {'Generation': 0, 'Route_set':[
                                                       [[20.0, 19.0, 47.0, 56.0, 43.0, 53.0, 18.0, -1.0, -1.0, -1.0, -1.0, -1.0], [20.0, 51.0, 46.0, 37.0, 2.0, 57.0, 49.0, 36.0, 25.0, 5.0, 4.0, 34.0], [54.0, 23.0, 5.0, 46.0, 34.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0], [57.0, 48.0, 46.0, 35.0, 25.0, 27.0, 52.0, 8.0, 39.0, 22.0, 51.0, 28.0], [57.0, 16.0, 45.0, 25.0, 49.0, 38.0, 0.0, 46.0, 13.0, 18.0, 19.0, 20.0], [21.0, 11.0, 6.0, 33.0, 25.0, 49.0, 57.0, 29.0, 12.0, 3.0, -1.0, -1.0], [9.0, 15.0, 47.0, 42.0, 25.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0], [51.0, 25.0, 22.0, 14.0, 39.0, 8.0, 40.0, 0.0, 10.0, 26.0, 32.0, 47.0], [1.0, 33.0, 24.0, 46.0, 56.0, 30.0, 48.0, 51.0, -1.0, -1.0, -1.0, -1.0], [25.0, 31.0, 50.0, 17.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0], [57.0, 12.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0], [20.0, 41.0, 47.0, 15.0, 46.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0], [14.0, 44.0, 39.0, 25.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0], [20.0, 51.0, 25.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0], [57.0, 49.0, 5.0, 20.0, 37.0, 46.0, 36.0, 25.0, 39.0, 51.0, 48.0, -1.0], [5.0, 0.0, 33.0, 55.0, 25.0, 48.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0], [51.0, 32.0, 33.0, 24.0, 35.0, 8.0, 25.0, 4.0, 46.0, 1.0, 7.0, -1.0], [5.0, 25.0, 34.0, 46.0, 1.0, 9.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0], [38.0, 57.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0], [12.0, 57.0, 49.0, 25.0, 9.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]],
                                                      ]})

data = df.Route_set.to_numpy()[0]

df = pd.DataFrame.from_dict(dict(zip(['route{}'.format(i) for i in range(1, len(data)+1)], [data[i] for i in range(len(data))])), orient='index').transpose()
df = df.apply(lambda x: x.explode() if 'route' in x.name  else x)

df[sorted(df.columns)]
print(df.to_markdown())

|    |   route1 |   route2 |   route3 |   route4 |   route5 |   route6 |   route7 |   route8 |   route9 |   route10 |   route11 |   route12 |   route13 |   route14 |   route15 |   route16 |   route17 |   route18 |   route19 |   route20 |
|---:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|
|  0 |       20 |       20 |       54 |       57 |       57 |       21 |        9 |       51 |        1 |        25 |        57 |        20 |        14 |        20 |        57 |         5 |        51 |         5 |        38 |        12 |
|  1 |       19 |       51 |       23 |       48 |       16 |       11 |       15 |       25 |       33 |        31 |        12 |        41 |        44 |        51 |        49 |         0 |        32 |        25 |        57 |        57 |
|  2 |       47 |       46 |        5 |       46 |       45 |        6 |       47 |       22 |       24 |        50 |        -1 |        47 |        39 |        25 |         5 |        33 |        33 |        34 |        -1 |        49 |
|  3 |       56 |       37 |       46 |       35 |       25 |       33 |       42 |       14 |       46 |        17 |        -1 |        15 |        25 |        -1 |        20 |        55 |        24 |        46 |        -1 |        25 |
|  4 |       43 |        2 |       34 |       25 |       49 |       25 |       25 |       39 |       56 |        -1 |        -1 |        46 |        -1 |        -1 |        37 |        25 |        35 |         1 |        -1 |         9 |
|  5 |       53 |       57 |       -1 |       27 |       38 |       49 |       -1 |        8 |       30 |        -1 |        -1 |        -1 |        -1 |        -1 |        46 |        48 |         8 |         9 |        -1 |        -1 |
|  6 |       18 |       49 |       -1 |       52 |        0 |       57 |       -1 |       40 |       48 |        -1 |        -1 |        -1 |        -1 |        -1 |        36 |        -1 |        25 |        -1 |        -1 |        -1 |
|  7 |       -1 |       36 |       -1 |        8 |       46 |       29 |       -1 |        0 |       51 |        -1 |        -1 |        -1 |        -1 |        -1 |        25 |        -1 |         4 |        -1 |        -1 |        -1 |
|  8 |       -1 |       25 |       -1 |       39 |       13 |       12 |       -1 |       10 |       -1 |        -1 |        -1 |        -1 |        -1 |        -1 |        39 |        -1 |        46 |        -1 |        -1 |        -1 |
|  9 |       -1 |        5 |       -1 |       22 |       18 |        3 |       -1 |       26 |       -1 |        -1 |        -1 |        -1 |        -1 |        -1 |        51 |        -1 |         1 |        -1 |        -1 |        -1 |
| 10 |       -1 |        4 |       -1 |       51 |       19 |       -1 |       -1 |       32 |       -1 |        -1 |        -1 |        -1 |        -1 |        -1 |        48 |        -1 |         7 |        -1 |        -1 |        -1 |
| 11 |       -1 |       34 |       -1 |       28 |       20 |       -1 |       -1 |       47 |       -1 |        -1 |        -1 |        -1 |        -1 |        -1 |        -1 |        -1 |        -1 |        -1 |        -1 |        -1 |

"
"I have an ASGI middleware that adds fields to the POST request body before it hits the route in my fastapi app.
from starlette.types import ASGIApp, Message, Scope, Receive, Send

class MyMiddleware:
    &quot;&quot;&quot;
    This middleware implements a raw ASGI middleware instead of a starlette.middleware.base.BaseHTTPMiddleware
    because the BaseHTTPMiddleware does not allow us to modify the request body.
    For documentation see https://www.starlette.io/middleware/#pure-asgi-middleware
    &quot;&quot;&quot;
    def __init__(self, app: ASGIApp):
        self.app = app

    async def __call__(self, scope: Scope, receive: Receive, send: Send):
        if scope[&quot;type&quot;] != &quot;http&quot;:
            await self.app(scope, receive, send)
            return &quot;&quot;

        async def modify_message():
            message: dict = await receive()
            if message.get(&quot;type&quot;, &quot;&quot;) != &quot;http.request&quot;:
                return message
            if not message.get(&quot;body&quot;, None):
                return message
            body: dict = json.loads(message.get(&quot;body&quot;, b&quot;'{}'&quot;).decode(&quot;utf-8&quot;))
            body[&quot;some_field&quot;] = &quot;foobar&quot;
            message[&quot;body&quot;] = json.dumps(body).encode(&quot;utf-8&quot;)
            return message

        await self.app(scope, modify_message, send)

Is there an example on how to unit test an ASGI middleware? I would like to test directly the __call__ part which is difficult as it does not return anything. Do I need to use a test api client (e.g. TestClient from fastapi) to then create some dummy endpoint which returns the request as response and thereby check if the middleware was successful or is there a more &quot;direct&quot; way?
","I've faced the similar problem recently, so I want to share my solution for fastapi and pytest.
I had to implement per request logs for the fastapi app using middlewares.
I've checked Starlette's test suite as Marcelo Trylesinski suggested and adapted the code to fit fastapi. Thank you for the recommendation, Marcelo!
Here is my middleware that logs information from every request and response.
# middlewares.py
import logging

from starlette.types import ASGIApp, Scope, Receive, Send


logger = logging.getLogger(&quot;app&quot;)


class LogRequestsMiddleware:
    def __init__(self, app: ASGIApp) -&gt; None:
        self.app = app

    async def __call__(
        self, scope: Scope, receive: Receive, send: Send
    ) -&gt; None:
        async def send_with_logs(message):
            &quot;&quot;&quot;Log every request info and response status code.&quot;&quot;&quot;
            if message[&quot;type&quot;] == &quot;http.response.start&quot;:
                # request info is stored in the scope
                # status code is stored in the message
                logger.info(
                    f'{scope[&quot;client&quot;][0]}:{scope[&quot;client&quot;][1]} - '
                    f'&quot;{scope[&quot;method&quot;]} {scope[&quot;path&quot;]} '
                    f'{scope[&quot;scheme&quot;]}/{scope[&quot;http_version&quot;]}&quot; '
                    f'{message[&quot;status&quot;]}'
                )
            await send(message)

        await self.app(scope, receive, send_with_logs)

To test a middleware, I had to create test_factory_client fixture:
# conftest.py
import pytest

from fastapi.testclient import TestClient


@pytest.fixture
def test_client_factory() -&gt; TestClient:
    return TestClient

In the test, I mocked logger.info() call within the middleware and asserted if the method was called.
# test_middlewares.py
from unittest import mock
from fastapi.testclient import TestClient
from fastapi import FastAPI
from .middlewares import LogRequestsMiddleware

# mock logger call within the pure middleware
@mock.patch(&quot;path.to.middlewares.logger.info&quot;)
def test_log_requests_middleware(
    mock_logger, test_client_factory: TestClient
):
    # create a fresh app instance to isolate tested middlewares
    app = FastAPI()
    app.add_middleware(LogRequestsMiddleware)
    
    # create an endpoint to test middlewares
    @app.get(&quot;/&quot;)
    def homepage():
        return {&quot;hello&quot;: &quot;world&quot;}

    # create a client for the app using fixure
    client = test_client_factory(app)

    # call an endpoint
    response = client.get(&quot;/&quot;)

    # sanity check
    assert response.status_code == 200
    # check if the logger was called
    mock_logger.assert_called_once()
    

"
"I wanted to count the number of three way conversations that have occured in a dataset.
A chat group_x can consist of multiple members.
What is a three way conversation?

1st way - red_x sends a message in the group_x.
2nd way - green_x replies in the same group_x.
3rd way - red_x sends a reply in the same group_x.

This can be called a three way conversation.
The sequence has to be exactly red_#, green_#, red_#.
What is touchpoint?

Touchpoint 1 - red_x's first message.
Touchpoint 2 - green_x's first message.
Touchpoint 3 - red_x's second message.

Code to easily generate a sample dataset I'm working with.
import pandas as pd
from pandas import Timestamp

t1_df = pd.DataFrame({'from_red': [True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True], 
              'sent_time': [Timestamp('2021-05-01 06:26:00'), Timestamp('2021-05-04 10:35:00'), Timestamp('2021-05-07 12:16:00'), Timestamp('2021-05-07 12:16:00'), Timestamp('2021-05-09 13:39:00'), Timestamp('2021-05-11 10:02:00'), Timestamp('2021-05-12 13:10:00'), Timestamp('2021-05-12 13:10:00'), Timestamp('2021-05-13 09:46:00'), Timestamp('2021-05-13 22:30:00'), Timestamp('2021-05-14 14:14:00'), Timestamp('2021-05-14 17:08:00'), Timestamp('2021-06-01 09:22:00'), Timestamp('2021-06-01 21:26:00'), Timestamp('2021-06-03 20:19:00'), Timestamp('2021-06-03 20:19:00'), Timestamp('2021-06-09 07:24:00'), Timestamp('2021-05-01 06:44:00'), Timestamp('2021-05-01 08:01:00'), Timestamp('2021-05-01 08:09:00')], 
              'w_uid': ['w_000001', 'w_112681', 'w_002516', 'w_002514', 'w_004073', 'w_005349', 'w_006803', 'w_006804', 'w_008454', 'w_009373', 'w_010063', 'w_010957', 'w_066840', 'w_071471', 'w_081446', 'w_081445', 'w_106472', 'w_000002', 'w_111906', 'w_000003'], 
              'user_id': ['red_00001', 'green_0263', 'red_01071', 'red_01071', 'red_01552', 'red_01552', 'red_02282', 'red_02282', 'red_02600', 'red_02854', 'red_02854', 'red_02600', 'red_00001', 'red_09935', 'red_10592', 'red_10592', 'red_12292', 'red_00002', 'green_0001', 'red_00003'], 
              'group_id': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], 
              'touchpoint': [1, 2, 1, 3, 1, 3, 1, 3, 1, 1, 3, 3, 3, 1, 1, 3, 1, 1, 2, 1]}, 
                     columns = ['from_red', 'sent_time', 'w_uid', 'user_id', 'group_id', 'touchpoint'])

t1_df['sent_time'] = pd.to_datetime(t1_df['sent_time'], format = &quot;%d-%m-%Y&quot;)
t1_df

The dataset looks like this:




from_red
sent_time
w_uid
user_id
group_id
touchpoint




True
2021-05-01 06:26:00
w_000001
red_00001
0
1


False
2021-05-04 10:35:00
w_112681
green_0263
0
2


True
2021-05-07 12:16:00
w_002516
red_01071
0
1


True
2021-05-07 12:16:00
w_002514
red_01071
0
3


True
2021-05-09 13:39:00
w_004073
red_01552
0
1


True
2021-05-11 10:02:00
w_005349
red_01552
0
3


True
2021-05-12 13:10:00
w_006803
red_02282
0
1


True
2021-05-12 13:10:00
w_006804
red_02282
0
3


True
2021-05-13 09:46:00
w_008454
red_02600
0
1


True
2021-05-13 22:30:00
w_009373
red_02854
0
1


True
2021-05-14 14:14:00
w_010063
red_02854
0
3


True
2021-05-14 17:08:00
w_010957
red_02600
0
3


True
2021-06-01 09:22:00
w_066840
red_00001
0
3


True
2021-06-01 21:26:00
w_071471
red_09935
0
1


True
2021-06-03 20:19:00
w_081446
red_10592
0
1


True
2021-06-03 20:19:00
w_081445
red_10592
0
3


True
2021-06-09 07:24:00
w_106472
red_12292
0
1


True
2021-05-01 06:44:00
w_000002
red_00002
1
1


False
2021-05-01 08:01:00
w_111906
green_0001
1
2


True
2021-05-01 08:09:00
w_000003
red_00003
1
1




Here is what I have tried, but the query is taking too long. Is there a faster way to achieve the same?
test_df = pd.DataFrame()
for i in range(len(t1_df['sent_time'])-1):
    if t1_df.query(f&quot;group_id == {i}&quot;)['from_red'].nunique() == 2:
        y = t1_df.query(f&quot;group_id == {i} &amp; touchpoint == 2&quot;).loc[:, ['sent_time']].values[0][0]
        x = t1_df.query(f&quot;group_id == {i} &amp; sent_time &gt; @y &amp; (touchpoint == 3)&quot;).sort_values('sent_time')
        test_df = pd.concat([test_df, x])
        test_df.merge(x, how = &quot;outer&quot;)
        
    else:
        pass

test_df

","For me it's not clear how you define the &quot;three way conversation&quot;. Within on group, if you have the input messages what option(s) do you consider as &quot;three way conversation&quot;? There are several options:
Input  : red_0, red_2, green_0, red_1, red_0, red_2, red_1
Option1:        red_2, green_0, red_1
Option2: red_0,        green_0,        red_0
   +   :        red_2, green_0,               red_2

and many more. Your code example returns the second msg of a user when sent after green:
OptionX:               green_0,         red_0
   +   :               green_0,               red_2
   +   :               green_0,                      red_1

without keeping track if some red user sent a msg before green. Another question is, what happens if green is sending multiple times within one group.
Input  : red_0, red_2, green_0, green_0, red_1, red_0, green_1, red_1

Based on your description &quot;The sequence has to be exactly red_#, green_#, red_#.&quot; I guess, Option1 is what you are looking for and maybe that it's even independent from the color: color0_#, color1_#, color0_#. Correct me if I'm wrong ;).
Prepare the DataFrame
To get the operation more generic, I would first prepare the DataFrame, e.g. extract the color of the user and get a integer represenation for the color
# extract the user color and id
t1_df[['color', 'id']] = t1_df.pop('user_id').str.split('_', expand=True)
# get the dtypes right, also it is not needed here
t1_df.id = t1_df.id.astype(int)
t1_df.color = t1_df.color.astype('category')
# get color as intager
t1_df['color_as_int'] =pd.factorize(t1_df.color)[0]

Detect the sequence color0_#, color1_#, color0_#
# a three way conversation is where color_as_int is [...,a,b,a,...]
# expressed as difference it's color_as_int.diff() is [...,c,-c,...]
# get the difference with tracking the group, therefore first sort
t1_df.sort_values(['group_id', 'sent_time'], inplace=True)
d_color = t1_df.groupby(['group_id']).color_as_int.diff()
m = (d_color != 0) &amp; (d_color == -d_color.shift(-1))  # detect [...,c,-c,...]
# count up for each three way conversation
m[m] = m[m].cumsum()
m = m.astype(int)

# get the labels for the dataframe [...,a,b,a,...]
t1_df['three_way_conversation'] = m + m.shift(1, fill_value=0) + m.shift(-1, fill_value=0)

which returns and works for any color
columns = ['sent_time', 'group_id', 'color', 'id', 'touchpoint']
print(t1_df.loc[t1_df['three_way_conversation']&gt;0, columns])

             sent_time  group_id  color    id  touchpoint
0  2021-05-01 06:26:00         0    red     1           1
1  2021-05-04 10:35:00         0  green   263           2
2  2021-05-07 12:16:00         0    red  1071           1
17 2021-05-01 06:44:00         1    red     2           1
18 2021-05-01 08:01:00         1  green     1           2
19 2021-05-01 08:09:00         1    red     3           1

Bonus
with the DataFrame preparation you can easily count the msg per color or user within a group or get the first and last time of a msg from a color or user. cumcount is faster as count and pd.merg() afterwards.
t1_df['color_msg_count'] = t1_df.groupby(['group_id', 'color']).cumcount() + 1
t1_df['user_msg_count'] = t1_df.groupby(['group_id', 'color', 'id']).cumcount() + 1

t1_df['user_sent_time_min'] = t1_df.sort_values('sent_time').groupby(['group_id', 'color', 'id']).sent_time.cummin()
t1_df['user_sent_time_max'] = t1_df.sort_values('sent_time', ascending=False).groupby(['group_id', 'color', 'id']).sent_time.cummax()

"
"I want to create on Abstract Model class for future inheriting like this:
class AbstractModel(models.Model):

    created_at = models.DateTimeField(
        auto_now_add=True,
        blank=True,
        null=True,
    )

    created_by = models.ForeignKey(
        settings.AUTH_USER_MODEL,
        on_delete=models.SET_NULL,
        related_name='XXX_created_by',
        blank=True,
        null=True,
    )

    class Meta:
        abstract = True

Field 'created_at' is working fine, but how to generate related_name in 'created_by' for my child classes to prevent clashing?
","As the Be careful with related_name and related_query_name section of the documentation says, you can:

To work around this problem, when you are using related_name or related_query_name in an abstract base class (only), part of the value should contain '%(app_label)s' and '%(class)s'.

'%(class)s' is replaced by the lowercased name of the child class that the field is used in.

'%(app_label)s' is replaced by the lowercased name of the app the child class is contained within. Each installed application name must be unique and the model class names within each app must also be unique, therefore the resulting name will end up being different.



You thus can work with:
class AbstractModel(models.Model):
    # &hellip;
    created_by = models.ForeignKey(
        settings.AUTH_USER_MODEL,
        on_delete=models.SET_NULL,
        related_name='%(class)s_created_by',
        blank=True,
        null=True,
    )

    class Meta:
        abstract = True
Then the related_name will be foo_created_by if the name of the model that inherits is named foo.
Or if the same model name can occur in different apps:
class AbstractModel(models.Model):
    # &hellip;
    created_by = models.ForeignKey(
        settings.AUTH_USER_MODEL,
        on_delete=models.SET_NULL,
        related_name='%(app_label)s_%(class)s_created_by',
        blank=True,
        null=True,
    )

    class Meta:
        abstract = True
Then the related_name will be bar_foo_created_by if the name of the model that inherits is named foo in an app named bar.
"
"Creating a vector store with the Python library langchain may take a while. How can I add a progress bar?

Example of code where a vector store is created with langchain:
import pprint
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.docstore.document import Document

model = &quot;sentence-transformers/multi-qa-MiniLM-L6-cos-v1&quot;
embeddings = HuggingFaceEmbeddings(model_name = model)

def main():
    doc1 = Document(page_content=&quot;The sky is blue.&quot;,    metadata={&quot;document_id&quot;: &quot;10&quot;})
    doc2 = Document(page_content=&quot;The forest is green&quot;, metadata={&quot;document_id&quot;: &quot;62&quot;})
    docs = []
    docs.append(doc1)
    docs.append(doc2)

    for doc in docs:
        doc.metadata['summary'] = 'hello'

    pprint.pprint(docs)
    db = FAISS.from_documents(docs, embeddings)
    db.save_local(&quot;faiss_index&quot;)
    new_db = FAISS.load_local(&quot;faiss_index&quot;, embeddings)

    query = &quot;Which color is the sky?&quot;
    docs = new_db.similarity_search_with_score(query)
    print('Retrieved docs:', docs)
    print('Metadata of the most relevant document:', docs[0][0].metadata)

if __name__ == '__main__':
    main()

Tested with Python 3.11 with:
pip install langchain==0.1.1 langchain_openai==0.0.2.post1 sentence-transformers==2.2.2 langchain_community==0.0.13 faiss-cpu==1.7.4

The vector store is created with db = FAISS.from_documents(docs, embeddings).
","Langchain does not natively support any progress bar for this at the moment with release of 1.0.0
I also had similar case, so instead of sending all the documents, I send independent document for ingestion and tracked progress at my end. This was helpful for me.
You can do the ingestion in the following way
    with tqdm(total=len(docs), desc=&quot;Ingesting documents&quot;) as pbar:
        for d in docs:
            if db:
                db.add_documents([d])
            else:
                db = FAISS.from_documents([d], embeddings)
            pbar.update(1)  


From what I checked from langchain code https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/retrievers.py#L31 they are making call to add_texts as well, so no major operation is being performed here other than parsing.
I had simple documents, and I didn't observe much difference. Probably others who has tried on huge documents can add if it adds latency in their usecase.
Below is your updated code
import pprint
from tqdm import tqdm
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.docstore.document import Document

model = &quot;sentence-transformers/multi-qa-MiniLM-L6-cos-v1&quot;
embeddings = HuggingFaceEmbeddings(model_name = model)

def main():
    doc1 = Document(page_content=&quot;The sky is blue.&quot;,    metadata={&quot;document_id&quot;: &quot;10&quot;})
    doc2 = Document(page_content=&quot;The forest is green&quot;, metadata={&quot;document_id&quot;: &quot;62&quot;})
    docs = []
    docs.append(doc1)
    docs.append(doc2)

    for doc in docs:
        doc.metadata['summary'] = 'hello'

    db = None
    with tqdm(total=len(docs), desc=&quot;Ingesting documents&quot;) as pbar:
        for d in docs:
            if db:
                db.add_documents([d])
            else:
                db = FAISS.from_documents([d], embeddings)
            pbar.update(1)  

    # pprint.pprint(docs)
    # db = FAISS.from_documents(docs, embeddings)
    db.save_local(&quot;faiss_index&quot;)
    new_db = FAISS.load_local(&quot;faiss_index&quot;, embeddings)

    query = &quot;Which color is the sky?&quot;
    docs = new_db.similarity_search_with_score(query)
    print('Retrieved docs:', docs)
    print('Metadata of the most relevant document:', docs[0][0].metadata)

if __name__ == '__main__':
    main()

"
"Is it possible to add a progress bar to a Polars apply loop with a custom function?
For example, how would I add a progress bar to the following toy example:
df = pl.DataFrame(
    {
        &quot;team&quot;: [&quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;C&quot;],
        &quot;conference&quot;: [&quot;East&quot;, &quot;East&quot;, &quot;East&quot;, &quot;West&quot;, &quot;West&quot;, &quot;East&quot;],
        &quot;points&quot;: [11, 8, 10, 6, 6, 5],
        &quot;rebounds&quot;: [7, 7, 6, 9, 12, 8]
    }
)

df.group_by(&quot;team&quot;).map_groups(lambda x: x.select(pl.col(&quot;points&quot;).mean()))

Edit 1:
After help from @Jcurious, I have the following 'tools' that can be re-used for other functions, however it does not print to console correctly.
def pl_progress_applier(func, task_id, progress, **kwargs):
    progress.update(task_id, advance=1, refresh=True)
    return func(**kwargs)

def pl_groupby_progress_apply(data, group_by, func, drop_cols=[], **kwargs):
    global progress
    with Progress() as progress:
        num_groups = len(data.select(group_by).unique())
        task_id = progress.add_task('Applying', total=num_groups)
        return (
            data
                .group_by(group_by)
                .map_groups(lambda x: pl_progress_applier(
                    x=x.drop(drop_cols), func=func, task_id=task_id, progress=progress, **kwargs)
                )
        )

# and using the function custom_func, we can return a table, howevef the progress bar jumps to 100%

def custom_func(x):
    return x.select(pl.col('points').mean())

pl_groupby_progress_apply(
    data=df,
    group_by='team',
    func=custom_func
)

Any ideas on how to get the progress bar to actually work?
Edit 2:
It seems like the above functions do indeed work, however if you're using PyCharm (like me), then it does not work. Enjoy non-PyCharm users!
","I like the progress bars from Rich (which also comes bundled with pip)
There's probably a neater way to package this up, but something like:
from pip._vendor.rich.progress import (
    Progress, SpinnerColumn, TimeElapsedColumn
)

def polars_bar(total, title=&quot;Processing&quot;, transient=True):
    bar = Progress( 
        SpinnerColumn(),
        *Progress.get_default_columns(),
        TimeElapsedColumn(),
        transient=transient # remove bar when finished
    )
    
    def _run(func, *args, **kwargs):
        task_id = bar.add_task(title, total=total)
        def _execute(*args, **kwargs):
            bar.update(task_id, advance=1)
            return func(*args, **kwargs)
        return lambda self: _execute(self, *args, **kwargs)
        
    bar.run = _run
    
    return bar

Examples
.map_groups()
def my_custom_group_udf(group, expr):
    time.sleep(.7)
    return group.select(expr)
    
num_groups = df[&quot;team&quot;].n_unique()

with polars_bar(total=num_groups) as bar:
    (df.group_by(&quot;team&quot;)
       .map_groups(
           bar.run(
               my_custom_group_udf, 
               expr=pl.col(&quot;points&quot;).mean().name.suffix(&quot;_mean&quot;)
           )
       )
    )


.map_elements()
def my_custom_udf(points, multiplier=1):
    time.sleep(.3) # simulate some work
    return (points + 100) * multiplier
    
with polars_bar(total=df.height) as bar:
    df.with_columns(
        pl.col(&quot;points&quot;).map_elements(
            bar.run(my_custom_udf, multiplier=5),
            return_dtype = pl.Int64
        )
        .alias(&quot;udf&quot;)
    )


Note: tqdm also has Rich support: https://tqdm.github.io/docs/rich/
"
"I have the following model in pydantic (Version 2.0.3)
from typing import Tuple
from pydantic import BaseModel

class Model(BaseModel):
    test_field: Tuple[int]

But when I enter
model = Model(test_field=(1,2))

I get as error:
Traceback (most recent call last):
  File &quot;/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/code.py&quot;, line 90, in runcode
    exec(code, self.locals)
  File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/Users/tobi/Documents/scraiber/z_legacy/fastapi_test_app/venv/lib/python3.10/site-packages/pydantic/main.py&quot;, line 150, in __init__
    __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)
pydantic_core._pydantic_core.ValidationError: 1 validation error for Model
test_field
  Tuple should have at most 1 item after validation, not 2 [type=too_long, input_value=(1, 2), input_type=tuple]
    For further information visit https://errors.pydantic.dev/2.0.3/v/too_long

Do you know how I can fix that?
","Following @Tim Robert's Answer, the linked PR suggests using the Ellipsis ... is the syntax you're after!
https://github.com/pydantic/pydantic/pull/512/files
class Model(BaseModel):
    test_field: Tuple[int, ...]

&gt;&gt;&gt; Model(test_field=(1,2))
Model(test_field=(1, 2))


Additionally, and though I don't think it's really advisable (prefer codegen or reconsider the design), you can generate and expand given an exact count of the fields
count = 5
class Model_Five(BaseModel):
    test_field: Tuple[*([int]*count)]

&gt;&gt;&gt; Model_Five(test_field=(1,2,3,4,5))
Model_Five(test_field=(1, 2, 3, 4, 5))
&gt;&gt;&gt; Model_Five(test_field=(1,2,3,4))
[..] omitted
test_field.4
  Field required [type=missing, input_value=(1, 2, 3, 4), input_type=tuple]
    For further information visit https://docs.pydantic.dev/dev/errors/validation_errors/#missing

Finally, there might be cases where you need a longer form like *(cls for _ in range(count)) (or the even more troublesome for sustainability *(some_class_factory() for _ in range(count))) to expressly avoid having the inner values refer to the same object
"
"I am trying to build a machine learning model which predicts a single number from a series of numbers. I am using a Sequential model from the keras API of Tensorflow.
You can imagine my dataset to look something like this:




Index
x data
y data




0
np.ndarray(shape (1209278,) )
numpy.float32


1
np.ndarray(shape (1211140,) )
numpy.float32


2
np.ndarray(shape (1418411,) )
numpy.float32


3
np.ndarray(shape (1077132,) )
numpy.float32


...
...
...




This was my first attempt:
I tried using a numpy ndarray which contains numpy ndarrays which finally contain floats as my xdata, so something like this:
array([
    array([3.59280851, 3.60459062, 3.60459062, ..., 4.02911493])
    array([3.54752101, 3.56740332, 3.56740332, ..., 4.02837855])
    array([3.61048168, 3.62152741, 3.62152741, ..., 4.02764217])
])

My y data is a numpy ndarray containing floats, which looks something like this
array([2.9864411, 3.0562437, ... , 2.7750807, 2.8712902], dtype=float32)

But when I tried to train the model using model.fit() it yields this error:
ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).

I was able to solve this error by asking a question related to this:
How can I have a series of numpy ndarrays as the input data to train a tensorflow machine learning model?
My latest attempt:
Because Tensorflow does not seem to be able to convert a ndarray of ndarrays to a tensor, I tried to convert my x data to a list of ndarrays like this:
[
    array([3.59280851, 3.60459062, 3.60459062, ..., 4.02911493])
    array([3.54752101, 3.56740332, 3.56740332, ..., 4.02837855])
    array([3.61048168, 3.62152741, 3.62152741, ..., 4.02764217])
]

I left my y data untouched, so as a ndarray of floats.
Sadly my attempt of using a list of ndarrays instead of a ndarray of ndarrays yielded this error:
ValueError: Data cardinality is ambiguous:
  x sizes: 1304593, 1209278, 1407624, ...
  y sizes: 46
Make sure all arrays contain the same number of samples.

As you can see, my x data consists of arrays which all have a different shape.
But I don't think that this should be a problem.
Question:
My guess is that Tensorflow tries to use my list of arrays as multiple inputs.
Tensorflow fit() documentation
But I don't want to use my x data as multiple inputs.
Easily said I just want my model to predict a number from a sequence of numbers.
For example like this:

array([3.59280851, 3.60459062, 3.60459062, ...]) =&gt; 2.8989773
array([3.54752101, 3.56740332, 3.56740332, ...]) =&gt; 3.0893357
...

How can I use a sequence of numbers to predict a single number in Tensorflow?
EDIT
Maybe I should have added that I want to use a RNN, especially a LSTM.
I have had a look at the Keras documentation, and in their simplest example they are using a Embedding layer. But I don't really know what to do.
All in all I think that my question ist pretty general and should be easy to answer if you know how to tackle this problem, unlike me.
Thanks in advance!
","Try something like this:
import numpy as np
import tensorflow as tf

# add additional dimension for lstm layer
x_train = np.asarray(train_set[&quot;x data&quot;].values))[..., None] 
y_train = np.asarray(train_set[&quot;y data&quot;]).astype(np.float32)

model = tf.keras.Sequential()
model.add(tf.keras.layers.LSTM(units=32))
model.add(tf.keras.layers.Dense(units=1))
model.compile(loss=&quot;mean_squared_error&quot;, optimizer=&quot;adam&quot;, metrics=&quot;mse&quot;)
model.fit(x=x_train,y=y_train,epochs=10)

Or with a ragged input for different sequence lengths:
x_train = tf.ragged.constant(train_set[&quot;x data&quot;].values[..., None]) # add additional dimension for lstm layer
y_train = np.asarray(train_set[&quot;y data&quot;]).astype(np.float32)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Input(shape=[None, x_train.bounding_shape()[-1]], batch_size=2, dtype=tf.float32, ragged=True))
model.add(tf.keras.layers.LSTM(units=32))
model.add(tf.keras.layers.Dense(units=1))
model.compile(loss=&quot;mean_squared_error&quot;, optimizer=&quot;adam&quot;, metrics=&quot;mse&quot;)
model.fit(x=x_train,y=y_train,epochs=10)

Or:
x_train = tf.ragged.constant([np.array(list(v))[..., None] for v in train_set[&quot;x data&quot;].values]) # add additional dimension for lstm layer

"
"np.cumsum([1, 2, 3, np.nan, 4, 5, 6]) will return nan for every value after the first np.nan. Moreover, it will do the same for any generator. However, np.cumsum(df['column']) will not. What does np.cumsum(...) do, such that dataframes are treated specially?
In [2]: df = pd.DataFrame({'column': [1, 2, 3, np.nan, 4, 5, 6]})

In [3]: np.cumsum(df['column'])
Out[3]: 
0     1.0
1     3.0
2     6.0
3     NaN
4    10.0
5    15.0
6    21.0
Name: column, dtype: float64

","When you call np.cumsum(object) with an object that is not a numpy array, it will try calling object.cumsum() See this thread for details
. You can also see it in the Numpy source.
The pandas method has a default of skipna=True. So np.cumsum(df) gets turned into the equivalent of df.cumsum(axis=None, skipna=True, *args, **kwargs), which, of course skips the NaN values. The Numpy method does not have a skipna option.
You can also verify this yourself by overriding the pandas method with your own:
class DF(pd.DataFrame):
    def cumsum(self, axis=None, skipna=True, *args, **kwargs):
        print('calling pandas cumsum')
        return super().cumsum(axis=None, skipna=True, *args, **kwargs)

df = DF({'column': [1, 2, 3, np.nan, 4, 5, 6]})

# does calling the numpy function call your pandas method?   
np.cumsum(df)

This will print
calling pandas cumsum

and return the expected result:
    column
0   1.0
1   3.0
2   6.0
3   NaN
4   10.0
5   15.0
6   21.0

You can then experiment with the result of changing skipna=True.
"
"How can I read/write data to Raspberry Pi Pico using Python/MicroPython over the USB connection?
","
Use Thonny to put MicroPython code on Raspberry Pi Pico. Save it as 'main.py'.
Unplug Raspberry Pi Pico USB.
Plug Raspberry Pi Pico USB back in. (don't hold do the boot button).
Run the PC Python code to send and receive data between PC and Raspberry Pi Pico.

Code for Raspberry Pi Pico:

Read data from sys.stdin.
Write data using print.
poll to check if data is in the buffer.

import select
import sys
import time

# Set up the poll object
poll_obj = select.poll()
poll_obj.register(sys.stdin, select.POLLIN)

# Loop indefinitely
while True:
    # Wait for input on stdin
    poll_results = poll_obj.poll(1) # the '1' is how long it will wait for message before looping again (in microseconds)
    if poll_results:
        # Read the data from stdin (read data coming from PC)
        data = sys.stdin.readline().strip()
        # Write the data to the input file
        sys.stdout.write(&quot;received data: &quot; + data + &quot;\r&quot;)
    else:
        # do something if no message received (like feed a watchdog timer)
        continue

Code for PC:
import serial


def main():
    s = serial.Serial(port=&quot;COM3&quot;, parity=serial.PARITY_EVEN, stopbits=serial.STOPBITS_ONE, timeout=1)
    s.flush()

    s.write(&quot;data\r&quot;.encode())
    mes = s.read_until().strip()
    print(mes.decode())


if __name__ == &quot;__main__&quot;:
    main()

serial is PySerial.
"
"I am attempting to create an Iceberg Table on S3 using the Glue Catalog and the PyIceberg library. My goal is to define a schema, partitioning specifications, and then create a table using PyIceberg. However, despite multiple attempts, I haven't been able to achieve this successfully and keep encountering an error related to empty path components in metadata paths.
Here's a simplified version of the code I'm using:
import boto3
from pyiceberg.catalog import load_catalog
from pyiceberg.schema import Schema
from pyiceberg.types import TimestampType, DoubleType, StringType, NestedField
from pyiceberg.partitioning import PartitionSpec, PartitionField
from pyiceberg.transforms import YearTransform, MonthTransform, DayTransform

def create_iceberg_table():
    # Replace with your S3 bucket and table names
    s3_bucket = &quot;my-bucket-name&quot;
    table_name = &quot;my-table-name&quot;
    database_name = &quot;iceberg_catalog&quot;

    # Define the table schema
    schema = Schema(
        NestedField(field_id=1, name=&quot;field1&quot;, field_type=DoubleType(), required=False),
        NestedField(field_id=2, name=&quot;field2&quot;, field_type=StringType(), required=False),
        # ... more fields ...
    )

    # Define the partitioning specification with transformations
    partition_spec = PartitionSpec(
        PartitionField(field_id=3, source_id=3, transform=YearTransform(), name=&quot;year&quot;),
        PartitionField(field_id=3, source_id=3, transform=MonthTransform(), name=&quot;month&quot;),
        # ... more partition fields ...
    )

    # Create the Glue client
    glue_client = boto3.client(&quot;glue&quot;)

    # Specify the catalog URI where Glue should store the metadata
    catalog_uri = f&quot;s3://{s3_bucket}/catalog&quot;
    # Load the Glue catalog for the specified database
    catalog = load_catalog(&quot;test&quot;, client=glue_client, uri=catalog_uri, type=&quot;GLUE&quot;)

    # Create the Iceberg table in the Glue Catalog
    catalog.create_table(
        identifier=f&quot;{database_name}.{table_name}&quot;,
        schema=schema,
        partition_spec=partition_spec,
        location=f&quot;s3://{s3_bucket}/{table_name}/&quot;
    )

    print(&quot;Iceberg table created successfully!&quot;)

if __name__ == &quot;__main__&quot;:
    create_iceberg_table()

My understanding is that the PyIceberg library interacts with the Glue Catalog to manage metadata, schema, and partitions, but I seem to be missing a crucial step or misconfiguring something.
How can I properly generate an Iceberg Table on S3 using the Glue Catalog and PyIceberg?
Traceback:
Traceback (most recent call last):
  File &quot;/home/workspaceuser/app/create_iceberg_tbl.py&quot;, line 72, in &lt;module&gt;
    create_iceberg_table()
  File &quot;/home/workspaceuser/app/create_iceberg_tbl.py&quot;, line 62, in create_iceberg_table
    catalog.create_table(
  File &quot;/home/workspaceuser/layers/paketo-buildpacks_cpython/cpython/lib/python3.8/site-packages/pyiceberg/catalog/glue.py&quot;, line 220, in create_table
    self._write_metadata(metadata, io, metadata_location)
  File &quot;/home/workspaceuser/layers/paketo-buildpacks_cpython/cpython/lib/python3.8/site-packages/pyiceberg/catalog/__init__.py&quot;, line 544, in _write_metadata
    ToOutputFile.table_metadata(metadata, io.new_output(metadata_path))
  File &quot;/home/workspaceuser/layers/paketo-buildpacks_cpython/cpython/lib/python3.8/site-packages/pyiceberg/serializers.py&quot;, line 71, in table_metadata
    with output_file.create(overwrite=overwrite) as output_stream:
  File &quot;/home/workspaceuser/layers/paketo-buildpacks_cpython/cpython/lib/python3.8/site-packages/pyiceberg/io/pyarrow.py&quot;, line 256, in create
    if not overwrite and self.exists() is True:
  File &quot;/home/workspaceuser/layers/paketo-buildpacks_cpython/cpython/lib/python3.8/site-packages/pyiceberg/io/pyarrow.py&quot;, line 200, in exists
    self._file_info()  # raises FileNotFoundError if it does not exist
  File &quot;/home/workspaceuser/layers/paketo-buildpacks_cpython/cpython/lib/python3.8/site-packages/pyiceberg/io/pyarrow.py&quot;, line 182, in _file_info
    file_info = self._filesystem.get_file_info(self._path)
  File &quot;pyarrow/_fs.pyx&quot;, line 571, in pyarrow._fs.FileSystem.get_file_info
  File &quot;pyarrow/error.pxi&quot;, line 144, in pyarrow.lib.pyarrow_internal_check_status
  File &quot;pyarrow/error.pxi&quot;, line 100, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Empty path component in path ua-weather-data/hourly_forecasts//metadata/00000-232e3e60-1c1a-4eb8-959e-6940b563acd4.metadata.json

","I came across this post in LinkedIn that had an example of how to accomplish this - thanks dipankar mazumdar!!!
Removed the boto3 library, instantiated the glue catalog with the proper syntax, and created a properly formed catalog.create_table command.
Here is the adjusted working code:
from pyiceberg.catalog import load_catalog
from pyiceberg.table import Table
from pyiceberg.schema import Schema
from pyiceberg.types import DoubleType, StringType, TimestampType, NestedField
from pyiceberg.partitioning import PartitionSpec, PartitionField
from pyiceberg.transforms import YearTransform, MonthTransform, DayTransform
from pyiceberg.table.sorting import SortOrder, SortField
from pyiceberg.transforms import IdentityTransform

def create_iceberg_table():
    # Specify the Glue Catalog database name and URI
    glue_database_name = &quot;iceberg_catalog&quot;
    glue_catalog_uri = &quot;s3://ua-weather-data/catalog&quot;  # Replace with your Glue Catalog URI

    # Instantiate glue catalog
    catalog = load_catalog(&quot;glue&quot;, **{&quot;type&quot;: &quot;glue&quot;})
    #catalog = load_catalog(catalog_impl=&quot;org.apache.iceberg.aws.glue.GlueCatalog&quot;, name=glue_database_name, uri=glue_catalog_uri)

    # Define the Iceberg schema
    schema = Schema(
        NestedField(field_id=1, name=&quot;cloudCover&quot;, field_type=DoubleType(), required=False),
        NestedField(field_id=2, name=&quot;dayOfWeek&quot;, field_type=StringType(), required=False),
        NestedField(field_id=3, name=&quot;dayOrNight&quot;, field_type=StringType(), required=False),
        NestedField(field_id=4, name=&quot;expirationTimeUtc&quot;, field_type=TimestampType(), required=False),
        NestedField(field_id=5, name=&quot;iconCode&quot;, field_type=DoubleType(), required=False),
        NestedField(field_id=6, name=&quot;iconCodeExtend&quot;, field_type=DoubleType(), required=False),
        NestedField(field_id=7, name=&quot;precipChance&quot;, field_type=DoubleType(), required=False),
        NestedField(field_id=8, name=&quot;precipType&quot;, field_type=StringType(), required=False),
        NestedField(field_id=9, name=&quot;pressureMeanSeaLevel&quot;, field_type=DoubleType(), required=False),
        NestedField(field_id=10, name=&quot;qpf&quot;, field_type=DoubleType(), required=False),
        NestedField(field_id=11, name=&quot;qpfSnow&quot;, field_type=DoubleType(), required=False),
        NestedField(field_id=12, name=&quot;relativeHumidity&quot;, field_type=DoubleType(), required=False),
        NestedField(field_id=13, name=&quot;temperature&quot;, field_type=DoubleType(), required=False),
        NestedField(field_id=14, name=&quot;temperatureFeelsLike&quot;, field_type=DoubleType(), required=False),
        NestedField(field_id=15, name=&quot;temperatureHeatIndex&quot;, field_type=DoubleType(), required=False),
        NestedField(field_id=16, name=&quot;temperatureWindChill&quot;, field_type=DoubleType(), required=False),
        NestedField(field_id=17, name=&quot;uvDescription&quot;, field_type=StringType(), required=False),
        NestedField(field_id=18, name=&quot;uvIndex&quot;, field_type=DoubleType(), required=False),
        NestedField(field_id=19, name=&quot;validTimeLocal&quot;, field_type=TimestampType(), required=True),
        NestedField(field_id=20, name=&quot;validTimeUtc&quot;, field_type=DoubleType(), required=False),
        NestedField(field_id=21, name=&quot;visibility&quot;, field_type=DoubleType(), required=False),
        NestedField(field_id=22, name=&quot;windDirection&quot;, field_type=DoubleType(), required=False),
        NestedField(field_id=23, name=&quot;windDirectionCardinal&quot;, field_type=StringType(), required=False),
        NestedField(field_id=24, name=&quot;windGust&quot;, field_type=DoubleType(), required=False),
        NestedField(field_id=25, name=&quot;windSpeed&quot;, field_type=DoubleType(), required=False),
        NestedField(field_id=26, name=&quot;wxPhraseLong&quot;, field_type=StringType(), required=False),
        NestedField(field_id=27, name=&quot;wxPhraseShort&quot;, field_type=StringType(), required=False),
        NestedField(field_id=28, name=&quot;wxSeverity&quot;, field_type=DoubleType(), required=False),
        NestedField(field_id=29, name=&quot;data_origin&quot;, field_type=StringType(), required=True)
    )

    # Define the partitioning specification with year, month, and day
    partition_spec = PartitionSpec(
        PartitionField(field_id=19, source_id=19, transform=YearTransform(), name=&quot;validTimeLocal_year&quot;),
        PartitionField(field_id=19, source_id=19, transform=MonthTransform(), name=&quot;validTimeLocal_month&quot;),
        PartitionField(field_id=19, source_id=19, transform=DayTransform(), name=&quot;validTimeLocal_day&quot;)
    )

    # Define the sorting order using validTimeUtc field
    sort_order = SortOrder(SortField(source_id=20, transform=IdentityTransform()))

    # Create the Iceberg table using the Iceberg catalog
    table_name = &quot;iceberg_catalog.hourly_forecasts&quot;
    catalog.create_table(
        identifier=table_name,
        location=&quot;s3://ua-weather-data/catalog&quot;,
        schema=schema,
        partition_spec=partition_spec,
        sort_order=sort_order
    )

    print(&quot;Iceberg table created using AWS Glue Catalog.&quot;)

if __name__ == &quot;__main__&quot;:
    create_iceberg_table()


"
"I have a dataframe with a certain number of groups, containing a weight column and a list of values, which can be of arbitrary length, so for example:
df = pl.DataFrame(
    {
        &quot;Group&quot;: [&quot;Group1&quot;, &quot;Group2&quot;, &quot;Group3&quot;],
        &quot;Weight&quot;: [100.0, 200.0, 300.0],
        &quot;Vals&quot;: [[0.5, 0.5, 0.8],[0.5, 0.5, 0.8], [0.7, 0.9]]
    }
)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Group  â”† Weight â”† Vals            â”‚
â”‚ ---    â”† ---    â”† ---             â”‚
â”‚ str    â”† f64    â”† list[f64]       â”‚
â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ Group1 â”† 100.0  â”† [0.5, 0.5, 0.8] â”‚
â”‚ Group2 â”† 200.0  â”† [0.5, 0.5, 0.8] â”‚
â”‚ Group3 â”† 300.0  â”† [0.7, 0.9]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

My goal is to calculate a 'weighted' column, which would be the multiple of each item in the values list with the value in the weight column:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Group  â”† Weight â”† Vals            â”† Weighted        â”‚
â”‚ ---    â”† ---    â”† ---             â”† ---             â”‚
â”‚ str    â”† f64    â”† list[f64]       â”† list[i64]       â”‚
â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ Group1 â”† 100.0  â”† [0.5, 0.5, 0.8] â”† [50, 50, 80]    â”‚
â”‚ Group2 â”† 200.0  â”† [0.5, 0.5, 0.8] â”† [100, 100, 160] â”‚
â”‚ Group3 â”† 300.0  â”† [0.7, 0.9]      â”† [210, 270]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

I've tried a few different things:
df.with_columns(
    pl.col(&quot;Vals&quot;).list.eval(pl.element() * 3).alias(&quot;Weight1&quot;), #Multiplying with literal works
    pl.col(&quot;Vals&quot;).list.eval(pl.element() * pl.col(&quot;Weight&quot;)).alias(&quot;Weight2&quot;), #Does not work
    pl.col(&quot;Vals&quot;).list.eval(pl.element() * pl.col(&quot;Unknown&quot;)).alias(&quot;Weight3&quot;), #Unknown columns give same value
    pl.col(&quot;Vals&quot;).list.eval(pl.col(&quot;Vals&quot;) * pl.col(&quot;Weight&quot;)).alias(&quot;Weight4&quot;), #Same effect
    # pl.col('Vals') * 3 -&gt; gives an error
)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Group  â”† Weight â”† Vals       â”† Weight1    â”† Weight2      â”† Weight3      â”† Weight4            â”‚
â”‚ ---    â”† ---    â”† ---        â”† ---        â”† ---          â”† ---          â”† ---                â”‚
â”‚ str    â”† f64    â”† list[f64]  â”† list[f64]  â”† list[f64]    â”† list[f64]    â”† list[f64]          â”‚
â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ Group1 â”† 100.0  â”† [0.5, 0.5, â”† [1.5, 1.5, â”† [0.25, 0.25, â”† [0.25, 0.25, â”† [0.25, 0.25, 0.64] â”‚
â”‚        â”†        â”† 0.8]       â”† 2.4]       â”† 0.64]        â”† 0.64]        â”†                    â”‚
â”‚ Group2 â”† 200.0  â”† [0.5, 0.5, â”† [1.5, 1.5, â”† [0.25, 0.25, â”† [0.25, 0.25, â”† [0.25, 0.25, 0.64] â”‚
â”‚        â”†        â”† 0.8]       â”† 2.4]       â”† 0.64]        â”† 0.64]        â”†                    â”‚
â”‚ Group3 â”† 300.0  â”† [0.7, 0.9] â”† [2.1, 2.7] â”† [0.49, 0.81] â”† [0.49, 0.81] â”† [0.49, 0.81]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Unless I'm not understanding it correctly, it seems like you're unable to access columns outside of the list from within the eval function. Perhaps there might be a way to use list comprehension within the statement, but that doesn't really seem like a neat solution.
What would be the recommended approach here? Any help would be appreciated!
","EDIT - Polars update:
As of the latest version of Polars, this is now a the correct syntax:
df = pl.DataFrame(
    {
        &quot;Group&quot;: [&quot;Group1&quot;, &quot;Group2&quot;, &quot;Group3&quot;],
        &quot;Weight&quot;: [100.0, 200.0, 300.0],
        &quot;Vals&quot;: [[0.5, 0.5, 0.8],[0.5, 0.5, 0.8], [0.7, 0.9]]
    }
)

(df
    .explode('Vals')
    .with_columns(Weighted = pl.col('Weight')*pl.col('Vals'))
    .group_by('Group')
    .agg(
        pl.col('Weight').first(),                                                                                                             
        pl.col('Vals'),
        pl.col('Weighted')
        )                                                                                                 
)

shape: (3, 4)
┌────────┬────────┬─────────────────┬───────────────────────┐
│ Group  ┆ Weight ┆ Vals            ┆ Weighted              │
│ ---    ┆ ---    ┆ ---             ┆ ---                   │
│ str    ┆ f64    ┆ list[f64]       ┆ list[f64]             │
╞════════╪════════╪═════════════════╪═══════════════════════╡
│ Group3 ┆ 300.0  ┆ [0.7, 0.9]      ┆ [210.0, 270.0]        │
│ Group1 ┆ 100.0  ┆ [0.5, 0.5, 0.8] ┆ [50.0, 50.0, 80.0]    │
│ Group2 ┆ 200.0  ┆ [0.5, 0.5, 0.8] ┆ [100.0, 100.0, 160.0] │
└────────┴────────┴─────────────────┴───────────────────────┘

"
"Let's say I have an array of permutations perm which could look like:
perm = np.array([[0, 1, 2], [1, 2, 0], [0, 2, 1], [2, 1, 0]])

If I want to apply it to one axis, I can write something like:
v = np.arange(9).reshape(3, 3)
print(v[perm])

Output:
array([[[0, 1, 2],
        [3, 4, 5],
        [6, 7, 8]],

       [[3, 4, 5],
        [6, 7, 8],
        [0, 1, 2]],

       [[0, 1, 2],
        [6, 7, 8],
        [3, 4, 5]],

       [[6, 7, 8],
        [3, 4, 5],
        [0, 1, 2]]])

Now I would like to apply it to two axes at the same time. I figured out that I can do it via:
np.array([v[tuple(np.meshgrid(p, p, indexing=&quot;ij&quot;))] for p in perm])

But I find it quite inefficient, because it has to create a mesh grid, and it also requires a for loop. I made a small array in this example but in reality I have a lot larger arrays with a lot of permutations, so I would really love to have something that's as quick and simple as the one-axis version.
","How about:
p1 = perm[:, :, np.newaxis]
p2 = perm[:, np.newaxis, :]
v[p1, p2]

The zeroth axis of p1 and p2 is just the &quot;batch&quot; dimension of perm, which allows you to do many permutations in one operation.
The other dimension of perm, which corresponds with the indices, is aligned along the first axis in p1 and the second in p2. Because the axes are orthogonal, the arrays get broadcasted, basically like the arrays you got using meshgrid - but these still have the batch dimension.
That's the best I can do from my cell phone : ) I can try to clarify later if needed, but the key idea is broadcasting.
Comparison:
import numpy as np
perm = np.array([[0, 1, 2], [1, 2, 0], [0, 2, 1], [2, 1, 0]])
v = np.arange(9).reshape(3, 3)

ref = np.array([v[tuple(np.meshgrid(p, p, indexing=&quot;ij&quot;))] for p in perm])

p1 = perm[:, :, np.newaxis]
p2 = perm[:, np.newaxis, :]
res = v[p1, p2]

np.testing.assert_equal(res, ref)
# passes

%timeit np.array([v[tuple(np.meshgrid(p, p, indexing=&quot;ij&quot;))] for p in perm])
# 107 µs ± 20.6 µs per loop

%timeit v[perm[:, :, np.newaxis], perm[:, np.newaxis, :]]
# 3.73 µs ± 1.07 µs per loop


A simpler (without batch dimension) example of broadcasting indices:
import numpy as np
i = np.arange(3)
ref = np.meshgrid(i, i, indexing=&quot;ij&quot;)
res = np.broadcast_arrays(i[:, np.newaxis], i[np.newaxis, :])
np.testing.assert_equal(res, ref)
# passes

In the solution code at the top, the broadcasting is implicit. We don't need to call broadcast_arrays because it happens automatically during the indexing.
"
"I'm having trouble calling typing.get_type_hints() for classes that have forward references as strings. My code works with not defined inside of a function. I've reproduced a minimal example below in Python 3.10:
import typing
class B:
  pass
class A:
  some_b: &quot;B&quot; 
print(typing.get_type_hints(A)) # prints {'some_b': &lt;class '__main__.B'&gt;}

import typing
def func():
  class B:
    pass
  class A:
    some_b: &quot;B&quot; 
  print(typing.get_type_hints(A)) 
func() # NameError: name 'B' is not defined

Is this expected behavior? Is there any way to get around this, and make sure that forward references with strings get evaluated in the correct scope?
","typing.get_type_hints allows you to explicitly pass the local namespace to use for resolving references via the localns parameter.
from typing import get_type_hints


def func():
    class A:
        some_b: &quot;B&quot;

    class B:
        pass

    print(get_type_hints(A, localns=locals()))


func()

Output: {'some_b': &lt;class '__main__.func.&lt;locals&gt;.B'&gt;}
See the docs for locals.

Side note: By utilizing postponed evaluation of annotations (PEP 563) you can omit the quotation marks:
from __future__ import annotations
from typing import get_type_hints


def func():
    class A:
        some_b: B

    class B:
        pass

    print(get_type_hints(A, localns=locals()))

"
"I have a Spark data frame (df1) with a particular schema, and I have another dataframe with the same columns, but different schema. I know how to do it column by column, but since I have a large set of columns, it would be quite lengthy. To keep the schema consistent across dataframes, I was wondering if I could be able to apply one schema to another data frame or creating a function that do the job.
Here is an example:
df1
# root
#  |-- A: date (nullable = true)
#  |-- B: integer (nullable = true)
#  |-- C: string (nullable = true)

df2
# root
#  |-- A: string (nullable = true)
#  |-- B: string (nullable = true)
#  |-- C: string (nullable = true)`

I want to copy apply the schema of df1 to df2.
I tried this approach for one column. Given that I have a large number of columns, it would be quite a lengthy way to do it.
df2 = df2.withColumn(&quot;B&quot;, df2[&quot;B&quot;].cast('int'))

","Yes, its possible dynamically with dataframe.schema.fields
df2.select(*[(col(x.name).cast(x.dataType)) for x in df1.schema.fields])
Example:
from pyspark.sql.functions import *
df1 = spark.createDataFrame([('2022-02-02',2,'a')],['A','B','C']).withColumn(&quot;A&quot;,to_date(col(&quot;A&quot;)))
print(&quot;df1 Schema&quot;)
df1.printSchema()
#df1 Schema
#root
# |-- A: date (nullable = true)
# |-- B: long (nullable = true)
# |-- C: string (nullable = true)

df2 = spark.createDataFrame([('2022-02-02','2','a')],['A','B','C'])
print(&quot;df2 Schema&quot;)
df2.printSchema()
#df2 Schema
#root
# |-- A: string (nullable = true)
# |-- B: string (nullable = true)
# |-- C: string (nullable = true)
#

#casting the df2 columns by getting df1 schema using select clause
df3 = df2.select(*[(col(x.name).cast(x.dataType)) for x in df1.schema.fields])
df3.show(10,False)
print(&quot;df3 Schema&quot;)
df3.printSchema()

#+----------+---+---+
#|A         |B  |C  |
#+----------+---+---+
#|2022-02-02|2  |a  |
#+----------+---+---+

#df3 Schema
#root
# |-- A: date (nullable = true)
# |-- B: long (nullable = true)
# |-- C: string (nullable = true)

In this example I have df1 defined with Integer,date,long types.
df2 is defined with string type.
df3 is defined by using df2 as source data and attached df1 schema.
"
"I have built a web application using streamlit and hosted it on the Google Cloud Platform (App Engine). The URL is something like https://xxx-11111.uc.r.appspot.com/ which is given for the Stream URL.
I enabled Google Analytics 2 days back but apparently, it is not set up correctly.
It was given that I need to add in the head tag.
This is the code where I added the Google Analytics tag...
What is wrong??
def page_header():
    st.set_page_config(page_title=&quot;xx&quot;, page_icon=&quot;images/logo.png&quot;)
    header = st.container()
    with header:
        # Add banner image
        logo = Image.open(&quot;images/logo.png&quot;)
        st.image(logo, width=300)

        # Add Google Analytics code to the header
        ga_code = &quot;&quot;&quot;
        &lt;!-- Google tag (gtag.js) --&gt;
        &lt;script async src=&quot;https://www.googletagmanager.com/gtag/js?id=G-xxxxxx&quot;&gt;&lt;/script&gt;
        &lt;script&gt;
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'G-xxxxxx');
        &lt;/script&gt;
        &quot;&quot;&quot;
        st.markdown(ga_code, unsafe_allow_html=True)


# Define the main function to run the app
def main():

    # Render the page header
    page_header()

    .....

if __name__ == &quot;__main__&quot;:
    main()

","One way to implement Google Analytics into your GAE Streamlit app would be to add the GA global site tag JS code to the default /site-packages/streamlit/static/index.html file
NOTE: This script should be run prior to running the streamlit server
from bs4 import BeautifulSoup
import shutil
import pathlib
import logging
import streamlit as st


def add_analytics_tag():
    # replace G-XXXXXXXXXX to your web app's ID
    
    analytics_js = &quot;&quot;&quot;
    &lt;!-- Global site tag (gtag.js) - Google Analytics --&gt;
    &lt;script async src=&quot;https://www.googletagmanager.com/gtag/js?id=G-XXXXXXXXXX&quot;&gt;&lt;/script&gt;
    &lt;script&gt;
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-XXXXXXXXXX');
    &lt;/script&gt;
    &lt;div id=&quot;G-XXXXXXXXXX&quot;&gt;&lt;/div&gt;
    &quot;&quot;&quot;
    analytics_id = &quot;G-XXXXXXXXXX&quot;

    
    # Identify html path of streamlit
    index_path = pathlib.Path(st.__file__).parent / &quot;static&quot; / &quot;index.html&quot;
    logging.info(f'editing {index_path}')
    soup = BeautifulSoup(index_path.read_text(), features=&quot;html.parser&quot;)
    if not soup.find(id=analytics_id): # if id not found within html file
        bck_index = index_path.with_suffix('.bck')
        if bck_index.exists():
            shutil.copy(bck_index, index_path)  # backup recovery
        else:
            shutil.copy(index_path, bck_index)  # save backup
        html = str(soup)
        new_html = html.replace('&lt;head&gt;', '&lt;head&gt;\n' + analytics_js) 
        index_path.write_text(new_html) # insert analytics tag at top of head

"
"In a more complicated setup using the python dependency injector framework I use the lifespan function for the FastAPI app object to correctly wire everything.
When testing I'd like to replace some of the objects with different versions (fakes), and the natural way to accomplish that seems to me like I should override or mock the lifespan function of the app object. However I can't seem to figure out if/how I can do that.
MRE follows
import pytest
from contextlib import asynccontextmanager
from fastapi.testclient import TestClient
from fastapi import FastAPI, Response, status


greeting = None

@asynccontextmanager
async def _lifespan(app: FastAPI):
    # Initialize dependency injection
    global greeting
    greeting = &quot;Hello&quot;
    yield


@asynccontextmanager
async def _lifespan_override(app: FastAPI):
    # Initialize dependency injection
    global greeting
    greeting = &quot;Hi&quot;
    yield


app = FastAPI(title=&quot;Test&quot;, lifespan=_lifespan)


@app.get(&quot;/&quot;)
async def root():
    return Response(status_code=status.HTTP_200_OK, content=greeting)


@pytest.fixture
def fake_client():
    with TestClient(app) as client:
        yield client


def test_override(fake_client):
    response = fake_client.get(&quot;/&quot;)
    assert response.text == &quot;Hi&quot;

So basically in the fake_client fixture I'd like to change it to use the _lifespan_override instead of the original _lifespan, making the dummy test-case above pass
I'd have expected something like with TestClient(app, lifespan=_lifespan_override) as client: to work, but that's not supported. Is there some way I can mock it to get the behavior I want?
(The mre above works if you replace &quot;Hi&quot; with &quot;Hello&quot; in the assert statement)
pyproject.toml below with needed dependencies
[tool.poetry]
name = &quot;mre&quot;
version = &quot;0.1.0&quot;
description = &quot;mre&quot;
authors = []

[tool.poetry.dependencies]
python = &quot;^3.10&quot;
fastapi = &quot;^0.103.2&quot;

[tool.poetry.group.dev.dependencies]
pytest = &quot;^7.1.2&quot;
httpx = &quot;^0.25.0&quot;


[build-system]
requires = [&quot;poetry-core&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;

EDIT:
Tried extending my code with the suggestion from Hamed Akhavan below as follows
@pytest.fixture
def fake_client():
    app.dependency_overrides[_lifespan] = _lifespan_override
    with TestClient(app) as client:
        yield client

but it doesn't work, even though it looks like it should be the right approach. Syntax problem?
","I found a solution to my problem that didn't include overriding the lifespan function, so not a general solution to my questions above.
As I mentioned my specific problem in the real application was using the python dependency injector framework, and it provides and override method for it's containers. So the solution was to use that override functionality when wiring the dependencies during testing, which means the lifespan function doesn't need to be touched
Here's a complete working MRE in case anyone is interested.
import pytest
from contextlib import asynccontextmanager
from fastapi.testclient import TestClient
from fastapi import FastAPI, Response, status, Depends
from dependency_injector import containers, providers
from dependency_injector.wiring import Provide, inject


class HelloGreeter():
    def greet(self):
        return &quot;Hello&quot;


class Container(containers.DeclarativeContainer):
    greeter = providers.Singleton(HelloGreeter)


@asynccontextmanager
async def _lifespan(app: FastAPI):
    # Initialize dependency injection
    container = Container()
    container.wire(modules=[__name__])
    yield


app = FastAPI(title=&quot;Test&quot;, lifespan=_lifespan)


@app.get(&quot;/&quot;)
@inject
async def root(greeter=Depends(Provide[Container.greeter])):
    return Response(status_code=status.HTTP_200_OK, content=greeter.greet())


@pytest.fixture
def fake_client():
    class HiGreeter():
        def greet(self):
            return &quot;Hi&quot;
    with Container.greeter.override(HiGreeter()):
        with TestClient(app) as client:
            yield client


def test_override(fake_client):
    response = fake_client.get(&quot;/&quot;)
    assert response.text == &quot;Hi&quot;

"
"I have the following list of 20 values:
values = [143.15,143.1,143.06,143.01,143.03,143.09,143.14,143.18,143.2,143.2,143.2,143.31,143.38,143.35,143.34,143.25,143.33,143.3,143.33,143.36]

In order to find the Exponential Moving Average, across a span of 9 values, I can do the following in Python:
def calculate_ema(values, periods, smoothing=2):
    ema = [sum(values[:periods]) / periods]
    
    for price in values[periods:]:
        ema.append((price * (smoothing / (1 + periods))) + ema[-1] * (1 - (smoothing / (1 + periods))))
    return ema

ema_9 = calculate_ema(values, periods=9)

[143.10666666666668,
 143.12533333333334,
 143.14026666666666,
 143.17421333333334,
 143.21537066666667,
 143.24229653333333,
 143.26183722666667,
 143.25946978133334,
 143.27357582506667,
 143.27886066005334,
 143.28908852804267,
 143.30327082243414]

The resulting list of EMA values is 12 items long, the first value [0] corresponding to the 9th [8] value from values.
Using Pandas and TA-Lib, I can perform the following:
import pandas as pd
import talib as ta

df_pan = pd.DataFrame(
    {
        'value': values
    }
)

df_pan['ema_9'] = ta.EMA(df_pan['value'], timeperiod=9)

df_pan

    value   ema_9
0   143.15  NaN
1   143.10  NaN
2   143.06  NaN
3   143.01  NaN
4   143.03  NaN
5   143.09  NaN
6   143.14  NaN
7   143.18  NaN
8   143.20  143.106667
9   143.20  143.125333
10  143.20  143.140267
11  143.31  143.174213
12  143.38  143.215371
13  143.35  143.242297
14  143.34  143.261837
15  143.25  143.259470
16  143.33  143.273576
17  143.30  143.278861
18  143.33  143.289089
19  143.36  143.303271

The Pandas / TA-Lib output corresponds with that of my Python function.
However, when I try to replicate this using funtionality purely in Polars:
import polars as pl

df = (
    pl.DataFrame(
        {
            'value': values
        }
    )
    .with_columns(
        pl.col('value').ewm_mean(span=9, min_periods=9,).alias('ema_9')
    )
)

df

I get different values:
value   ema_9
f64 f64
143.15  null
143.1   null
143.06  null
143.01  null
143.03  null
143.09  null
143.14  null
143.18  null
143.2   143.128695
143.2   143.144672
143.2   143.156777
143.31  143.189683
143.38  143.229961
143.35  143.255073
143.34  143.272678
143.25  143.268011
143.33  143.280694
143.3   143.284626
143.33  143.293834
143.36  143.307221

Can anyone please explain what adjustments I need to make to my Polars code in order get the expected results?
","Two things here:

Reading the ewm_mean docs closely, you want adjust=False (default is True).
min_periods is still doing the calculations as if you didn't skip any values, it just replaces those calculated values with null up to the min_periodsth row, so to speak. Try removing min_periods and see how the tail values don't change at all.

To actually change the calculation (starting with the mean of the first min_periods values), we can do a a pl.when with cum_count (a handy way to get the row index of a value). The calculations will all still be done under the hood, but the ewm_mean will stay at this constant value, of course, until row 9, and min_periods=9 will null them out in the end.
All together:
df.with_columns(
    pl.when(pl.col('value').cum_count() &lt;= 9) # NOTE: Polars cum_count starts at 1
    .then(pl.col('value').head(9).mean())
    .otherwise(pl.col('value'))
    .ewm_mean(span=9, min_periods=9, adjust=False)
    .alias('ema_9')
)

shape: (20, 2)
┌────────┬────────────┐
│ value  ┆ ema_9      │
│ ---    ┆ ---        │
│ f64    ┆ f64        │
╞════════╪════════════╡
│ 143.15 ┆ null       │
│ 143.1  ┆ null       │
│ 143.06 ┆ null       │
│ 143.01 ┆ null       │
│ 143.03 ┆ null       │
│ 143.09 ┆ null       │
│ 143.14 ┆ null       │
│ 143.18 ┆ null       │
│ 143.2  ┆ 143.106667 │
│ 143.2  ┆ 143.125333 │
│ 143.2  ┆ 143.140267 │
│ 143.31 ┆ 143.174213 │
│ 143.38 ┆ 143.215371 │
│ 143.35 ┆ 143.242297 │
│ 143.34 ┆ 143.261837 │
│ 143.25 ┆ 143.25947  │
│ 143.33 ┆ 143.273576 │
│ 143.3  ┆ 143.278861 │
│ 143.33 ┆ 143.289089 │
│ 143.36 ┆ 143.303271 │
└────────┴────────────┘

"
"How to tell whether an argument in click is coming from the user or is the default value?
For example:
import click

@click.command()
@click.option('--value', default=1, help='a value.')
def hello(value):
    print(value)

if __name__ == &quot;__main__&quot;:
    hello()

Now if I run python script.py --value 1, the value is now coming from the user input as opposed to the default value (which is set to 1). Is there any way to discern where this value is coming from?
","You can use Context.get_parameter_source to get what you want. This returns an enum of 4 possible values (or None if the value does not exist), you can then use them to decide what you want to do.
COMMANDLINE - The value was provided by the command line args.
ENVIRONMENT - The value was provided with an environment variable.
DEFAULT - Used the default specified by the parameter.
DEFAULT_MAP - Used a default provided by :attr:`Context.default_map`.
PROMPT - Used a prompt to confirm a default or provide a value.

import click
from click.core import ParameterSource

@click.command()
@click.option('--value', default=1, help='a value.')
def hello(value):
    parameter_source = click.get_current_context().get_parameter_source('value')

    if parameter_source == ParameterSource.DEFAULT:
        print('default value')

    elif parameter_source == ParameterSource.COMMANDLINE:
        print('from command line')

    # other conditions go here
    print(value)

if __name__ == &quot;__main__&quot;:
    hello()

In this case, the value is taken from default and can be seen in the output below.
Output
default value
1

"
"I just stumbled accross this surprising behaviour with Python datetimes while creating datetimes accross DST shift.
Adding a timedelta to a local datetime might not add the amount of time we expect.
import datetime as dt
from zoneinfo import ZoneInfo

# Midnight
d0 = dt.datetime(2020, 3, 29, 0, 0, tzinfo=ZoneInfo(&quot;Europe/Paris&quot;))
# datetime.datetime(2020, 3, 29, 0, 0, tzinfo=zoneinfo.ZoneInfo(key='Europe/Paris'))
d0.isoformat()
# '2020-03-29T00:00:00+01:00'

# Before DST shift
d1 = d0 + dt.timedelta(hours=2)
# datetime.datetime(2020, 3, 29, 2, 0, tzinfo=zoneinfo.ZoneInfo(key='Europe/Paris'))
d1.isoformat()
# '2020-03-29T02:00:00+01:00'

# After DST shift
d2 = d0 + dt.timedelta(hours=3)
# datetime.datetime(2020, 3, 29, 3, 0, tzinfo=zoneinfo.ZoneInfo(key='Europe/Paris'))
d2.isoformat()
# '2020-03-29T03:00:00+02:00'

# Convert to UCT
d1u = d1.astimezone(dt.timezone.utc)
# datetime.datetime(2020, 3, 29, 1, 0, tzinfo=datetime.timezone.utc)
d2u = d2.astimezone(dt.timezone.utc)
# datetime.datetime(2020, 3, 29, 1, 0, tzinfo=datetime.timezone.utc)

# Compute timedeltas
d2 - d1
# datetime.timedelta(seconds=3600)
d2u - d1u
# datetime.timedelta(0)

I agree d1 and d2 are the same, but shouldn't d2 be '2020-03-29T04:00:00+02:00', then?
d3 = d0 + dt.timedelta(hours=4)
# datetime.datetime(2020, 3, 29, 4, 0, tzinfo=zoneinfo.ZoneInfo(key='Europe/Paris'))

Apparently, when adding a timedelta (ex. 3 hours) to a local datetime, it is added regardless of the timezone and the delta between the two datetimes (in real time / UTC) is not guaranteed to be that timedelta (i.e. it may be 2 hours due to DST). This is a bit of a pitfall.
What is the rationale? Is this documented somewhere?
","The rationale is : timedelta arithmetic is wall time arithmetic. That is, it includes the DST transition hours (or excludes, depending on the change). See also P. Ganssle's blog post on the topic .
An illustration:
import datetime as dt
from zoneinfo import ZoneInfo

# Midnight
d0 = dt.datetime(2020, 3, 29, 0, 0, tzinfo=ZoneInfo(&quot;Europe/Paris&quot;))

for h in range(1, 4):
    print(h)
    print(d0 + dt.timedelta(hours=h))
    print((d0 + dt.timedelta(hours=h)).astimezone(ZoneInfo(&quot;UTC&quot;)), end=&quot;\n\n&quot;)

1
2020-03-29 01:00:00+01:00
2020-03-29 00:00:00+00:00 # as expected, 1 hour added

2
2020-03-29 02:00:00+01:00 # that's a non-existing datetime...
2020-03-29 01:00:00+00:00 # looks normal

3
2020-03-29 03:00:00+02:00
2020-03-29 01:00:00+00:00 # oops, 3 hours timedelta is only 2 hours actually!


Need more confusion? Use naive datetime. Given that the tz of my machine (Europe/Berlin) has the same DST transitions as the tz used above:
d0 = dt.datetime(2020, 3, 29, 0, 0)

for h in range(1, 4):
    print(h)
    print(d0 + dt.timedelta(hours=h))
    print((d0 + dt.timedelta(hours=h)).astimezone(ZoneInfo(&quot;UTC&quot;)), end=&quot;\n\n&quot;)

1
2020-03-29 01:00:00       # 1 hour as expected
2020-03-29 00:00:00+00:00 # we're on UTC+1

2
2020-03-29 02:00:00       # ok 2 hours...
2020-03-29 00:00:00+00:00 # wait, what?!

3
2020-03-29 03:00:00
2020-03-29 01:00:00+00:00

"
"I'm wondering what the story -- whether sound design or inherited legacy -- is behind these functools.partial and inspect.signature facts (talking python 3.8 here).
Set up:
from functools import partial
from inspect import signature

def bar(a, b):
    return a / b

All starts well with the following, which seems compliant with curry-standards.
We're fixing a to 3 positionally, a disappears from the signature and it's value is indeed bound to 3:
f = partial(bar, 3)
assert str(signature(f)) == '(b)'
assert f(6) == 0.5 == f(b=6)

If we try to specify an alternate value for a, f won't tell us that we got an unexpected keyword, but rather that it got multiple values for argument a:
f(a=2, b=6)  # TypeError: bar() got multiple values for argument 'a'
f(c=2, b=6)  # TypeError: bar() got an unexpected keyword argument 'c'

But now if we fix b=3 through a keyword, b is not removed from the signature, it's kind changes to keyword-only, and we can still use it (overwrite the default, as a normal default, which we couldn't do with a in the previous case):
f = partial(bar, b=3)
assert str(signature(f)) == '(a, *, b=3)'
assert f(6) == 2.0 == f(6, b=3)
assert f(6, b=1) == 6.0

Why such asymmetry?
It gets even stranger, we can do this:
f = partial(bar, a=3)
assert str(signature(f)) == '(*, a=3, b)'  # whaaa?! non-default argument follows default argument?

Fine: For keyword-only arguments, there can be no confusing of what parameter a default is assigned to, but I still wonder what design-thinking or constraints are behind these choices.
","Using partial with a Positional Argument
f = partial(bar, 3)

By design, upon calling a function, positional arguments are assigned first. Then logically, 3 should be assigned to a with partial. It makes sense to remove it from the signature as there is no way to assign anything to it again!
when you have f(a=2, b=6), you are actually doing
bar(3, a=2, b=6)

when you have f(2, 2), you are actually doing
bar (3, 2, 2)

We never get rid of 3
For the new partial function:

We can't give a a different value with another positional argument
We can't use the keyword a to assign a different value to it as it is already &quot;filled&quot;


If there is a parameter with the same name as the keyword, then the argument value is assigned to that parameter slot. However, if the parameter slot is already filled, then that is an error.

I recommend reading the function calling behavior section of pep-3102 to get a better grasp of this matter.
Using partial with a Keyword Argument
f = partial(bar, b=3)

This is a different use case. We are applying a keyword argument to bar.
You are functionally turning
def bar(a, b):
    ...

into
def f(a, *, b=3):
    ...

where b becomes a keyword-only argument
instead of
def f(a, b=3):
    ...

inspect.signature correctly reflects a design decision of partial. The keyword arguments passed to partial are designed to append additional positional arguments (source).
Note that this behavior does not necessarily override the keyword arguments supplied with f = partial(bar, b=3), i.e., b=3 will be applied regardless of whether you supply the second positional argument or not (and there will be a TypeError if you do so). This is different from a positional argument with a default value.
&gt;&gt;&gt; f(1, 2)
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
TypeError: f() takes 1 positional argument but 2 were given

where f(1, 2) is equivalent to bar(1, 2, b=3)
The only way to override it is with a keyword argument
&gt;&gt;&gt; f(2, b=2)

An argument that can only be assigned with a keyword but positionally? This is a keyword-only argument. Thus (a, *, b=3) instead of (a, b=3).
The Rationale of Non-default Argument follows Default Argument
f = partial(bar, a=3)
assert str(signature(f)) == '(*, a=3, b)'  # whaaa?! non-default argument follows default argument?


You can't do def bar(a=3, b). a and b are so called positional-or-keyword arguments.
You can do def bar(*, a=3, b). a and b are keyword-only arguments.

Even though semantically, a has a default value and thus it is optional, we can't leave it unassigned because b, which is a positional-or-keyword argument needs to be assigned a value if we want to use b positionally. If we do not supply a value for a, we have to use b as a keyword argument.
Checkmate! There is no way for b to be a positional-or-keyword argument as we intended.
The PEP for positonal-only arguments also kind of shows the rationale behind it.
This also has something to do with the aforementioned &quot;function calling behavior&quot;.
partial != Currying &amp; Implementation Details
partial by its implementation wraps the original function while storing the fixed arguments you passed to it.
IT IS NOT IMPLEMENTED WITH CURRYING. It is rather partial application instead of currying in the sense of functional programming. partial is essentially applying the fixed arguments first, then the arguments you called with the wrapper:
def __call__(self, /, *args, **keywords):
    keywords = {**self.keywords, **keywords}
    return self.func(*self.args, *args, **keywords)

This explains f(a=2, b=6)  # TypeError: bar() got multiple values for argument 'a'.
See also: Why is partial called partial instead of curry
Under the Hood of inspect
The outputs of inspect is another story.
inspect itself is a tool that produces user-friendly outputs. For partial() in particular (and partialmethod(), similarly), it follows the wrapped function while taking the fixed parameters into account:
if isinstance(obj, functools.partial):
    wrapped_sig = _get_signature_of(obj.func)
    return _signature_get_partial(wrapped_sig, obj)

Do note that it is not inspect.signature's goal to show you the actual signature of the wrapped function in the AST.
def _signature_get_partial(wrapped_sig, partial, extra_args=()):
    &quot;&quot;&quot;Private helper to calculate how 'wrapped_sig' signature will
    look like after applying a 'functools.partial' object (or alike)
    on it.
    &quot;&quot;&quot;
    ...

So we have a nice and ideal signature for f = partial(bar, 3)
but get f(a=2, b=6)  # TypeError: bar() got multiple values for argument 'a' in reality.
Follow-up
If you want currying so badly, how do you implement it in Python, in the way which gives you the expected TypeError?
"
"I'm trying to flatten some mixed arrays in Python using LC. I'm having some trouble figuring out how to structure it.
Here's the array's i'm trying to flatten
arr_1 = [1, [2, 3], 4, 5]
arr_2 = [1,[2,3],[[4,5]]]

I tried this methods for arr_1 but get &quot;TypeError: 'int' object is not iterable&quot;
print([item if type(items) is list else items for items in arr_1 for item in items])

So I decided to break it into parts to see where it's failing by using this
def check(item):
return item;

print([check(item) if type(items) is list else check(items) for items in [1, [2, 3], 4, 5] for items in arr_2]) 

Through the debugger I found that it's failing at the 2d array in
for items in [1, [2, 3], 4, 5]

I don't need the LC to be in one line but I just wanted to know how to do it in a single nested LC if its even possible.
","Using an internal stack and iter's second form to simulate a while loop:
def flatten(obj):
    return [x
            for stack in [[obj]]
            for x, in iter(lambda: stack and [stack.pop()], [])
            if isinstance(x, int)
            or stack.extend(reversed(x))]

print(flatten([1, [2, 3], 4, 5]))
print(flatten([1, [2, 3], [[4, 5]]]))
print(flatten([1, [2, [], 3], [[4, 5]]]))

Output (Try it online!):
[1, 2, 3, 4, 5]
[1, 2, 3, 4, 5]
[1, 2, 3, 4, 5]

Slight variation, splitting the &quot;long&quot; line into two (Try it online!):
def flatten(obj):
    return [x
            for stack in [[obj]]
            for _ in iter(lambda: stack, [])
            for x in [stack.pop()]
            if isinstance(x, int)
            or stack.extend(reversed(x))]

To explain it a bit, here's roughly the same with ordinary code:
def flatten(obj):
    result = []
    stack = [obj]
    while stack:
        x = stack.pop()
        if isinstance(x, int):
            result.append(x)
        else:
            stack.extend(reversed(x))
    return result

If the order doesn't matter, we can use a queue instead (inspired by 0x263A's comment), although it's less memory-efficient (Try it online!):
def flatten(obj):
    return [x
            for queue in [[obj]]
            for x in queue
            if isinstance(x, int) or queue.extend(x)]

We can fix the order if instead of putting each list's contents at the end of the queue, we insert them right after the list (which is less time-efficient) in the &quot;priority&quot; queue (Try it online!):
def flatten(obj):
    return [x
            for pqueue in [[obj]]
            for i, x in enumerate(pqueue, 1)
            if isinstance(x, int) or pqueue.__setitem__(slice(i, i), x)]

"
"For the last 5 days, I am trying to make Keras/Tensorflow packages work in R. I am using RStudio for installation and have used conda, miniconda, virtualenv but it crashes each time in the end. Installing a library should not be a nightmare especially when we are talking about R (one of the best statistical languages) and TensorFlow (one of the best deep learning libraries). Can someone share a reliable way to install Keras/Tensorflow on CentOS 7?
Following are the steps I am using to install tensorflow in RStudio.
Since RStudio simply crashes each time I run tensorflow::tf_config() I have no way to check what is going wrong.

devtools::install_github(&quot;rstudio/reticulate&quot;)
devtools::install_github(&quot;rstudio/keras&quot;) # This package also installs tensorflow
library(reticulate)
reticulate::install_miniconda()
reticulate::use_miniconda(&quot;r-reticulate&quot;)
library(tensorflow)
tensorflow::tf_config() **# Crashes at this point**

sessionInfo()


R version 3.6.0 (2019-04-26)
Platform: x86_64-redhat-linux-gnu (64-bit)
Running under: CentOS Linux 7 (Core)

Matrix products: default
BLAS/LAPACK: /usr/lib64/R/lib/libRblas.so

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] tensorflow_2.7.0.9000 keras_2.7.0.9000      reticulate_1.22-9000 

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.7      lattice_0.20-45 png_0.1-7       zeallot_0.1.0  
 [5] rappdirs_0.3.3  grid_3.6.0      R6_2.5.1        jsonlite_1.7.2 
 [9] magrittr_2.0.1  tfruns_1.5.0    rlang_0.4.12    whisker_0.4    
[13] Matrix_1.3-4    generics_0.1.1  tools_3.6.0     compiler_3.6.0 
[17] base64enc_0.1-3



Update 1
The only way RStudio does not crash while installing tensorflow is by executing following steps -
First, I created a new virtual environment using conda
conda create --name py38 python=3.8.0
conda activate py38
conda install tensorflow=2.4

Then from within RStudio, I installed reticulate and activated the virtual environment which I earlier created using conda
devtools::install_github(&quot;rstudio/reticulate&quot;)
library(reticulate)
reticulate::use_condaenv(&quot;/root/.conda/envs/py38&quot;, required = TRUE)
reticulate::use_python(&quot;/root/.conda/envs/py38/bin/python3.8&quot;, required = TRUE)
reticulate::py_available(initialize = TRUE)
ts &lt;- reticulate::import(&quot;tensorflow&quot;)

As soon as I try to import tensorflow in RStudio, it loads the library /lib64/libstdc++.so.6 instead of /root/.conda/envs/py38/lib/libstdc++.so.6 and I get the following error -
Error in py_module_import(module, convert = convert) : 
  ImportError: Traceback (most recent call last):
  File &quot;/root/.conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py&quot;, line 64, in &lt;module&gt;
    from tensorflow.python._pywrap_tensorflow_internal import *
  File &quot;/home/R/x86_64-redhat-linux-gnu-library/3.6/reticulate/python/rpytools/loader.py&quot;, line 39, in _import_hook
    module = _import(
ImportError: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by /root/.conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

Here is what inside /lib64/libstdc++.so.6
&gt; strings /lib64/libstdc++.so.6 | grep GLIBC

GLIBCXX_3.4
GLIBCXX_3.4.1
GLIBCXX_3.4.2
GLIBCXX_3.4.3
GLIBCXX_3.4.4
GLIBCXX_3.4.5
GLIBCXX_3.4.6
GLIBCXX_3.4.7
GLIBCXX_3.4.8
GLIBCXX_3.4.9
GLIBCXX_3.4.10
GLIBCXX_3.4.11
GLIBCXX_3.4.12
GLIBCXX_3.4.13
GLIBCXX_3.4.14
GLIBCXX_3.4.15
GLIBCXX_3.4.16
GLIBCXX_3.4.17
GLIBCXX_3.4.18
GLIBCXX_3.4.19
GLIBC_2.3
GLIBC_2.2.5
GLIBC_2.14
GLIBC_2.4
GLIBC_2.3.2
GLIBCXX_DEBUG_MESSAGE_LENGTH

To resolve the library issue, I added the path of the correct libstdc++.so.6 library having GLIBCXX_3.4.20 in RStudio.
system('export LD_LIBRARY_PATH=/root/.conda/envs/py38/lib/:$LD_LIBRARY_PATH')

and, also
Sys.setenv(&quot;LD_LIBRARY_PATH&quot; = &quot;/root/.conda/envs/py38/lib&quot;)

But still I get the same error ImportError: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20'. Somehow RStudio still loads /lib64/libstdc++.so.6 first instead of /root/.conda/envs/py38/lib/libstdc++.so.6
Instead of RStudio, if I execute the above steps in the R console, then also I get the exact same error.
Update 2:
A solution is posted here
","Update on 29 July, 2022
After months of solving this problem, I feel so stupid to have wasted time coding R on CentOS. The most popular and stable OS to code R is Ubuntu. By default, CentOS supports only the 3.6 version of R while the most stable current version of R is 4.2. With the default 3.6 version of R on CentOS, most of the libraries are outdated and they conflict with other libraries which are updated for R 4.2+. From my experience, you are going to avoid a lot of misery and frustration if you start coding R on Ubuntu. I am not sponsoring Ubuntu, the above statement is just from my experience and others might have different experiences.
Original Answer
Took me more than 15 days and I finally solved this problem.
Boot up a clean CentOS 7 VM, install R and dependencies (taken from Jared's answer) -
yum install epel-release
yum install R
yum install libxml2-devel
yum install openssl-devel
yum install libcurl-devel
yum install libXcomposite libXcursor libXi libXtst libXrandr alsa-lib mesa-libEGL libXdamage mesa-libGL libXScrnSaver

Now, create a conda environment
yum install conda
conda clean -a     # Clean cache and remove old packages, if you already have conda installed
# Install all the packages together and let conda handle versioning. It is important to give a Python version while setting up the environment. Since Tensorflow supports python 3.9.0, I have used this version 
conda create -y -n &quot;tf&quot; python=3.9.0 ipython tensorflow keras r-essentials r-reticulate r-tensorflow
conda activate tf

Open a new port (7878 or choose any port number you want) on the server to access RStudio with new conda environment libraries
iptables -A INPUT -p tcp --dport 7878 -j ACCEPT
/sbin/service iptables save

then launch RStudio as follows -
/usr/lib/rstudio-server/bin/rserver \
   --server-daemonize=0 \
   --www-port 7878 \
   --rsession-which-r=$(which R) \
   --rsession-ld-library-path=$CONDA_PREFIX/lib

You will have your earlier environment intact on default port 8787 and a new environment with Tensorflow and Keras on 7878.
The following code now works fine in RStudio
install.packages(&quot;reticulate&quot;)
install.packages(&quot;tensorflow&quot;)
library(reticulate)
library(tensorflow)
ts &lt;- reticulate::import(&quot;tensorflow&quot;)

"
"I have a polars dataframe with columns a_0, a_1, a_2, b_0, b_1, b_2. I want to convert it to a longer and thinner dataframe (3 x rows, but just 2 columns a and b), so that a contains a_0[0], a_1[0], a_2[0], a_0[1], a_1[1], a_2[1],... and the same for b. How can I do that?
","You can use concat_list() to join the columns you want together and then use explode() to convert them into rows.
Let's take simple data frame as an example:
df = pl.DataFrame(
    data=[[x for x in range(6)]],
    schema=[f&quot;a_{i}&quot; for i in range(3)] + [f&quot;b_{i}&quot; for i in range(3)]
)

┌─────┬─────┬─────┬─────┬─────┬─────┐
│ a_0 ┆ a_1 ┆ a_2 ┆ b_0 ┆ b_1 ┆ b_2 │
│ --- ┆ --- ┆ --- ┆ --- ┆ --- ┆ --- │
│ i64 ┆ i64 ┆ i64 ┆ i64 ┆ i64 ┆ i64 │
╞═════╪═════╪═════╪═════╪═════╪═════╡
│ 0   ┆ 1   ┆ 2   ┆ 3   ┆ 4   ┆ 5   │
└─────┴─────┴─────┴─────┴─────┴─────┘

Now, you can reshape it. First, concat the columns into lists and rename the columns for the final result:
import polars.selectors as cs

df.select(
    pl.concat_list(cs.starts_with(x)).alias(x) for x in ['a','b']
)

┌───────────┬───────────┐
│ a         ┆ b         │
│ ---       ┆ ---       │
│ list[i64] ┆ list[i64] │
╞═══════════╪═══════════╡
│ [0, 1, 2] ┆ [3, 4, 5] │
└───────────┴───────────┘

No, explode lists into rows:
df.select(
    pl.concat_list(cs.starts_with(x)).alias(x) for x in ['a','b']
).explode(pl.all())

┌─────┬─────┐
│ a   ┆ b   │
│ --- ┆ --- │
│ i64 ┆ i64 │
╞═════╪═════╡
│ 0   ┆ 3   │
│ 1   ┆ 4   │
│ 2   ┆ 5   │
└─────┴─────┘

"
"I have Polars dataframe
data = {
    &quot;col1&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;],
    &quot;col2&quot;: [[-0.06066, 0.072485, 0.548874, 0.158507],
             [-0.536674, 0.10478, 0.926022, -0.083722],
             [-0.21311, -0.030623, 0.300583, 0.261814],
             [-0.308025, 0.006694, 0.176335, 0.533835]],
}

df = pl.DataFrame(data)

I want to calculate cosine similarity for each combination of column col1
The desired output should be the following:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ col1_col2       â”† a    â”† b    â”† c    â”† d    â”‚
â”‚ ---             â”† ---  â”† ---  â”† ---  â”† ---  â”‚
â”‚ str             â”† f64  â”† f64  â”† f64  â”† f64  â”‚
â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•¡
â”‚ a               â”† 1.0  â”† 0.86 â”† 0.83 â”† 0.54 â”‚
â”‚ b               â”† 0.86 â”† 1.0  â”† 0.75 â”† 0.41 â”‚
â”‚ c               â”† 0.83 â”† 0.75 â”† 1.0  â”† 0.89 â”‚
â”‚ d               â”† 0.54 â”† 0.41 â”† 0.89 â”† 1.0  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜

Where each value represents cosine similarity between respective column values.
I'm using following cosine similarity function
from numpy.linalg import norm

cosine_similarity = lambda a,b: (a @ b.T) / (norm(a)*norm(b))

I tried to use it with pivot method
df.pivot(on=&quot;col1&quot;, values=&quot;col2&quot;, index=&quot;col1&quot;, aggregate_function=cosine_similarity)

However I'm getting the following error
AttributeError: 'function' object has no attribute '_pyexpr'

","Update: Polars 1.8.0 added native list arithmetic allowing us to write a much more efficient cosine similarity expression.

Combinations
We can add a row index and use
.join_where() to generate the row &quot;combinations&quot;.
df = df.with_row_index().lazy()

df.join_where(df, pl.col.index &lt;= pl.col.index_right).collect() 

shape: (10, 6)
┌───────┬──────┬─────────────────────────────────┬─────────────┬────────────┬─────────────────────────────────┐
│ index ┆ col1 ┆ col2                            ┆ index_right ┆ col1_right ┆ col2_right                      │
│ ---   ┆ ---  ┆ ---                             ┆ ---         ┆ ---        ┆ ---                             │
│ u32   ┆ str  ┆ list[f64]                       ┆ u32         ┆ str        ┆ list[f64]                       │
╞═══════╪══════╪═════════════════════════════════╪═════════════╪════════════╪═════════════════════════════════╡
│ 0     ┆ a    ┆ [-0.06066, 0.072485, … 0.15850… ┆ 0           ┆ a          ┆ [-0.06066, 0.072485, … 0.15850… │
│ 0     ┆ a    ┆ [-0.06066, 0.072485, … 0.15850… ┆ 1           ┆ b          ┆ [-0.536674, 0.10478, … -0.0837… │
│ 0     ┆ a    ┆ [-0.06066, 0.072485, … 0.15850… ┆ 2           ┆ c          ┆ [-0.21311, -0.030623, … 0.2618… │
│ 0     ┆ a    ┆ [-0.06066, 0.072485, … 0.15850… ┆ 3           ┆ d          ┆ [-0.308025, 0.006694, … 0.5338… │
│ 1     ┆ b    ┆ [-0.536674, 0.10478, … -0.0837… ┆ 1           ┆ b          ┆ [-0.536674, 0.10478, … -0.0837… │
│ 1     ┆ b    ┆ [-0.536674, 0.10478, … -0.0837… ┆ 2           ┆ c          ┆ [-0.21311, -0.030623, … 0.2618… │
│ 1     ┆ b    ┆ [-0.536674, 0.10478, … -0.0837… ┆ 3           ┆ d          ┆ [-0.308025, 0.006694, … 0.5338… │
│ 2     ┆ c    ┆ [-0.21311, -0.030623, … 0.2618… ┆ 2           ┆ c          ┆ [-0.21311, -0.030623, … 0.2618… │
│ 2     ┆ c    ┆ [-0.21311, -0.030623, … 0.2618… ┆ 3           ┆ d          ┆ [-0.308025, 0.006694, … 0.5338… │
│ 3     ┆ d    ┆ [-0.308025, 0.006694, … 0.5338… ┆ 3           ┆ d          ┆ [-0.308025, 0.006694, … 0.5338… │
└───────┴──────┴─────────────────────────────────┴─────────────┴────────────┴─────────────────────────────────┘

Cosine Similarity
You can write the formula using Expressions e.g. list arithmetic, .list.sum() and Expr.sqrt().
cosine_similarity = lambda x, y: (
    (x * y).list.sum() / (
        (x * x).list.sum().sqrt() * (y * y).list.sum().sqrt()
    )
)

out = (
   df.join_where(df, pl.col.index &lt;= pl.col.index_right)
     .select(
        col = &quot;col1&quot;,
        other = &quot;col1_right&quot;,
        cosine = cosine_similarity(
           x = pl.col.col2,
           y = pl.col.col2_right
        )
     )
)

# out.collect()
shape: (10, 3)
┌─────┬───────┬──────────┐
│ col ┆ other ┆ cosine   │
│ --- ┆ ---   ┆ ---      │
│ str ┆ str   ┆ f64      │
╞═════╪═══════╪══════════╡
│ a   ┆ a     ┆ 1.0      │
│ a   ┆ b     ┆ 0.856754 │
│ a   ┆ c     ┆ 0.827877 │
│ a   ┆ d     ┆ 0.540282 │
│ b   ┆ b     ┆ 1.0      │
│ b   ┆ c     ┆ 0.752199 │
│ b   ┆ d     ┆ 0.411564 │
│ c   ┆ c     ┆ 1.0      │
│ c   ┆ d     ┆ 0.889009 │
│ d   ┆ d     ┆ 1.0      │
└─────┴───────┴──────────┘

Pivot
You can vertically concat/stack the reverse pairings and then .pivot() for the matrix shape.
pl.concat(
   [
      out, 
      out.filter(pl.col.col != pl.col.other).select(col=&quot;other&quot;, other=&quot;col&quot;, cosine=&quot;cosine&quot;)
   ]
).collect().pivot(&quot;other&quot;, index=&quot;col&quot;)

shape: (4, 5)
┌─────┬──────────┬──────────┬──────────┬──────────┐
│ col ┆ a        ┆ b        ┆ c        ┆ d        │
│ --- ┆ ---      ┆ ---      ┆ ---      ┆ ---      │
│ str ┆ f64      ┆ f64      ┆ f64      ┆ f64      │
╞═════╪══════════╪══════════╪══════════╪══════════╡
│ a   ┆ 1.0      ┆ 0.856754 ┆ 0.827877 ┆ 0.540282 │
│ b   ┆ 0.856754 ┆ 1.0      ┆ 0.752199 ┆ 0.411564 │
│ c   ┆ 0.827877 ┆ 0.752199 ┆ 1.0      ┆ 0.889009 │
│ d   ┆ 0.540282 ┆ 0.411564 ┆ 0.889009 ┆ 1.0      │
└─────┴──────────┴──────────┴──────────┴──────────┘

"
"It is known that np.sum(arr) is quite a lot slower  than arr.sum().  For example:
import numpy as np
np.random.seed(7)
A = np.random.random(1000)
%timeit np.sum(A)
2.94 Âµs Â± 13.8 ns per loop (mean Â± std. dev. of 7 runs, 100,000 loops each)
%timeit A.sum()
1.8 Âµs Â± 40.8 ns per loop (mean Â± std. dev. of 7 runs, 1,000,000 loops each)

Can anyone give a detailed code-based explanation of what np.sum(arr) is doing that arr.sum() is not?
The difference is insignificant for much longer arrays. But it is relatively significant for arrays of length 1000 or less, for example.
In my code I do millions of array sums so the difference is particularly significant.
","When I run np.sum(a) in debug mode on my PC, it steps into the following code.
https://github.com/numpy/numpy/blob/v1.26.5/numpy/core/fromnumeric.py#L2178
The following is the part of the code where it is relevant.
import numpy as np
import types


def _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs):
    passkwargs = {k: v for k, v in kwargs.items()
                  if v is not np._NoValue}

    if type(obj) is not np.ndarray:
        raise NotImplementedError

    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)


def copied_np_sum(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, initial=np._NoValue, where=np._NoValue):
    if isinstance(a, types.GeneratorType):
        raise NotImplementedError

    return _wrapreduction(
        a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,
        initial=initial, where=where
    )

Note that this ends up calling np.add.reduce(a).
Benchmark:
import timeit


def benchmark(setup, stmt, repeat, number):
    print(f&quot;{stmt:16}: {min(timeit.repeat(setup=setup, stmt=stmt, globals=globals(), repeat=repeat, number=number)) / number}&quot;)


n_item = 10 ** 3
n_loop = 1000
n_set = 1000

data_setup = f&quot;&quot;&quot;\
import numpy as np
rng = np.random.default_rng(0)
a = rng.random({n_item})
&quot;&quot;&quot;

benchmark(setup=data_setup, stmt=&quot;np.sum(a)&quot;, repeat=n_set, number=n_loop)
benchmark(setup=data_setup, stmt=&quot;a.sum()&quot;, repeat=n_set, number=n_loop)
benchmark(setup=data_setup, stmt=&quot;copied_np_sum(a)&quot;, repeat=n_set, number=n_loop)
benchmark(setup=data_setup, stmt=&quot;np.add.reduce(a)&quot;, repeat=n_set, number=n_loop)

np.sum(a)       : 2.6407251134514808e-06
a.sum()         : 1.3474803417921066e-06
copied_np_sum(a): 2.50667380169034e-06
np.add.reduce(a): 1.195137854665518e-06

As you can see, copied_np_sum performs similarly to np.sum, and np.add.reduce is similar to a.sum.
So the majority of the difference between np.sum and a.sum is likely due to what copied_np_sum does before calling np.add.reduce.
In other words, it's the overhead caused by the dict comprehension and the additional function calls.
However, although there is a significant difference in the above benchmark that reproduces the OP's one, as pointed out in the comment, this may be overstated.
Because timeit repeatedly executes the code and uses the (best of) average, with a small array like in this benchmark, the array may already be in the CPU cache when it is measured.
This is not necessarily an unfair condition. The same thing could happen in actual use. Rather, it should be so whenever possible.
That being said, for a canonical answer, we should measure it.
Based on @user3666197 advice, we can create a large array immediately after creating a to evicts a from the cache.
Note that I decided to use np.arange here, which I confirmed has the same effect but runs faster.
import timeit


def benchmark(setup, stmt, repeat, number):
    print(f&quot;{stmt:16}: {min(timeit.repeat(setup=setup, stmt=stmt, globals=globals(), repeat=repeat, number=number)) / number}&quot;)


n_item = 10 ** 3
n_loop = 1
n_set = 100

data_setup = f&quot;&quot;&quot;\
import numpy as np
rng = np.random.default_rng(0)
a = rng.random({n_item})
_ = np.arange(10 ** 9, dtype=np.uint8)  # To evict `a` from the CPU cache.
&quot;&quot;&quot;

benchmark(setup=data_setup, stmt=&quot;np.sum(a)&quot;, repeat=n_set, number=n_loop)
benchmark(setup=data_setup, stmt=&quot;a.sum()&quot;, repeat=n_set, number=n_loop)
benchmark(setup=data_setup, stmt=&quot;copied_np_sum(a)&quot;, repeat=n_set, number=n_loop)
benchmark(setup=data_setup, stmt=&quot;np.add.reduce(a)&quot;, repeat=n_set, number=n_loop)

Without eviction (With cache):
np.sum(a)       : 2.6407251134514808e-06
a.sum()         : 1.3474803417921066e-06
copied_np_sum(a): 2.50667380169034e-06
np.add.reduce(a): 1.195137854665518e-06

With eviction (Without cache):
np.sum(a)       : 4.916824400424957e-05
a.sum()         : 3.245798870921135e-05
copied_np_sum(a): 4.7205016016960144e-05
np.add.reduce(a): 3.0195806175470352e-05

Naturally, the presence or absence of cache makes a huge impact on performance.
However, although the difference has become smaller, it can still be said to be a significant difference.
Also, since these four relationships remain the same as before, the conclusion also remains the same.
There are a few things I should add.
Note1
The claim regarding method loading is incorrect.
benchmark(setup=f&quot;{data_setup}f = np.sum&quot;, stmt=&quot;f(a)&quot;, repeat=n_set, number=n_loop)
benchmark(setup=f&quot;{data_setup}f = a.sum&quot;, stmt=&quot;f()&quot;, repeat=n_set, number=n_loop)

np.sum(a)       : 4.916824400424957e-05
a.sum()         : 3.245798870921135e-05
f(a)            : 4.6479981392621994e-05  &lt;-- Same as np.sum.
f()             : 3.27317975461483e-05    &lt;-- Same as a.sum.
np.add.reduce(a): 3.0195806175470352e-05  &lt;-- Also, note that this one is fast.

Note2
As all benchmarks show, np.add.reduce is the fastest (least overhead). If your actual application also deals only with 1D arrays, and such a small difference is important to you, you should consider using np.add.reduce.
Note3
Actually, numba may be the fastest in this case.
from numba import njit
import numpy as np
import math


@njit(cache=True)
def nb_numpy_sum(a):
    # This will be a reduce sum.
    return np.sum(a)


@njit(cache=True)
def nb_pairwise_sum(a):
    # https://en.wikipedia.org/wiki/Pairwise_summation
    N = 2
    if len(a) &lt;= N:
        return np.sum(a)  # reduce sum
    else:
        m = len(a) // 2
        return nb_pairwise_sum(a[:m]) + nb_pairwise_sum(a[m:])


@njit(cache=True)
def nb_kahan_sum(a):
    # https://en.wikipedia.org/wiki/Kahan_summation_algorithm
    total = a.dtype.type(0.0)
    c = total
    for i in range(len(a)):
        y = a[i] - c
        t = total + y
        c = (t - total) - y
        total = t
    return total


def test():
    candidates = [
        (&quot;np.sum&quot;, np.sum),
        (&quot;math.fsum&quot;, math.fsum),
        (&quot;nb_numpy_sum&quot;, nb_numpy_sum),
        (&quot;nb_pairwise_sum&quot;, nb_pairwise_sum),
        (&quot;nb_kahan_sum&quot;, nb_kahan_sum),
    ]

    n = 10 ** 7 + 1
    a = np.full(n, 0.1, dtype=np.float64)
    for name, f in candidates:
        print(f&quot;{name:16}: {f(a)}&quot;)


test()

Accuracy:
np.sum          : 1000000.0999999782
math.fsum       : 1000000.1000000001
nb_numpy_sum    : 1000000.0998389754
nb_pairwise_sum : 1000000.1
nb_kahan_sum    : 1000000.1000000001

Timing:
np.sum(a)         : 4.7777313739061356e-05
a.sum()           : 3.219071435928345e-05
np.add.reduce(a)  : 2.9000919312238693e-05
nb_numpy_sum(a)   : 1.0361894965171814e-05
nb_pairwise_sum(a): 1.4733988791704178e-05
nb_kahan_sum(a)   : 1.2937933206558228e-05

Note that although nb_pairwise_sum and nb_kahan_sum have mathematical accuracy comparable to NumPy, neither is intended to be an exact replica of NumPy's implementation.
So there is no guarantee that the results will be exactly the same as NumPy's.
It should also be clarified that this difference is due to the amount of overhead, and NumPy is significantly faster for large arrays (e.g. &gt;10000).

The following section was added after this answer was accepted. Below is an improved version of @JérômeRichard's pairwise sum that sacrifices some accuracy for faster performance on larger arrays. See the comments for more details.
import numba as nb
import numpy as np

# Very fast function which should be inlined by LLVM.
# The loop should be completely unrolled and designed so the SLP-vectorizer 
# could emit SIMD instructions, though in practice it does not...
@nb.njit(cache=True)
def nb_sum_x16(a):
    v1 = a[0]
    v2 = a[1]
    for i in range(2, 16, 2):
        v1 += a[i]
        v2 += a[i+1]
    return v1 + v2

@nb.njit(cache=True)
def nb_pairwise_sum(a):
    n = len(a)
    m = n // 2

    # Trivial case for tiny arrays
    if n &lt; 16:
        return sum(a[:m]) + sum(a[m:])

    # Computation of a chunk (of 16~256 items) using an iterative 
    # implementation so to reduce the overhead of function calls.
    if n &lt;= 256:
        v = nb_sum_x16(a[0:16])
        i = 16
        # Main loop iterating on blocks (of exactly 16 items)
        while i + 15 &lt; n:
            v += nb_sum_x16(a[i:i+16])
            i += 16
        return v + sum(a[i:])

    # OPTIONAL OPTIMIZATION: only for array with 1_000~100_000 items
    # Same logic than above but with bigger chunks
    # It is meant to reduce branch prediction issues with small 
    # chunks by splitting them in equal size.
    if n &lt;= 4096:
        v = nb_pairwise_sum(a[:256])
        i = 256
        while i + 255 &lt; n:
            v += nb_pairwise_sum(a[i:i+256])
            i += 256
        return v + nb_pairwise_sum(a[i:])

    return nb_pairwise_sum(a[:m]) + nb_pairwise_sum(a[m:])

"
"We have a discriminator field type which we want to hide from the Swagger UI docs:
class Foo(BDCBaseModel):
    type: Literal[&quot;Foo&quot;] = Field(&quot;Foo&quot;, exclude=True)
    Name: str

class Bar(BDCBaseModel):
    type: Literal[&quot;Bar&quot;] = Field(&quot;Bar&quot;, exclude=True)
    Name: str

class Demo(BDCBaseModel):
    example: Union[Foo, Bar] = Field(discriminator=&quot;type&quot;)

The following router:
@router.post(&quot;/demo&quot;)
async def demo(
    foo: Foo,
):
    demo = Demo(example=foo)
    return demo

And this is shown in the Swagger docs:

We don't want the user to see the type field as it is useless for him/her anyways.
We tried making the field private: _type which hides it from the docs but then it cannot be used as discriminator anymore:
    class Demo(BDCBaseModel):
  File &quot;pydantic\main.py&quot;, line 205, in pydantic.main.ModelMetaclass.__new__
  File &quot;pydantic\fields.py&quot;, line 491, in pydantic.fields.ModelField.infer
  File &quot;pydantic\fields.py&quot;, line 421, in pydantic.fields.ModelField.__init__
  File &quot;pydantic\fields.py&quot;, line 537, in pydantic.fields.ModelField.prepare
  File &quot;pydantic\fields.py&quot;, line 639, in pydantic.fields.ModelField._type_analysis
  File &quot;pydantic\fields.py&quot;, line 753, in pydantic.fields.ModelField.prepare_discriminated_union_sub_fields
  File &quot;pydantic\utils.py&quot;, line 739, in pydantic.utils.get_discriminator_alias_and_values
pydantic.errors.ConfigError: Model 'Foo' needs a discriminator field for key '_type'

","This is a very common situation and the solution is farily simple. Factor out that type field into its own separate model.
The typical way to go about this is to create one FooBase with all the fields, validators etc. that all child models will share (in this example only name) and then subclass it as needed. In this example you would create one Foo subclass with that type field that you then use for the Demo annotation, and one FooRequest class without any additions.
Here is a full working example:
from typing import Literal, Union

from fastapi import FastAPI
from pydantic import BaseModel, Field

class FooBase(BaseModel):
    name: str

class FooRequest(FooBase):
    pass  # possibly configure other request specific things here

class Foo(FooBase):
    type: Literal[&quot;Foo&quot;] = Field(&quot;Foo&quot;, exclude=True)

    class Config:
        orm_mode = True

class Bar(BaseModel):
    type: Literal[&quot;Bar&quot;] = Field(&quot;Bar&quot;, exclude=True)
    name: str

class Demo(BaseModel):
    example: Union[Foo, Bar] = Field(discriminator=&quot;type&quot;)

api = FastAPI()

@api.post(&quot;/demo&quot;)
async def demo(foo: FooRequest):
    foo = Foo.from_orm(foo)
    return Demo(example=foo)

Note that I used the orm_mode = True setting just to have a very concise way of converting a FooRequest instance into a Foo instance inside the route handler function. This is not necessary. You could also just do foo = Foo.parse_obj(foo.dict()) there.
Also, the addition of the FooRequest model is redundant here of course. You can just as well use the FooBase as the request model. I wrote it this way just to demonstrate a typical pattern because sometimes the request model has additional things that distinguish it from its siblings. In your example it is overkill.
"
"I got the following Javascript code and I need to convert it to Python(I'm not an expert in hashing so sorry for my knowledge on this subject)
function generateAuthHeader(dataToSign) {
    let apiSecretHash = new Buffer(&quot;Rbju7azu87qCTvZRWbtGqg==&quot;, 'base64');
    let apiSecret = apiSecretHash.toString('ascii');
    var hash = CryptoJS.HmacSHA256(dataToSign, apiSecret);
    return hash.toString(CryptoJS.enc.Base64);
}

when I ran generateAuthHeader(&quot;abc&quot;) it returned +jgBeooUuFbhMirhh1KmQLQ8bV4EXjRorK3bR/oW37Q=
So I tried writing the following Python code:
def generate_auth_header(data_to_sign):
    api_secret_hash = bytearray(base64.b64decode(&quot;Rbju7azu87qCTvZRWbtGqg==&quot;))
    hash = hmac.new(api_secret_hash, data_to_sign.encode(), digestmod=hashlib.sha256).digest()
    return base64.b64encode(hash).decode()

But when I ran generate_auth_header(&quot;abc&quot;) it returned a different result aOGo1XCa5LgT1CIR8C1a10UARvw2sqyzWWemCJBJ1ww=
Can someone tell me what is wrong with my Python code and what I need to change?
The base64 is the string I generated myself for this post
UPDATE:
this is the document I'm working with
//Converting the Rbju7azu87qCTvZRWbtGqg== (key) into byte array 
//Converting the data_to_sign into byte array 
//Generate the hmac signature

it seems like apiSecretHash and api_secret_hash is different, but I don't quite understand as the equivalent of new Buffer() in NodeJS is bytearray() in python
","It took me 2 days to look it up and ask for people in python discord and I finally got an answer. Let me summarize the problems:

API secret hash from both return differents hash of the byte array
javascript

Javascript
apiSecret = &quot;E8nm,ns:\u0002NvQY;F*&quot;

Python
api_secret_hash = b'E\xb8\xee\xed\xac\xee\xf3\xba\x82N\xf6QY\xbbF\xaa'

once we replaced the hash with python code it return the same result
def generate_auth_header(data_to_sign):
    api_secret_hash = &quot;E8nm,ns:\u0002NvQY;F*&quot;.encode()

    hash = hmac.new(api_secret_hash, data_to_sign.encode(), digestmod=hashlib.sha256).digest()
    return base64.b64encode(hash).decode()

encoding for ASCII in node.js you can find here https://github.com/nodejs/node/blob/a2a32d8beef4d6db3a8c520572e8a23e0e51a2f8/src/string_bytes.cc#L636-L647
case ASCII:
  if (contains_non_ascii(buf, buflen)) {
    char* out = node::UncheckedMalloc(buflen);
    if (out == nullptr) {
      *error = node::ERR_MEMORY_ALLOCATION_FAILED(isolate);
      return MaybeLocal&lt;Value&gt;();
    }
    force_ascii(buf, out, buflen);
    return ExternOneByteString::New(isolate, out, buflen, error);
  } else {
    return ExternOneByteString::NewFromCopy(isolate, buf, buflen, error);
  }

there is this force_ascii() function that is called when the data contains non-ASCII characters which is implemented here https://github.com/nodejs/node/blob/a2a32d8beef4d6db3a8c520572e8a23e0e51a2f8/src/string_bytes.cc#L531-L573
so we need to check for the hash the same as NodeJS one, so we get the final version of the Python code:
def generate_auth_header(data_to_sign):
    # convert to bytearray so the for loop below can modify the values
    api_secret_hash = bytearray(base64.b64decode(&quot;Rbju7azu87qCTvZRWbtGqg==&quot;))
    
    # &quot;force&quot; characters to be in ASCII range
    for i in range(len(api_secret_hash)):
        api_secret_hash[i] &amp;= 0x7f;

    hash = hmac.new(api_secret_hash, data_to_sign.encode(), digestmod=hashlib.sha256).digest()
    return base64.b64encode(hash).decode()

now it returned the same result as NodeJS one
Thank you Mark from the python discord for helping me understand and fix this!
Hope anyone in the future trying to convert byte array from javascript to python know about this different of NodeJS Buffer() function
"
"I'm trying to find out if Pandas.read_json performs some level of autodetection. For example, I have the following data:
data_records = [
    {
        &quot;device&quot;: &quot;rtr1&quot;,
        &quot;dc&quot;: &quot;London&quot;,
        &quot;vendor&quot;: &quot;Cisco&quot;,
    },
    {
        &quot;device&quot;: &quot;rtr2&quot;,
        &quot;dc&quot;: &quot;London&quot;,
        &quot;vendor&quot;: &quot;Cisco&quot;,
    },
    {
        &quot;device&quot;: &quot;rtr3&quot;,
        &quot;dc&quot;: &quot;London&quot;,
        &quot;vendor&quot;: &quot;Cisco&quot;,
    },
]

data_index = {
    &quot;rtr1&quot;: {&quot;dc&quot;: &quot;London&quot;, &quot;vendor&quot;: &quot;Cisco&quot;},
    &quot;rtr2&quot;: {&quot;dc&quot;: &quot;London&quot;, &quot;vendor&quot;: &quot;Cisco&quot;},
    &quot;rtr3&quot;: {&quot;dc&quot;: &quot;London&quot;, &quot;vendor&quot;: &quot;Cisco&quot;},
}

If I do the following:
import pandas as pd
import json

pd.read_json(json.dumps(data_records))
---
  device      dc vendor
0   rtr1  London  Cisco
1   rtr2  London  Cisco
2   rtr3  London  Cisco

though I get the output that I desired, the data is record based. Being that the default orient is columns, I would have not thought this would have worked.
Therefore is there some level of autodetection going on? With index based inputs the behaviour seems more inline. As this shows appears to have parsed the data based on a column orient by default.
pd.read_json(json.dumps(data_index))

          rtr1    rtr2    rtr3
dc      London  London  London
vendor   Cisco   Cisco   Cisco

pd.read_json(json.dumps(data_index), orient=&quot;index&quot;)

          dc vendor
rtr1  London  Cisco
rtr2  London  Cisco
rtr3  London  Cisco

","TL;DR
When using pd.read_json() with orient=None, the representation of the data is automatically determined through pd.DataFrame().
Explanation
The pandas documentation is a bit misleading here. When not specifying orient, the parser for 'columns' is used, which is self.obj = pd.DataFrame(json.loads(json)). So
pd.read_json(json.dumps(data_records))

is equivalent to
pd.DataFrame(json.loads(json.dumps(data_records)))

which again is equivalent to
pd.DataFrame(data_records)

I.e., you pass a list of dicts to the DataFrame constructor, which then performs the automatic determination of the data representation. Note that this does not mean that orient is auto-detected. Instead, simple heuristics (see below) on how the data should be loaded into a DataFrame are applied.
Loading JSON-like data through pd.DataFrame()
For the 3 most relevant cases of JSON-structured data, the DataFrame construction through pd.DataFrame() is:

Dict of lists

In[1]: data = {&quot;a&quot;: [1, 2, 3], &quot;b&quot;: [9, 8, 7]}
  ...: pd.DataFrame(data)
Out[1]: 
   a  b
0  1  9
1  2  8
2  3  7


Dict of dicts

In[2]: data = {&quot;a&quot;: {&quot;x&quot;: 1, &quot;y&quot;: 2, &quot;z&quot;: 3}, &quot;b&quot;: {&quot;x&quot;: 9, &quot;y&quot;: 8, &quot;z&quot;: 7}}
  ...: pd.DataFrame(data)
Out[2]: 
   a  b
x  1  9
y  2  8
z  3  7


List of dicts

In[3]: data = [{'a': 1, 'b': 9}, {'a': 2, 'b': 8}, {'a': 3, 'b': 7}]
  ...: pd.DataFrame(data)
Out[3]: 
   a  b
0  1  9
1  2  8
2  3  7

"
"I have two different lists and I would like to know how I can get each element of one list print with each element of another list. I know I could use two for loops (each for one of the lists), however I want to use the zip() function because there's more that I will be doing in this for loop for which I will require parallel iteration.
I therefore attempted the following but the output is as shown below.
lasts = ['x', 'y', 'z']
firsts = ['a', 'b', 'c']

for last, first in zip(lasts, firsts):
    print (last, first, &quot;\n&quot;)

Output:
x a 
y b 
z c 

Expected Output:
x a
x b
x c
y a
y b
y c
z a
z b
z c

","I believe the function you are looking for is itertools.product:
lasts = ['x', 'y', 'z']
firsts = ['a', 'b', 'c']

from itertools import product
for last, first in product(lasts, firsts):
    print (last, first)

x a
x b
x c
y a
y b
y c
z a
z b
z c

Another alternative, that also produces an iterator is to use a nested comprehension:
iPairs=( (l,f) for l in lasts for f in firsts)
for last, first in iPairs:
    print (last, first)

If you must use zip(), both inputs must be extended to the total length (product of lengths) with the first one repeating items and the second one repeating the list:
iPairs = zip( (l for l in lasts for _ in firsts),
              (f for _ in lasts for f in firsts) )

"
"I have been trying to use the scikit-learn library to solve this problem.  Roughly:
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# Make or load an n x p data matrix X and n x 1 array y of the corresponding
# function values.

poly = PolynomialFeatures(degree=2)
Xp = poly.fit_transform(X)
model = LinearRegression()
model.fit(Xp, y)

# Approximate the derivatives of the gradient and Hessian using the relevant
# finite-difference equations and model.predict.

As the above illustrates, sklearn makes the design choice to separate polynomial regression into PolynomialFeatures and LinearRegression rather than combine these into a single function.  This separation has conceptual advantages but also a major drawback:  it effectively prevents model from offering the methods gradient and hessian, and model would be significantly more useful if it did.
My current work-around uses finite-difference equations and model.predict to approximate the elements of the gradient and Hessian (as described here).  But I don't love this approach â€” it is sensitive to floating-point error and the &quot;exact&quot; information needed to build the gradient and Hessian is already contained in model.coef_.
Is there any more elegant or accurate method to fit a p-dimensional polynomial and find its gradient and Hessian within Python? I would be fine with one that uses a different library.
","To compute the gradient or the Hessian of a polynomial, one needs to know exponents of variables in each monomial and the corresponding monomial coefficients. The first piece of this information is provided by poly.powers_, the second by model.coef_:
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
import numpy as np

np.set_printoptions(precision=2, suppress=True)

X = np.arange(6).reshape(3, 2)
y = np.arange(3)
poly = PolynomialFeatures(degree=2)
Xp = poly.fit_transform(X)
model = LinearRegression()
model.fit(Xp, y)

print(&quot;Exponents:&quot;)
print(poly.powers_.T)
print(&quot;Coefficients:&quot;)
print(model.coef_)

This gives:
Exponents:
[[0 1 0 2 1 0]
 [0 0 1 0 1 2]]
Coefficients:
[ 0.    0.13  0.13 -0.12 -0.    0.13]

The following function can be then used to compute the gradient at a point given by an array x:
def gradient(x, powers, coeffs):
    x = np.array(x)
    gp = np.maximum(0, powers[:, np.newaxis] - np.eye(powers.shape[1], dtype=int))
    gp = gp.transpose(1, 2, 0)
    gc = coeffs * powers.T
    return (((x[:, np.newaxis] ** gp).prod(axis=1)) * gc).sum(axis=1)

For example, we can use it to compute the gradient at the point [0, 1]:
print(gradient([0, 1],  poly.powers_, model.coef_))

This gives:
[0.13 0.38]

The Hessian at a given point can be computed in a similar way.
"
"Using pydantic setting management, how can I load env variables on nested setting objects on a main settings class? In the code below, the sub_field env variable field doesn't get loaded. field_one and field_two load fine. How can I load an environment file so the values are propagated down to the nested sub_settings object?
from typing import Optional
from pydantic import BaseSettings, Field


class SubSettings(BaseSettings):
    sub_field: Optional[str] = Field(None, env='SUB_FIELD')


class Settings(BaseSettings):
    field_one: Optional[str] = Field(None, env='FIELD_ONE')
    field_two: Optional[int] = Field(None, env='FIELD_TWO')
    sub_settings: SubSettings = SubSettings()


settings = Settings(_env_file='local.env')

","There are some examples of nested loading of pydantic env variables in the docs.
Option 1
If you're willing to adjust your variable names, one strategy is to use env_nested_delimiter to denote nested fields. This appears to be the way that pydantic expects nested settings to be loaded, so it should be preferred when possible.
So with a local.env like this:
FIELD_ONE=one
FIELD_TWO=2
SUB_SETTINGS__SUB_FIELD=value

You should be able to load the settings in this way
from typing import Optional
from pydantic import BaseModel, BaseSettings


class SubSettings(BaseModel):
     # ^ Note that this inherits from BaseModel, not BaseSettings
    sub_field: Optional[str]


class Settings(BaseSettings):
    field_one: Optional[str]
    field_two: Optional[int]
    sub_settings: SubSettings

    class Config:
        env_nested_delimiter = '__'

Alternate
If you don't want to use the env_nested_delimiter functionality, you could load both sets of settings from the same local.env file. Then pass the loaded SubSettings to Settings directly. This can be done by overriding the Settings class __init__ method
With this local.env
FIELD_ONE=one
FIELD_TWO=2
SUB_FIELD=value

use the following to load the settings
from typing import Optional
from pydantic import BaseModel, BaseSettings, Field

class SubSettings(BaseSettings):
    sub_field: Optional[str]

class Settings(BaseSettings):
    field_one: Optional[str]
    field_two: Optional[int]
    sub_settings: SubSettings

    def __init__(self, *args, **kwargs):
        kwargs['sub_settings'] = SubSettings(_env_file=kwargs['_env_file'])
        super().__init__(*args, **kwargs)


settings = Settings(_env_file='local.env')

"
"Considering this abstract class and a class implementing it:
from abc import ABC

class FooBase(ABC):
    foo: str
    bar: str
    baz: int

    def __init__(self):
        self.bar = &quot;bar&quot;
        self.baz = &quot;baz&quot;

class Foo(FooBase):
    foo: str = &quot;hello&quot;

The idea here is that a Foo class that implements FooBase would be required to specify the value of the foo attribute, but the other attributes (bar and baz) would not need to be overwritten, as they're already handle by a method provided by the abstract class.
From a MyPy type-checking perspective, is it possible to force Foo to declare the attribute foo and raise a type-checking error otherwise?
EDIT:
The rationale is that FooBase is part of a library, and the client code should be prevented from implementing it without specifying a value for foo. For bar and baz however, these are entirely managed by the library and the client doesn't care about them.
","This is a partial answer. You can use
class FooBase(ABC):
    @property
    @classmethod
    @abstractmethod
    def foo(cls) -&gt; str:
        ...

class Foo(FooBase):
    foo = &quot;hi&quot;

def go(f: FooBase) -&gt; str:
    return f.foo

It's only partial because you'll only get a mypy error if you try to instantiate Foo without an initialized foo, like
class Foo(FooBase):
    ...

Foo()  # error: Cannot instantiate abstract class &quot;Foo&quot; with abstract attribute &quot;foo&quot;

This is the same behaviour as when you have a simple @abstractmethod. Only when instantiating it is the error raised. This is expected because Foo might not be intended as a concrete class, and may itself be subclassed. You can mitigate this somewhat by stating it is a concrete class with typing.final. The following will raise an error on the class itself.
@final
class Foo(FooBase):  # error: Final class __main__.Foo has abstract attributes &quot;foo&quot;
   ...

"
"I am trying to build a Python package, that contains sub-modules and sub-packages (&quot;libraries&quot;).
I was looking everywhere for the right way to do it, but amazingly I find it very complicated. Also went through multiple threads in StackOverFlow of course..
The problem is as follows:

In order to import a module or a package from another directory, it seems to me that there are 2 options:
a. Adding the absolute path to sys.path.
b. Installing the package with the setuptools.setup function in a setup.py file, in the main directory of the package - which installs the package into the site-packages directory of the specific Python version that in use.

Option a seems too clumsy for me. Option b is great, however I find it impractical becasue I am currently working and editing the package's source code - and the changes are not updating on the installed directory of the package, of course. In addition the installed directory of the package is not tracked by Git, and needless to say I use Git the original directory.


To conclude the question:
What is the best practice to import modules and sub-packages freely and nicely from within sub-directories of a Python package that is currently under construction?
I feel I am missing something but couldn't find a decent solution so far.
Thanks!
","This is a great question, and I wish more people would think along these lines. Making a module importable and ultimately installable is absolutely necessary before it can be easily used by others.
On sys.path munging
Before I answer I will say I do use sys.path munging when I do initial development on a file outside of an existing package structure. I have an editor snippet that constructs code like this:
import sys, os
sys.path.append(os.path.expanduser('~/path/to/parent'))
from module_of_interest import *  # NOQA

Given the path to the current file I use:
import ubelt as ub
fpath = ub.Path('/home/username/path/to/parent/module_of_interest.py')
modpath, modname = ub.split_modpath(fpath, check=False)
modpath = ub.Path(modpath).shrinkuser()  # abstract home directory

To construct the necessary parts the snippet will insert into the file so I can interact with it from within IPython. I find taking the little bit of extra time to remove the reference to my explicit homefolder such that the code still works as long as users have the same relative path structure wrt to the home directory makes this slightly more portable.
Proper Python Package Management
That being said, sys.path munging is not a sustainable solution. Ultimately you want your package to be managed by a python package manger. I know a lot of people use poetry, but I like plain old pip, so I can describe that process, but know this isn't the only way to do it.
To do this we need to go over some basics.
Basics

You must know what Python environment you are working in. Ideally this is a virtual environment managed with pyenv (or conda or mamba or poetry ...). But it's also possible to do this in your global sytem Python environment, although that is not recommended. I like working in a single default Python virtual environment that is always activated in my .bashrc. Its always easy to switch to a new one or blow it away / start fresh.

You need to consider two root paths: the root of your repository, which I will call your repo path, and your root to your package, the package path or module path, which should be a folder with the name of the top-level Python package. You will use this name to import it. This package path must live inside the repo path. Some repos, like xdoctest, like to put the module path in a src directory. Others , like ubelt, like to have the repo path at the top-level of the repository. I think the second case is conceptually easier for new package creators / maintainers, so let's go with that.


Setting up the repo path
So now, you are in an activated Python virtual environment, and we have designated a path we will checkout the repo. I like to clone repos in $HOME/code, so perhaps the repo path is $HOME/code/my_project.
In this repo path you should have your root package path. Lets say your package is named mypymod. Any directory that contains an __init__.py file is conceptually a python module, where the contents of __init__.py are what you get when you import that directory name. The only difference between a directory module and a normal file module is that a directory module/package can have submodules or subpackages.
For example if you are in the my_project repo, i.e. when you ls you see mypymod, and you have a file structure that looks something like this...
+ my_project
    + mypymod
        + __init__.py
        + submod1.py
        + subpkg
            + __init__.py
            + submod2.py


you can import the following modules:
import mypymod
import mypymod.submod1
import mypymod.subpkg
import mypymod.subpkg.submod2


If you ensured that your current working directory was always the repo root, or you put the repo root into sys.path, then this would be all you need. Being visible in sys.path or the CWD is all that is needed for another module could see your module.
The package manifest: setup.py / pyproject.toml
Now the trick is: how do you ensure your other packages / scripts can always see this module? That is where the package manager comes in. For this we will need a setup.py or the newer pyproject.toml variant. I'll describe the older setup.py way of doing things.
All you need to do is put the setup.py in your repo root. Note: it does not go in your package directory. There are plenty of resources for how to write a setup.py so I wont describe it in much detail, but basically all you need is to populate it with enough information so it knows about the name of the package, its location, and its version.
from setuptools import setup, find_packages
setup(
    name='mypymod',
    version='0.1.0',
    packages=find_packages(include=['mypymod', 'mypymod.*']),
    install_requires=[],
)


So your package structure will look like this:
+ my_project
    + setup.py
    + mypymod
        + __init__.py
        + submod1.py
        + subpkg
            + __init__.py
            + submod2.py


There are plenty of other things you can specify, I recommend looking at ubelt and xdoctest as examples. I'll note they contain a non-standard way of parsing requirements out of a requirements.txt or requirements/*.txt files, which I think is generally better than the standard way people handle requirements. But I digress.
Given something that pip or some other package manager (e.g. pipx, poetry) recognizes as a package manifest - a file that describes the contents of your package, you can now install it. If you are still developing it you can install it in editable mode, so instead of the package being copied into your site-packages, only a symbolic link is made, so any changes in your code are reflected each time you invoke Python (or immediately if you have autoreload on with IPython).
With pip it is as simple as running pip install -e &lt;path-to-repo-root&gt;, which is typically done by navigating into the repo and running pip install -e ..
Congrats, you now have a package you can reference from anywhere.
Making the most of your package
The python -m invocation
Now that you have a package you can reference as if it was installed via pip from pypi. There are a few tricks for using it effectively. The first is running scripts.
You don't need to specify a path to a file to run it as a script in Python. It is possible to run a script as __main__ using only its module name. This is done with the -m argument to Python. For instance you can run python -m mypymod.submod1 which will invoke $HOME/code/my_project/mypymod/submod1.py as the main module (i.e. it's __name__ attribute will be set to &quot;__main__&quot;).
Furthermore if you want to do this with a directory module you can make a special file called __main__.py in that directory, and that is the script that will be executed. For instance if we modify our package structure
+ my_project
    + setup.py
    + mypymod
        + __init__.py
        + __main__.py
        + submod1.py
        + subpkg
            + __init__.py
            + __main__.py
            + submod2.py

Now python -m mypymod will execute $HOME/code/my_project/mypymod/__main__.py and python -m mypymod.subpkg will execute $HOME/code/my_project/mypymod/subpkg/__main__.py. This is a very handy way to make a module double as both a importable package and a command line executable (e.g. xdoctest does this).
Easier imports
One pain point you might notice is that in the above code if you run:
import mypymod
mypymod.submod1

You will get an error because by default a package doesn't know about its submodules until they are imported. You need to populate the __init__.py to expose any attributes you desire to be accessible at the top-level. You could populate the mypymod/__init__.py with:
from mypymod import submod1

And now the above code would work.
This has a tradeoff though. The more thing you make accessible immediately, the more time it takes to import the module, and with big packages it can get fairly cumbersome. Also you have to manually write the code to expose what you want, so that is a pain if you want everything.
If you took a look at ubelt's init.py you will see it has a good deal of code to explicitly make every function in every submodule accessible at a top-level. I've written yet another library called mkinit that actually automates this process, and it also has the option of using the lazy_loader library to mitigate the performance impact of exposing all attributes at the top-level. I find the mkinit tool very helpful when writing large nested packages.
Summary
To summarize the above content:

Make sure you are working in a Python virtualenv (I recommend pyenv)
Identify your &quot;package path&quot; inside of your &quot;repo path&quot;.
Put an __init__.py in every directory you want to be a Python package or subpackage.
Optionally, use mkinit to autogenerate the content of your __init__.py files.
Put a setup.py / pyproject.toml in the root of your &quot;repo path&quot;.
Use pip install -e . to install the package in editable mode while you develop it.
Use python -m to invoke module names as scripts.

Hope this helps.
"
"As of matplotlib 3.4.0, Axes.bar_label method allows for labelling bar charts.
However, the labelling format option works with old style formatting, e.g. fmt='%g'
How can I make it work with new style formatting that would allow me to do things like percentages, thousands separators, etc:  '{:,.2f}', '{:.2%}', ...
The first thing that comes to my mind is somehow taking the initial labels from ax.containers and then reformatting them but it also needs to work for different bar structures, grouped bars with different formats and so on.
","
How can I make bar_label work with new style formatting like percentages, thousands separators, etc?


As of matplotlib 3.7
The fmt param now directly supports {}-based format strings, e.g.:
# &gt;= 3.7
plt.bar_label(bars, fmt='{:,.2f}')
#                       ^no f here (not an actual f-string)


Prior to matplotlib 3.7
The fmt param does not support {}-based format strings, so use the labels param. Format the bar container's datavalues with an f-string and set those as the labels, e.g.:
# &lt; 3.7
plt.bar_label(bars, labels=[f'{x:,.2f}' for x in bars.datavalues])




Examples:

Thousands separator labels
bars = plt.bar(list('ABC'), [12344.56, 23456.78, 34567.89])

# &gt;= v3.7
plt.bar_label(bars, fmt='${:,.2f}')

# &lt; v3.7
plt.bar_label(bars, labels=[f'${x:,.2f}' for x in bars.datavalues])



Percentage labels
bars = plt.bar(list('ABC'), [0.123456, 0.567890, 0.789012])

# &gt;= 3.7
plt.bar_label(bars, fmt='{:.2%}')  # &gt;= 3.7

# &lt; 3.7
plt.bar_label(bars, labels=[f'{x:.2%}' for x in bars.datavalues])



Stacked percentage labels
x = list('ABC')
y = [0.7654, 0.6543, 0.5432]

fig, ax = plt.subplots()
ax.bar(x, y)
ax.bar(x, 1 - np.array(y), bottom=y)

# now 2 bar containers: white labels for blue bars, black labels for orange bars
colors = list('wk')

# &gt;= 3.7
for bars, color in zip(ax.containers, colors):
    ax.bar_label(bars, fmt='{:.1%}', color=color, label_type='center')

# &lt; 3.7
for bars, color in zip(ax.containers, colors):
    labels = [f'{x:.1%}' for x in bars.datavalues]
    ax.bar_label(bars, labels=labels, color=color, label_type='center')




"
"I have a DataFrame (df) that contains columns: ID, Initial Date, Final Date, and Value, and another DataFrame (dates) that contains all the days for each ID from df.
On the dates dataframe i want to sum the values if exist on the range of each ID
Here is my code
import polars as pl
from datetime import datetime

data = {
    &quot;ID&quot; : [1, 2, 3, 4, 5],
    &quot;Initial Date&quot; : [&quot;2022-01-01&quot;, &quot;2022-01-02&quot;, &quot;2022-01-03&quot;, &quot;2022-01-04&quot;, &quot;2022-01-05&quot;],
    &quot;Final Date&quot; : [&quot;2022-01-03&quot;, &quot;2022-01-06&quot;, &quot;2022-01-07&quot;, &quot;2022-01-09&quot;, &quot;2022-01-07&quot;],
    &quot;Value&quot; : [10, 20, 30, 40, 50]


}

df = pl.DataFrame(data)

dates = pl.datetime_range(
    start=datetime(2022,1,1),
    end=datetime(2022,1,7),
    interval=&quot;1d&quot;,
    eager = True,
    closed = &quot;both&quot;
    ).to_frame(&quot;date&quot;)

shape: (5, 4)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ID  â”† Initial Date â”† Final Date â”† Value â”‚
â”‚ --- â”† ---          â”† ---        â”† ---   â”‚
â”‚ i64 â”† str          â”† str        â”† i64   â”‚
â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡
â”‚ 1   â”† 2022-01-01   â”† 2022-01-03 â”† 10    â”‚
â”‚ 2   â”† 2022-01-02   â”† 2022-01-06 â”† 20    â”‚
â”‚ 3   â”† 2022-01-03   â”† 2022-01-07 â”† 30    â”‚
â”‚ 4   â”† 2022-01-04   â”† 2022-01-09 â”† 40    â”‚
â”‚ 5   â”† 2022-01-05   â”† 2022-01-07 â”† 50    â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜

shape: (7, 1)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ date                â”‚
â”‚ ---                 â”‚
â”‚ datetime[Î¼s]        â”‚
â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ 2022-01-01 00:00:00 â”‚
â”‚ 2022-01-02 00:00:00 â”‚
â”‚ 2022-01-03 00:00:00 â”‚
â”‚ 2022-01-04 00:00:00 â”‚
â”‚ 2022-01-05 00:00:00 â”‚
â”‚ 2022-01-06 00:00:00 â”‚
â”‚ 2022-01-07 00:00:00 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

In this case, on 2022-01-01 the value would be 10. On 2022-01-02, it would be 10 + 20, and on 2022-01-03, it would be 10 + 20 + 30, and so on. In other words, I want to check if the date exists within the range of each row in the DataFrame (df), and if it does, sum the values.
I think the aproach for this is like this:
(
    dates.with_columns(
        pl.sum(
            pl.when(
                (df[&quot;Initial Date&quot;] &lt;= pl.col(&quot;date&quot;)) &amp; (df[&quot;Final Date&quot;] &gt;= pl.col(&quot;date&quot;))
            ).then(df[&quot;Value&quot;]).otherwise(0)
        ).alias(&quot;Summed Value&quot;)
    )
    
)

","update join_where() was added in Polars 1.7.0
(
    dates.join_where(
        df,
        pl.col(&quot;date&quot;) &gt;= pl.col(&quot;Initial Date&quot;),
        pl.col(&quot;date&quot;) &lt;= pl.col(&quot;Final Date&quot;),
    ).group_by(&quot;date&quot;)
    .agg(pl.col(&quot;Value&quot;).sum())
)

┌─────────────────────┬───────┐
│ date                ┆ Value │
│ ---                 ┆ ---   │
│ datetime[μs]        ┆ i64   │
╞═════════════════════╪═══════╡
│ 2022-01-01 00:00:00 ┆ 10    │
│ 2022-01-02 00:00:00 ┆ 30    │
│ 2022-01-03 00:00:00 ┆ 60    │
│ 2022-01-04 00:00:00 ┆ 90    │
│ 2022-01-05 00:00:00 ┆ 140   │
│ 2022-01-06 00:00:00 ┆ 140   │
│ 2022-01-07 00:00:00 ┆ 120   │
└─────────────────────┴───────┘

previous
If you just want to know sum of values on each date within ranges in df, you don't even need dates dataframe.

date_ranges() to create column with date ranges based on initial and final date.
explode() to convert date ranges into rows.
group_by() and agg() to sum the values.

(
    df
    .with_columns(date = pl.date_ranges(&quot;Initial Date&quot;, &quot;Final Date&quot;))
    .explode(&quot;date&quot;)
    .group_by(&quot;date&quot;, maintain_order = True)
    .agg(pl.col.Value.sum())
)

┌────────────┬───────┐
│ date       ┆ Value │
│ ---        ┆ ---   │
│ date       ┆ i64   │
╞════════════╪═══════╡
│ 2022-01-01 ┆ 10    │
│ 2022-01-02 ┆ 30    │
│ 2022-01-03 ┆ 60    │
│ 2022-01-04 ┆ 90    │
│ 2022-01-05 ┆ 140   │
│ 2022-01-06 ┆ 140   │
│ 2022-01-07 ┆ 120   │
│ 2022-01-08 ┆ 40    │
│ 2022-01-09 ┆ 40    │
└────────────┴───────┘

If you really want to use dates then you can join() result on dates dataframe:
(
    df
    .with_columns(date = pl.date_ranges(&quot;Initial Date&quot;, &quot;Final Date&quot;))
    .explode(&quot;date&quot;)
    .group_by(&quot;date&quot;, maintain_order = True)
    .agg(pl.col.Value.sum())
    .join(dates, on=&quot;date&quot;, how=&quot;semi&quot;)
)

┌────────────┬───────┐
│ date       ┆ Value │
│ ---        ┆ ---   │
│ date       ┆ i64   │
╞════════════╪═══════╡
│ 2022-01-01 ┆ 10    │
│ 2022-01-02 ┆ 30    │
│ 2022-01-03 ┆ 60    │
│ 2022-01-04 ┆ 90    │
│ 2022-01-05 ┆ 140   │
│ 2022-01-06 ┆ 140   │
│ 2022-01-07 ┆ 120   │
└────────────┴───────┘

Or just filter() the result:
(
    df
    .with_columns(date = pl.date_ranges(&quot;Initial Date&quot;, &quot;Final Date&quot;))
    .explode(&quot;date&quot;)
    .group_by(&quot;date&quot;, maintain_order = True)
    .agg(pl.col.Value.sum())
    .filter(pl.col.date.is_between(datetime(2022,1,1), datetime(2022,1,7)))
)

┌────────────┬───────┐
│ date       ┆ Value │
│ ---        ┆ ---   │
│ date       ┆ i64   │
╞════════════╪═══════╡
│ 2022-01-01 ┆ 10    │
│ 2022-01-02 ┆ 30    │
│ 2022-01-03 ┆ 60    │
│ 2022-01-04 ┆ 90    │
│ 2022-01-05 ┆ 140   │
│ 2022-01-06 ┆ 140   │
│ 2022-01-07 ┆ 120   │
└────────────┴───────┘

Alternative solution would be to use join on inequality, but polars is not great on it (yet). But in this case you can use DuckDB integration with Polars.
duckdb.sql(&quot;&quot;&quot;
    select
        d.date,
        sum(df.value) as value
    from df
        inner join dates as d on
            d.date between df.&quot;Initial Date&quot; and df.&quot;Final Date&quot;
    group by
        d.date
    order by
        d.date
&quot;&quot;&quot;).pl()

┌─────────────────────┬───────────────┐
│ date                ┆ value         │
│ ---                 ┆ ---           │
│ datetime[μs]        ┆ decimal[38,0] │
╞═════════════════════╪═══════════════╡
│ 2022-01-01 00:00:00 ┆ 10            │
│ 2022-01-02 00:00:00 ┆ 30            │
│ 2022-01-03 00:00:00 ┆ 60            │
│ 2022-01-04 00:00:00 ┆ 90            │
│ 2022-01-05 00:00:00 ┆ 140           │
│ 2022-01-06 00:00:00 ┆ 140           │
│ 2022-01-07 00:00:00 ┆ 120           │
└─────────────────────┴───────────────┘

"
"I don't understand how I should be using ArrayLike in my code. If check mypy, I keep getting errors when I try to use the variables for anything without calling cast. I am trying to define function signatures that work with ndarray as well as regular lists.
For example, the code below
import numpy.typing as npt
import numpy as np

from typing import Any

def f(a: npt.ArrayLike) -&gt; int:
    return len(a)

def g(a: npt.ArrayLike) -&gt; Any:
    return a[0]

print(f(np.array([0, 1])), g(np.array([0, 1])))
print(f([0, 1]), g([0, 1]))

give me theses errors for f() and g():
Argument 1 to &quot;len&quot; has incompatible type &quot;Union[_SupportsArray[dtype[Any]], _NestedSequence[_SupportsArray[dtype[Any]]], bool, int, float, complex, str, bytes, _NestedSequence[Union[bool, int, float, complex, str, bytes]]]&quot;; expected &quot;Sized&quot;  [arg-type]

Value of type &quot;Union[_SupportsArray[dtype[Any]], _NestedSequence[_SupportsArray[dtype[Any]]], bool, int, float, complex, str, bytes, _NestedSequence[Union[bool, int, float, complex, str, bytes]]]&quot; is not indexable  [index]

","The purpose of numpy.typing.ArrayLike is to be able to annotate

objects that can be coerced into an ndarray.

With that purpose in mind, they defined the type to be the following union:
Union[
    _SupportsArray[dtype[Any]],
    _NestedSequence[_SupportsArray[dtype[Any]]],
    bool,
    int,
    float,
    complex,
    str,
    bytes,
    _NestedSequence[Union[bool, int, float, complex, str, bytes]]
]

_SupportsArray is simply a protocol with an __array__ method. It neither requires __len__ (for use with the len function) nor __getitem__ (for indexing) to be implemented.
_NestedSequence is a more restrictive protocol that does actually require __len__ and __getitem__.
But the problem with this code is that the parameter annotation is that union:
import numpy.typing as npt

...

def f(a: npt.ArrayLike) -&gt; int:
    return len(a)

So a could be a sequence-like object that supports __len__, but it could also be just an object that supports __array__ and nothing else. It could even be just an int for example (see the union again). Therefore the call len(a) is unsafe.
Similarly, here the item access is not type safe because a might not implement __getitem__:
...

def g(a: npt.ArrayLike) -&gt; Any:
    return a[0]

So the reason it is not working for you is that it is not meant to be used as an annotation for things that are numpy arrays or other sequences; it is meant to be used for things that can be turned into numpy arrays.

If you want to annotate your functions f and g to take both lists and numpy arrays, you could just use a union of list and NDArray like list[Any] | npt.NDArray[Any].
If you want to have a wider annotation to accommodate any type that has __len__ and __getitem__, you need to define your own protocol:
from typing import Any, Protocol, TypeVar

import numpy as np

T = TypeVar(&quot;T&quot;, covariant=True)


class SequenceLike(Protocol[T]):
    def __len__(self) -&gt; int: ...
    def __getitem__(self, item: int) -&gt; T: ...


def f(a: SequenceLike[Any]) -&gt; int:
    return len(a)


def g(a: SequenceLike[T]) -&gt; T:
    return a[0]


print(f(np.array([0, 1])), g(np.array([0, 1])))
print(f([0, 1]), g([0, 1]))

To be more precise __getitem__ should probably also take slice objects, but the overloads may be overkill for you.
"
"I am trying to parse/scrape https://etherscan.io/tokens website using requests in Python but I get the following error:

etherscan.io
Checking if the site connection is secure
etherscan.io needs to review the security of your connection before
proceeding. Ray ID: 73b56fc71bc276ed Performance &amp; security by
Cloudflare

Now, I found a solution here: https://stackoverflow.com/a/62687390/4190159 but when I try to use this solution, I am still not being able to read the actual content of the website and getting a different error stated below.
My code as follows:
import requests
from collections import OrderedDict
from requests import Session
import socket

answers = socket.getaddrinfo('etherscan.io', 443)
(family, type, proto, canonname, (address, port)) = answers[0]
s = Session()
headers = OrderedDict({
    'Accept-Encoding': 'gzip, deflate, br',
    'Host': &quot;grimaldis.myguestaccount.com&quot;,
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0'
})
s.headers = headers
response = s.get(f&quot;https://{address}/tokens&quot;, headers=headers, verify=False).text
print(response)

Error for the above code as follows:

Traceback (most recent call last):   File
&quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py&quot;,
line 677, in urlopen
chunked=chunked,   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py&quot;,
line 381, in _make_request
self._validate_conn(conn)   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py&quot;,
line 976, in validate_conn
conn.connect()   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connection.py&quot;,
line 370, in connect
ssl_context=context,   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/util/ssl.py&quot;,
line 390, in ssl_wrap_socket
return context.wrap_socket(sock)   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py&quot;,
line 407, in wrap_socket
_context=self, _session=session)   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py&quot;,
line 814, in init
self.do_handshake()   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py&quot;,
line 1068, in do_handshake
self._sslobj.do_handshake()   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py&quot;,
line 689, in do_handshake
self._sslobj.do_handshake() ssl.SSLError: [SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure
(_ssl.c:833)
During handling of the above exception, another exception occurred:
Traceback (most recent call last):   File
&quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/adapters.py&quot;,
line 449, in send
timeout=timeout   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py&quot;,
line 725, in urlopen
method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]   File
&quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/util/retry.py&quot;,
line 439, in increment
raise MaxRetryError(_pool, url, error or ResponseError(cause)) urllib3.exceptions.MaxRetryError:
HTTPSConnectionPool(host='172.67.8.107', port=443): Max retries
exceeded with url: /tokens (Caused by SSLError(SSLError(1, '[SSL:
SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure
(_ssl.c:833)'),))
During handling of the above exception, another exception occurred:
Traceback (most recent call last):   File &quot;label_scrapper.py&quot;, line
16, in 
response = s.get(f&quot;https://{address}/tokens&quot;, headers=headers, verify=False).text   File
&quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/sessions.py&quot;,
line 543, in get
return self.request('GET', url, **kwargs)   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/sessions.py&quot;,
line 530, in request
resp = self.send(prep, **send_kwargs)   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/sessions.py&quot;,
line 643, in send
r = adapter.send(request, **kwargs)   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/adapters.py&quot;,
line 514, in send
raise SSLError(e, request=request) requests.exceptions.SSLError: HTTPSConnectionPool(host='172.67.8.107', port=443): Max retries
exceeded with url: /tokens (Caused by SSLError(SSLError(1, '[SSL:
SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure
(_ssl.c:833)'),)) Somdips-MacBook-Pro:Downloads somdipdey$ python3
label_scrapper.py  Traceback (most recent call last):   File
&quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py&quot;,
line 677, in urlopen
chunked=chunked,   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py&quot;,
line 381, in _make_request
self._validate_conn(conn)   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py&quot;,
line 976, in validate_conn
conn.connect()   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connection.py&quot;,
line 370, in connect
ssl_context=context,   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/util/ssl.py&quot;,
line 390, in ssl_wrap_socket
return context.wrap_socket(sock)   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py&quot;,
line 407, in wrap_socket
_context=self, _session=session)   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py&quot;,
line 814, in init
self.do_handshake()   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py&quot;,
line 1068, in do_handshake
self._sslobj.do_handshake()   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py&quot;,
line 689, in do_handshake
self._sslobj.do_handshake() ssl.SSLError: [SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure
(_ssl.c:833)
During handling of the above exception, another exception occurred:
Traceback (most recent call last):   File
&quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/adapters.py&quot;,
line 449, in send
timeout=timeout   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py&quot;,
line 725, in urlopen
method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]   File
&quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/util/retry.py&quot;,
line 439, in increment
raise MaxRetryError(_pool, url, error or ResponseError(cause)) urllib3.exceptions.MaxRetryError:
HTTPSConnectionPool(host='172.67.8.107', port=443): Max retries
exceeded with url: /tokens (Caused by SSLError(SSLError(1, '[SSL:
SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure
(_ssl.c:833)'),))
During handling of the above exception, another exception occurred:
Traceback (most recent call last):   File &quot;label_scrapper.py&quot;, line
16, in 
response = s.get(f&quot;https://{address}/tokens&quot;, headers=headers, verify=False).text   File
&quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/sessions.py&quot;,
line 543, in get
return self.request('GET', url, **kwargs) Somdips-MacBook-Pro:Downloads somdipdey$ python3 label_scrapper.py
Traceback (most recent call last):   File
&quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py&quot;,
line 677, in urlopen
chunked=chunked,   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py&quot;,
line 381, in _make_request
self._validate_conn(conn)   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py&quot;,
line 976, in validate_conn
conn.connect()   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connection.py&quot;,
line 370, in connect
ssl_context=context,   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/util/ssl.py&quot;,
line 390, in ssl_wrap_socket
return context.wrap_socket(sock)   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py&quot;,
line 407, in wrap_socket
_context=self, _session=session)   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py&quot;,
line 814, in init
self.do_handshake()   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py&quot;,
line 1068, in do_handshake
self._sslobj.do_handshake()   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py&quot;,
line 689, in do_handshake
self._sslobj.do_handshake() ssl.SSLError: [SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure
(_ssl.c:833)
During handling of the above exception, another exception occurred:
Traceback (most recent call last):   File
&quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/adapters.py&quot;,
line 449, in send
timeout=timeout   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py&quot;,
line 725, in urlopen
method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]   File
&quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/util/retry.py&quot;,
line 439, in increment
raise MaxRetryError(_pool, url, error or ResponseError(cause)) urllib3.exceptions.MaxRetryError:
HTTPSConnectionPool(host='172.67.8.107', port=443): Max retries
exceeded with url: /tokens (Caused by SSLError(SSLError(1, '[SSL:
SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure
(_ssl.c:833)'),))
During handling of the above exception, another exception occurred:
Traceback (most recent call last):   File &quot;label_scrapper.py&quot;, line
16, in 
response = s.get(f&quot;https://{address}/tokens&quot;, headers=headers, verify=False).text   File
&quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/sessions.py&quot;,
line 543, in get
return self.request('GET', url, **kwargs)   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/sessions.py&quot;,
line 530, in request
resp = self.send(prep, **send_kwargs)   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/sessions.py&quot;,
line 643, in send
r = adapter.send(request, **kwargs)   File &quot;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/adapters.py&quot;,
line 514, in send
raise SSLError(e, request=request) requests.exceptions.SSLError: HTTPSConnectionPool(host='172.67.8.107', port=443): Max retries
exceeded with url: /tokens (Caused by SSLError(SSLError(1, '[SSL:
SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure
(_ssl.c:833)'),))

How to resolve this?
","The website is under cloudflare protection. So you can use cloudscraper instead of requests to get rid of the protection. Now it's working fine.
Example:
from bs4 import BeautifulSoup
import cloudscraper
scraper = cloudscraper.create_scraper(delay=10,   browser={'custom': 'ScraperBot/1.0',})
url = 'https://etherscan.io/tokens'
req = scraper.get(url)

soup = BeautifulSoup(req.content,'lxml')
for tr in soup.select('table#tblResult tbody tr'):
    d= list(tr.stripped_strings)
    print(d)

Output:
['1', 'Tether USD (USDT)', 'Tether gives you the joint benefits of open blockchain technology and traditional currency by converting your cash into a stable digital currency equivalent.', '$1.005', '0.000042\xa0Btc', '0.000534\xa0Eth', '0.40%', '$56,223,000,912.00', '$67,594,315,221.00', '$40,022,235,444.23', '4,384,118', '0.003%']
['2', 'USD Coin (USDC)', 'USDC is a fully collateralized US Dollar stablecoin developed by CENTRE, the open source project with Circle being the first of several forthcoming issuers.', '$1.006', '0.000042\xa0Btc', '0.000534\xa0Eth', '0.44%', '$7,028,538,289.00', '$53,707,876,130.00', '$46,882,045,425.04', '1,470,173', '0.132%']
['3', 'BNB (BNB)', 'Binance aims to build a world-class crypto exchange, powering the future\nof crypto finance.', '$316.6558', '0.013290\xa0Btc', '0.168135\xa0Eth', '-0.87%', '$1,131,380,099.00', '$51,088,380,246.00', '$5,250,000,297.97', '322,395', '0.001%']
['4', 'Binance USD (BUSD)', 'Binance USD (BUSD) is a dollar-backed stablecoin issued and custodied by Paxos Trust Company, and regulated by the New York State Department of Financial Services. BUSD is available directly for sale 1:1 with USD on Paxos.com and will be listed for trading on Binance.', '$0.9976', '0.000042\xa0Btc', '0.000530\xa0Eth', '-0.30%', '$6,219,951,485.00', '$17,920,238,921.00', '$17,532,299,450.15', '125,632', '0.197%']
['5', 'HEX (HEX)', &quot;HEX.com averages 25% APY interest recently. HEX virtually lends value from stakers to non-stakers as staking reduces supply. The launch ends Nov. 19th, 2020 when HEX 
stakers get credited ~200B HEX. HEX's total supply is now ~350B. Audited 3 times, 2 security, and 1 economics.&quot;, '$0.0626', '0.000003\xa0Btc', '0.000033\xa0Eth', '-5.10%', '$22,598,794.00', '$10,856,229,132.00', '$36,158,058,204.94', '308,035', '-0.040%']
['6', 'SHIBA INU (SHIB)', 'SHIBA INU is a 100% decentralized community experiment with it claims that 1/2 the tokens have been sent to Vitalik and the other half were locked to a Uniswap pool and the keys burned.', '$0.00', '0.000000\xa0Btc', '0.000000\xa0Eth', '-9.20%', '$2,070,477,368.00', '$9,155,756,506.00', '$15,489,873,503.95', '1,206,115', '0.088%']
['7', 'stETH (stETH)', 'stETH is a token that represents staked ether in Lido, combining the 
value of initial deposit + staking rewards. stETH tokens are pegged 1:1 to the ETH staked with Lido and can be used as one would use ether, allowing users to earn Eth2 staking rewards whilst benefiting from Defi yields.', '$1,844.28', '0.077404\xa0Btc', '0.979260\xa0Eth', '-2.26%', '$3,408,944.00', '$7,909,446,933.00', '$3,418,574,006.52', '94,316', '0.215%']
['8', 'Matic Token (MATIC)', 'Matic Network brings massive scale to Ethereum using an adapted version of Plasma with PoS based side chains. Polygon is a well-structured, easy-to-use platform for Ethereum scaling and infrastructure development.', '$0.9397', '0.000039\xa0Btc', '0.000499\xa0Eth', '-6.39%', '$529,032,596.00', '$7,551,024,649.00', '$9,397,310,544.00', '466,641', '0.083%']
['9', 'Dai Stablecoin (DAI)', 'Multi-Collateral Dai, brings a lot of new and exciting features, such as support for new CDP collateral types and Dai Savings Rate.', '$1.005', '0.000042\xa0Btc', '0.000534\xa0Eth', '0.36%', '$635,956,564.00', '$6,800,555,162.00', '$9,848,650,590.65', '479,078', '-0.008%']
['10', 'Wrapped BTC (WBTC)', 'Wrapped Bitcoin (WBTC) is an ERC20 token backed 1:1 with Bitcoin.\nCompletely transparent. 100% verifiable. Community led.', '$23,983.00', '1.006567\xa0Btc', '12.734291\xa0Eth', '-1.26%', '$263,338,154.00', '$5,928,934,190.00', '$6,279,085,162.00', 
'51,459', '0.058%']

... so on
cloudscraper
"
"I have pandas DataFrame A. I am struggling transforming this into my desired format, see DataFrame B. I tried pivot or melt but I am not sure how I could make it conditional (string values to FIELD_STR_VALUE, numeric values to FIELD_NUM_VALUE). I was hoping you could point me the right direction.
A: Input DataFrame
|FIELD_A |FIELD_B |FIELD_C |FIELD_D |
|--------|--------|--------|--------|
|123123  |8       |a       |23423   |
|123124  |7       |c       |6464    |
|123144  |99      |x       |234     |

B: Desired output DataFrame
|ID |FIELD_A |FIELD_NAME |FIELD_STR_VALUE |FIELD_NUM_VALUE |
|---|--------|-----------|----------------|----------------|
|1  |123123  |B          |                |8               |
|2  |123123  |C          |a               |                |
|3  |123123  |D          |                |23423           |
|4  |123124  |B          |                |7               |
|5  |123124  |C          |c               |                |
|6  |123124  |D          |                |6464            |
|7  |123144  |B          |                |99              |
|8  |123144  |C          |x               |                |
|9  |123144  |D          |                |234             |

","You can use:
# dic = {np.int64: 'NUM', object: 'STR'}

(df.set_index('FIELD_A')
   .pipe(lambda d: d.set_axis(pd.MultiIndex.from_arrays(
          [d.columns, d.dtypes],
         # or for custom NAMES
         #[d.columns, d.dtypes.map(dic)],
                              names=['FIELD_NAME', None]),
                              axis=1)
        )
   .stack(0).add_prefix('FIELD_').add_suffix('_VALUE')
   .reset_index()
)

NB. if you really want STR/NUM, map those strings from the dtypes (see comments in code).
Output:
   FIELD_A FIELD_NAME  FIELD_int64_VALUE FIELD_object_VALUE
0   123123    FIELD_B                8.0                NaN
1   123123    FIELD_C                NaN                  a
2   123123    FIELD_D            23423.0                NaN
3   123124    FIELD_B                7.0                NaN
4   123124    FIELD_C                NaN                  c
5   123124    FIELD_D             6464.0                NaN
6   123144    FIELD_B               99.0                NaN
7   123144    FIELD_C                NaN                  x
8   123144    FIELD_D              234.0                NaN

"
"I'm fairly new to python packaging and I'm trying to create a command line tool so that I can send to client to interact with my service in AWS.
My goal is to have a command line tool to upload files that are in the folder resources to s3 that will later be used by other services.
It's my first time using setuptools for that but I'm seem to be lost at some point.
My project structure is something like:
ProjectRoot
â”œâ”€â”€ MANIFEST.in
â”œâ”€â”€ Pipfile
â”œâ”€â”€ Pipfile.lock
â”œâ”€â”€ dist
â”‚   â”œâ”€â”€ myscript-0.0.1.whl
â”‚   â””â”€â”€ myscript-0.0.1.tar.gz
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ resources
â”‚   â”œâ”€â”€ artifacts
â”‚   â”‚   â”œâ”€â”€ code1.jar
â”‚   â”‚   â”œâ”€â”€ code2.jar
â”‚   â”‚   â”œâ”€â”€ api.keys
â”‚   â”‚   â”œâ”€â”€ package1.tar.gz
â”‚   â”‚   â”œâ”€â”€ install-linux.sh
â”‚   â”‚   â””â”€â”€ confs.yaml
â”‚   â”œâ”€â”€ recipe.template.yaml
â””â”€â”€ src
    â””â”€â”€ code
        â”œâ”€â”€ __init__.py
        â””â”€â”€ myscript.py

I've tried to make setuptools add the files to the .tar package with the pyproject.toml with this:
[build-system]
requires = [&quot;setuptools&quot;]
build-backend = &quot;setuptools.build_meta&quot;

[project]
name = &quot;myscript&quot;
version = &quot;0.0.1&quot;
dependencies = [
    'Click',
    'boto3',
    'botocore',
]

[project.scripts]
myscript = &quot;code.main:run&quot;

[tool.setuptools]
include-package-data = true

[tool.setuptools.packages.find]
where = [&quot;src&quot;,&quot;resources&quot;] 
include = [&quot;code*&quot;]
exclude = [] 

[tool.setuptools.package-data]
&quot;resources.artifacts&quot; = [&quot;*&quot;]
recipe = [&quot;*.yaml&quot;]

After that I try to install the wheel generated file with pip install dist/generated_file.whl, but I can't find the resources/ folder anywhere during installation.
ps.: I also got a little lost if I need the whl and the tar package together.
I tried using relative paths to find the resources, but I saw they weren't installed in the sites_packages.
My latest try was using from importlib_resources import files but it also can't seem to find the resources.
I can't find the resources folder files.
","Starting point
With the given project structure
📁 &lt;project root&gt;/
├─📄 pyproject.toml
├─📁 src/
│ └─📁 code/
│   ├─📄 __init__.py
│   └─📄 myscript.py
├─📁 resources/
  └─📁 artifacts/
    └─📄 code1.jar

and by specifying
[tool.setuptools.packages.find]
where = [&quot;src&quot;,&quot;resources&quot;]
include = [&quot;code*&quot;]
exclude = []

[tool.setuptools.package-data]
&quot;resources.artifacts&quot; = [&quot;*&quot;]
recipe = [&quot;*.yaml&quot;]

you'll get a wheel with following contents
📁 myscript-0.0.1-py3-none-any/
└─📁 code/
  ├─📄 __init__.py
  └─📄 myscript.py


The reason for this is that the only package found is code. A package is a folder with python (.py) files, and usually with __init__.py file (if not talking about namespace packages which are a bit of a special thing).
What I would do?

First, renaming your main package folder. You've called your project myscript in pyproject (so you would install it with pip install myscript), but then the file structure would imply that the import name  is code; so you would need to do import code.myscript (code being the main package). I'll change the project name to be in this example myproj, by changing the name in pyproject.toml and ./code folder to ./myproj

Second, the name of the &quot;package-data&quot; to me says that it is data inside a package. The ./resources is not a package as it does not contain any python files. If you add there an empty __init__.py, it will become a package. But there is another problem in pyproject.toml: your package should be found under a folder called ./resources, but that is actually in root folder (.). Therefore, you should either change where = ['src', '.'] (which creates new problems) or move ./resources to ./resources/resources, but you could also do it easier (third point)

Third, you could simplify things by putting the data files inside you package (./myproj). That's fair more common practice, and also makes pip install the resources with your code, inside site-packages/myproj, which is nice (although, there are other possibilities). So, I propose these changes to pyproject.toml:


[project]
name = &quot;myproj&quot; # &lt;-- this changed
version = &quot;0.0.1&quot;

[tool.setuptools.packages.find]
where = [&quot;src&quot;] # &lt;-- this changed

[tool.setuptools.package-data]
&quot;*&quot; = [&quot;*.*&quot;] # &lt;-- this changed

and then the folder structure to
📁 &lt;project root&gt;/
├─📄 pyproject.toml
└─📁 src/
  └─📁 myproj/
    ├─📄 __init__.py
    ├─📁 __assets__/ # non-code files here
    │ └─📄 code1.jar     
    └─📄 myscript.py

That will then create a wheel with following folder structure:
└─📁 myproj-0.0.1-py3-none-any/
  └─📁 myproj/
    ├─📄 __init__.py
    ├─📁 __assets__/
    │ └─📄 code1.jar
    └─📄 myscript.py

"
"In polars I can get the horizontal max (maximum value of a set of columns for reach row) like this:
df = pl.DataFrame(
    {
        &quot;a&quot;: [1, 8, 3],
        &quot;b&quot;: [4, 5, None],
    }
)

df.with_columns(max = pl.max_horizontal(&quot;a&quot;, &quot;b&quot;))
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ a   â”† b    â”† max â”‚
â”‚ --- â”† ---  â”† --- â”‚
â”‚ i64 â”† i64  â”† i64 â”‚
â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•¡
â”‚ 1   â”† 4    â”† 4   â”‚
â”‚ 8   â”† 5    â”† 8   â”‚
â”‚ 3   â”† null â”† 3   â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

This corresponds to Pandas df[[&quot;a&quot;, &quot;b&quot;]].max(axis=1).
Now, how do I get the column names instead of the actual max value?
In other words, what is the Polars version of Pandas' df[CHANGE_COLS].idxmax(axis=1)?
The expected output would be:
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ a   â”† b    â”† max â”‚
â”‚ --- â”† ---  â”† --- â”‚
â”‚ i64 â”† i64  â”† str â”‚
â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•¡
â”‚ 1   â”† 4    â”† b   â”‚
â”‚ 8   â”† 5    â”† a   â”‚
â”‚ 3   â”† null â”† a   â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

","You can concatenate the elements into a list using pl.concat_list, get the index of the largest element using pl.Expr.list.arg_max, and replace the index with the column name using pl.Expr.replace.
mapping = {0: &quot;a&quot;, 1: &quot;b&quot;}
(
    df
    .with_columns(
        pl.concat_list([&quot;a&quot;, &quot;b&quot;]).list.arg_max().replace(mapping).alias(&quot;max_col&quot;)
    )
)

This can all be wrapped into a function to also handle the creation of the mapping dict.
def max_col(cols) -&gt; str:
    mapping = dict(enumerate(cols))
    return pl.concat_list(cols).list.arg_max().replace(mapping)

df.with_columns(max_col([&quot;a&quot;, &quot;b&quot;]).alias(&quot;max_col&quot;))

Output.
shape: (3, 3)
┌─────┬──────┬─────────┐
│ a   ┆ b    ┆ max_col │
│ --- ┆ ---  ┆ ---     │
│ i64 ┆ i64  ┆ str     │
╞═════╪══════╪═════════╡
│ 1   ┆ 4    ┆ b       │
│ 8   ┆ 5    ┆ a       │
│ 3   ┆ null ┆ a       │
└─────┴──────┴─────────┘

"
"Suppose I have a numpy array [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16], How do I take 4 elements every 8 elements). Here is the expected result:
a -&gt; [1,2,3,4, 9,10,11,12]
b -&gt; [5,6,7,8, 13,14,15,16]

My array has hundreds of elements. I went through the numpy array documentation but I never succeeded to perform this computation other then a loop which is very slow.
EDIT:
The array can have up to 3 interleave sub-array of 4 elements
4 elt sample0, 4 elt sample 1, 4 elt  sample2, 4 elt sample0, 4 elt sample 1, 4 elt sample2, 4 elt sample0, 4 elt sample 1, 4 elt sample2 ...

My array has 499875840 elements !
","For a generic and pure numpy approach, you could argsort then split:
N = 4 # number of consecutive elements
M = 2 # number of output arrays

idx = np.argsort(np.arange(len(arr))%(N*M)//N, kind='stable')
# array([ 0,  1,  2,  3,  8,  9, 10, 11,  4,  5,  6,  7, 12, 13, 14, 15])

a, b = np.split(arr[idx], M)

As a one liner:
out = np.split(arr[np.argsort(np.arange(len(arr))%(N*M)//N, kind='stable')], M)

Output:
# a / out[0]
array([ 1,  2,  3,  4,  9, 10, 11, 12])

# b / out[1]
array([ 5,  6,  7,  8, 13, 14, 15, 16])

Output with arr = np.arange(32) as input:
# a
array([ 0,  1,  2,  3,  8,  9, 10, 11, 16, 17, 18, 19, 24, 25, 26, 27])

# b
array([ 4,  5,  6,  7, 12, 13, 14, 15, 20, 21, 22, 23, 28, 29, 30, 31])

Output with arr = np.arange(32), N = 4, M = 4:
(array([ 0,  1,  2,  3, 16, 17, 18, 19]),
 array([ 4,  5,  6,  7, 20, 21, 22, 23]),
 array([ 8,  9, 10, 11, 24, 25, 26, 27]),
 array([12, 13, 14, 15, 28, 29, 30, 31])

timings
Paul's approach is faster than mine (but limited to 2 arrays as output).

generalization
A reshaping approach, as proposed by @hpaulj, can be generalized using:
N = 4 # number of consecutive elements
M = 3 # number of samples/output arrays

out = arr.reshape(-1, M, N).transpose(1, 0, 2).reshape(-1, arr.size//M)

# or to map to individual variables
a,b,c = arr.reshape(-1, M, N).transpose(1, 0, 2).reshape(-1, arr.size//M) 

@U13-Forward's approach only works when M = 2, if can however be generalized using a list comprehension:
N = 4 # number of consecutive elements
M = 3 # number of samples/output arrays

reshaped = arr.reshape(-1, N*M)
out = [reshaped[:, n*N:n*N+N].ravel() for n in range(M)]


"
"As an example, consider the following:
class FooMeta(type):
    def __len__(cls):
        return 9000


class GoodBar(metaclass=FooMeta):
    def __len__(self):
        return 9001


class BadBar(metaclass=FooMeta):
    @classmethod
    def __len__(cls):
        return 9002

len(GoodBar) -&gt; 9000
len(GoodBar()) -&gt; 9001
GoodBar.__len__() -&gt; TypeError (missing 1 required positional argument)
GoodBar().__len__() -&gt; 9001
len(BadBar) -&gt; 9000 (!!!)
len(BadBar()) -&gt; 9002
BadBar.__len__() -&gt; 9002
BadBar().__len__() -&gt; 9002

The issue being with len(BadBar) returning 9000 instead of 9002 which is the intended behaviour.
This behaviour is (somewhat) documented in Python Data Model - Special Method Lookup, but it doesn't mention anything about classmethods, and I don't really understand the interaction with the @classmethod decorator.
Aside from the obvious metaclass solution (ie, replace/extend FooMeta) is there a way to override or extend the metaclass function so that len(BadBar) -&gt; 9002?
Edit:
To clarify, in my specific use case I can't edit the metaclass, and I don't want to subclass it and/or make my own metaclass, unless it is the only possible way of doing this.
","The __len__ defined in the class will always be ignored when using len(...) for the class itself:  when executing its operators, and methods like &quot;hash&quot;, &quot;iter&quot;, &quot;len&quot; can be roughly said to have &quot;operator status&quot;, Python always retrieve the corresponding method from the class of the target,  by directly acessing the memory structure of the class. These dunder methods have &quot;physical&quot; slot in the memory layout for the class: if the method exists in the class of your instance (and in this case, the &quot;instances&quot; are the classes &quot;GoodBar&quot; and &quot;BadBar&quot;, instances of &quot;FooMeta&quot;), or one of its superclasses, it is called - otherwise the operator fails.
So, this is the reasoning that applies on len(GoodBar()): it will call the __len__ defined in GoodBar()'s class, and len(GoodBar) and len(BadBar) will call the __len__ defined in their class, FooMeta

I don't really understand the interaction with the @classmethod
decorator.

The &quot;classmethod&quot; decorator creates a special descriptor out of the decorated function, so that when it is retrieved, via &quot;getattr&quot; from the class it is bound too, Python creates a &quot;partial&quot; object with the &quot;cls&quot; argument already in place. Just as retrieving an ordinary method from an instance creates an object with &quot;self&quot; pre-bound:
Both things are carried through the &quot;descriptor&quot; protocol - which means, both an ordinary method and a classmethod are retrieved by calling its __get__ method. This method takes 3 parameters: &quot;self&quot;, the descriptor itself, &quot;instance&quot;, the instance its bound to, and &quot;owner&quot;: the class it is ound to. The thing is that for ordinary methods (functions), when the second (instance) parameter to __get__ is None, the function itself is returned. @classmethod wraps a function with an object with a different __get__: one that returns the equivalent to partial(method, cls), regardless of the second parameter to __get__.
In other words, this simple pure Python code replicates the working of the classmethod decorator:
class myclassmethod:
    def __init__(self, meth):
         self.meth = meth
    def __get__(self, instance, owner):
         return lambda *args, **kwargs: self.meth(owner, *args, **kwargs)

That is why you see the same behavior when calling a classmethod explicitly with klass.__get__() and klass().__get__(): the instance is ignored.
TL;DR: len(klass)  will always go through the metaclass slot, and klass.__len__() will retrieve __len__  via the getattr mechanism, and then bind the classmethod properly before calling it.

Aside from the obvious metaclass solution (ie, replace/extend FooMeta)
is there a way to override or extend the metaclass function so that
len(BadBar) -&gt; 9002?


(...)
To clarify, in my specific use case I can't edit the metaclass, and I
don't want to subclass it and/or make my own metaclass, unless it is
the only possible way of doing this.

There is no other way. len(BadBar) will always go through the metaclass __len__.
Extending the metaclass might not be all that painful, though.
It can be done with a simple call to type passing the new __len__ method:
In [13]: class BadBar(metaclass=type(&quot;&quot;, (FooMeta,), {&quot;__len__&quot;: lambda cls:9002})):
    ...:     pass
    

In [14]: len(BadBar)
Out[14]: 9002

Only if BadBar will later be combined in multiple inheritance with another class hierarchy with a different custom metaclass you will have to worry. Even if there are other classes that have FooMeta as metaclass, the snippet above will work: the dynamically created metaclass will be the metaclass for the new subclass, as the &quot;most derived subclass&quot;.
If however, there is a hierarchy of subclasses and they have differing metaclasses, even if created by this method, you will have to combine both metaclasses in a common subclass_of_the_metaclasses before creating the new  &quot;ordinary&quot; subclass.
If that is the case, note that you can have one single paramtrizable metaclass, extending your original one (can't dodge that, though)
class SubMeta(FooMeta):
    def __new__(mcls, name, bases, ns, *,class_len):
         cls = super().__new__(mcls, name, bases, ns)
         cls._class_len = class_len
         return cls

    def __len__(cls):
        return cls._class_len if hasattr(cls, &quot;_class_len&quot;) else super().__len__()

And:

In [19]: class Foo2(metaclass=SubMeta, class_len=9002): pass

In [20]: len(Foo2)
Out[20]: 9002


"
"I need to delete duplicated rows based on combination of two columns (person1 and person2 columns) which have strings.
For example person1: ryan and person2: delta or person 1: delta and person2: ryan is same and provides the same value in messages column. Need to drop one of these two rows. Return the non duplicated rows as well.
Code to recreate df 
df = pd.DataFrame({&quot;&quot;: [0,1,2,3,4,5,6],
                     &quot;person1&quot;: [&quot;ryan&quot;, &quot;delta&quot;, &quot;delta&quot;, &quot;delta&quot;,&quot;bravo&quot;,&quot;alpha&quot;,&quot;ryan&quot;], 
                     &quot;person2&quot;: [&quot;delta&quot;, &quot;ryan&quot;, &quot;alpha&quot;, &quot;bravo&quot;,&quot;delta&quot;,&quot;ryan&quot;,&quot;alpha&quot;], 
                     &quot;messages&quot;: [1, 1, 2, 3,3,9,9]})

 df
        person1 person2 messages
0   0   ryan    delta   1
1   1   delta   ryan    1
2   2   delta   alpha   2
3   3   delta   bravo   3
4   4   bravo   delta   3
5   5   alpha   ryan    9
6   6   ryan    alpha   9

Answer df should be:
 finaldf
        person1 person2 messages
0   0   ryan    delta   1
1   2   delta   alpha   2
2   3   delta   bravo   3
3   5   alpha   ryan    9

","Try as follows:
res = (df[~df.filter(like='person').apply(frozenset, axis=1).duplicated()]
       .reset_index(drop=True))

print(res)

     person1 person2  messages
0  0    ryan   delta         1
1  2   delta   alpha         2
2  3   delta   bravo         3
3  5   alpha    ryan         9

Explanation

First, we use df.filter to select just the columns with person*.
For these columns only we use df.apply to turn each row (axis=1) into a frozenset. So, at this stage, we are looking at a pd.Series like this:

0     (ryan, delta)
1     (ryan, delta)
2    (alpha, delta)
3    (bravo, delta)
4    (bravo, delta)
5     (alpha, ryan)
6     (alpha, ryan)
dtype: object


Now, we want to select the duplicate rows, using Series.duplicated and add ~ as a prefix to the resulting boolean series to select the inverse from the original df.
Finally, we reset the index with df.reset_index.

"
"I would like to create a DataFrame that has an &quot;index&quot; (integer) from a number of (sparse) Series, where the index (or primary key) is NOT necessarily consecutive integers. Each Series is like a vector of (index, value) tuple or {index: value} mapping.
(1) A small example
In Pandas, this is very easy as we can create a DataFrame at a time, like
&gt;&gt;&gt; pd.DataFrame({
   &quot;A&quot;: {0:  'a', 20: 'b', 40: 'c'},
   &quot;B&quot;: {10: 'd', 20: 'e', 30: 'f'},
   &quot;C&quot;: {20: 'g', 30: 'h'},
}).sort_index()

      A    B    C
0     a  NaN  NaN
10  NaN    d  NaN
20    b    e    g
30  NaN    f    h
40    c  NaN  NaN

but I can't find an easy way to achieve a similar result with Polars. As described in Coming from Pandas, Polars does not use an index unlike Pandas, and each row is indexed by its integer position in the table; so I might need to represent an &quot;indexed&quot; Series with a 2-column DataFrame:
A = pl.DataFrame({ &quot;index&quot;: [0, 20, 40], &quot;A&quot;: ['a', 'b', 'c'] })
B = pl.DataFrame({ &quot;index&quot;: [10, 20, 30], &quot;B&quot;: ['d', 'e', 'f'] })
C = pl.DataFrame({ &quot;index&quot;: [20, 30], &quot;C&quot;: ['g', 'h'] })

I tried to combine these multiple DataFrames, joining on the index column:
&gt;&gt;&gt; A.join(B, on='index', how='full', coalesce=True).join(C, on='index', how='full', coalesce=True).sort(by='index')

shape: (5, 4)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ index â”† A    â”† B    â”† C    â”‚
â”‚ ---   â”† ---  â”† ---  â”† ---  â”‚
â”‚ i64   â”† str  â”† str  â”† str  â”‚
â•žâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•¡
â”‚ 0     â”† a    â”† null â”† null â”‚
â”‚ 10    â”† null â”† d    â”† null â”‚
â”‚ 20    â”† b    â”† e    â”† g    â”‚
â”‚ 30    â”† null â”† f    â”† h    â”‚
â”‚ 40    â”† c    â”† null â”† null â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜

This gives the result I want, but I wonder:

(i) if there is there more concise way to do this over many columns, and
(ii) how make this operation as efficient as possible.

Alternatives?
I also tried outer joins as this is one way to combine Dataframes with different number of columns and rows, as described above.
Other alternatives I tried includes diagonal concatenation, but this does not deduplicate or join on index:
&gt;&gt;&gt; pl.concat([A, B, C], how='diagonal')

   index     A     B     C
0      0     a  None  None
1     20     b  None  None
2     40     c  None  None
3     10  None     d  None
4     20  None     e  None
5     30  None     f  None
6     20  None  None     g
7     30  None  None     h

(2) Efficiently Building a Large Table
The approach I found above gives desired results I'd want but I feel there must be a better way in terms of performance. Consider a case with more large tables; say 300,000 rows and 20 columns:
N, C = 300000, 20
pls = []
pds = []

for i in range(C):
    A = pl.DataFrame({
        &quot;index&quot;: np.linspace(i, N*3-i, num=N, dtype=np.int32),
        f&quot;A{i}&quot;: np.arange(N, dtype=np.float32),
    })
    pls.append(A)
    
    B = A.to_pandas().set_index(&quot;index&quot;)
    pds.append(B)

The approach of joining two columns in a row is somewhat slow than I expected:
%%time
F = functools.reduce(lambda a, b: a.join(b, on='index', how='full', coalesce=True), pls)
F.sort(by='index')

CPU times: user 1.49 s, sys: 97.8 ms, total: 1.59 s
Wall time: 611 ms

or than one-pass creation in pd.DataFrame:
%%time
pd.DataFrame({
    f&quot;A{i}&quot;: pds[i][f'A{i}'] for i in range(C)
}).sort_index()

CPU times: user 230 ms, sys: 50.7 ms, total: 281 ms
Wall time: 281 ms

","Following your example, but only informing polars on the fact that the &quot;index&quot; column is sorted (polars will use fast paths if data is sorted).
You can use align_frames together with functools.reduce to get what you want.
This is your data creation snippet:
import functools
import polars as pl

N, C = 300000, 20
pls = []
pds = []

for i in range(C):
    A = pl.DataFrame({
        &quot;index&quot;: np.linspace(i, N*3-i, num=N, dtype=np.int32),
        f&quot;A{i}&quot;: np.arange(N, dtype=np.float32),
    }).with_columns(pl.col(&quot;index&quot;).set_sorted())
    
    pls.append(A)
    
    B = A.to_pandas().set_index(&quot;index&quot;)
    pds.append(B)

Creating the frame aligned by index. We need to use functools.reduce because align_frames returns a list of new DataFrame objects that are aligned by index.
frames = pl.align_frames(*pls, on=&quot;index&quot;)
functools.reduce(lambda a, b: a.with_columns(b.get_columns()), frames)

Performance
The performance is better than the pandas sort_index method.
Pandas
&gt;&gt;&gt; %%timeit
&gt;&gt;&gt; pd.DataFrame({
...     f&quot;A{i}&quot;: pds[i][f'A{i}'] for i in range(C)
... }).sort_index()
389 ms ± 8.96 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

Polars
&gt;&gt;&gt; %%timeit
&gt;&gt;&gt; frames = pl.align_frames(*pls, on=&quot;index&quot;)
&gt;&gt;&gt; functools.reduce(lambda a, b: a.with_columns(b.get_columns()), frames)
348 ms ± 11.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

"
"We are using the PyPI repos built into our gitlab deployment to share our internal packages with multiple internal projects. When we build our docker images we need to install those packages as part of image creation. However the gitlab CI token that we use to get access to the gitlab PyPI repository is a one-off token, and so is different every time we run the build.
Our Dockerfile starts something like this:
FROM python:3.9

WORKDIR /project

COPY poetry.lock pyproject.toml
RUN pip install poetry

ARG CI_JOB_TOKEN
RUN poetry config http-basic.gitlab-pypi-repo gitlab-ci-token ${CI_JOB_TOKEN}
RUN poetry install --no-interaction

Now because we're using poetry and the versions are locked in poetry.lock, when we get to the poetry steps we shouldn't need to reinstall poetry unless the poetry.lock file has changed, but because the CI_JOB_TOKEN is always different we always miss the cache and have to rebuild poetry and everything downstream (which is actually where most of the work is) as well.
So is there a way that we can pass CI_JOB_TOKEN into the docker build but in a way that is ignored for the purposes of the cache? Or maybe there's another way to achieve this?
","Use build secrets instead (requires build kit)
You can mount the secret at build time using the --mount argument to the RUN instruction. Suppose you have the following in a dockerfile:
# ...
RUN --mount=type=secret,id=mysecret echo &quot;$(cat /run/secrets/mysecret)&quot; &gt; .foo
RUN echo &quot;another layer&quot; &gt; .bar

Then you can pass the secret into the build using the --secret flag.
On the first run, you'll see the RUN instruction executed and if you were to inspect the .foo file, it would contain the secret (because we echoed it to the file in the RUN command -- in practice, this might be your poetry configuration, for example).
$ echo -n supersecret &gt; ../secret.txt
$ docker build --secret id=mysecret,src=../secret.txt -t test .
# ...
 =&gt; [3/4] RUN --mount=type=secret,id=mysecret echo &quot;$(cat /run/secrets/mysecret)&quot; &gt; .foo                                                0.2s
 =&gt; [4/4] RUN echo &quot;another layer&quot; &gt; .bar                                                                                                         0.4s
# ...

Even if your secret changes, on subsequent runs, you'll see the relevant layers still remain cached:
$ echo -n newvalue &gt; ../secret.txt
$ docker build --secret id=mysecret,src=../secret.txt -t test .
# ...
 =&gt; CACHED [3/4] RUN --mount=type=secret,id=mysecret echo &quot;$(cat /run/secrets/mysecret)&quot; &gt; .foo                                         0.0s
 =&gt; CACHED [4/4] RUN echo &quot;another layer&quot; &gt; .bar                                                                                                  0.0s
# ...

Of course, because the RUN instruction was cached, you would see the old secret value in .foo in the resulting build.

As a separate note, you should be aware that your poetry config command is writing to disk. This means that your secret will be contained in the resulting image layers, which may not be ideal from a security standpoint.
"
"I want to create BigInteger Identity column in SQLAlchemy ORM. Documentation does not have any example of either ORM Identity or BigInteger Identity.

Is this possible at all? I don't see any parameter for Identity type that would allow specifying inner integer type
How to do this? Do I have to create custom type and pass it inside Mapping[] brackets?

","This seems to work:
import sqlalchemy as sa    
from sqlalchemy.orm import mapped_column, Mapped, DeclarativeBase    
    
      
class Base(DeclarativeBase):    
    pass    
    
      
class Test(Base):    
    __tablename__ = 't75312537'    
    
    id: Mapped[int] = mapped_column(    
        sa.BigInteger, sa.Identity(), primary_key=True    
    )    
    
    
engine = sa.create_engine('postgresql+psycopg2:///test', echo=True)    
Base.metadata.drop_all(engine, checkfirst=True)    
Base.metadata.create_all(engine)

Output:
CREATE TABLE t75312537 (
    id BIGINT GENERATED BY DEFAULT AS IDENTITY, 
    PRIMARY KEY (id)
)

See the docs at Identity Columns (GENERATED {ALWAYS | BY DEFAULT} AS IDENTITY).
"
"In looking at the guide What do blueprints offer that just importing doesn't?
Here are some points that are unclear:

It says to have a file called http_blueprint.py in which you'd define some routes but it just looks like the regular http trigger but the decorator is a bp.route instead of an app.route.  Are these also app.functions since the main file has 2 decorators per def?

Does everything in the blueprint have to be an http trigger or is that just an example that they used?

Can you have multiple blueprint files or are we limited to the single one?


","Example of repo structure.
project
│   file001.py
│   file002.py
│   function_app.py
│   README.md
│   host.json
│   local.settings.json

file001.py:
import azure.function as func
import json
    
bp01 = func.Blueprint()
@bp01.route(route=&quot;route01&quot;)
def method01(req:func.HttpRequest) -&gt; func.HttpRequest:
    return func.HttpResponse (
        json.dumps({
        'version': 1
        })
    )

file002.py:
import azure.function as func
import json
    
bp02 = func.Blueprint()
@bp02.route(route=&quot;route02&quot;)
def method02(req:func.HttpRequest) -&gt; func.HttpRequest:
    return func.HttpResponse (
        json.dumps({
        'version': 2
        })
    )

This is how you register more than one blueprint in function_app.py:
import azure.functions as func 
from file001.py import bp01
from file002.py import bp02

app = func.FunctionApp(http_auth_level=func.AuthLevel.ANONYMOUS) 

app.register_functions(bp01)
app.register_functions(bp01)


My guess blueprints are useful when you want separation of concerns but I don't know for certain.
The most confusing part to me was that the main methods from the two blueprints must have a unique name otherwise the azure function will get confused and not show neither.
"
"I'm facing an issue which my dag cannot be imported, but cannot figure out why:
from airflow.sensors.sql import SqlSensor
import pendulum
from airflow.decorators import task,dag

@dag(
dag_id = &quot;database_monitor&quot;,
schedule_interval = '*/10 * * * *',
start_date=pendulum.datetime(2023, 7, 16, 21,0,tz=&quot;UTC&quot;),
catchup=False,)
def Pipeline():

    check_db_alive = SqlSensor(
        task_id=&quot;check_db_alive&quot;,
        conn_id=&quot;evergreen&quot;,
        sql=&quot;SELECT pg_is_in_recovery()&quot;,
        success= lambda x: x == False,
        poke_interval= 60,
        #timeout = 60 * 2,
        mode = &quot;reschedule&quot;,
    )


    @task()
    def alert_of_db_inrecovery():
        import requests
        # result = f&quot;Former primary instance is in recovery, task_instance_key_str: {kwargs['task_instance_key_str']}&quot;

        data = {&quot;@key&quot;:&quot;kkll&quot;,
                &quot;@version&quot; : &quot;alertapi-0.1&quot;,
                &quot;@type&quot;:&quot;ALERT&quot;,
                &quot;object&quot; : &quot;Testobject&quot;,
                &quot;severity&quot; : &quot;MINOR&quot;,
                &quot;text&quot; : str(&quot;Former primary instance is in recovery&quot;)
            }
        requests.post('https://httpevents.systems/api/sendAlert',verify=False,data=data)


    check_db_alive &gt;&gt; alert_of_db_inrecovery


dag = Pipeline()

I get this error:

AttributeError: '_TaskDecorator' object has no attribute 'update_relative'

","You need to call the Python task flow operator i.e
change check_db_alive &gt;&gt; alert_of_db_inrecovery to check_db_alive &gt;&gt; alert_of_db_inrecovery()
check correct code
from airflow.sensors.sql import SqlSensor
import pendulum
from airflow.decorators import task, dag


@dag(
    dag_id=&quot;database_monitor&quot;,
    schedule_interval='*/10 * * * *',
    start_date=pendulum.datetime(2023, 7, 16, 21, 0, tz=&quot;UTC&quot;),
    catchup=False,
)
def Pipeline():
    check_db_alive = SqlSensor(
        task_id=&quot;check_db_alive&quot;,
        conn_id=&quot;evergreen&quot;,
        sql=&quot;SELECT pg_is_in_recovery()&quot;,
        success=lambda x: x == False,
        poke_interval=60,
        # timeout = 60 * 2,
        mode=&quot;reschedule&quot;,
    )

    @task
    def alert_of_db_inrecovery():
        import requests
        # result = f&quot;Former primary instance is in recovery, task_instance_key_str: {kwargs['task_instance_key_str']}&quot;

        data = {&quot;@key&quot;: &quot;kkll&quot;,
            &quot;@version&quot;: &quot;alertapi-0.1&quot;,
            &quot;@type&quot;: &quot;ALERT&quot;,
            &quot;object&quot;: &quot;Testobject&quot;,
            &quot;severity&quot;: &quot;MINOR&quot;,
            &quot;text&quot;: str(&quot;Former primary instance is in recovery&quot;)
            }
        requests.post('https://httpevents.systems/api/sendAlert', verify=False, data=data)

    check_db_alive &gt;&gt; alert_of_db_inrecovery()


dag = Pipeline()

Ref: https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html
"
"I want to fill the null values of a column with the content of another column of the same row in a lazy data frame in Polars.
Is this possible with reasonable performance?
","There's a function for this: fill_null.
Let's say we have this data:
import polars as pl

df = pl.DataFrame({'a': [1, None, 3, 4],
                   'b': [10, 20, 30, 40]
                   }).lazy()
print(df.collect())

shape: (4, 2)
┌──────┬─────┐
│ a    ┆ b   │
│ ---  ┆ --- │
│ i64  ┆ i64 │
╞══════╪═════╡
│ 1    ┆ 10  │
│ null ┆ 20  │
│ 3    ┆ 30  │
│ 4    ┆ 40  │
└──────┴─────┘

We can fill the null values in column a with values in column b:
df.with_columns(pl.col('a').fill_null(pl.col('b'))).collect()

shape: (4, 2)
┌─────┬─────┐
│ a   ┆ b   │
│ --- ┆ --- │
│ i64 ┆ i64 │
╞═════╪═════╡
│ 1   ┆ 10  │
│ 20  ┆ 20  │
│ 3   ┆ 30  │
│ 4   ┆ 40  │
└─────┴─────┘

The performance of this will be quite good.
"
"I have an array of datetimes that I need to convert to a list of datetimes. My array looks like this:
import numpy as np

my_array = np.array(['2017-06-28T22:47:51.213500000', '2017-06-28T22:48:37.570900000',
                     '2017-06-28T22:49:46.736800000', '2017-06-28T22:50:41.866800000',
                     '2017-06-28T22:51:17.024100000', '2017-06-28T22:51:24.038300000'], dtype='datetime64[ns]')

my_list = my_array.tolist()

I need a list of datetime values, but when I do my_array.tolist(), I get a list of numerical time stamps:
[1498690071213500000,
 1498690117570900000,
 1498690186736800000,
 1498690241866800000,
 1498690277024100000,
 1498690284038300000]

My question is how do I preserve the datetime format when going from an array to a list, or how do I convert the list of time stamps to a list datetime values?
","Explicitly casting the numpy.ndarray as a native Python list will preserve the contents as numpy.datetime64 objects:
&gt;&gt;&gt; list(my_array)
[numpy.datetime64('2017-06-28T22:47:51.213500000'),
 numpy.datetime64('2017-06-28T22:48:37.570900000'),
 numpy.datetime64('2017-06-28T22:49:46.736800000'),
 numpy.datetime64('2017-06-28T22:50:41.866800000'),
 numpy.datetime64('2017-06-28T22:51:17.024100000'),
 numpy.datetime64('2017-06-28T22:51:24.038300000')]

However, if you wanted to go back from an integer timestamp to a numpy.datetime64 object, the number given here by numpy.ndarray.tolist is given in nanosecond format, so you could also use a list comprehension like the following:
&gt;&gt;&gt; [np.datetime64(x, &quot;ns&quot;) for x in my_list]
[numpy.datetime64('2017-06-28T22:47:51.213500000'),
 numpy.datetime64('2017-06-28T22:48:37.570900000'),
 numpy.datetime64('2017-06-28T22:49:46.736800000'),
 numpy.datetime64('2017-06-28T22:50:41.866800000'),
 numpy.datetime64('2017-06-28T22:51:17.024100000'),
 numpy.datetime64('2017-06-28T22:51:24.038300000')]

And if you want the final result as a Python datetime.datetime object instead of a numpy.datetime64 object, you can use a method like this (adjusted as needed for locality):
&gt;&gt;&gt; from datetime import datetime
&gt;&gt;&gt; list(map(datetime.utcfromtimestamp, my_array.astype(np.uint64) / 1e9))
[datetime.datetime(2017, 6, 28, 22, 47, 51, 213500),
 datetime.datetime(2017, 6, 28, 22, 48, 37, 570900),
 datetime.datetime(2017, 6, 28, 22, 49, 46, 736800),
 datetime.datetime(2017, 6, 28, 22, 50, 41, 866800),
 datetime.datetime(2017, 6, 28, 22, 51, 17, 24100),
 datetime.datetime(2017, 6, 28, 22, 51, 24, 38300)]

Edit: Warren Weckesser's answer provides a more straightforward approach to go from a numpy.datetime64[ns] array to a list of Python datetime.datetime objects than is described here.
"
"I really like the factory boy style of generated factories that can handle things like sequences, complex relationships etc.
For a FastAPI app with fully async database access using factory boy seems likely problematic. There is dated discussion here and an old PR to add async support that seems stuck.
Is there a good solution for these kinds of fixtures that has full async support?
","I haven't seen further progress from factory boy on this issue, but ultimately implemented a solution using pytest fixtures as factories that is working well for me.
The core idea is to build fixtures that return a factory method that can be used in tests. Here is a concrete example that generates users:
@pytest.fixture(scope=&quot;function&quot;)
def user_factory(db: Session):
    &quot;&quot;&quot;A factory for creating users&quot;&quot;&quot;
    last_user = 0

    def create_user(
        email=None, name=None, role=None, org: Organization | None = None
    ) -&gt; User:
        &quot;&quot;&quot;Return a new user, optionally with role for an existing organization&quot;&quot;&quot;
        nonlocal last_user
        last_user += 1
        email = email or f&quot;user{last_user}@example.com&quot;
        name = name or f&quot;User {last_user}&quot;
        user = User(email=email, name=name, auth_id=auth_id)
        db.add(user)

        if role:
            role = OrganizationRole(user_id=user_id, organization_id=org.id, role=role)
            db.add(role)

        db.commit()
        db.refresh(user)
        return user

    return create_user

# use in test 
def test_something(user_factory) -&gt; None:
  user = user_factory()
  # ...

I picked an example with SQLAlchemy ORM for simplicity of demonstrating the concept but you can persist however you like, pull in other factories as dependencies, etc.
There is a pretty good discussion of how to approach this in this article about using factories as fixtures.
This article also has good ideas for how to address sequences, constraints and a lot of the things I would use factory boy for by just using simple python.
"
"Poetry has the version command to increment a package version. Does uv package manager has anything similar?
","Currently uv package manager does not have a built-in command to bump package versions like poetry's version command. You can manually edit pyproject.toml or automate it with a script.
For example:
import toml
from typing import Literal

def bump_version(file_path: str, part: Literal[&quot;major&quot;, &quot;minor&quot;, &quot;patch&quot;] = &quot;patch&quot;) -&gt; None:
    with open(file_path, &quot;r&quot;) as f:
        pyproject = toml.load(f)

    version = pyproject[&quot;tool&quot;][&quot;poetry&quot;][&quot;version&quot;]
    major, minor, patch = map(int, version.split(&quot;.&quot;))

    if part == &quot;major&quot;:
        major += 1
        minor = 0
        patch = 0
    elif part == &quot;minor&quot;:
        minor += 1
        patch = 0
    elif part == &quot;patch&quot;:
        patch += 1
    else:
        raise ValueError(&quot;Invalid part value. Choose 'major', 'minor', or 'patch'.&quot;)

    pyproject[&quot;tool&quot;][&quot;poetry&quot;][&quot;version&quot;] = f&quot;{major}.{minor}.{patch}&quot;

    with open(file_path, &quot;w&quot;) as f:
        toml.dump(pyproject, f)

    print(f&quot;Version bumped to {major}.{minor}.{patch}&quot;)

"
"I am using joblib to run four processes on four cores in parallel. I would like to see the progress of the four processes separately on different lines.  However, what I see is the progress being written on top of each other to the same line until the first process finishes.
from math import factorial
from decimal import Decimal, getcontext
from joblib import Parallel, delayed
from tqdm import trange
import time

def calc(n_digits):
    # number of iterations
    n = int(n_digits+1/14.181647462725477)
    n = n if n &gt;= 1 else 1

    # set the number of digits for our numbers
    getcontext().prec = n_digits+1

    t    = Decimal(0)
    pi   = Decimal(0)
    deno = Decimal(0)

    for k in trange(n):
        t = ((-1)**k)*(factorial(6*k))*(13591409+545140134*k)
        deno = factorial(3*k)*(factorial(k)**3)*(640320**(3*k))
        pi += Decimal(t)/Decimal(deno)

    pi = pi * Decimal(12) / Decimal(640320 ** Decimal(1.5))
    pi = 1/pi
    
    # no need to round
    return pi


def parallel_with_joblib():
    # Define the number of cores to use
    n_cores = 4

    # Define the tasks (e.g., compute first 100, 200, 300, 400 digits of pi)
    tasks = [1200, 1700, 900, 1400]


    # Run tasks in parallel
    results = Parallel(n_jobs=n_cores)(delayed(calc)(n) for n in tasks)


if __name__ == &quot;__main__&quot;:
    parallel_with_joblib()

I would also like the four lines to be labelled &quot;Job 1 of 4&quot;, &quot;Job 2 of 4&quot; etc.

Following the method of @Swifty and changing the number of cores to 3 and the number of tasks to 7 and changing leave=False to leave=True I have this code:
from math import factorial
from decimal import Decimal, getcontext
from joblib import Parallel, delayed
from tqdm import trange
import time


def calc(n_digits, pos, total):
    # number of iterations
    n = int(n_digits + 1 / 14.181647462725477)
    n = n if n &gt;= 1 else 1

    # set the number of digits for our numbers
    getcontext().prec = n_digits + 1

    t = Decimal(0)
    pi = Decimal(0)
    deno = Decimal(0)

    for k in trange(n, position=pos, desc=f&quot;Job {pos + 1} of {total}&quot;, leave=True):
        t = ((-1) ** k) * (factorial(6 * k)) * (13591409 + 545140134 * k)
        deno = factorial(3 * k) * (factorial(k) ** 3) * (640320 ** (3 * k))
        pi += Decimal(t) / Decimal(deno)

    pi = pi * Decimal(12) / Decimal(640320 ** Decimal(1.5))
    pi = 1 / pi

    # no need to round
    return pi


def parallel_with_joblib():
    # Define the number of cores to use
    n_cores = 3

    # Define the tasks (e.g., compute first 100, 200, 300, 400 digits of pi)
    tasks =  [1200, 1700, 900, 1400, 800, 600, 500]

    # Run tasks in parallel
    results = Parallel(n_jobs=n_cores)(delayed(calc)(n, pos, len(tasks)) for (pos, n) in enumerate(tasks))


if __name__ == &quot;__main__&quot;:
    parallel_with_joblib()

I have change it to leave=True as I don't want the blank lines that appear otherwise.
This however gives me:

and then at the end it creates even more mess:

How can this be fixed?
","My idea was to create all the task bars in the main process and to create a single multiprocessing queue that each pool process would have access to. Then when calc completed an iteration it would place on the queue an integer representing its corresponding task bar. The main process would continue to get these integers from the queue and update the correct task bar. Each calc instance would place a sentinel value on the queue telling the main process that it had no more updates to enqueue.
With a multiprocessing.pool.Pool instance we can use a &quot;pool initializer&quot; function to initialize a global variable queue in each pool process, which will be accessed by calc. Unfortunately, joblib provides no authorized equivalent pool initializer. I tried various workarounds mentioned on the web, but none worked. So if you can live with not using joblib, then try this:
from math import factorial
from decimal import Decimal, getcontext
from multiprocessing import Pool, Queue
from tqdm import tqdm
import time

def init_pool(_queue):
    global queue

    queue = _queue

def calc(n_digits, pos):
    # number of iterations
    n = int(n_digits + 1 / 14.181647462725477)
    n = n if n &gt;= 1 else 1

    # set the number of digits for our numbers
    getcontext().prec = n_digits + 1

    t = Decimal(0)
    pi = Decimal(0)
    deno = Decimal(0)

    for k in range(n):
        t = ((-1) ** k) * (factorial(6 * k)) * (13591409 + 545140134 * k)
        deno = factorial(3 * k) * (factorial(k) ** 3) * (640320 ** (3 * k))
        pi += Decimal(t) / Decimal(deno)
        # Tell the main process to update the appropriate bar:
        queue.put(pos)

    pi = pi * Decimal(12) / Decimal(640320 ** Decimal(1.5))
    pi = 1 / pi

    # no need to round
    queue.put(None)  # Let updater know we have no more updates
    return pi

def parallel_with_pool():
    # Define the number of cores to use
    n_cores = 4

    # Define the tasks (e.g., compute first 100, 200, 300, 400 digits of pi)
    tasks =  [1200, 1700, 900, 1400] # Edit to make code for longer
    n_tasks = len(tasks)

    queue = Queue()

    LEAVE_PROGRESS_BAR = False

    # Create the bars:
    pbars = [
        tqdm(total=tasks[idx],
             position=idx,
             desc=f&quot;Job {idx + 1} of {n_tasks}&quot;,
             leave=LEAVE_PROGRESS_BAR
             )
        for idx in range(n_tasks)
    ]

    # Run tasks in parallel
    with Pool(n_cores, initializer=init_pool, initargs=(queue,)) as pool:
        # This doesn't block and allows us to retrieve items from the queue:
        async_result = pool.starmap_async(calc, zip(tasks, range(n_tasks)))

        n = n_tasks
        while n:
            pos = queue.get()
            # Is this a sentinel value?
            if pos is None:
                n -= 1  # One less task to await
            else:
                pbars[pos].update()

        # We have no more updates to perform, so wait for the results:
        results = async_result.get()

        # Cause the bars to be removed before we display results
        # (See following Notes):
        for pbar in pbars:
            pbar.close()
        # So that the next print call starts at the start of the line
        # (required if leave=False is specified):
        if not LEAVE_PROGRESS_BAR:
            print('\r')

        for result in results:
            print(result)

if __name__ == &quot;__main__&quot;:
    parallel_with_pool()

Notes
In the above code the progress bars are instantiated with the argument leave=False signifying that we do not want the bars to remain. Consider the following code:
from tqdm import tqdm
import time

with tqdm(total=10, leave=False) as pbar:
    for _ in range(10):
        pbar.update()
        time.sleep(.5)

print('Done!')

When the with block is terminated, the progress bar will disappear as a result of the implicit call to pbar.__exit__ that occurs. But if we had instead:
pbar = tqdm(total=10, leave=False)
for _ in range(10):
    pbar.update()
    time.sleep(.5)

print('Done')

We would see instead:
C:\Ron\test&gt;test.py
100%|██████████████████████| 10/10 [00:04&lt;00:00,  2.03it/s]Done

Since, in the posted answer we are not using the progress bar as context manager the progress bar are not immediately erased and if we had a print statement to output the actual results of our PI calculations, we would have the problem. The solution is to explicitly call close() on each progress bar:
...
def parallel_with_pool():
        ...

        # We have no more updates to perform, so wait for the results:
        results = async_result.get()

        # Cause the bars to be removed before we display results.
        for pbar in pbars:
            pbar.close()
            # So that the next print call starts at the start of the line
            # (required if leave=False is specified):
            print('\r')

        for result in results:
            print(result)

If you want the progress bars to remain even after they have completed, then specify leave=True as follows:
    pbars = [
        tqdm(total=tasks[idx],
             position=idx,
             desc=f&quot;Job {idx + 1} of {n_tasks}&quot;,
             leave=True
             )
        for idx in range(n_tasks)
    ]

It is no longer necessary to call close for each bar, but it does not hurt to do so.
Update
Instead of using a multiprocessing.Queue instance to communicate we can instead create a multiprocessing.Array instance (which uses shared memory) of N counters where N is the number of progress bars whose progress is being tracked. Every iteration of calc will include an increment of the appropriate counter. The main process now has to periodically (say every .1 seconds) check the counters and update the progress bar accordingly:
from math import factorial
from decimal import Decimal, getcontext
from multiprocessing import Pool, Array
from tqdm import tqdm
import time

def init_pool(_progress_cntrs):
    global progress_cntrs

    progress_cntrs = _progress_cntrs

def calc(n_digits, pos):
    # number of iterations
    n = int(n_digits + 1 / 14.181647462725477)
    n = n if n &gt;= 1 else 1

    # set the number of digits for our numbers
    getcontext().prec = n_digits + 1

    t = Decimal(0)
    pi = Decimal(0)
    deno = Decimal(0)

    for k in range(n):
        t = ((-1) ** k) * (factorial(6 * k)) * (13591409 + 545140134 * k)
        deno = factorial(3 * k) * (factorial(k) ** 3) * (640320 ** (3 * k))
        pi += Decimal(t) / Decimal(deno)
        progress_cntrs[pos] += 1

    pi = pi * Decimal(12) / Decimal(640320 ** Decimal(1.5))
    pi = 1 / pi

    return pi

def parallel_with_pool():
    # Define the number of cores to use
    n_cores = 4

    # Define the tasks (e.g., compute first 100, 200, 300, 400 digits of pi)
    tasks =  [1200, 1700, 900, 1400] # Edit to make code for longer
    n_tasks = len(tasks)
    progress_cntrs = Array('i', [0] * n_tasks, lock=False)

    LEAVE_PROGRESS_BAR = True

    # Create the bars:
    pbars = [
        tqdm(total=tasks[idx],
             position=idx,
             desc=f&quot;Job {idx + 1} of {n_tasks}&quot;,
             leave=LEAVE_PROGRESS_BAR
             )
        for idx in range(n_tasks)
    ]

    # Run tasks in parallel
    with Pool(n_cores, initializer=init_pool, initargs=(progress_cntrs,)) as pool:
        # This doesn't block and allows us to retrieve items form the queue:
        async_result = pool.starmap_async(calc, zip(tasks, range(n_tasks)))

        n = n_tasks
        while n:
            time.sleep(.1)

            for idx in range(n_tasks):
                ctr = progress_cntrs[idx]
                if ctr != -1:
                    # This bar isn't complete
                    pbars[idx].n = ctr
                    pbars[idx].refresh()
                    if ctr == tasks[idx]:
                        # This bar is now complete
                        progress_cntrs[idx] = -1 # So we do not process this bar again
                        n -= 1

        # We have no more updates to perform, so wait for the results:
        results = async_result.get()

        # Cause the bars to be removed before we display results
        # (See following Notes):
        for pbar in pbars:
            pbar.close()
        # So that the next print call starts at the start of the line
        # (required if leave=False is specified)
        if not LEAVE_PROGRESS_BAR:
            print('\r')

        for result in results:
            print(result)

if __name__ == '__main__':
    parallel_with_pool()

"
"I'm making a test class for pytest, I want to set a class attribute a that will be used for several test methods. To do so, I used a fixture set_a, which is launched automatically autouse=True, and invoked only once for the class (scope='class'), because setting a is costly. Here is my code:
import pytest
import time


class Test:

    @pytest.fixture(scope='class', autouse=True)
    def set_a(self):
        print('Setting a...')
        time.sleep(5)
        self.a = 1

    def test_1(self):
        print('TEST 1')
        assert self.a == 1


But the test fails with the following error:
========================================================================= FAILURES ==========================================================================
________________________________________________________________________ Test.test_1 ________________________________________________________________________

self = &lt;tests.test_file.Test object at 0x116d953a0&gt;

    def test_1(self):
        print('TEST 1')
&gt;       assert self.a == 1
E       AttributeError: 'Test' object has no attribute 'a'

tests/test_file.py:15: AttributeError
------------------------------------------------------------------- Captured stdout setup -------------------------------------------------------------------
Setting a...
------------------------------------------------------------------- Captured stdout call --------------------------------------------------------------------
TEST 1

It looks like a wasn't set even if set_a was invoked, like if a new instance of the class was created when the test is executed.
It works well if I change the fixture scope to function, but I don't wan't to set a for each test.
Any idea what's the problem here ?
","You shouldn’t set the scope since you are already in the class.
class Test:
    @pytest.fixture(autouse=True)
    def set_a(self):
        print(&quot;Setting a...&quot;)
        time.sleep(5)
        self.a = 1

    def test_1(self):
        print(&quot;TEST 1&quot;)
        assert self.a == 1

This is how you should use the scope=class, meaning it will work for any class in your module:
@pytest.fixture(scope=&quot;class&quot;, autouse=True)
def a(request):
    print(&quot;Setting a...&quot;)
    time.sleep(5)
    request.cls.a = 1


class Test:
    def test_1(self):
        print(&quot;TEST 1&quot;)
        assert self.a == 1

"
"Let me edit my question again. I know how flatten works but I am looking if it possible to remove the inside braces and just simple two outside braces just like in MATLAB and maintain the same shape of (3,4). here it is arrays inside array, and I want to have just one array so I can plot it easily also get the same results is it is in Matlab.
For example I have the following matrix (which is arrays inside array):
s=np.arange(12).reshape(3,4)
print(s)
[[ 0  1  2  3]
 [ 4  5  6  7]
 [ 8  9 10 11]]

Is it possible to reshape or flatten() it and get results like this:
[ 0  1  2  3
  4  5  6  7
  8  9 10 11]

","First answer
If I understood correctly your question (and 4 other answers say I didn't), your problem is not how to flatten() or reshape(-1) an array, but how to ensure that even after reshaping, it still display with 4 elements per line.
I don't think you can, strictly speaking. Arrays are just a bunch of elements. They don't contain indication about how we want to see them. That's a printing problem, you are supposed to solve when printing.  You can see [here][1] that people who want to do that... start with reshaping array in 2D.
That being said, without creating your own printing function, you can control how numpy display arrays, using np.set_printoptions.
Still, it is tricky so, because this function allows you only to specify how many characters, not elements, are printed per line. So you need to know how many chars each element will need, to force linebreaks.
In your example:
np.set_printoptions(formatter={&quot;all&quot;:lambda x:&quot;{:&gt;6}&quot;.format(x)}, linewidth=7+(6+2)*4)

The formatter ensure that each number use 6 chars.
The linewidth, taking into account &quot;array([&quot; part, and the closing &quot;])&quot; (9 chars) plus the 2 &quot;, &quot; between each elements, knowing we want 4 elements, must be 9+6×4+2×3: 9 chars for &quot;array([...])&quot;, 6×4 for each 4 numbers, 2×3 for each 3 &quot;, &quot; separator. Or 7+(6+2)×4.
You can use it only for one printing
with np.printoptions(formatter={&quot;all&quot;:lambda x:&quot;{:&gt;6}&quot;.format(x)}, linewidth=7+(6+2)*4):
    print(s.reshape(-1))

Edit after some times : subclass
Another method that came to my mind, would be to subclass ndarray, to make it behave as you would want
import numpy as np


class MyArr(np.ndarray):
# To create a new array, with args ls: number of element to print per line, and arr, normal array to take data from
    def __new__(cls, ls, arr):
        n=np.ndarray.__new__(MyArr, (len(arr,)))
        n.ls=ls
        n[:]=arr[:]
        return n

    def __init__(self, *args):
        pass

    # So that this .ls is viral: when ever the array is created from an operation from an array that has this .ls, the .ls is copyied in the new array
    def __array_finalize__(self, obj):
        if not hasattr(self, 'ls') and type(obj)==MyArr and hasattr(obj, 'ls'):
            self.ls=obj.ls

    # Function to print an array with .ls elements per line
    def __repr__(self):
        # For other than 1D array, just use standard representation
        if len(self.shape)!=1:
            return super().__repr__()

        mxsize=max(len(str(s)) for s in self)
        s='['
        for i in range(len(self)):
            if i%self.ls==0 and i&gt;0:
                 s+='\n '
            s+=f'{{:{mxsize}}}'.format(self[i])
            if i+1&lt;len(self): s+=', '
        s+=']'
        return s

Now you can use this MyArr to build your own 1D array
MyArr(4, range(12))

shows
[ 0.0,  1.0,  2.0,  3.0, 
  4.0,  5.0,  6.0,  7.0, 
  8.0,  9.0, 10.0, 11.0]

And you can use it anywhere a 1d ndarray is legal. And most of the time, the .ls attribute will follows (I say &quot;most of the time&quot;, because I cannot guarantee that some functions wont build a new ndarray, and fill them with the data from this one)
a=MyArr(4, range(12))
a*2
#[ 0.0,  2.0,  4.0,  6.0, 
#  8.0, 10.0, 12.0, 14.0, 
# 16.0, 18.0, 20.0, 22.0]
a*a
#[  0.0,   1.0,   4.0,   9.0, 
#  16.0,  25.0,  36.0,  49.0, 
#  64.0,  81.0, 100.0, 121.0]
a[8::-1]
#[8.0, 7.0, 6.0, 5.0, 
# 4.0, 3.0, 2.0, 1.0, 
# 0.0]

# It even resists reshaping
b=a.reshape((3,4))
b
#MyArr([[ 0.,  1.,  2.,  3.],
#       [ 4.,  5.,  6.,  7.],
#       [ 8.,  9., 10., 11.]])
b.reshape((12,))
#[ 0.0,  1.0,  2.0,  3.0, 
#  4.0,  5.0,  6.0,  7.0, 
#  8.0,  9.0, 10.0, 11.0]

# Or fancy indexing
a[np.array([1,2,5,5,5])]
#[1.0, 2.0, 5.0, 5.0,
# 5.0]


# Or matrix operations
M=np.eye(12,k=1)+2*M.identity(12) # Just a matrix
M@a
#[ 1.0,  4.0,  7.0, 10.0, 
# 13.0, 16.0, 19.0, 22.0, 
# 25.0, 28.0, 31.0, 22.0]
np.diag(M*a)
#[ 0.0,  2.0,  4.0,  6.0, 
#  8.0, 10.0, 12.0, 14.0, 
# 16.0, 18.0, 20.0, 22.0]

# But of course, some time you loose the MyArr class
import pandas as pd
pd.DataFrame(a, columns=['v']).v.values
#array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])


  [1]: https://stackoverflow.com/questions/25991666/how-to-efficiently-output-n-items-per-line-from-numpy-array

"
"I am testing polars performance by LazyDataFrame API polars.scan_csv with filter. The performance is much better than I expect. Filtering a CSV file is even faster than the disk speed!  WHY???
The CSV file is about 1.51 GB on my PC HDD.
testing code:
import polars as pl
t0 = time.time()
lazy_df = pl.scan_csv(&quot;kline.csv&quot;)
df = lazy_df.filter(pl.col('ts') == '2015-01-01').collect().to_pandas()
print(time.time() - t0)

&gt; Output: 1.8616907596588135

It takes less than 2 seconds to scan the whole CSV file, which means that the scan speed is faster than 750MB/S. It is much faster than the disk speed, apparently.
","What you're probably seeing is a common problem in benchmarking: the caching of files by your operating system.  Most modern operating systems will attempt to cache files that are accessed, if the amount of RAM permits.
The first time you accessed the file, your operating system likely cached the 1.51 GB file in RAM (possibly even when you created the file).  As such, subsequent retrievals are not really accessing your HDD -- they are running against the cached file in RAM, a process which is far faster than reading from your HDD.  (Which is kind of the point of caching files in RAM.)
An Example
As an example, I created a 29.9 GB csv file, and purposely placed it on my NAS (network-attached storage) rather than on my local hard drive.  For reference, my NAS and my machine are connected by a 10 gigabit/sec network.
Running this benchmarking code the first time took about 54 seconds.
import polars as pl
import time
start = time.perf_counter()
(
    pl.scan_csv('/mnt/bak-projects/StackOverflow/benchmark.csv')
    .filter(pl.col('col_0') == 100)
    .collect()
)
print(time.perf_counter() - start)

shape: (1, 27)
┌─────┬───────┬───────┬───────┬─────┬────────┬────────┬────────┬────────┐
│ id  ┆ col_0 ┆ col_1 ┆ col_2 ┆ ... ┆ col_22 ┆ col_23 ┆ col_24 ┆ col_25 │
│ --- ┆ ---   ┆ ---   ┆ ---   ┆     ┆ ---    ┆ ---    ┆ ---    ┆ ---    │
│ f64 ┆ i64   ┆ i64   ┆ i64   ┆     ┆ i64    ┆ i64    ┆ i64    ┆ i64    │
╞═════╪═══════╪═══════╪═══════╪═════╪════════╪════════╪════════╪════════╡
│ 1.0 ┆ 100   ┆ 100   ┆ 100   ┆ ... ┆ 100    ┆ 100    ┆ 100    ┆ 100    │
└─────┴───────┴───────┴───────┴─────┴────────┴────────┴────────┴────────┘
&gt;&gt;&gt; print(time.perf_counter() - start)
53.92608916899917

So, reading a 29.9 GB file in 54 seconds is roughly 29.9 GB * (8 bits-per-byte) / 54 seconds = 4.4 gigabits per second.  Not bad for retrieving files from a network drive. And certainly within the realm of possibility on my 10 gigabit/sec network.
However, the file is now cached by my operating system (Linux) in RAM (I have 512 GB of RAM).  So when I run the same benchmarking code a second time, it took a mere 3.5 seconds:
shape: (1, 27)
┌─────┬───────┬───────┬───────┬─────┬────────┬────────┬────────┬────────┐
│ id  ┆ col_0 ┆ col_1 ┆ col_2 ┆ ... ┆ col_22 ┆ col_23 ┆ col_24 ┆ col_25 │
│ --- ┆ ---   ┆ ---   ┆ ---   ┆     ┆ ---    ┆ ---    ┆ ---    ┆ ---    │
│ f64 ┆ i64   ┆ i64   ┆ i64   ┆     ┆ i64    ┆ i64    ┆ i64    ┆ i64    │
╞═════╪═══════╪═══════╪═══════╪═════╪════════╪════════╪════════╪════════╡
│ 1.0 ┆ 100   ┆ 100   ┆ 100   ┆ ... ┆ 100    ┆ 100    ┆ 100    ┆ 100    │
└─────┴───────┴───────┴───────┴─────┴────────┴────────┴────────┴────────┘
&gt;&gt;&gt; print(time.perf_counter() - start)
3.5459880090020306

If my 29.9 GB file was really pulled across my network, this would imply a network speed of at least 29.9 * 8 / 3.5 sec = 68 gigabits per second.  (Clearly, not possible on my 10 Gigabit/sec network.)
And a third time: 2.9 seconds
shape: (1, 27)
┌─────┬───────┬───────┬───────┬─────┬────────┬────────┬────────┬────────┐
│ id  ┆ col_0 ┆ col_1 ┆ col_2 ┆ ... ┆ col_22 ┆ col_23 ┆ col_24 ┆ col_25 │
│ --- ┆ ---   ┆ ---   ┆ ---   ┆     ┆ ---    ┆ ---    ┆ ---    ┆ ---    │
│ f64 ┆ i64   ┆ i64   ┆ i64   ┆     ┆ i64    ┆ i64    ┆ i64    ┆ i64    │
╞═════╪═══════╪═══════╪═══════╪═════╪════════╪════════╪════════╪════════╡
│ 1.0 ┆ 100   ┆ 100   ┆ 100   ┆ ... ┆ 100    ┆ 100    ┆ 100    ┆ 100    │
└─────┴───────┴───────┴───────┴─────┴────────┴────────┴────────┴────────┘
&gt;&gt;&gt; print(time.perf_counter() - start)
2.8593162479992316

Depending on your operating system, there is a way to flush cached files from RAM before benchmarking.
"
"I have player A and B who both played against different opponents.



player
opponent
days ago




A
C
1


A
C
2


A
D
10


A
F
100


A
F
101


A
F
102


A
G
1


B
C
1


B
C
2


B
D
10


B
F
100


B
F
101


B
F
102


B
G
1


B
G
2


B
G
3


B
G
4


B
G
5


B
G
6


B
G
7


B
G
8



First, I want to find the opponent that is the most common one. My definition of &quot;most common&quot; is not the total number of matches but more like the balanced number of matches.
If for example, player 1 and 2 played respectively 99 and 1 time(s) against player 3 I prefer opponent 4 where A and B played both 49 times against.
In order to measure the &quot;balanceness&quot; I write the following function:
import numpy as np
from collections import Counter


def balanceness(array: np.ndarray):
    classes = [(c, cnt) for c, cnt in Counter(array).items()]
    m = len(classes)
    n = len(array)

    H = -sum([(cnt / n) * np.log((cnt / n)) for c, cnt in classes])

    return H / np.log(m)

This functions works as expected:
&gt;&gt; balanceness(array=np.array([0, 0, 0, 1, 1, 1]))
1.0

If I run the function on the different opponents I see the following results:



opponent
balanceness
n_matches




C
1
4


D
1
2


F
1
6


G
0.5032583347756457
9



Clearly, opponent F is the most common one. However, the matches of A and B against F are relatively old.
How should I incorporate a recency-factor into my calculation to find the &quot;most recent common opponent&quot;?
Edit
After thinking more about it I decided to weight each match using the following function
def weight(days_ago: int, epilson: float=0.005) -&gt; float:
    return np.exp(-1 * days_ago * epilson)

I sum the weight of all the matches against each opponent



opponent
balanceness
n_matches
weighted_n_matches




C
1
4
3.9701246258837


D
1
2
1.90245884900143


F
1
6
3.62106362790388


G
0.5032583347756457
9
8.81753570603108



Now, opponent C is the &quot;most-recent balanced opponent&quot;.
Nevertheless, this method ignores the &quot;recentness&quot; on a player-level because we sum the values. There could be a scenario where player 1 played recently a lot of matches against player 3 whereas player 2 faced player 3 in the distant past.
How can we find the opponent that is

the most balanced / equally-distributed between two players
the opponent with the most recent matches against the two players

","First, I think &quot;balanceness&quot; needs to consider how many days ago the matches were played. For example, suppose A and B played 1 match against C, both 100 days ago. Again, let A and B both play 1 match against E, 1 day and 199 days ago respectively. Although the number of matches is the same, their recency is different, and they shouldn't have the same balanceness score.
By using the defined weight(days_ago) function, it will be as if A and B both played 0.60 matches against C, while they played 0.995 and 0.36 matches against E respectively. These two scenarios should have different balanceness.
Second, just balanceness is obviously not enough. If A and B played 1 match each against D, both 100 years ago, and against E, both 200 years ago---both scenarios are equally &quot;balanced&quot;. You need to define a &quot;recency&quot; score (between 0 and 1); I think average weight might work. And then you can combine the two metrics together in some way, e.g. B * R, or (B * R)/(B + R), or alpha * B + (1 - alpha) * R.
import numpy as np
import pandas as pd

data = [
    [&quot;A&quot;, &quot;C&quot;, 2],
    [&quot;A&quot;, &quot;D&quot;, 10],
    [&quot;A&quot;, &quot;F&quot;, 100],
    [&quot;A&quot;, &quot;F&quot;, 101],
    [&quot;A&quot;, &quot;F&quot;, 102],
    [&quot;A&quot;, &quot;G&quot;, 1],
    [&quot;B&quot;, &quot;C&quot;, 1],
    [&quot;B&quot;, &quot;C&quot;, 2],
    [&quot;B&quot;, &quot;D&quot;, 10],
    [&quot;B&quot;, &quot;F&quot;, 100],
    [&quot;B&quot;, &quot;F&quot;, 101],
    [&quot;B&quot;, &quot;F&quot;, 102],
    [&quot;B&quot;, &quot;G&quot;, 1],
    [&quot;B&quot;, &quot;G&quot;, 2],
    [&quot;B&quot;, &quot;G&quot;, 3],
    [&quot;B&quot;, &quot;G&quot;, 4],
    [&quot;B&quot;, &quot;G&quot;, 5],
    [&quot;B&quot;, &quot;G&quot;, 6],
    [&quot;B&quot;, &quot;G&quot;, 7],
    [&quot;B&quot;, &quot;G&quot;, 8]
]

def weight(days_ago: int, epilson: float=0.005) -&gt; float:
    return np.exp(-1 * days_ago * epilson)

def weighted_balanceness(array: np.ndarray, weights: np.ndarray):
    classes = np.unique(array)
    cnt = np.array([weights[array == c].sum() for c in classes])
    m = len(classes)
    n = weights.sum()

    H = -(cnt / n * np.log(cnt / n)).sum() 
    return H / np.log(m)


df = pd.DataFrame(data=data, columns=[&quot;player&quot;, &quot;opponent&quot;, &quot;days_ago&quot;])
df[&quot;effective_count&quot;] = weight(df[&quot;days_ago&quot;])

scores = []
for opponent in df[&quot;opponent&quot;].unique():
    df_o = df.loc[df[&quot;opponent&quot;] == opponent]
    player = np.where(df_o[&quot;player&quot;].values == &quot;A&quot;, 0, 1)
    balanceness = weighted_balanceness(array=player, weights=df_o[&quot;effective_count&quot;])

    recency = df_o[&quot;effective_count&quot;].mean()
    scores.append([opponent, balanceness, recency])


df_out = pd.DataFrame(scores, columns=[&quot;opponent&quot;, &quot;balanceness&quot;, &quot;recency&quot;])
df_out[&quot;br&quot;] = df_out[&quot;balanceness&quot;] * df_out[&quot;recency&quot;]
df_out[&quot;mean_br&quot;] = 0.5 * df_out[&quot;balanceness&quot;] + 0.5 * df_out[&quot;recency&quot;]
df_out[&quot;harmonic_mean_br&quot;] = df_out[&quot;balanceness&quot;] * df_out[&quot;recency&quot;] / ( (df_out[&quot;balanceness&quot;] + df_out[&quot;recency&quot;]))

print(df_out)

This gives me the following:
  opponent  balanceness   recency        br   mean_br  harmonic_mean_br
0        C     0.917739  0.991704  0.910125  0.954721          0.476644
1        D     1.000000  0.951229  0.951229  0.975615          0.487503
2        F     1.000000  0.603511  0.603511  0.801755          0.376368
3        G     0.508437  0.979726  0.498129  0.744082          0.334728

Note that D and F have perfect balanceness. They both played with A &amp; B with same number of matches and same days ago. However, F played a while back (100-102 days ago), so they have a lower recency score, which hurts their combined scores.
Depending on how you combine b and r, most likely D or C would be the best choice (C may win if you give more weight to recency).
"
"I have two pandas DataFrames:
import pandas as pd

data1 = {
    'score': [1, 2],
    'seconds': [1140, 2100],
}

data2 = {
    'prize': [5.5, 14.5, 14.6, 21, 23, 24, 26, 38, 39, 40, 50],
    'seconds': [840, 1080, 1380, 1620, 1650, 1680, 1700, 1740, 2040, 2100, 2160],
}

df1 = pd.DataFrame.from_dict(data1)
df2 = pd.DataFrame.from_dict(data2)

Output: df1
   score  seconds
0      1     1140
1      2     2100

Output: df2
    prize  seconds
0     5.5      840
1    14.5     1080
2    14.6     1380
3    21.0     1620
4    23.0     1650
5    24.0     1680
6    26.0     1700
7    38.0     1740
8    39.0     2040
9    40.0     2100
10   50.0     2160

For each value in seconds column from df1, I would like to get the match (or the closest to) row from df2 and also the closest 2 rows above and below the match.
The seconds columns contains only sorted unique values.
As result, I expect this:
Output: result
    prize  seconds
0     5.5      840
1    14.5     1080 # closest match to 1140
2    14.6     1380
3    21.0     1620
7    38.0     1740
8    39.0     2040
9    40.0     2100 # match 2100
10   50.0     2160

","You can use a merge_asof to identify the closest value to each value in df1, then a rolling.max to extend the selection to the neighboring N rows:
N = 2 # number of surronding rows to keep

s1 = df1['seconds'].sort_values()
s2 = df2['seconds'].sort_values().rename('_')

keep = pd.merge_asof(s1, s2, left_on='seconds', right_on='_',
                     direction='nearest')['_']

out = df2[s2.isin(keep)
            .rolling(2*N+1, center=True, min_periods=1)
            .max().astype(bool)]

NB. if the seconds are already sorted, you can skip the .sort_values().
Output:
    prize  seconds
0     5.5      840
1    14.5     1080
2    14.6     1380
3    21.0     1620
7    38.0     1740
8    39.0     2040
9    40.0     2100
10   50.0     2160

Intermediates:
    prize  seconds  closest  isin(keep)  rolling.max
0     5.5      840      NaN       False         True
1    14.5     1080   1140.0        True         True
2    14.6     1380      NaN       False         True
3    21.0     1620      NaN       False         True
4    23.0     1650      NaN       False        False
5    24.0     1680      NaN       False        False
6    26.0     1700      NaN       False        False
7    38.0     1740      NaN       False         True
8    39.0     2040      NaN       False         True
9    40.0     2100   2100.0        True         True
10   50.0     2160      NaN       False         True

"
"I would like to create automated examples of valid data based on my pydantic models. How can I do this?
Example:
import pydantic
from typing import Any


class ExampleData(pydantic.BaseModel):
    a: int
    b: str = pydantic.Field(min_length=10, max_length=10)
    
    @staticmethod
    def example() -&gt; dict[str, Any]:
        # some logic
        return {}
        


a.example()
&quot;&quot;&quot;Returns
{
    &quot;a&quot;: 1,
    &quot;b&quot;: &quot;0123456789&quot;
}
&quot;&quot;&quot;

P.S. I suspect that pydantic provides this functionality because fastapi generates sample data, but I'm not sure if this is exactly its functionality and I couldn't find such a method. Can any one help me understand this?
","Here's how you can do it using pydantic and Faker:

Installation

pip install Faker
pip install pydantic


Script

import uuid
from datetime import date, datetime, timedelta
from typing import List, Union
from pydantic import BaseModel, UUID4
from faker import Faker

# your pydantic model
class Person(BaseModel):
    id: UUID4
    name: str
    hobbies: List[str]
    age: Union[float, int]
    birthday: Union[datetime, date]

class PersonFactory:
    @classmethod
    def generate_id(cls):
        return str(uuid.uuid4())

    @classmethod
    def generate_name(cls):
        # Implement your own logic to generate realistic names
        return Faker().name()

    @classmethod
    def generate_hobbies(cls):
        # Implement your own logic to generate hobbies
        return Faker().words(nb=1)

    @classmethod
    def generate_age(cls):
        # Implement your own logic to generate realistic ages
        return Faker().random_int(1940, 2023)
        
    
    @classmethod
    def generate_birthday(cls):
        # Implement your own logic to generate realistic birthdays
        return Faker().date_of_birth(tzinfo=None, minimum_age=18, maximum_age=80)

    @classmethod
    def build(cls):
        id = cls.generate_id()
        name = cls.generate_name()
        hobbies = cls.generate_hobbies()
        birthday = cls.generate_birthday()
        age = datetime.now().year - birthday.year
        
        return Person(id=id, name=name, hobbies=hobbies, age=age, birthday=birthday)

result = PersonFactory.build()
print(result)


Output:

id=UUID('4b7ffc04-48a1-4f4d-8c5b-d3167717dd69')
name='Katherine Brown'
hobbies=['stay']
age=30
birthday=datetime.date(1993, 2, 17)

This will generate a fake person data everytime you run PersonFactory.build()
"
"I want to use pathlib.glob() to find directories with a specific name pattern (*data) in the current working dir. I don't want to explicitly check via .isdir() or something else.
Input data
This is the relevant listing with three folders as the expected result and one file with the same pattern but that should be part of the result.
ls -ld *data
drwxr-xr-x 2 user user 4,0K  9. Sep 10:22 2021-02-11_68923_data/
drwxr-xr-x 2 user user 4,0K  9. Sep 10:22 2021-04-03_38923_data/
drwxr-xr-x 2 user user 4,0K  9. Sep 10:22 2022-01-03_38923_data/
-rw-r--r-- 1 user user    0  9. Sep 10:24 2011-12-43_3423_data

Expected result
[
    '2021-02-11_68923_data/', 
    '2021-04-03_38923_data/',
    '2022-01-03_38923_data/'
]

Minimal working example
from pathlib import Path
cwd = Path.cwd()

result = cwd.glob('*_data/')
result = list(result)

That gives me the 3 folders but also the file.
Also tried the variant cwd.glob('**/*_data/').
","The trailing path separator certainly should be respected in pathlib.glob patterns. This is the expected behaviour in shells on all platforms, and is also how the glob module works:

If the pattern is followed by an os.sep or os.altsep then files will not match.

However, there is a bug in pathlib that was fixed in bpo-22276, and merged in Python-3.11.0rc1 (see what's new: pathlib).
In the meantime, as a work-around you can use the glob module to get the behaviour you want:
$ ls -ld *data
drwxr-xr-x 2 user user 4096 Sep  9 22:45 2022-01-03_38923_data
drwxr-xr-x 2 user user 4096 Sep  9 22:44 2021-04-03_38923_data
drwxr-xr-x 2 user user 4096 Sep  9 22:44 2021-02-11_68923_data
-rw-r--r-- 1 user user    0 Sep  9 22:45 2011-12-43_3423_data

&gt;&gt;&gt; import glob
&gt;&gt;&gt; res = glob.glob('*_data')
&gt;&gt;&gt; print('\n'.join(res))
2022-01-03_38923_data
2011-12-43_3423_data
2021-02-11_68923_data
2021-04-03_38923_data

&gt;&gt;&gt; res = glob.glob('*_data/')
&gt;&gt;&gt; print('\n'.join(res))
2022-01-03_38923_data/
2021-02-11_68923_data/
2021-04-03_38923_data/

"
"I want to generate 10 whole numbers that add up to 40 and are in the range of 2-6.
For example:
2 + 6 + 2 + 5 + 6 + 2 + 2 + 6 + 3 + 6 = 40
Ten random numbers between 2 and 6 that add up to 40.
","Given the relatively small search space, you could use itertools.combinations_with_replacement() to generate all possible sequences of 10 numbers between 2 and 6, save the ones that sum to 40 - then pick and shuffle one at random when requested:
from itertools import combinations_with_replacement as combine
from random import choice, shuffle

sequences = [list(combo) for combo in combine(range(2, 6+1), 10) if sum(combo) == 40]

def get_random_sequence_of_sum_40():
    seq = choice(sequences)
    shuffle(seq)
    return seq

# ... later when you need random sequences of sum=40
for i in range(10):
    rand_sequence = get_random_sequence_of_sum_40()
    print(f&quot;The sum of {rand_sequence} is {sum(rand_sequence)}&quot;)

Sample output:
The sum of [6, 3, 4, 4, 3, 3, 4, 6, 5, 2] is 40
The sum of [3, 3, 5, 3, 5, 5, 3, 3, 5, 5] is 40
The sum of [3, 3, 6, 3, 4, 6, 3, 4, 4, 4] is 40
The sum of [6, 6, 5, 3, 4, 3, 3, 2, 4, 4] is 40
The sum of [5, 2, 2, 4, 4, 4, 5, 4, 4, 6] is 40
The sum of [4, 4, 4, 3, 4, 4, 3, 6, 4, 4] is 40
The sum of [4, 4, 5, 4, 2, 4, 4, 5, 5, 3] is 40
The sum of [4, 2, 6, 2, 5, 6, 2, 5, 4, 4] is 40
The sum of [3, 6, 3, 4, 3, 3, 4, 4, 6, 4] is 40
The sum of [2, 2, 6, 2, 3, 5, 6, 4, 4, 6] is 40

"
"So this is my code basically:
df = pd.read_csv('XBT_60.csv', index_col = 'date', parse_dates = True)
df.index.freq = 'H'

I load a csv, set the index to the date column and want to set the frequency to 'H'. But this raises this error:
ValueError: Inferred frequency None from passed values does not conform to passed frequency H

The format of the dates column is: 2017-01-01 00:00:00
I already tried loading the csv without setting the index column and used pd.to_datetime on the dates column before I set it as index, but still i am unable to set the frequency. How can I solve this?
BTW: my aim is to use the seasonal_decompose() method from statsmodels, so I need the frequency there.
","You can't set frequency if you have missing index values:
&gt;&gt;&gt; df
            val
2019-09-15    0
2019-09-16    1
2019-09-18    3

&gt;&gt;&gt; df.index.freq = 'D'
...
ValueError: Inferred frequency None from passed values does not conform to passed frequency D

To find missing index, use:
&gt;&gt;&gt; df = df.resample('D').first()
            val
2019-09-15  0.0
2019-09-16  1.0
2019-09-17  NaN
2019-09-18  3.0

&gt;&gt;&gt; df.index.freq
&lt;Day&gt;

To debug, find missing indexes:
&gt;&gt;&gt; pd.date_range(df.index.min(), df.index.max(), freq='D').difference(df.index)
DatetimeIndex(['2019-09-17'], dtype='datetime64[ns]', freq=None)

"
"Hi I've made a simple Binary Tree and added a pre-order traversal method. After throwing around some ideas I got stuck on finding a way to return each value from the traverse_pre() method in an array.
class BST:
    def __init__(self, val):
        self.value = val
        self.left = None
        self.right = None

    def add_child(self, val):
        if self.value:
            if val &lt; self.value:
                if self.left == None:
                    self.left = BST(val)
                else:
                    self.left.add_child(val)
            else:
                if val &gt; self.value:
                    if self.right == None:
                        self.right = BST(val)
                    else:
                        self.right.add_child(val)
        else:
            self.value = val

    def traverse_pre(self):
        if self.left:
            self.left.traverse_pre()
        print(self.value)

        if self.right:
            self.right.traverse_pre()


Tree = BST(5)
Tree.add_child(10)
Tree.add_child(8)
Tree.add_child(2)
Tree.add_child(4)
Tree.add_child(7)

Tree.traverse_pre()

How would I modify the traverse_pre() function to return an array consisting of the node values. Is there a good example of this process for me to understand this further, I'm a bit stuck on how values can be appended to an array within recursion.
","I would not recommend copying the entire tree to an intermediate list using  .append or .extend. Instead use yield which makes your tree iterable and capable of working directly with many built-in Python functions -
class BST:
    # ...
    def preorder(self):
        # value
        yield self.value
        # left
        if self.left: yield from self.left.preorder()
        # right
        if self.right: yield from self.right.preorder()

We can simply reorder the lines this to offer different traversals like inorder -
class BST:
    # ...
    def inorder(self):
        # left
        if self.left: yield from self.left.inorder()
        # value
        yield self.value
        # right
        if self.right: yield from self.right.inorder()

And postorder -
class BST:
    # ...
    def postorder(self):
        # left
        if self.left: yield from self.left.postorder()
        # right
        if self.right: yield from self.right.postorder()
        # value
        yield self.value

Usage of generators provides inversion of control. Rather than the traversal function deciding what happens to each node, the the caller is left with the decision on what to do. If a list is indeed the desired target, simply use list -
list(mytree.preorder())

# =&gt; [ ... ]

That said, there's room for improvement with the rest of your code. There's no need to mutate nodes and tangle self context and recursive methods within your BST class directly. A functional approach with a thin class wrapper will make it easier for you to grow the functionality of your tree. For more information on this technique, see this related Q&amp;A.
If you need to facilitate trees of significant size, a different traversal technique may be required. Just ask in the comments and someone can help you find what you are looking for.
"
"Hi everyone. I'm developing my first flask project and I got stuck on the following problem:
I have a simple Flask app:
from flask import Flask, render_template
import map_plotting_test as mpt

app = Flask(__name__)


@app.route('/')
def render_the_map():
    mpt.create_map()
    return render_template(&quot;map.html&quot;)


if __name__ == '__main__':
    app.run(debug=True)


Problem
mpt.create_map() function here is just making the map, rendering it, then creating the map.html file and saving it to the templates folder: templates/map.html. It works pretty fine, but it takes some noticeable time to finish making the map (around 10-15 seconds).
The problem is that while this function is performed, I see just a blank screen in the browser, and only then does Flask render the finished map.html file.
What I want
What I want to do is to show the loading screen instead of a blank screen while the create_map() function is running. And when the function finishes its work and creates a map.html file - show rendered template to user just like return render_template(&quot;map.html&quot;) does.
Is there a way to achieve this without much effort? I'm new to Flask, and I would be very grateful for a good explanation.
Thank you!!!
","Finally I found the solution!

Thanks to Laurel's answer. I'll just make it more nice and clear.

What I've done
I redesigned my Flask app, so it looks like this:
from flask import Flask, render_template
import map_plotting_module as mpm

app = Flask(__name__)


@app.route('/')
def loading():
    return render_template(&quot;loading.html&quot;)


@app.route('/map')
def show_map():
    return render_template(&quot;map.html&quot;)


@app.route('/create_map')
def create_map():
    mpm.create_map()
    return &quot;Map created&quot;


if __name__ == '__main__':
    app.run()

When user opens the page, flask stars rendering the loading.html file.
In this file you have to add the following code to the &lt;head&gt; section:
&lt;script&gt;
    function navigate() {
        window.location.href = 'map';  // Redirects user to the /map route when 'create_map' is finished
    }
    fetch('create_map').then(navigate); // Performing 'create_map' and then calls navigate() function, declared above
&lt;/script&gt;

Then, add a loading wheel div to your &lt;body&gt; section:
&lt;body&gt;
    &lt;div class=&quot;loader&quot;&gt;&lt;/div&gt;
&lt;/body&gt;

If it's still not clear for you - please check my example at the end of the answer
Explanation
In the &lt;script&gt; section we have to declare navigate() function. It just redirects the user to the desired reference, /map in the current case.
fetch() is the analog to jQuery.ajax() - read more. It's just fetching the app route to /create_map, awaits it to be done in the &quot;backround&quot;, and then performs action in the .then() block - navigate() function in our case.
So the workflow is:

User opens the page, @app.route('/') function is performed, which is loading page.
Loading page fetching @app.route('/create_map') and runs its functions in the background.
When create_map() function is completed - user is being redirected to the @app.route('/map') which function is rendering all-done map.html template.

Few recommendations from me

If you want your loading page to have an icon, just add the following tab to your &lt;head&gt; section of loading.html:

&lt;link rel=&quot;icon&quot; href=&quot;/static/&lt;icon_file&gt;&quot;&gt;


Pay attention, Flask is searching for media in the /static folder. Put your media in it. Otherwise, media will not be rendered!


If you want your loading page to have a nice CSS loader, consider visiting this page: loading.io. I really enjoyed it.

Finally, here's code snippet as an example of loader:


.loader {
  position: absolute;
  top: 50%;
  left: 50%;
  margin: -56px 0 0 -56px;
}

.loader:after {
  content: "" "";
  display: block;
  width: 110px;
  height: 110px;
  border-radius: 50%;
  border: 1px solid;
  border-color: #0aa13a transparent #47a90e transparent;
  animation: ring 1.2s linear infinite;
}

@keyframes ring {
  0% {
    transform: rotate(0deg);
  }
  100% {
    transform: rotate(360deg);
  }
 
&lt;!DOCTYPE html&gt;
&lt;html lang=""en""&gt;

&lt;head&gt;
  &lt;meta charset=""UTF-8""&gt;
  &lt;title&gt;Loading...&lt;/title&gt;
&lt;/head&gt;

&lt;body&gt;
  &lt;div class=""loader""&gt;&lt;/div&gt;
&lt;/body&gt;

&lt;/html&gt;



"
"I can't find any Python code for the equivalent of
python -m http.server port --bind addr --directory dir

So I need basically a working server class that process at least GET requests. Most of the things I found on Google were either an HTTP server with some special needs or something like that, where you need to code the response behaviour be yourself:
from http.server import BaseHTTPRequestHandler, HTTPServer

def run(server_class=HTTPServer, handler_class=BaseHTTPRequestHandler):
    server_address = ('', 8000)
    httpd = server_class(server_address, handler_class)
    httpd.serve_forever()

run()

All that I need is a default working skeleton of a Python HTTP server, where you can provide address, port and directory, and it would normally process GET requests.
","That's what I ended up with:
# python -m http.server 8000 --directory ./my_dir

from http.server import HTTPServer as BaseHTTPServer, SimpleHTTPRequestHandler
import os


class HTTPHandler(SimpleHTTPRequestHandler):
    &quot;&quot;&quot;This handler uses server.base_path instead of always using os.getcwd()&quot;&quot;&quot;

    def translate_path(self, path):
        path = SimpleHTTPRequestHandler.translate_path(self, path)
        relpath = os.path.relpath(path, os.getcwd())
        fullpath = os.path.join(self.server.base_path, relpath)
        return fullpath


class HTTPServer(BaseHTTPServer):
    &quot;&quot;&quot;The main server, you pass in base_path which is the path you want to serve requests from&quot;&quot;&quot;

    def __init__(self, base_path, server_address, RequestHandlerClass=HTTPHandler):
        self.base_path = base_path
        BaseHTTPServer.__init__(self, server_address, RequestHandlerClass)


web_dir = os.path.join(os.path.dirname(__file__), 'my_dir')
httpd = HTTPServer(web_dir, (&quot;&quot;, 8000))
httpd.serve_forever()

A simple HTTP server that handles GET requests with, working with a certain directory.
"
"I'm using FastAPI to create backend for my project. I have a method that allows to upload a file. I implemented it as follows:
from fastapi import APIRouter, UploadFile, File

from app.models.schemas.files import FileInResponse

router = APIRouter()


@router.post(&quot;&quot;, name=&quot;files:create-file&quot;, response_model=FileInResponse)
async def create(file: UploadFile = File(...)) -&gt; FileInResponse:
    pass

As you can see, I use a dedicated pydantic model for a method resultâ€”FileInResponse:
from pathlib import Path

from pydantic import BaseModel


class FileInResponse(BaseModel):
    path: Path

And I follow this naming pattern for models (naming models as &lt;Entity&gt;InCreate, &lt;Entity&gt;InResponse, and so on) throughout the API. However, I couldn't create a pydantic model with a field of the type File, so I had to declare it directly in the route definition (i.e. without a model containing it). As a result, I have this long auto generated name Body_files_create_file_api_files_post in the OpenAPI docs:

Is there a way to change the schema name?
","In case someone is interested into renaming auto generated fields from FastAPI in openapi.json file, you should add an operation_id param to your route. This will allow FastAPI to use this id to generate a clearer attribute name for the generated schema.
Before:
@router.post(
    &quot;/{opportunity_id}/files&quot;,
    status_code=status.HTTP_201_CREATED,
)
async def attach_opportunity_file(
    db: Database,
    uploaded_file: UploadFile = File(title=&quot;File to upload&quot;),
) -&gt; OpportunityFile:
    pass


After:
@router.post(
    &quot;/{opportunity_id}/files&quot;,
    status_code=status.HTTP_201_CREATED,
    operation_id=&quot;attach_opportunity_file&quot;, # New operation_id added 
)
async def attach_opportunity_file(
    db: Database,
    uploaded_file: UploadFile = File(title=&quot;File to upload&quot;),
) -&gt; OpportunityFile:
    pass


"
"I am trying to implement a code for image style transfer based on FastAPI. I found it effective to convert the byte of the image into base64 and transmit it.
So, I designed my client codeto encode the image into a base64 string and send it to the server, which received it succesfully. However, I face some difficulties in restoring the image bytes to ndarray.
I get the following this errors:
image_array = np.frombuffer(base64.b64decode(image_byte)).reshape(image_shape)

ValueError: cannot reshape array of size 524288 into shape (512,512,4)

This is my client code :
import base64
import requests
import numpy as np
import json
from matplotlib.pyplot import imread
from skimage.transform import resize


if __name__ == '__main__':
    path_to_img = &quot;my image path&quot;

    image = imread(path_to_img)
    image = resize(image, (512, 512))

    image_byte = base64.b64encode(image.tobytes())
    data = {&quot;shape&quot;: image.shape, &quot;image&quot;: image_byte.decode()}

    response = requests.get('http://127.0.0.1:8000/myapp/v1/filter/a', data=json.dumps(data))

and this is my server code:
import json
import base64
import uvicorn
import model_loader
import numpy as np

from fastapi import FastAPI
from typing import Optional


app = FastAPI()

@app.get(&quot;/&quot;)
def read_root():
    return {&quot;Hello&quot;: &quot;World&quot;}


@app.get(&quot;/myapp/v1/filter/a&quot;)
async def style_transfer(data: dict):
    image_byte = data.get('image').encode()
    image_shape = tuple(data.get('shape'))
    image_array = np.frombuffer(base64.b64decode(image_byte)).reshape(image_shape)

if __name__ == '__main__':
    uvicorn.run(app, port='8000', host=&quot;127.0.0.1&quot;)

","Option 1
As previously mentioned here, as well as here and here, one should use UploadFile, in order to upload files from client apps (for async read/write have a look at this answer). For example:
server side:
@app.post(&quot;/upload&quot;)
def upload(file: UploadFile = File(...)):
    try:
        contents = file.file.read()
        with open(file.filename, 'wb') as f:
            f.write(contents)
    except Exception:
        return {&quot;message&quot;: &quot;There was an error uploading the file&quot;}
    finally:
        file.file.close()
        
    return {&quot;message&quot;: f&quot;Successfuly uploaded {file.filename}&quot;}

client side:
import requests

url = 'http://127.0.0.1:8000/upload'
file = {'file': open('images/1.png', 'rb')}
resp = requests.post(url=url, files=file) 
print(resp.json())

Option 2
If, however, you still need to send a base64 encoded image, you can do it as previously described here (Option 2). On client side, you can encode the image to base64 and send it using a POST  request as follows:
client side:
import base64
import requests

url = 'http://127.0.0.1:8000/upload'
with open(&quot;photo.png&quot;, &quot;rb&quot;) as image_file:
    encoded_string = base64.b64encode(image_file.read())
    
payload ={&quot;filename&quot;: &quot;photo.png&quot;, &quot;filedata&quot;: encoded_string}
resp = requests.post(url=url, data=payload) 

On server side you can receive the image using a Form field, and decode the image as follows:
server side:
@app.post(&quot;/upload&quot;)
def upload(filename: str = Form(...), filedata: str = Form(...)):
    image_as_bytes = str.encode(filedata)  # convert string to bytes
    img_recovered = base64.b64decode(image_as_bytes)  # decode base64string
    try:
        with open(&quot;uploaded_&quot; + filename, &quot;wb&quot;) as f:
            f.write(img_recovered)
    except Exception:
        return {&quot;message&quot;: &quot;There was an error uploading the file&quot;}
        
    return {&quot;message&quot;: f&quot;Successfuly uploaded {filename}&quot;} 

"
"I have a number of large base64 strings to decode, ranging from a few hundred of MB up to ~5 GB each.
The obvious solution is a single call to base64.b64decode (&quot;reference implementation&quot;).
I'm trying to speed up the process by using multiprocessing, but, surprisingly, it is much slower than the reference implementation.
On my machine I get:
reference_implementation
decoding time = 7.37

implmementation1
Verify result Ok
decoding time = 7.59

threaded_impl
Verify result Ok
decoding time = 13.24

mutiproc_impl
Verify result Ok
decoding time = 11.82

What I am doing wrong?
(Warning: memory hungry code!)
import base64

from time import perf_counter
from binascii import a2b_base64
import concurrent.futures as fut
from time import sleep
from gc import collect
from multiprocessing import cpu_count

def reference_implementation(encoded):
    &quot;&quot;&quot;This is the implementation that gives the desired result&quot;&quot;&quot;
    return base64.b64decode(encoded)


def implmementation1(encoded):
    &quot;&quot;&quot;Try to call the directly the underlying library&quot;&quot;&quot;
    return a2b_base64(encoded)


def threaded_impl(encoded, N):
    &quot;&quot;&quot;Try multi threading calling the underlying library&quot;&quot;&quot;
    # split the string into pieces
    d = len(encoded) // N            # number of splits
    lbatch = (d // 4) * 4           # lenght of first N-1 batches, the last is len(source) - lbatch*N
    batches = []
    for i in range(N-1):
        start = i * lbatch
        end = (i + 1) * lbatch
        # print(i, start, end)
        batches.append(encoded[start:end])
    batches.append(encoded[end:])
    # Decode
    ret = bytes()
    with fut.ThreadPoolExecutor(max_workers=N) as executor:
        # Submit tasks for execution and put pieces together
        for result  in executor.map(a2b_base64, batches):
            ret = ret + result
    return ret


def mutiproc_impl(encoded, N):
    &quot;&quot;&quot;Try multi processing calling the underlying library&quot;&quot;&quot;
    # split the string into pieces
    d = len(encoded) // N            # number of splits
    lbatch = (d // 4) * 4           # lenght of first N-1 batches, the last is len(source) - lbatch*N
    batches = []
    for i in range(N-1):
        start = i * lbatch
        end = (i + 1) * lbatch
        # print(i, start, end)
        batches.append(encoded[start:end])
    batches.append(encoded[end:])
    # Decode
    ret = bytes()
    with fut.ProcessPoolExecutor(max_workers=N) as executor:
        # Submit tasks for execution and put pieces together
        for result  in executor.map(a2b_base64, batches):
            ret = ret + result
    return ret

if __name__ == &quot;__main__&quot;:
    CPU_NUM = cpu_count()

    # Prepare a 4.6 GB byte string (with less than 32 GB ram you may experience swapping on virtual memory)
    repeat = 60000000
    large_b64_string = b'VGhpcyBzdHJpbmcgaXMgZm9ybWF0dGVkIHRvIGJlIGVuY29kZWQgd2l0aG91dCBwYWRkaW5nIGJ5dGVz' * repeat

    # Compare implementations
    print(&quot;\nreference_implementation&quot;)
    t_start = perf_counter()
    dec1 = reference_implementation(large_b64_string)
    t_end = perf_counter()
    print('decoding time =', (t_end - t_start))

    sleep(1)

    print(&quot;\nimplmementation1&quot;)
    t_start = perf_counter()
    dec2 = implmementation1(large_b64_string)
    t_end = perf_counter()
    print(&quot;Verify result&quot;, &quot;Ok&quot; if dec2==dec1 else &quot;FAIL&quot;)
    print('decoding time =', (t_end - t_start))
    del dec2; collect()     # force freeing memory to avoid swapping on virtual mem

    sleep(1)

    print(&quot;\nthreaded_impl&quot;)
    t_start = perf_counter()
    dec3 = threaded_impl(large_b64_string, CPU_NUM)
    t_end = perf_counter()
    print(&quot;Verify result&quot;, &quot;Ok&quot; if dec3==dec1 else &quot;FAIL&quot;)
    print('decoding time =', (t_end - t_start))
    del dec3; collect()

    sleep(1)

    print(&quot;\nmutiproc_impl&quot;)
    t_start = perf_counter()
    dec4 = mutiproc_impl(large_b64_string, CPU_NUM)
    t_end = perf_counter()
    print(&quot;Verify result&quot;, &quot;Ok&quot; if dec4==dec1 else &quot;FAIL&quot;)
    print('decoding time =', (t_end - t_start))
    del dec4; collect()

","TL;DR: Python parallelism sucks due to the global interpreter lock and inter-process communication. Data copies also introduce overheads making your parallel implementations even slower, especially since the operation tends to be memory-bound. A native CPython module can be written to overpass the CPython's limits and strongly speed up the computation.

First things first, multi-threading in CPython is limited by the global interpreter lock (GIL) which prevents such a computation to be faster than a sequential one (like in nearly all cases except generally I/Os). This point has been pointed out by Barmar in comments.
Moreover, multi-processing is limited by the inter-process communication (IPC) between workers which is generally slow. This is especially true here since the computation is rather memory intensive and IPC is done using relatively slow pickling internally. Not to mention this IPC operation is mostly done sequentially impacting even further the scalability of the operation if it is not completely memory-bound on the target platform.
On top of that, operations like encoded[start:end] creates a new bytes which is a (partial) copy of encoded. This increase even further the memory bandwidth pressure which should be already an issue (it is clearly the case on my laptop). The same thing is true for ret = ret + result which create a new growing copy for every process resulting in a quadratic execution.
With so many copies in a rather memory-bound operation, this is not surprising for the operation to be slower than the sequential part. The thing is you can hardly do better in Python! Without any convoluted tricks, there is no other way to parallelize the operation in pure-Python. I mean all module have to create either CPython threads (GIL bound) or CPython processes (IPC bound). The GIL cannot be released as long as you work on any CPython objects in multiple threads. The only solution is to use native threads operating on bytes' internal buffer (which does not require the GIL to be locked.
This can be done either using native languages (e.g. C/C++/Rust), Numba or Cython. However, there is another big issue impacting performance to consider: bytes' copies. AFAIK, Numba and Cython prevent you to avoid that. The best you can do is to extract the input memory buffer, write the output in parallel in a native array (not limited by the GIL), and finally then creates a bytes object. The thing is creating this last object take &gt;60% of the time on my machine and there is no way to make it faster because bytes objects are immutable.
A native Python module written in native language can overpass this limit. Technically, this is also possible in Cython by directly calling C API functions and managing object yourself but this is pretty low-level, and in the end, this looks like more a C code than Python one (with additional Cython annotations). Indeed, the CPython API provides a PyBytes_FromStringAndSize function so to creates a bytes object and it allows developers to write in the bytes' buffer only after creating it without associated buffer (i.e. the first parameter must be NULL). This is the only way to avoid an expensive copy.
That being said, note that a new bytes object needs to be created and filling its internal buffer results in a lot of pages faults slowing things a bit. AFAIK, there is no way to avoid that in CPython besides not creating huge buffers. In fact, the computation would be faster if you could process the whole string chunk by chunk (if possible) so to benefit from CPU cache and memory recycling of the standard allocator. Indeed, this can strongly reduce the DRAM pressure and avoid page-faults. The bad news is that if you do the chunk by chunk computation in CPython, then I think you cannot benefit from multiple threads anymore. Indeed, chunks will be too small for multiple threads to really worth it, especially on large-scale servers (where multiple threads are also required to saturate the RAM and the L3 cache). CPython's parallelism simply sucks (especially due to the GIL).
I also found out that base64.b64decode is surprisingly not so efficient. I wrote a faster (but less safe) implementation. There are ways to write a fast and safe implementation (typically thanks to SIMD), but this is complicated and not the purpose of this post. Besides, using multiple threads is enough to make the computation memory-bound on most machines (including mine) so it does not worth it to optimize further the resulting (rather naive) sequential implementation.
Note I used OpenMP so to parallelize the C loop with only 1 line (for large inputs).
Here is the base64.c main file of the fast parallel CPython native module (assuming the input is correctly formatted):
#define PY_SSIZE_T_CLEAN // Required for large bytes objects on 64-bit machines
#include &lt;Python.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;string.h&gt;
#include &lt;assert.h&gt;
#include &lt;omp.h&gt;

int base64_table[256];

// Generate a conversion table for sake of performance
static inline void init_table()
{
    static const unsigned char base64_chars[] = &quot;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/&quot;;

    for (int i = 0; i &lt; 64; ++i)
        base64_table[i] = -1;

    for (int i = 0; i &lt; 64; ++i)
        base64_table[base64_chars[i]] = i;

    base64_table['='] = 0;
}

static inline int decode_char(unsigned char c)
{
    return base64_table[c];
}

// Assume the input is correctly formatted
static PyObject* decode(PyObject* self, PyObject* args)
{
    PyObject* input_obj;

    // Extract the input parameter and check its type
    if(!PyArg_ParseTuple(args, &quot;O!&quot;, &amp;PyBytes_Type, &amp;input_obj))
        return NULL;

    char* input = PyBytes_AS_STRING(input_obj);
    Py_ssize_t input_length = PyBytes_GET_SIZE(input_obj);
    assert(input_length % 4 == 0);

    int padding = 0;
    padding += input_length &gt;= 1 &amp;&amp; input[input_length - 1] == '=';
    padding += input_length &gt;= 2 &amp;&amp; input[input_length - 2] == '=';

    // Assume there is enough memory
    Py_ssize_t output_length = (input_length / 4) * 3 - padding;
    PyObject* output_obj = PyBytes_FromStringAndSize(NULL, output_length);
    assert(output_obj != NULL);
    char* output = PyBytes_AS_STRING(output_obj);
    assert(output != NULL);

    #pragma omp parallel for schedule(guided) if(input_length &gt;= 8*1024*1024)
    for(Py_ssize_t k = 0; k &lt; input_length / 4; ++k)
    {
        const Py_ssize_t i = k * 4;
        const Py_ssize_t j = k * 3;

        const int a = decode_char(input[i]);
        const int b = decode_char(input[i + 1]);
        const int c = decode_char(input[i + 2]);
        const int d = decode_char(input[i + 3]);
        assert(a &gt; 0 &amp;&amp; b &gt; 0 &amp;&amp; c &gt; 0 &amp;&amp; d &gt; 0);

        const int merged = (a &lt;&lt; 18) + (b &lt;&lt; 12) + (c &lt;&lt; 6) + d;

        if(j &lt; output_length) output[j]     = (merged &gt;&gt; 16) &amp; 0xFF;
        if(j &lt; output_length) output[j + 1] = (merged &gt;&gt; 8) &amp; 0xFF;
        if(j &lt; output_length) output[j + 2] = merged &amp; 0xFF;
    }

    return output_obj;
}

static PyMethodDef MyMethods[] = 
{
    {&quot;decode&quot;, decode, METH_VARARGS, &quot;Parallel base64 decoding function.&quot;},
    {NULL, NULL, 0, NULL}
};

static struct PyModuleDef parallel_base64 = 
{
    PyModuleDef_HEAD_INIT,
    &quot;parallel_base64&quot;,
    NULL,
    -1,
    MyMethods
};

PyMODINIT_FUNC PyInit_parallel_base64(void) 
{
    init_table();
    return PyModule_Create(&amp;parallel_base64);
}

and here is the setup.py file to build it:
from setuptools import setup, Extension

module = Extension(
    'parallel_base64', 
    sources=['base64.c'],
    extra_compile_args=['-fopenmp'],
    extra_link_args=['-fopenmp']
)

setup(
    name='parallel_base64',
    version='1.0',
    description='A parallel base64 module written in C',
    ext_modules=[module],
)

You can test it and call it with python setup.py build_ext --inplace. Then you can import it with import parallel_base64 and just call parallel_base64.decode(large_b64_string).

Performance results
Using repeat = 30000000, on my Linux laptop with an AMD Ryzen 7 5700U (configured in performance mode) and Python 3.12.3, I get the following results:
decoding time = 3.6366550829989137

implmementation1
Verify result Ok
decoding time = 3.5178445390010893

threaded_impl
Verify result Ok
decoding time = 9.623698087001685

mutiproc_impl
Verify result Ok
decoding time = 13.102449985999556

c_module_impl
Verify result Ok
decoding time = 0.29033970499949646

We can see that the native parallel implementation is 12.5 times faster using 8 cores. This is because it not only use multiple cores but also benefit from a more efficient computation. The DRAM throughput reaches 23 GiB/s which is pretty good. It should be far enough for data read from a high-end SSD.
Note that if you want to read data from a SSD efficiently, then reading it (all at once) from Python to bytes is inefficient (because of copies and page faults). Memory mapping is generally faster, especially on a high-end SSD. This can be done with mmap on Linux. Note that Numpy provides such a feature (though munmap is missing) and Numpy arrays have the benefit to be mutable so they can be reused many times which might help to avoid page-fault performance issues and enable further optimizations.
In the end, Python is maybe simply not the right too to get good performance for such kind of computation though native modules can help a lot to speed up some specific parts (otherwise, it is not really a Python code anymore).
"
"I am trying to fetch as rows the different values inside each href element from the following website: https://www.bmv.com.mx/es/mercados/capitales
There should be 1 row that matches each field on the provided headers for each different href element on the HTML file.
This is one of the portions of the HTML that I am trying to scrape:

  &lt;tbody&gt;
    
  &lt;tr role=&quot;row&quot; class=&quot;odd&quot;&gt;
&lt;td class=&quot;sorting_1&quot;&gt;&lt;a href=&quot;/es/mercados/cotizacion/1959&quot;&gt;AC
  
&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;span class=&quot;series&quot;&gt;*&lt;/span&gt;
&lt;/td&gt;&lt;td&gt;03:20&lt;/td&gt;&lt;td&gt;&lt;span class=&quot;color-2&quot;&gt;191.04

&lt;/span&gt;&lt;/td&gt;&lt;td&gt;191.32&lt;/td&gt;
&lt;td&gt;194.51&lt;/td&gt;
&lt;td&gt;193.92&lt;/td&gt;
&lt;td&gt;191.01&lt;/td&gt;
&lt;td&gt;380,544&lt;/td&gt;
&lt;td&gt;73,122,008.42&lt;/td&gt;
&lt;td&gt;2,793&lt;/td&gt;
&lt;td&gt;-3.19&lt;/td&gt;&lt;td&gt;-1.64&lt;/td&gt;&lt;/tr&gt;&lt;tr role=&quot;row&quot; class=&quot;even&quot;&gt;
  &lt;td class=&quot;sorting_1&quot;&gt;&lt;a href=&quot;/es/mercados/cotizacion/203&quot;&gt;ACCELSA&lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;&lt;span class=&quot;series&quot;&gt;B&lt;/span&gt;
  &lt;/td&gt;&lt;td&gt;03:20&lt;/td&gt;&lt;td&gt;
    &lt;span class=&quot;&quot;&gt;22.5&lt;/span&gt;&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;
    &lt;td&gt;22.5&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0

    &lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;67.20&lt;/td&gt;
    &lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;
    &lt;tr role=&quot;row&quot; class=&quot;odd&quot;&gt;
      &lt;td class=&quot;sorting_1&quot;&gt;
        &lt;a href=&quot;/es/mercados/cotizacion/6096&quot;&gt;ACTINVR&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;span class=&quot;series&quot;&gt;B&lt;/span&gt;&lt;/td&gt;&lt;td&gt;03:20&lt;/td&gt;&lt;td&gt;
        &lt;span class=&quot;&quot;&gt;15.13&lt;/span&gt;&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;15.13&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;&lt;td&gt;13&lt;/td&gt;&lt;td&gt;196.69&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;
        &lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr role=&quot;row&quot; class=&quot;even&quot;&gt;&lt;td class=&quot;sorting_1&quot;&gt;
          &lt;a href=&quot;/es/mercados/cotizacion/339083&quot;&gt;AGUA&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;&lt;span class=&quot;series&quot;&gt;*&lt;/span&gt;
          &lt;/td&gt;&lt;td&gt;03:20&lt;/td&gt;&lt;td&gt;
            &lt;span class=&quot;color-1&quot;&gt;29&lt;/span&gt;
          &lt;/td&gt;&lt;td&gt;28.98&lt;/td&gt;&lt;td&gt;28.09&lt;/td&gt;
            &lt;td&gt;29&lt;/td&gt;&lt;td&gt;28&lt;/td&gt;&lt;td&gt;296,871&lt;/td&gt;
            &lt;td&gt;8,491,144.74&lt;/td&gt;&lt;td&gt;2,104&lt;/td&gt;&lt;td&gt;0.89&lt;/td&gt;
            &lt;td&gt;3.17&lt;/td&gt;&lt;/tr&gt;&lt;tr role=&quot;row&quot; class=&quot;odd&quot;&gt;&lt;td class=&quot;sorting_1&quot;&gt;
              &lt;a href=&quot;/es/mercados/cotizacion/30&quot;&gt;ALFA&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;span class=&quot;series&quot;&gt;A&lt;/span&gt;&lt;/td&gt;
              &lt;td&gt;03:20&lt;/td&gt;
              &lt;td&gt;&lt;span class=&quot;color-2&quot;&gt;13.48&lt;/span&gt;
              &lt;/td&gt;&lt;td&gt;13.46&lt;/td&gt;
              &lt;td&gt;13.53&lt;/td&gt;&lt;td&gt;13.62&lt;/td&gt;&lt;td&gt;13.32&lt;/td&gt;
              &lt;td&gt;2,706,398&lt;/td&gt;
              td&gt;36,494,913.42&lt;/td&gt;&lt;td&gt;7,206&lt;/td&gt;&lt;td&gt;-0.07&lt;/td&gt;
              &lt;td&gt;-0.52&lt;/td&gt;
            &lt;/tr&gt;&lt;tr role=&quot;row&quot; class=&quot;even&quot;&gt;&lt;td class=&quot;sorting_1&quot;&gt;
              &lt;a href=&quot;/es/mercados/cotizacion/7684&quot;&gt;ALPEK&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;span class=&quot;series&quot;&gt;A&lt;/span&gt;
              &lt;/td&gt;&lt;td&gt;03:20&lt;/td&gt;&lt;td&gt;&lt;span class=&quot;color-2&quot;&gt;10.65&lt;/span&gt;
            &lt;/td&gt;&lt;td&gt;10.64&lt;/td&gt;&lt;td&gt;10.98&lt;/td&gt;&lt;td&gt;10.88&lt;/td&gt;&lt;td&gt;10.53&lt;/td&gt;
            &lt;td&gt;1,284,847&lt;/td&gt;&lt;td&gt;13,729,368.46&lt;/td&gt;&lt;td&gt;6,025&lt;/td&gt;&lt;td&gt;-0.34&lt;/td&gt;
            &lt;td&gt;-3.10&lt;/td&gt;&lt;/tr&gt;&lt;tr role=&quot;row&quot; class=&quot;odd&quot;&gt;&lt;td class=&quot;sorting_1&quot;&gt;
              &lt;a href=&quot;/es/mercados/cotizacion/1729&quot;&gt;ALSEA&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;span class=&quot;series&quot;&gt;*&lt;/span&gt;
            &lt;/td&gt;&lt;td&gt;03:20&lt;/td&gt;&lt;td&gt;&lt;span class=&quot;color-2&quot;&gt;65.08&lt;/span&gt;&lt;/td&gt;&lt;td&gt;64.94&lt;/td&gt;&lt;td&gt;65.44&lt;/td&gt;&lt;td&gt;66.78&lt;/td&gt;&lt;td&gt;64.66&lt;/td&gt;&lt;td&gt;588,826&lt;/td&gt;&lt;td&gt;38,519,244.51&lt;/td&gt;&lt;td&gt;4,442&lt;/td&gt;&lt;td&gt;-0.5&lt;/td&gt;&lt;td&gt;-0.76&lt;/td&gt;&lt;/tr&gt;
            &lt;tr role=&quot;row&quot; class=&quot;even&quot;&gt;&lt;td class=&quot;sorting_1&quot;&gt;
              &lt;a href=&quot;/es/mercados/cotizacion/424518&quot;&gt;ALTERNA&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;span class=&quot;series&quot;&gt;B&lt;/span&gt;&lt;/td&gt;&lt;td&gt;03:20&lt;/td&gt;&lt;td&gt;&lt;span class=&quot;&quot;&gt;1.5&lt;/span&gt;&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1.5&lt;/td&gt;
              &lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr role=&quot;row&quot; class=&quot;odd&quot;&gt;&lt;td class=&quot;sorting_1&quot;&gt;
              &lt;a href=&quot;/es/mercados/cotizacion/1862&quot;&gt;AMX&lt;/a&gt;&lt;/td&gt;
              &lt;td&gt;&lt;span class=&quot;series&quot;&gt;B&lt;/span&gt;&lt;/td&gt;&lt;td&gt;03:20&lt;/td&gt;
              &lt;td&gt;&lt;span class=&quot;color-2&quot;&gt;14.56&lt;/span&gt;&lt;/td&gt;&lt;td&gt;14.58&lt;/td&gt;
              &lt;td&gt;14.69&lt;/td&gt;&lt;td&gt;14.68&lt;/td&gt;&lt;td&gt;14.5&lt;/td&gt;&lt;td&gt;86,023,759&lt;/td&gt;
              &lt;td&gt;1,254,412,623.59&lt;/td&gt;&lt;td&gt;41,913&lt;/td&gt;&lt;td&gt;-0.11&lt;/td&gt;
              &lt;td&gt;-0.75&lt;/td&gt;&lt;/tr&gt;&lt;tr role=&quot;row&quot; class=&quot;even&quot;&gt;
                &lt;td class=&quot;sorting_1&quot;&gt;&lt;a href=&quot;/es/mercados/cotizacion/6507&quot;&gt;ANGELD&lt;/a&gt;
              &lt;/td&gt;&lt;td&gt;&lt;span class=&quot;series&quot;&gt;10&lt;/span&gt;&lt;/td&gt;&lt;td&gt;03:20&lt;/td&gt;&lt;td&gt;
                &lt;span class=&quot;color-2&quot;&gt;21.09&lt;/span&gt;
              &lt;/td&gt;&lt;td&gt;21.1&lt;/td&gt;&lt;td&gt;21.44&lt;/td&gt;&lt;td&gt;21.23&lt;/td&gt;&lt;td&gt;21.09&lt;/td&gt;
              &lt;td&gt;51,005&lt;/td&gt;&lt;td&gt;1,076,281.67&lt;/td&gt;
              &lt;td&gt;22&lt;/td&gt;&lt;td&gt;-0.34&lt;/td&gt;&lt;td&gt;-1.59&lt;/td&gt;&lt;/tr&gt;
      &lt;/tbody&gt;

And my current code results into an empty dataframe:
# create empty pandas dataframe
import pandas as pd
import requests
from bs4 import BeautifulSoup


# get response code from webhost
page = requests.get('https://www.bmv.com.mx/es/mercados/capitales')
soup = BeautifulSoup(page.text, 'lxml')
#print(soup.p.text)
# yet it doesn't bring the expected rows!

print('Read html!')

# get headers

tbody = soup.find(&quot;thead&quot;)
tr = tbody.find_all(&quot;tr&quot;)

headers= [t.get_text().strip().replace('\n', ',').split(',') for t in tr][0]

#print(headers)

df = pd.DataFrame(columns=headers)

# fetch rows into pandas dataframe# You can find children with multiple tags by passing a list of strings
rows = soup.find_all('tr', {&quot;role&quot;:&quot;row&quot;})
#rows

for row in rows:
    cells = row.findChildren('td')
    for cell in cells:
        value = cell.string

        #print(&quot;The value in this cell is %s&quot; % value)

        # append row in dataframe


I would like to know if it's possible to get a pandas dataframe whose fields are the ones portrayed in the headers list and the rows are each element from href.
For better perspective, the expected output should be equal to the table at the bottom of the provided website. Whose first row has the next schema:
EMISORA SERIE   HORA    ÃšLTIMO   PPP    ANTERIOR    MÃXIMO  MÃNIMO VOLUMEN  IMPORTE OPS.    VAR PUNTOS  VAR %
AC        *    3:20    191.04   191.32  194.51     193.92   191.01  380,544  73,122,008.42   2,793  -3.19    -1.64


Is this possible to create such dataset?
","As mentioned before, the table is loaded and rendered dynamically via JavaScript, something you could not handle with requests because it just get the static response and does not behave like a browser.
A solution to mimic a browsers behaviour is given by @thetaco using selenium but you could get your goal also with requests while using the source the data comes from.

Get the request url use your browsers dev tools to inspect the network traffic in this example it is: https://www.bmv.com.mx/es/Grupo_BMV/BmvJsonGeneric?idSitioPagina=4

Extract the string from the response (it is not valid JSON)
requests.get('https://www.bmv.com.mx/es/Grupo_BMV/BmvJsonGeneric?idSitioPagina=4').text.split(';(', 1)[-1].split(')')[0]


Convert the string into JSON (json.loads()) and tranform it with pandas.json_normalize() into a dataframe. Your data is under the path ['response']['resultado']['A']

The column names may differ a bit because they are build on the keyes from the JSON but they could be easily mapped.


The response contains all content, including that of the other groups (ACCIONES, CKD'S, FIBRAS, TÍTULOS OPCIONALES) which can also be extracted (A, CKDS, F, TO) would be the abbreviations that can be used analogously for the selection.
Example (all available information for ACCIONES from XHR Request)
import json, requests
import pandas as pd

df = pd.json_normalize(
    json.loads(
        requests.get('https://www.bmv.com.mx/es/Grupo_BMV/BmvJsonGeneric?idSitioPagina=4')\
            .text\
            .split(';(', 1)[-1]\   
            .split(')')[0]
        )['response']['resultado']['A']
)\
.dropna(axis=1, how='all')





idEmision
idTpvalor
cveSerie
cveCorta
idEmisora
datosEstadistica.hora
datosEstadistica.maximo
datosEstadistica.minimo
datosEstadistica.importeAcomulado
datosEstadistica.noOperaciones
datosEstadistica.variacionPuntos
datosEstadistica.variacionPorcentual
datosEstadistica.precioUltimoHecho
datosEstadistica.ppp
datosEstadistica.precioAnterior
datosEstadistica.volumenOperado
datosEstadistica.anioEjercicio
datosEstadistica.insumosPu




0
1959
1
*
AC
6081
03:20
192.98
189.01
9.54831e+07
3333
-2.59
-1.35
189.3
189.32
191.91
502297
0
0


1
203
1
B
ACCELSA
5015
03:20
0
0
22.4
1
0
0
22.5
0
22.5
1
0
0


...




















103
404833
1B
19
VMEX
34347
03:20
45.29
45.29
11007.9
8
0.14
0.31
45.29
0
45.15
243
0
0


104
327336
1
A
VOLAR
30023
03:20
12.76
12.42
1.5744e+07
5006
0.24
1.93
12.67
12.68
12.44
1246397
0
0


105
5
1
*
WALMEX
5214
03:20
70.37
67.83
1.21326e+09
19593
-2.02
-2.86
68.7
68.72
70.74
17639588
0
0




Coming closer to your result, you could post process the dataframe to your needs:
import re
# exclude all columns referencing an id information
df = df.loc[:, ~df.columns.str.startswith('id')]
# adjust the column names
df.columns = [re.sub(r&quot;(?&lt;=\w)([A-Z])&quot;, r&quot; \1&quot;, c).split('.')[-1].lstrip('cve').upper() for c in df.columns]
df




SERIE
CORTA
HORA
MAXIMO
MINIMO
IMPORTE ACOMULADO
NO OPERACIONES
ARIACION PUNTOS
ARIACION PORCENTUAL
PRECIO ULTIMO HECHO
PPP
PRECIO ANTERIOR
OLUMEN OPERADO
ANIO EJERCICIO
INSUMOS PU




*
AC
03:20
191.17
187.8
1.14863e+08
4175
0.64
0.34
189.65
189.96
189.32
604632
0
0


B
ACTINVR
03:20
15.03
15.03
36614.4
14
0
0
15.03
0
15.03
2436
0
0


...
















A
VOLAR
03:20
12.97
12.51
1.48613e+07
2832
0.07
0.55
12.83
12.75
12.68
1162684
0
0


*
WALMEX
03:20
69.03
67.66
7.2698e+08
22462
-0.71
-1.03
68
68.01
68.72
10672270
0
0



or simply map against the columns, to get exact column names:
map_dict = {'cveSerie':'SERIE', 'cveCorta':'EMISORA', 'datosEstadistica.hora':'HORA', 'datosEstadistica.maximo':'MÁXIMO',
       'datosEstadistica.minimo':'MÍNIMO', 'datosEstadistica.importeAcomulado':'IMPORTE', 'datosEstadistica.noOperaciones':'OPS.', 'datosEstadistica.variacionPuntos':'VAR PUNTOS',
       'datosEstadistica.variacionPorcentual':'VAR %', 'datosEstadistica.precioUltimoHecho':'ÚLTIMO', 'datosEstadistica.ppp':'PPP',
       'datosEstadistica.precioAnterior':'ANTERIOR', 'datosEstadistica.volumenOperado':'VOLUMEN'}
df.loc[:,[c for c in df.columns if c in map_dict.keys()]].rename(columns=map_dict)




SERIE
EMISORA
HORA
MÁXIMO
MÍNIMO
IMPORTE
OPS.
VAR PUNTOS
VAR %
ÚLTIMO
PPP
ANTERIOR
VOLUMEN




*
AC
03:20
191.17
187.8
1.14863e+08
4175
0.64
0.34
189.65
189.96
189.32
604632



...
"
"I am using pandas version 1.0.5
The example dataframe below lists time intervals, recorded over three days, and I seek where some time intervals overlap every day.

For example,
one of the overlapping time across all the three dates (yellow highlighted) is 1:16 - 2:13. The other (blue highlighted) would be 18:45 - 19:00
So my expected output would be like: [57,15] because

57 - Minutes between 1:16 - 2:13.
15 - Minutes between 18:45 - 19:00

Please use this generator of the input dataframe:
import pandas as pd
dat1 = [
    ['2023-12-27','2023-12-27 00:00:00','2023-12-27 02:14:00'],
    ['2023-12-27','2023-12-27 03:16:00','2023-12-27 04:19:00'],
    ['2023-12-27','2023-12-27 18:11:00','2023-12-27 20:13:00'],
    ['2023-12-28','2023-12-28 01:16:00','2023-12-28 02:14:00'],
    ['2023-12-28','2023-12-28 02:16:00','2023-12-28 02:28:00'],
    ['2023-12-28','2023-12-28 02:30:00','2023-12-28 02:56:00'],
    ['2023-12-28','2023-12-28 18:45:00','2023-12-28 19:00:00'],
    ['2023-12-29','2023-12-29 01:16:00','2023-12-29 02:13:00'],
    ['2023-12-29','2023-12-29 04:16:00','2023-12-29 05:09:00'],
    ['2023-12-29','2023-12-29 05:11:00','2023-12-29 05:14:00'],
    ['2023-12-29','2023-12-29 18:00:00','2023-12-29 19:00:00']
       ]
df = pd.DataFrame(dat1,columns = ['date','Start_tmp','End_tmp'])
df[&quot;Start_tmp&quot;] = pd.to_datetime(df[&quot;Start_tmp&quot;])
df[&quot;End_tmp&quot;] = pd.to_datetime(df[&quot;End_tmp&quot;])

","This solution uses:

numpy, no uncommon Python modules, so using Python 1.0.5 you should, hopefully, be in the clear,
no nested loops to care for speed issues with growing dataset,

Method:

Draw the landscape of overlaps
Then select the overlaps corresponding to the number of documented days,
Finally describe the overlaps in terms of their lengths

Number of documented days: (as in Python: Convert timedelta to int in a dataframe)
n = 1 + ( max(df['End_tmp']) - min(df['Start_tmp']) ).days
n
3

Additive landscape:
# initial flat whole-day landcape (height: 0)
L = np.zeros(24*60, dtype='int')
# add up ranges: (reused @sammywemmy's perfect formula for time of day in minutes)
for start, end in zip(df['Start_tmp'].dt.hour.mul(60) + df['Start_tmp'].dt.minute,  # Start_tmp timestamps expressed in minutes
                      df['End_tmp'].dt.hour.mul(60)   + df['End_tmp'].dt.minute):   # End_tmp timestamps expressed in minutes
    L[start:end+1] += 1

plt.plot(L)
plt.hlines(y=[2,3],xmin=0,xmax=1400,colors=['green','red'], linestyles='dashed')
plt.xlabel('time of day (minutes)')
plt.ylabel('time range overlaps')


(Please excuse the typo: these are obviously minutes, not seconds)
Keep only overlaps over all days: (red line, n=3)
# Reduce heights &lt;n to 0 because not overlaping every day
L[L&lt;n]=0
# Simplify all greater values to 1 because only their presence matters
L[L&gt;0]=1
# Now only overlaps are highlighted
# (Actually this latest line is disposable, provided we filter all but the overlaps of rank n. Useful only if you were to include lower overlaps)

Extract overlap ranges and their lengths
# Highlight edges of overlaping intervals
D = np.diff(L)
# Describe overlaps as ranges
R = list(zip([a[0]   for a in np.argwhere(D&gt;0)],  # indices where overlaps *begin*, with scalar indices instead of arrays
             [a[0]-1 for a in np.argwhere(D&lt;0)])) # indices where overlaps *end*, with scalar indices instead of arrays
R
[(75, 132), (1124, 1139)]

# Finally their lengths
[b-a for a,b in R]

Final output: [57, 15]
"
"I have a Polars DataFrame containing a column with strings representing 'sparse' sector exposures, like this:
df = pl.DataFrame(
    pl.Series(&quot;sector_exposure&quot;, [
        &quot;Technology=0.207;Financials=0.090;Health Care=0.084;Consumer Discretionary=0.069&quot;, 
        &quot;Financials=0.250;Health Care=0.200;Consumer Staples=0.150;Industrials=0.400&quot;
    ])
)




sector_exposure




Technology=0.207;Financials=0.090;Health Care=0.084;Consumer Discretionary=0.069


Financials=0.250;Health Care=0.200;Consumer Staples=0.150;Industrials=0.400



I want to &quot;unpack&quot; this string into new columns for each sector (e.g., Technology, Financials, Health Care) with associated values or a polars struct with sector names as fields and exposure values.
I'm looking for a more efficient solution using polars expressions only, without resorting to Python loops (or python mapped functions). Can anyone provide guidance on how to accomplish this?
This is what I have come up with so far - which works in producing the desired struct but is a little slow.
(
    df[&quot;sector_exposure&quot;]
    .str
    .split(&quot;;&quot;)
    .map_elements(lambda x: {entry.split('=')[0]: float(entry.split('=')[1]) for entry in x},
                  skip_nulls=True,
                  )
)

Output:
shape: (2,)
Series: 'sector_exposure' [struct[6]]
[
    {0.207,0.09,0.084,0.069,null,null}
    {null,0.25,0.2,null,0.15,0.4}
]

Thanks!
","There are potentially two ways to do it that I can think of.
Regex extract
df.with_columns(pl.col('sector_exposure').str.extract(x+r&quot;=(\d+\.\d+)&quot;).cast(pl.Float64).alias(x) 
                for x in [&quot;Technology&quot;, &quot;Financials&quot;, &quot;Health Care&quot;, &quot;Consumer Discretionary&quot;,
                          &quot;Consumer Staples&quot;,&quot;Industrials&quot;])

shape: (2, 7)
┌────────────────┬────────────┬────────────┬─────────────┬────────────────┬──────────┬─────────────┐
│ sector_exposur ┆ Technology ┆ Financials ┆ Health Care ┆ Consumer       ┆ Consumer ┆ Industrials │
│ e              ┆ ---        ┆ ---        ┆ ---         ┆ Discretionary  ┆ Staples  ┆ ---         │
│ ---            ┆ f64        ┆ f64        ┆ f64         ┆ ---            ┆ ---      ┆ f64         │
│ str            ┆            ┆            ┆             ┆ f64            ┆ f64      ┆             │
╞════════════════╪════════════╪════════════╪═════════════╪════════════════╪══════════╪═════════════╡
│ Technology=0.2 ┆ 0.207      ┆ 0.09       ┆ 0.084       ┆ 0.069          ┆ null     ┆ null        │
│ 07;Financials= ┆            ┆            ┆             ┆                ┆          ┆             │
│ 0.090;Health   ┆            ┆            ┆             ┆                ┆          ┆             │
│ Care=0.084;Con ┆            ┆            ┆             ┆                ┆          ┆             │
│ sumer Discreti ┆            ┆            ┆             ┆                ┆          ┆             │
│ onary=0.069    ┆            ┆            ┆             ┆                ┆          ┆             │
│ Financials=0.2 ┆ null       ┆ 0.25       ┆ 0.2         ┆ null           ┆ 0.15     ┆ 0.4         │
│ 50;Health Care ┆            ┆            ┆             ┆                ┆          ┆             │
│ =0.200;Consume ┆            ┆            ┆             ┆                ┆          ┆             │
│ r Staples=0.15 ┆            ┆            ┆             ┆                ┆          ┆             │
│ 0;Industrials= ┆            ┆            ┆             ┆                ┆          ┆             │
│ 0.400          ┆            ┆            ┆             ┆                ┆          ┆             │
└────────────────┴────────────┴────────────┴─────────────┴────────────────┴──────────┴─────────────┘

In this one we're counting on all the numbers being decimal (you could tweak the regex to get around this a bit) and all the sectors being prespecified in the generator within with_columns
Split and pivot
(
    df
    .with_columns(str_split=pl.col('sector_exposure').str.split(';'))
    .explode('str_split')
    .with_columns(
        pl.col('str_split')
        .str.split('=')
        .list.to_struct(fields=['sector','value'])
        )
    .unnest('str_split')
    .pivot(values='value',index='sector_exposure',columns='sector',aggregate_function='first')
    .with_columns(pl.exclude('sector_exposure').cast(pl.Float64))
    )
shape: (2, 7)
┌────────────────┬────────────┬────────────┬─────────────┬────────────────┬──────────┬─────────────┐
│ sector_exposur ┆ Technology ┆ Financials ┆ Health Care ┆ Consumer       ┆ Consumer ┆ Industrials │
│ e              ┆ ---        ┆ ---        ┆ ---         ┆ Discretionary  ┆ Staples  ┆ ---         │
│ ---            ┆ f64        ┆ f64        ┆ f64         ┆ ---            ┆ ---      ┆ f64         │
│ str            ┆            ┆            ┆             ┆ f64            ┆ f64      ┆             │
╞════════════════╪════════════╪════════════╪═════════════╪════════════════╪══════════╪═════════════╡
│ Technology=0.2 ┆ 0.207      ┆ 0.09       ┆ 0.084       ┆ 0.069          ┆ null     ┆ null        │
│ 07;Financials= ┆            ┆            ┆             ┆                ┆          ┆             │
│ 0.090;Health   ┆            ┆            ┆             ┆                ┆          ┆             │
│ Care=0.084;Con ┆            ┆            ┆             ┆                ┆          ┆             │
│ sumer Discreti ┆            ┆            ┆             ┆                ┆          ┆             │
│ onary=0.069    ┆            ┆            ┆             ┆                ┆          ┆             │
│ Financials=0.2 ┆ null       ┆ 0.25       ┆ 0.2         ┆ null           ┆ 0.15     ┆ 0.4         │
│ 50;Health Care ┆            ┆            ┆             ┆                ┆          ┆             │
│ =0.200;Consume ┆            ┆            ┆             ┆                ┆          ┆             │
│ r Staples=0.15 ┆            ┆            ┆             ┆                ┆          ┆             │
│ 0;Industrials= ┆            ┆            ┆             ┆                ┆          ┆             │
│ 0.400          ┆            ┆            ┆             ┆                ┆          ┆             │
└────────────────┴────────────┴────────────┴─────────────┴────────────────┴──────────┴─────────────┘

In this one you do a &quot;round&quot; of splitting at the semi colon and then explode. Then you split again on the equal but you turn that into a struct which you then unnest. From there you pivot the sectors up to columns.
If the sectors existed in the same order then you could use str.extract_groups but with varying orders I don't think it works.
"
"I am trying to write a Polars DataFrame to a duckdb database. I have the following simple code which I expected to work:
import polars as pl
import duckdb

pldf = pl.DataFrame({'mynum': [1,2,3,4]})
with duckdb.connect(database=&quot;scratch.db&quot;, read_only=False) as con:
    pldf.write_database(table_name='test_table', connection=con)

However, I get the following error:
sqlalchemy.exc.ArgumentError: Expected string or URL object, got &lt;duckdb.duckdb.DuckDBPyConnection object

I get a similar error if I use the non-default engine='adbc' instead of df.write_database()'s default engine='sqlalchemy'.
So it seemed it should be easy enough to just swap in a URI for my ducdkb database, but I haven't been able to get that to work either. Potentially it's complicated by my being on Windows?
","In-memory database. If you just want to use DuckDB to query a polars dataframe, this can simply be achieved as long as the table exists in the current scope.
duckdb.sql(&quot;SELECT * FROM df&quot;).show()

Persistent database If you want to use a persistent database, you could install duckdb-engine and write the database using the connection URI string.
df.write_database(
    table_name='test_table',
    connection=&quot;duckdb:///scratch.db&quot;,
)

Reading the data back in using DuckDB works as usual.
with duckdb.connect(database=&quot;scratch.db&quot;, read_only=False) as con:
    con.query(&quot;SELECT * FROM test_table&quot;).show()

┌───────┐
│ mynum │
│ int64 │
├───────┤
│     1 │
│     2 │
│     3 │
│     4 │
└───────┘

"
"I created this program to calculate the sha256 or sha512 hash of a given file and digest calculations to hex.
It consists of 5 files, 4 are custom modules and 1 is the main.
I have two functions in different modules but the only difference in these functions is one variable. See below:
From sha256.py
def get_hash_sha256():
    global sha256_hash
    filename = input(&quot;Enter the file name: &quot;)
    sha256_hash = hashlib.sha256()
    with open(filename, &quot;rb&quot;) as f:
        for byte_block in iter(lambda: f.read(4096),b&quot;&quot;):
            sha256_hash.update(byte_block)
#       print(&quot;sha256 valule: \n&quot; + Color.GREEN + sha256_hash.hexdigest())
        print(Color.DARKCYAN + &quot;sha256 value has been calculated&quot;)
        color_reset()

From sha512.py
def get_hash_sha512():
    global sha512_hash
    filename = input(&quot;Enter the file name: &quot;)
    sha512_hash = hashlib.sha512()
    with open(filename, &quot;rb&quot;) as f:
        for byte_block in iter(lambda: f.read(4096),b&quot;&quot;):
            sha512_hash.update(byte_block)
#       print(&quot;sha512 valule: \n&quot; + Color.GREEN + sha512_hash.hexdigest())
        print(Color.DARKCYAN + &quot;sha512 value has been calculated&quot;)
        color_reset()

These functions are called in my simple_sha_find.py file:
def which_hash():
    sha256_or_sha512 = input(&quot;Which hash do you want to calculate: sha256 or sha512? \n&quot;)
    if sha256_or_sha512 == &quot;sha256&quot;:
        get_hash_sha256()
        verify_checksum_sha256()
    elif sha256_or_sha512 == &quot;sha512&quot;:
        get_hash_sha512()
        verify_checksum_sha512()
    else:
        print(&quot;Type either sha256 or sha512. If you type anything else the program will close...like this.&quot;)
        sys.exit()

if __name__ == &quot;__main__&quot;:
    which_hash()

As you can see, the functions that will be called are based on the users input. If the user types sha256, then it triggers the functions from sha256.py, but if they type sha512 then they trigger the functions from sha512.py
The application works, but I know I can make it less redundant but I do not know how.
How can I define the get_hash_sha---() and verify_checksum_sha---() functions once and they perform the appropriate calculations based on whether the user chooses sha256 or sha512?
I have performed a few variations of coding this program.
I have created it as one single file as well as creating different modules and calling functions from these modules.
In either case I've had the repetition but I know that tends to defeat the purpose of automation.
","You could union these 2 functions into a single one:
import hashlib

def get_hash(hash_type):
    if hash_type == 'sha256':
        hash_obj= hashlib.sha256()
    elif hash_type == 'sha512':
        hash_obj = hashlib.sha512()
    else:
        print(&quot;Invalid hash type.Please choose 'sha256'or'sha512'&quot;)
        return

    filename = input(&quot;Enter the fileename:  &quot;)
    try:
        with open(filename,&quot;rb&quot;) as f:
            for byte_block in iter(lambda: f.read(4096), b&quot;&quot;):
                hash_obj.update(byte_block)
        print(Color.DARKCYAN + f&quot;{hash_type} value has been calculated&quot;)
        color_reset()
    except FileNotFoundError:
        print(f&quot;File '{filename}' not found.&quot;)

def which_hash():
    sha_type =input(&quot;Which hash do you want to calculate: sha256 or sha512? \n&quot;).lower()
    if sha_type in ['sha256', 'sha512']:
        get_hash(sha_type)
        verify_checksum(sha_type)
    else:
        print(&quot;Type sha256 or sha512. If you type anything else program will close. .&quot;)
        sys.exit()

if __name__ == &quot;__main__&quot;:
    which_hash() 

Also its a best practice to use Enum instead of plain text:
from enum import Enum

class HashType(Enum):
    SHA256 = 'sha256'
    SHA512 = 'sha512'

So you could change
if hash_type == HashType.SHA256:
    hash_obj = hashlib.sha256()
elif hash_type == HashType.SHA512:
    hash_obj = hashlib.sha512()

def which_hash():
    sha_type_input = input(&quot;Which hash do you want to calculate: sha256 or sha512? \n&quot;).lower()
    
    try:
        sha_type = HashType(sha_type_input)
        get_hash(sha_type)
        verify_checksum(sha_type)
    except ValueError:
        print(&quot;Type either sha256 or sha512. If you type anything else the program will close.&quot;)
        sys.exit()

"
"I'm trying to reorder the columns in a Polars dataframe and put 5 columns out of 100 first (the document must unfortunately be somewhat readable in excel). I can't seem to find an easy way to do this.
Ideally, I'd like something simple like
df.select(
    'col2',
    'col1',
    r'^.*$',  # the rest of the columns, but this throws a duplicate column name error
)

Negative lookahead is not supported so it's not possible to make a regex that excludes my selected columns. I could make two overlapping selections, drop the columns from one selection, and then join them, but this does not seem like it would be the intended way to do this. Every other solution I've found involves explicitly naming every single column, which I'm trying to avoid as the columns get added or change names somewhat frequently.
","You can combine pl.exclude with the walrus operator.
Suppose you have something like
df=pl.DataFrame(
    [
        pl.Series('c', [1, 2, 3], dtype=pl.Int64),
        pl.Series('b', [2, 3, 4], dtype=pl.Int64),
        pl.Series('fcvem', [4, 5, 6], dtype=pl.Int64),
        pl.Series('msoy', [4, 5, 6], dtype=pl.Int64),
        pl.Series('smrn', [4, 5, 6], dtype=pl.Int64),
        pl.Series('z', [4, 5, 6], dtype=pl.Int64),
        pl.Series('wxous', [4, 5, 6], dtype=pl.Int64),
        pl.Series('uusn', [4, 5, 6], dtype=pl.Int64),
        pl.Series('ydj', [4, 5, 6], dtype=pl.Int64),
        pl.Series('squr', [4, 5, 6], dtype=pl.Int64),
        pl.Series('yyx', [4, 5, 6], dtype=pl.Int64),
        pl.Series('nl', [4, 5, 6], dtype=pl.Int64),
        pl.Series('a', [0, 1, 2], dtype=pl.Int64),
    ]
)

and you want the first 3 columns to be 'a', 'b', 'c'. You can do:
df.select(*(start_cols:=['a','b','c']), pl.exclude(start_cols))

This creates a list called start_cols which contains 'a','b','c'. The asterisk unwraps the list and then pl.exclude uses the contents of start_cols to tell polars to return everything except start_cols.
If you prefer, you could do this syntax instead:
df.select((start_cols:=['a','b','c'])+ [pl.exclude(start_cols)])

"
"import polars as pl
import numpy as np

df_sim = pl.DataFrame({
   &quot;daily_n&quot;: [1000, 2000, 3000, 4000],
   &quot;prob&quot;: [.5, .5, .5, .6],
   &quot;size&quot;: 1
   })

df_sim = df_sim.with_columns(
  pl.struct([&quot;daily_n&quot;, &quot;prob&quot;, &quot;size&quot;])
  .map_elements(lambda x: 
      np.random.binomial(n=x['daily_n'], p=x['prob'], size=x['size']))
  .cast(pl.Int32)
  .alias('events')
  )

df_sim


However the following code would fail with the message
&quot;TypeError: float() argument must be a string or a number, not 'Expr'&quot;
df_sim.with_columns(
  np.random.binomial(n=col('daily_n'), p=col('prob'), size=col('size'))
  .alias('events')
  )

Why do some functions require use of struct(), map_elements() and lambda, while others do not?
In my case below I am able to simply refer to polars columns as function arguments by using col().
def local_double(x):
  return(2*x)

df_ab.with_columns(rev_2x = local_double(col(&quot;revenue&quot;)))


","Let's go back to what a context is/does.
polars DataFrames (or LazyFrame) have contexts which is just a generic way of referring to with_columns, select, agg, and group_by. The inputs to contexts are Expressions. To a limited extent, the python side of polars can convert python objects into polars expressions. For example a datetime or an int are easily converted to a polars expression and so when you input col('a')*2. It converts that into an expression of col('a').mul(lit(2)).
Functions that return expressions:
Here's your function with type annotations.
def local_double(x: pl.Expr) -&gt; pl.Expr:
  return(2*x)

It takes an Expr as input and returns another Expr as output. It doesn't do any work, it just gives polars a new Expr. Using this function is the same as doing df_ab.with_columns(rev_2x = 2*col(&quot;revenue&quot;)). In fact, polars isn't doing anything with your function when you do df_ab.with_columns(rev_2x = local_double(col(&quot;revenue&quot;))) because the order of operations by python is going to resolve your function so that python can give polars its output as an input to polars' context.
Why do we need map_batches and map_elements
Remember that polars contexts are expecting expressions. One of the reasons polars is so fast and efficient is that behind the API is its own query language and processing engine. That language speaks in expressions. To &quot;translate&quot; from python that it doesn't already know you have to use one of the map_* functions. What they do is convert your expression into values. In the case of map_batches it will give the whole pl.Series to whatever function you choose all at once. In the case of map_elements it will give the function one python value at a time. They are the translation layer so that polars can interact with arbitrary functions.
Why do we need to wrap columns in struct?
Polars is designed to operate multiple expressions in parallel. That means that each expression doesn't know what any other expression is doing. As a side effect of this it means that no expression can be the input of another expression in the same context. This may seem like a limiting design but it's really not because of structs. They are a type of column which can contain multiple columns in one.
If you're going to use a function that needs multiple inputs from your DataFrame then they give the way of converting multiple columns into just one expression. If you only need one column from your DataFrame to be handed to your function then you don't need to wrap it in a struct.
(bonus) Besides functions that return Exprs are there other times we don't need map_*?
Yes. Numpy has what they call Universal Functions, or ufunc. You can use a ufunc directly in a context giving it your col('a'), col('b') directly as inputs. For example, you can do
df.with_columns(log_a = np.log(pl.col('a')))

and it'll just work. You can even make your own ufunc with numba which will also just work. The mechanism behind why ufuncs just work is actually the same as Functions that return expressions, but with more hidden steps. When a ufunc gets an input that isn't an np.array, instead of raising an error (as you got with np.random.binomial), it will check if the input has __array_ufunc__ as a method. If it does then it'll run that method. polars implements that method in pl.Expr so the above gets converted into
df.with_columns(log_a = pl.col('a').map_batches(np.log))

If you have a ufunc that takes multiple inputs, it will even convert all of those inputs into a struct automatically.
Why do you need to use lambda sometimes?
You don't ever need lambda, it's just a way to make and use a function in one line. Instead of your example we could do this instead
def binomial_elements(x: dict) -&gt; float:
    return np.random.binomial(n=x['daily_n'], p=x['prob'], size=x['size'])


df_sim.with_columns(
  pl.struct([&quot;daily_n&quot;, &quot;prob&quot;, &quot;size&quot;])
  .map_elements(binomial_elements)
  .cast(pl.Int32)
  .alias('events')
  )

(bonus) When to use map_elements and when to use map_batches?
Spoiler alert: Your example should be map_batches
Anytime you're dealing with a vectorized function, map_batches is the better choice. I believe most (if not all) of numpy's functions are vectorized, as are scipy's. As such, your example would be more performant as:
def binomial_batches(x: pl.Series) -&gt; np.array:
    return np.random.binomial(n=x.struct['daily_n'], p=x.struct['prob'])


df_sim.with_columns(
  pl.struct(&quot;daily_n&quot;, &quot;prob&quot;)
  .map_batches(binomial_batches)
  .cast(pl.Int32)
  .alias('events')
  )

Notice that I took out the size parameter because numpy infers the output size from the size of daily_n and prob.
Also, when you do map_batches on the Expr, it becomes a Series rather than a dict. To access the individual fields within the struct Series, you need to use the .struct namespace so that's a bit different syntax to be aware of between map_elements and map_batches.
You could also do this as a lambda like
df_sim.with_columns(
  pl.struct(&quot;daily_n&quot;, &quot;prob&quot;)
  .map_batches(lambda x: np.random.binomial(n=x.struct['daily_n'], p=x.struct['prob']))
  .cast(pl.Int32)
  .alias('events')
  )

One last overlooked thing about map_batches
The function that you give map_batches is supposed to return a pl.Series except for in the above it returns an np.array. polars has pretty good interoperability with numpy so it's able to automatically convert the np.array into a pl.Series. One area where you might get tripped up is if you're using pyarrow.compute functions. Polars won't automatically convert that to pl.Series so you'd need to explicitly do it.
As an aside:
I made this gist of a decorator which will, in principle, take any function and make it look for the __array_ufunc__ method of inputs so that you don't have to use map_*. I say &quot;in principle&quot; because I haven't tested it extensively so don't want to over hype it.
A note on np.random.binomial (response to comment)
There are 2+1 modes of binomial (and really many np functions). What do I mean 2+1?

You can give it a single value in each of n and p and then give it a size to get a 1d array with a length of size. This is essentially what your map_elements approach is doing

You can give it an array for n or p and nothing for size then it'll give you a 1d array matching the size of the array you gave it for n. This is what the map_batches approach is doing.

(the +1) You can combine the previous two modes and give it an array for n, p, and for size you give it a tuple where the first element is the number of simulations for each n and p with the second element of the tuple being the length of n and p. With that, it'll give you a 2d array with rows equal to the number of simulations and columns for each of the input length.


You can get that 3rd mode in polars as long as you transpose to fit polars. That would look like this:
df_sim.with_columns(
  pl.struct(&quot;daily_n&quot;, &quot;prob&quot;)
  .map_batches(lambda x: (
      np.random.binomial(
          n=x.struct['daily_n'], 
          p=x.struct['prob'], 
          size=(3,x.shape[0])
          ).transpose()
      )
  )
  .alias('events')
  )

shape: (4, 4)
┌─────────┬──────┬──────┬────────────────────┐
│ daily_n ┆ prob ┆ size ┆ events             │
│ ---     ┆ ---  ┆ ---  ┆ ---                │
│ i64     ┆ f64  ┆ i32  ┆ list[i64]          │
╞═════════╪══════╪══════╪════════════════════╡
│ 1000    ┆ 0.5  ┆ 1    ┆ [491, 493, 482]    │
│ 2000    ┆ 0.5  ┆ 1    ┆ [1032, 966, 972]   │
│ 3000    ┆ 0.5  ┆ 1    ┆ [1528, 1504, 1483] │
│ 4000    ┆ 0.6  ┆ 1    ┆ [2401, 2422, 2367] │
└─────────┴──────┴──────┴────────────────────┘

"
"This is a sister question to How to set DEFAULT ON UPDATE CURRENT_TIMESTAMP in mysql with sqlalchemy?, but focused on Postgres instead of MySQL.
Say we want to create a table users with a column datemodified that updates by default to the current timestamp whenever a row is updated. The solution given in the sister PR for MySQL is:
user = Table(
    &quot;users&quot;,
    Metadata,
    Column(
        &quot;datemodified&quot;,
        TIMESTAMP,
        server_default=text(&quot;CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP&quot;),
    ),
)

How can I get the same functionality with a Postgres backend?
","Eventually I implemented this using triggers as suggested by a_horse_with_no_name in the comments. Full SQLAlchemy implementation and integration with Alembic follow.
SQLAlchemy implementation
# models.py

class User(Base):
    __tablename__ = &quot;user&quot;
    id = Column(Integer, primary_key=True)
    name = Column(Text)
    created_at = Column(DateTime, server_default=sqlalchemy.func.now(), nullable=False)
    updated_at = Column(DateTime)

# your_application_code.py

import sqlalchemy as sa

create_refresh_updated_at_func = &quot;&quot;&quot;
    CREATE FUNCTION {schema}.refresh_updated_at()
    RETURNS TRIGGER
    LANGUAGE plpgsql AS
    $func$
    BEGIN
       NEW.updated_at := now();
       RETURN NEW;
    END
    $func$;
    &quot;&quot;&quot;

create_trigger = &quot;&quot;&quot;
    CREATE TRIGGER trig_{table}_updated BEFORE UPDATE ON {schema}.{table}
    FOR EACH ROW EXECUTE PROCEDURE {schema}.refresh_updated_at();
    &quot;&quot;&quot;

my_schema = &quot;foo&quot;
engine.execute(sa.text(create_refresh_updated_at_func.format(schema=my_schema)))
engine.execute(sa.text(create_trigger.format(schema=my_schema, table=&quot;user&quot;)))


Integration with Alembic
In my case it was important to integrate the trigger creation with Alembic, and to add the trigger to n dimension tables (all of them having an updated_at column).
# alembic/versions/your_version.py

import sqlalchemy as sa

create_refresh_updated_at_func = &quot;&quot;&quot;
    CREATE FUNCTION {schema}.refresh_updated_at()
    RETURNS TRIGGER
    LANGUAGE plpgsql AS
    $func$
    BEGIN
       NEW.updated_at := now();
       RETURN NEW;
    END
    $func$;
    &quot;&quot;&quot;

create_trigger = &quot;&quot;&quot;
    CREATE TRIGGER trig_{table}_updated BEFORE UPDATE ON {schema}.{table}
    FOR EACH ROW EXECUTE PROCEDURE {schema}.refresh_updated_at();
    &quot;&quot;&quot;

def upgrade():
    op.create_table(..., schema=&quot;foo&quot;)
    ...

    # Add updated_at triggers for all tables
    op.execute(sa.text(create_refresh_updated_at_func.format(schema=&quot;foo&quot;)))
    for table in MY_LIST_OF_TABLES:
        op.execute(sa.text(create_trigger.format(schema=&quot;foo&quot;, table=table)))

def downgrade():
    op.drop_table(..., schema=&quot;foo&quot;)
    ...

    op.execute(sa.text(&quot;DROP FUNCTION foo.refresh_updated_at() CASCADE&quot;))

"
"Goal: Given a seaborn catplot (kind=&quot;bar&quot;) with multiple rows, grouped bars, and a mapped stripplot, how do I add statistical annotations (p-values).
The following code from @Trenton McKinney generates my figure without statistical annotation. I would like to insert statistical annotation into this figure:
import seaborn as sns

tips = sns.load_dataset(&quot;tips&quot;)

g = sns.catplot(x=&quot;sex&quot;, y=&quot;total_bill&quot;, hue=&quot;smoker&quot;, row=&quot;time&quot;, data=tips, kind=&quot;bar&quot;, ci = &quot;sd&quot;, 
    edgecolor=&quot;black&quot;, errcolor=&quot;black&quot;, errwidth=1.5, capsize = 0.1, height=4, aspect=.7,alpha=0.5)

g.map(sns.stripplot, 'sex', 'total_bill', 'smoker', hue_order=['Yes', 'No'], order=['Male', 'Female'],
  palette=sns.color_palette(), dodge=True, alpha=0.6, ec='k', linewidth=1)



What I tried: I tried to use statannotations.Annotator.Annotator.plot_and_annotate_facets(). However, I was not able to get it working properly.
I also tried to use statannotations.Annotator.Annotator.new_plot(). However, this just worked for barplots but not for catplots. This is the corresponding code based on @r-beginners:
import seaborn as sns
from statannotations.Annotator import Annotator
%matplotlib inline
import matplotlib.pyplot as plt

df = sns.load_dataset(&quot;tips&quot;)

x=&quot;sex&quot;
y=&quot;total_bill&quot;
hue=&quot;smoker&quot;
hue_order=['Yes', 'No']

pairs = [
    ((&quot;Male&quot;, &quot;Yes&quot;), (&quot;Male&quot;, &quot;No&quot;)),
    ((&quot;Female&quot;, &quot;Yes&quot;), (&quot;Female&quot;, &quot;No&quot;))]

ax = sns.barplot(data=df, x=x, y=y, hue=hue, hue_order=hue_order, seed=2021, ci=&quot;sd&quot;, 
    edgecolor=&quot;black&quot;, errcolor=&quot;black&quot;, errwidth=1.5, capsize = 0.1, alpha=0.5)

sns.stripplot(x=x, y=y, hue=hue, data=df, dodge=True, alpha=0.6, ax=ax)

annot = Annotator(None, pairs)

annot.new_plot(ax, pairs, plot='barplot',
           data=df, x=x, y=y, hue=hue, hue_order=hue_order, seed=2021)
annot.configure(test='Mann-Whitney', text_format='simple', loc='inside', verbose=2)
annot.apply_test().annotate()

plt.legend(loc='upper left', bbox_to_anchor=(1.03, 1), title=hue)


Question: Does anyone know how to insert statistical annotation into a figure-level plot, preferably a catplot (kind=&quot;bar&quot;)?
","I think you can just iterate over the axes in the FacetGrid and apply the Annotator element wise.
Here is a short example with your provided code:
import seaborn as sns
from statannotations.Annotator import Annotator
%matplotlib inline


tips = sns.load_dataset(&quot;tips&quot;)

args = dict(x=&quot;sex&quot;, y=&quot;total_bill&quot;, data=tips, hue=&quot;smoker&quot;, hue_order=[&quot;Yes&quot;,&quot;No&quot;], order=['Male', 'Female'])

g = sns.catplot(edgecolor=&quot;black&quot;, errcolor=&quot;black&quot;, errwidth=1.5, capsize = 0.1, height=4, aspect=.7,alpha=0.5, kind=&quot;bar&quot;, ci = &quot;sd&quot;, row=&quot;time&quot;, **args)
g.map(sns.stripplot, args[&quot;x&quot;], args[&quot;y&quot;], args[&quot;hue&quot;], hue_order=args[&quot;hue_order&quot;], order=args[&quot;order&quot;], palette=sns.color_palette(), dodge=True, alpha=0.6, ec='k', linewidth=1)

pairs = [
    ((&quot;Male&quot;, &quot;Yes&quot;), (&quot;Male&quot;, &quot;No&quot;)),
    ((&quot;Female&quot;, &quot;Yes&quot;), (&quot;Female&quot;, &quot;No&quot;))
]

for ax_n in g.axes:
    for ax in ax_n:
        annot = Annotator(ax, pairs, **args)
        annot.configure(test='Mann-Whitney', text_format='simple', loc='inside', verbose=2)
        annot.apply_test().annotate()

This produces the following plot:

"
"It was brought to my attention that the matmul function in numpy is performing significantly worse than the dot function when multiplying array views. In this case my array view is the real part of a complex array. Here is some code which reproduces the issue:
import numpy as np
from timeit import timeit
N = 1300
xx = np.random.randn(N, N) + 1j
yy = np.random.randn(N, N) + 1J

x = np.real(xx)
y = np.real(yy)
assert np.shares_memory(x, xx)
assert np.shares_memory(y, yy)

dot = timeit('np.dot(x,y)', number = 10, globals = globals())
matmul = timeit('np.matmul(x,y)', number = 10, globals = globals())

print('time for np.matmul: ', matmul)
print('time for np.dot: ', dot)

On my machine the output is as follows:
time for np.matmul:  23.023062199994456
time for np.dot:  0.2706864000065252

This clearly has something to do with the shared memory as replacing np.real(xx) with np.real(xx).copy() makes the performance discrepancy go away.
Trolling the numpy docs was not particularly helpful as the listed differences did not discuss implementation details when dealing with memory views.
","These timings indicate the dot is doing a copy with real:
In [22]: timeit np.dot(xx.real,xx.real)
232 ms ± 3.34 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [23]: timeit np.dot(xx.real.copy(),xx.real.copy())
232 ms ± 4.18 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

Applying that to matmul produces nearly the same times:
In [24]: timeit np.matmul(xx.real.copy(),xx.real.copy())
231 ms ± 3.54 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

Again, matmul with real is taking some slow route.  matmul/dot both do poorer when given int arrays, though not as slow as the matmul real case.  matmul/dot can also handle object dtypes, but that's even slower.
So there's a lot going on under the covers that we, as python level users, don't see (and isn't documented).
edit
I was tempted to change the title to focus on complex-real, but decided to check another view - a slice of a float array
In [42]: y=xx.real.copy()[::2,::2];y.shape,y.dtype
Out[42]: ((650, 650), dtype('float64'))

In [43]: timeit np.dot(y,y)
36.4 ms ± 63.4 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
In [44]: timeit np.dot(y.copy(),y.copy())
35.6 ms ± 191 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

Again it is evident that dot is using copies of the view.  matmul does not:
In [45]: timeit np.matmul(y,y)
1.89 s ± 3.01 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

but with copies the times are the same as dot:
In [46]: timeit np.matmul(y.copy(),y.copy())
35.3 ms ± 102 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

So my guess is that dot routinely makes a copy if it can't send the arrays directly to the BLAS routines.  matmul apparently takes a slower route instead.
edit
While their handling of 2d arrays is similar, dot and matmul are quite different in how they handle 3+d arrays.  In fact the main reason to add @ was to provide a convenient 'batch' notion to matrix multiplication.
Sticking with the large complex arrays, lets make one 3x larger:
In [49]: yy=np.array([xx,xx,xx]);yy.shape
Out[49]: (3, 1300, 1300)

In [50]: timeit np.dot(xx,xx)
794 ms ± 12.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)    
In [51]: timeit np.dot(xx,yy)       # (yy,xx) same timings
55.5 s ± 151 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
In [52]: timeit np.matmul(xx,yy)    # (yy,yy) same
2.58 s ± 362 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

matmul has just increased the time by 3; dot by 70.  I could explore things more, but not with timings in the minute range.
"
"I'm plotting a separate hexbin figure and json boundary file. The hexbin grid overlaps the boundary file though. I'm interested in displaying the African continent only. I'm aiming to cut-off or subset the hexbin grid within the African continent. So no grid square should be visualised outside the boundary file. Is there a way to achieve this using Plotly?
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objs as go
import plotly.figure_factory as ff
import geopandas as gpd
import json

data = pd.DataFrame({
    'LAT': [1,5,6,7,5,6,7,5,6,7,5,6,7,12,-40,50],
    'LON': [10,10,11,12,10,11,12,10,11,12,10,11,12,-20,40,50],
    })

gdf_poly = gpd.read_file(gpd.datasets.get_path(&quot;naturalearth_lowres&quot;))
gdf_poly = gdf_poly.drop('name', axis = 1)

Afr_gdf_area = gdf_poly[gdf_poly['continent'] == 'Africa'].reset_index(drop = True)

fig = ff.create_hexbin_mapbox(data_frame=data,
                       lat=&quot;LAT&quot;, 
                       lon=&quot;LON&quot;,
                       nx_hexagon=25,
                       opacity=0.4,
                       labels={&quot;color&quot;: &quot;Point Count&quot;},
                       mapbox_style='carto-positron',
                       zoom = 1
                       )

fig.update_layout(mapbox={
        &quot;layers&quot;: [
            {&quot;source&quot;: json.loads(Afr_gdf_area.geometry.to_json()),
                &quot;below&quot;: &quot;traces&quot;,
                &quot;type&quot;: &quot;fill&quot;,
                &quot;color&quot;: &quot;orange&quot;,
                &quot;opacity&quot; : 0.1,
                &quot;line&quot;: {&quot;width&quot;: 1}
            },
        ],
    })   

fig.show()

Intended output is to cut-off or clip squares outside the African continent, which is in orange.

","If you look inside fig.data[0], it's a Choroplethmapbox with several fields including customdata and geojson. The geojson contains all of the information that plotly needs to draw the hexbins, including the coordinates and unique id for each hexagon. The customdata is an array of shape [n_hexbins x 3] where each element of the array includes the id and the numeric values that plotly uses to determine the color of each hexbin.
'customdata': array([[0.0, '-0.3490658516205964,-0.7648749219440846', 0],
                         [0.0, '-0.3490658516205964,-0.6802309514438665', 0],
                         [0.0, '-0.3490658516205964,-0.5955869809436484', 0],
                         ...,
                         [0.0, '0.8482300176421051,0.8010385323099501', 0],
                         [0.0, '0.8482300176421051,0.8856825028101681', 0],
                         [0.0, '0.8482300176421051,0.9703264733103861', 0]], dtype=object),
    'geojson': {'features': [{'geometry': {'coordinates': [[[-20.00000007,
                                                           -41.31174966478728],
                                                           [-18.6000000672,
                                                           -40.70179509236059],
                                                           [-18.6000000672,
                                                           -39.464994178287064],
                                                           [-20.00000007,
                                                           -38.838189880150665],
                                                           [-21.4000000728,
                                                           -39.464994178287064],
                                                           [-21.4000000728,
                                                           -40.70179509236059],
                                                           [-20.00000007,
                                                           -41.31174966478728]]],
                                           'type': 'Polygon'},
                              'id': '-0.3490658516205964,-0.7648749219440846',
                              'type': 'Feature'},
                             {'geometry': {'coordinates': [[[-20.00000007,
                                                           -37.56790013078226],
                                                           [-18.6000000672,
                                                           -36.924474103794715],
                                                           [-18.6000000672,
                                                           -35.62123099996148],
                                                           [-20.00000007,
                                                           -34.96149172026768],
                                                           [-21.4000000728,
                                                           -35.62123099996148],
                                                           [-21.4000000728,
                                                           -36.924474103794715],
                                                           [-20.00000007,
                                                           -37.56790013078226]]],
                                           'type': 'Polygon'},
                              'id': '-0.3490658516205964,-0.6802309514438665',
                              'type': 'Feature'},
                             {'geometry': {'coordinates
...

To select the hexbins within the specified boundary, we can start by extracting the information from customdata and geojson within the fig.data[0] generated by plotly, and create a geopandas dataframe. Then we can create a new geopandas dataframe called hexbins_in_afr which is an inner join between our new gdf of hexbins and Afr_gdf_area (so that we are dropping all hexbins outside of Afr_gdf_area).
After we extract the geojson information from hexbins_in_afr as well as the customdata, we can explicitly set the following fields within fig.data[0]:
fig.data[0]['geojson']['features'] = new_geojson
fig.data[0]['customdata'] = hexbins_in_afr['customdata']

Here is the code with the necessary modifications:
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objs as go
import plotly.figure_factory as ff
import geopandas as gpd
from geopandas.tools import sjoin
from shapely.geometry import Polygon
import json


data = pd.DataFrame({
    'LAT': [1,5,6,7,5,6,7,5,6,7,5,6,7,12,-40,50],
    'LON': [10,10,11,12,10,11,12,10,11,12,10,11,12,-20,40,50],
    })

gdf_poly = gpd.read_file(gpd.datasets.get_path(&quot;naturalearth_lowres&quot;))
gdf_poly = gdf_poly.drop('name', axis = 1)

Afr_gdf_area = gdf_poly[gdf_poly['continent'] == 'Africa'].reset_index(drop = True)

fig = ff.create_hexbin_mapbox(data_frame=data,
                       lat=&quot;LAT&quot;, 
                       lon=&quot;LON&quot;,
                       nx_hexagon=25,
                       opacity=0.4,
                       labels={&quot;color&quot;: &quot;Point Count&quot;},
                       mapbox_style='carto-positron',
                       zoom = 1
                       )

gdf = gpd.GeoDataFrame({
    'customdata': fig.data[0]['customdata'].tolist(),
    'id':[item['id'] for item in fig.data[0]['geojson']['features']],
    'geometry':[Polygon(item['geometry']['coordinates'][0]) for item in fig.data[0]['geojson']['features']]
})
gdf.set_crs(epsg=4326, inplace=True)

hexbins_in_afr = sjoin(gdf, Afr_gdf_area, how='inner')

def get_coordinates(polygon):
    return [[list(i) for i in polygon.exterior.coords]]

hexbins_in_afr['coordinates'] = hexbins_in_afr['geometry'].apply(lambda x: get_coordinates(x))

## create a new geojson that matches the structure of fig.data[0]['geojson']['features']
new_geojson = [{
    'type': 'Feature', 
    'id': id, 
    'geometry': {
        'type': 'Polygon', 
        'coordinates': coordinate
    }
} for id, coordinate in zip(hexbins_in_afr['id'],hexbins_in_afr['coordinates'])]

fig.data[0]['geojson']['features'] = new_geojson
fig.data[0]['customdata'] = hexbins_in_afr['customdata']

fig.update_layout(mapbox={
        &quot;layers&quot;: [
            {&quot;source&quot;: json.loads(Afr_gdf_area.geometry.to_json()),
                &quot;below&quot;: &quot;traces&quot;,
                &quot;type&quot;: &quot;fill&quot;,
                &quot;color&quot;: &quot;orange&quot;,
                &quot;opacity&quot; : 0.1,
                &quot;line&quot;: {&quot;width&quot;: 1}
            },
        ],
    })   

fig.show()


"
"I would like to define a sort of &quot;wrapper&quot; Generic Type, say MyType[T], so that it has the same type interface as the wrapped type.
from typing import Generic, TypeVar

T = TypeVar(&quot;T&quot;)

class MyType(Generic):
    pass  # what to write here?

So, as an example, when I have a type MyType[int], the type-checker should treat it as if it was an int type.
Is that possible? If so, how?
","To confirm, you're looking at wanting the expression MyType[T] to mean to a static type checker &quot;A subclass of MyType and T&quot;, such that a declaration
class MyType:
    attr: object

will result in the following (e.g. using mypy and int.conjugate as an example):
&gt;&gt;&gt; reveal_type(MyType[int].conjugate)  # def (self: builtins.int) -&gt; builtins.int
&gt;&gt;&gt; obj: MyType[int] = MyType[int]()
&gt;&gt;&gt; reveal_type(obj.attr)  # builtins.object


No, this isn't a supported idiom in Python typing. Your use case should be covered by a future implementation of intersection types with a proposed syntax MyType &amp; T instead.
To a static type checker, the syntax MyType[T] means and only means that MyType is a generic type (including generic structural types), and the API that MyType has is derived from any metaclasses, any base classes, and anything declared under class MyType's body.

Any type parameterisation (e.g. passing int to T in MyType[T]) only affects generic types in the class bases and statements in the class body;

There is nothing you can write in a class body which tells a static type checker that the class inherits the API from another class, as this information is already given by the metaclass and base class(es);

&lt;type&gt;[T] is only well-defined as a statically-inferrable type when &lt;type&gt; is a user-defined generic type. The static types inferred from other arbitrary attempts to make &lt;type&gt;[T] a valid expression is unstable and implementation-defined, regardless of the runtime implementation.
from typing import *

T = TypeVar(&quot;T&quot;)

class M(type):
    def __getitem__(cls, type_: T, /) -&gt; T: ...

class A(metaclass=M): ...
class B:
    def __class_getitem__(cls, type_: T, /) -&gt; T: ...
class C(Generic[T]): ...

&gt;&gt;&gt; A_int: TypeAlias = A[int]  # mypy: &quot;A&quot; expects no type arguments, but 1 given  # pyright: Expected no type arguments for class &quot;A&quot;
&gt;&gt;&gt; B_int: TypeAlias = B[int]  # mypy: &quot;B&quot; expects no type arguments, but 1 given  # pyright: &lt;no messages&gt;
&gt;&gt;&gt; C_int: TypeAlias = C[int]
&gt;&gt;&gt;
&gt;&gt;&gt; reveal_type(A[int])  # mypy: &lt;overloads of `int.__new__`&gt;                           pyright: Type of &quot;A[int]&quot; is &quot;Type[int]&quot;
&gt;&gt;&gt; reveal_type(B[int])  # mypy: The type &quot;type[B]&quot; is not generic and not indexable    pyright: Type of &quot;B[int]&quot; is &quot;Type[int]&quot;
&gt;&gt;&gt; reveal_type(C[int])  # mypy: Revealed type is &quot;def () -&gt; __main__.C[builtins.int]&quot;  pyright: Type of &quot;C[int]&quot; is &quot;Type[C[int]]&quot;



"
"I have an 1d numpy array of values:
v = np.array([0, 1, 4, 0, 5])

Furthermore, I have a 2d numpy array of boolean masks (in production, there are millions of masks):
m = np.array([
    [True, True, False, False, False],
    [True, False, True, False, True],
    [True, True, True, True, True],
])

I want to apply each row from the mask to the array v, and then compute the mean of the masked values.
Expected behavior:
results = []
for mask in m:
    results.append(np.mean(v[mask]))

print(results) # [0.5, 3.0, 2.0]


Easy to do sequentially, but I am sure there is a beautiful version in parallel? One solution, that I've found:
mask = np.ones(m.shape)
mask[~m] = np.nan
np.nanmean(v * mask, axis=1) # [0.5, 3.0, 2.0]


Is there another solution, perhaps using np.ma module? I am looking for a solution that is faster than my current two solutions.
","Faster Numpy code
A faster Numpy way to do that is to perform a matrix multiplication:
(m @ v) / m.sum(axis=1)

We can optimize this further by avoiding implicit conversion and perform the summation with 8-bit integer (this is safe only because v.shape[1] is small -- ie. less than 127):
(m @ v) / m.view(np.int8).sum(dtype=np.int8, axis=1)



Even faster code with Numba multithreading
We can use Numba to write a similar function and even use multiple threads:
import numba as nb

@nb.njit('(float32[::1], bool_[:,::1])', parallel=True)
def compute(v, m):
    si, sj = m.shape
    res = np.empty(si, dtype=np.float32)
    for i in nb.prange(si):
        s = np.float32(0)
        count = 0
        for j in range(sj):
            if m[i, j]:
                s += v[j]
                count += 1
        if count &gt; 0:
            res[i] = s / count
        else:
            res[i] = np.nan
    return res


Results
Here are results on my i5-9600KF CPU (with 6 cores):
Initial vectorized code:   136 ms
jakevdp's solution:         74 ms
Numpy matmul:               28 ms
Numba sequential code:      21 ms
Optimized Numpy matmul:     20 ms   &lt;-----
Numba parallel code:         4 ms   &lt;-----

The sequential Numba implementation is unfortunately not much better than the Numpy one (Numba do not generate a very good code in this case), but it scale well so the parallel version is significantly faster.
The Numba parallel version is 34 times faster than the initial vectorized code and 18 times faster than the jakevdp's solution. The Numpy optimized matrix-multiplication-based solution is 3.7 times faster jakevdp's solution.
Note that the Numba code is also a bit better than the jakevdp's solution and the optimized Numpy one since it support the case where the mean is invalid (NaN) without printing a warning about a division by 0.
"
"I am facing a small (big) problem: I want to generate a high resolution speckle pattern and save it as a file that I can import into a laser engraver. Can be PNG, JPEG, PDF, SVG, or TIFF.
My script does a decent job of generating the pattern that I want:
The user needs to first define the inputs, these are:
############
#  INPUTS  #
############
dpi = 1000 # dots per inch
dpmm = 0.03937 * dpi # dots per mm
widthOfSampleMM = 50 # mm
heightOfSampleMM = 50 # mm
patternSizeMM = 0.1 # mm
density = 0.75 # 1 is very dense, 0 is not fine at all
variation = 0.75 # 1 is very bad, 0 is very good
############

After this, I generate the empty matrix and fill it with black shapes, in this case a circle.
# conversions to pixels
widthOfSamplesPX = int(np.ceil(widthOfSampleMM*dpmm)) # get the width
widthOfSamplesPX = widthOfSamplesPX + 10 - widthOfSamplesPX % 10 # round up the width to nearest 10
heightOfSamplePX = int(np.ceil(heightOfSampleMM*dpmm)) # get the height
heightOfSamplePX = heightOfSamplePX + 10 - heightOfSamplePX % 10 # round up the height to nearest 10
patternSizePX = patternSizeMM*dpmm # this is the size of the pattern, so far I am going with circles
# init an empty image
im = 255*np.ones((heightOfSamplePX, widthOfSamplesPX), dtype = np.uint8)
# horizontal circle centres
numPoints = int(density*heightOfSamplePX/patternSizePX) # get number of patterns possible
if numPoints==1:
    horizontal = [heightOfSamplePX // 2]
else:
    horizontal = [int(i * heightOfSamplePX / (numPoints + 1)) for i in range(1, numPoints + 1)]
# vertical circle centres
numPoints = int(density*widthOfSamplesPX/patternSizePX)
if numPoints==1:
    vertical = [widthOfSamplesPX // 2]
else:
    vertical = [int(i * widthOfSamplesPX / (numPoints + 1)) for i in range(1, numPoints + 1)]
for i in vertical:
    for j in horizontal:
        # generate the noisy information
        iWithNoise = i+variation*np.random.randint(-2*patternSizePX/density, +2*patternSizePX/density)
        jWithNoise = j+variation*np.random.randint(-2*patternSizePX/density, +2*patternSizePX/density)
        patternSizePXWithNoise = patternSizePX+patternSizePX*variation*(np.random.rand()-0.5)/2
        cv2.circle(im, (int(iWithNoise),int(jWithNoise)), int(patternSizePXWithNoise//2), 0, -1) # add circle

After this step, I can get im, here's a low quality example at dpi=1000:

And here's one with my target dpi (5280):

Now I would like to save im in a handlable way at high quality (DPI&gt;1000). Is there any way to do this?

Stuff that I have tried so far:

plotting and saving the plot image with PNG, TIFF, SVG, PDF with different DPI values
plt.savefig() with different dpi's
cv2.imwrite()
too large of a file, only solution here is to reduce DPI, which also reduces quality
SVG write from matrix:
I developed this function but ultimately, the files were too large:

import svgwrite
def matrix_to_svg(matrix, filename, padding = 0, cellSize=1):
    # get matrix dimensions and extremes
    rows, cols = matrix.shape
    minVal = np.min(matrix)
    maxVal = np.max(matrix)
    # get a drawing
    dwg = svgwrite.Drawing(filename, profile='tiny', 
                           size = (cols*cellSize+2*padding,rows*cellSize+2*padding))
    # define the colormap, in this case grayscale since black and white
    colorScale = lambda val: svgwrite.utils.rgb(int(255*(val-minVal)/(maxVal-minVal)),
                                                 int(255*(val-minVal)/(maxVal-minVal)),
                                                 int(255*(val-minVal)/(maxVal-minVal)))
    # get the color of each pixel in the matrix and draw it
    for i in range(rows):
        for j in range(cols):
            color = colorScale(matrix[i, j])
            dwg.add(dwg.rect(insert=(j * cellSize + padding, i * cellSize + padding),
                             size=(cellSize, cellSize),
                             fill=color))
    dwg.save() # save


PIL.save(). Files too large

The problem could be also solved by generating better shapes. This would not be an obstacle either.  I am open to re-write using a different method, would be grateful if someone would just point me in the right direction.
","Let's make some observations of the effects of changing the DPI:
DPI 1000   Height=1970   Width=1970    # Spots=140625  Raw pixels: 3880900
DPI 10000  Height=19690  Width=19690   # Spots=140625  Raw pixels: 387696100

We can see that while the number of spots drawn remains quite consistent (it does vary due to the various rounding in your calculations, but for all intents and purposes, we can consider it constant), the raw pixel count of a raster image generated increases quadratically. A vector representation would seem desireable, since it is freely scalable (quality depending on the capabilities of a renderer).
Unfortunately, the way you generate the SVG is flawed, since you've basically turned it into an extremely inefficient raster representation. This is because you generate a rectangle for each individual pixel (even for those that are technically background). Consider that in an 8-bit grayscale image, such as the PNGs you generate requires 1 byte to represent a raw pixel. On the other hand, your SVG representation of a single pixel looks something like this:
&lt;rect fill=&quot;rgb(255,255,255)&quot; height=&quot;1&quot; width=&quot;1&quot; x=&quot;12345&quot; y=&quot;15432&quot; /&gt;
Using ~70 bytes per pixel, when we're talking about tens of megapixels... clearly not the way to go.
However, let's recall that the number of spots doesn't depend on DPI. Can we just represent the spots in some efficient way? Well, the spots are actually circles,  parametrized by position, radius and colour. SVG supports circles, and their representation looks like this:
&lt;circle cx=&quot;84&quot; cy=&quot;108&quot; fill=&quot;rgb(0,0,0)&quot; r=&quot;2&quot; /&gt;
Let's look at the effects of changing the DPI now.
DPI 1000   # Spots=140625  Raw pixels: 3880900    SVG size: 7435966
DPI 10000  # Spots=140625  Raw pixels: 387696100  SVG size: 7857942

The slight increase in size is due to increased range of position/radius values.

I somewhat refactored your code example. Here's the result that demonstrates the SVG output.
import numpy as np
import cv2
import svgwrite

MM_IN_INCH = 0.03937

def round_int_to_10s(value):
    int_value = int(value)
    return int_value + 10 - int_value % 10

def get_sizes_pixels(height_mm, width_mm, pattern_size_mm, dpi):
    dpmm = MM_IN_INCH * dpi # dots per mm
    width_px = round_int_to_10s(np.ceil(width_mm * dpmm))
    height_px = round_int_to_10s(np.ceil(height_mm * dpmm))
    pattern_size_px = pattern_size_mm * dpmm
    return height_px, width_px, pattern_size_px
 
def get_grid_positions(size, pattern_size, density):
    count = int(density * size / pattern_size) # get number of patterns possible
    if count == 1:
        return [size // 2]
    return [int(i * size / (count + 1)) for i in range(1, count + 1)]
 
def get_spot_grid(height_px, width_px, pattern_size_px, density):
    vertical = get_grid_positions(height_px, pattern_size_px, density)
    horizontal = get_grid_positions(width_px, pattern_size_px, density)
    return vertical, horizontal

def generate_spots(vertical, horizontal, pattern_size, density, variation):
    spots = []
    noise_halfspan = 2 * pattern_size / density;
    noise_min, noise_max = (-noise_halfspan, noise_halfspan)
    for i in vertical:
        for j in horizontal:
            # generate the noisy information
            center = tuple(map(int, (j, i) + variation * np.random.randint(noise_min, noise_max, 2)))
            d = int(pattern_size + pattern_size * variation * (np.random.rand()-0.5) / 2)
            spots.append((center, d//2)) # add circle params
    return spots

def render_raster(height, width, spots):
    im = 255 * np.ones((height, width), dtype=np.uint8)
    for center, radius in spots:
        cv2.circle(im, center, radius, 0, -1) # add circle
    return im
    
def render_svg(height, width, spots):
    dwg = svgwrite.Drawing(profile='tiny', size = (width, height))
    fill_color = svgwrite.utils.rgb(0, 0, 0)
    for center, radius in spots:
        dwg.add(dwg.circle(center, radius, fill=fill_color)) # add circle
    return dwg.tostring()


#  INPUTS  #
############
dpi = 100 # dots per inch
WidthOfSample_mm = 50 # mm
HeightOfSample_mm = 50 # mm
PatternSize_mm = 1 # mm
density = 0.75 # 1 is very dense, 0 is not fine at all
Variation = 0.75 # 1 is very bad, 0 is very good
############

height, width, pattern_size = get_sizes_pixels(HeightOfSample_mm, WidthOfSample_mm, PatternSize_mm, dpi)
vertical, horizontal = get_spot_grid(height, width, pattern_size, density)
spots = generate_spots(vertical, horizontal, pattern_size, density, Variation)

img = render_raster(height, width, spots)
svg = render_svg(height, width, spots)

print(f&quot;Height={height}  Width={width}   # Spots={len(spots)}&quot;)
print(f&quot;Raw pixels: {img.size}&quot;)
print(f&quot;SVG size: {len(svg)}&quot;)

cv2.imwrite(&quot;timo.png&quot;, img)
with open(&quot;timo.svg&quot;, &quot;w&quot;) as f:
    f.write(svg)

This generates the following output:
PNG  | Rendered SVG 
Note: Since it's not possible to upload SVGs here, I put it on pastebin, and provide capture of it rendered by Firefox.

Further improvements to the size of the SVG are possible. For example, we're currently using the same colour over an over. Styling or grouping should help remove this redundancy.
Here's an example that groups all the spots in one group with constant fill colour:
def render_svg(height, width, spots):
    dwg = svgwrite.Drawing(profile='tiny', size = (width, height))
    dwg_spots = dwg.add(dwg.g(id='spots', fill='black'))
    for center, radius in spots:
        dwg_spots.add(dwg.circle(center, radius)) # add circle
    return dwg.tostring()

The output looks the same, but the file is now 4904718 bytes instead of 7435966 bytes.
An alternative (pointed out by AKX) if you only desire to draw in black, you may omit the fill specification as well as the grouping, since the default SVG fill colour is black.

The next thing to notice is that most of the spots have the same radius -- in fact, using your settings at DPI of 1000 the unique radii are [1, 2] and at DPI of 10000 they are [15, 16, 17, 18, 19, 20, 21, 22, 23].
How could we avoid repeatedly specifying the same radius? (As far as I can tell, we can't use groups to specify it) In fact, how can we omit repeatedly specifying it's a circle? Ideally we'd just tell it &quot;Draw this mark at all of those positions&quot; and just provide a list of points.
Turns out there are two features of SVG that let us do exactly that. First of all, we can specify custom markers, and later refer to them by an ID.
&lt;marker id=&quot;id1&quot; markerHeight=&quot;2&quot; markerWidth=&quot;2&quot; refX=&quot;1&quot; refY=&quot;1&quot;&gt;
  &lt;circle cx=&quot;1&quot; cy=&quot;1&quot; fill=&quot;black&quot; r=&quot;1&quot; /&gt;
&lt;/marker&gt;

Second, the polyline element can optionally draw markers at every vertex of the polyline. If we draw the polyline with no stroke and no fill, all we end up is with the markers.
&lt;polyline fill=&quot;none&quot; marker-end=&quot;url(#id1)&quot; marker-mid=&quot;url(#id1)&quot; marker-start=&quot;url(#id1)&quot;
  points=&quot;2,5 8,22 11,26 9,46 8,45 2,70 ... and so on&quot; stroke=&quot;none&quot; /&gt;

Here's the code:
def group_by_radius(spots):
    radii = set([r for _,r in spots])
    groups = {r: [] for r in radii}
    for c, r in spots:
        groups[r].append(c)
    return groups

def render_svg_v2(height, width, spots):
    dwg = svgwrite.Drawing(profile='full', size=(width, height))
    by_radius = group_by_radius(spots)
    dwg_grp = dwg.add(dwg.g(stroke='none', fill='none'))
    for r, centers in by_radius.items():
        dwg_marker = dwg.marker(id=f'r{r}', insert=(r, r), size=(2*r, 2*r))
        dwg_marker.add(dwg.circle((r, r), r=r))
        dwg.defs.add(dwg_marker)
        dwg_line = dwg_grp.add(dwg.polyline(centers))
        dwg_line.set_markers((dwg_marker, dwg_marker, dwg_marker))
    return dwg.tostring()

The output SVG still looks the same, but now the filesize at DPI of 1000 is down to 1248852 bytes.

With high enough DPI, a lot of the coordinates will be 3, 4 or even 5 digits. If we bin the coordinates into tiles of 100 or 1000 pixels, we can then take advantage of the use element, which lets us apply an offset to the referenced object. Thus, we can limit the polyline coordinates to 2 or 3 digits at the cost of some extra overhead (which is generally worth it).
Here's an initial (clumsy) implementation of that:
def bin_points(points, bin_size):
    bins = {}
    for x,y in points:
        bin = (max(0, x // bin_size), max(0, y // bin_size))
        base = (bin[0] * bin_size, bin[1] * bin_size)
        offset = (x - base[0], y - base[1])
        if base not in bins:
            bins[base] = []
        bins[base].append(offset)
    return bins

def render_svg_v3(height, width, spots, bin_size):
    dwg = svgwrite.Drawing(profile='full', size=(width, height))
    by_radius = group_by_radius(spots)
    dwg_grp = dwg.add(dwg.g(stroke='none', fill='none'))
    polyline_counter = 0
    for r, centers in by_radius.items():
        dwg_marker = dwg.marker(id=f'm{r}', insert=(r, r), size=(2*r, 2*r))
        dwg_marker.add(dwg.circle((r, r), r=r, fill='black'))
        dwg.defs.add(dwg_marker)
       
        dwg_marker_grp = dwg_grp.add(dwg.g())
        marker_iri = dwg_marker.get_funciri()
        for kind in ['start','end','mid']:
            dwg_marker_grp[f'marker-{kind}'] = marker_iri
        
        bins = bin_points(centers, bin_size)
        for base, offsets in bins.items():
            dwg_line = dwg.defs.add(dwg.polyline(id=f'p{polyline_counter}', points=offsets))
            polyline_counter += 1            
            dwg_marker_grp.add(dwg.use(dwg_line, insert=base))
    return dwg.tostring()

With bin size set to 100, and DPI of 1000, we get to a file size of 875012 bytes, which means about 6.23 bytes per spot. That's not so bad for XML based format. With DPI of 10000 we need bin size of 1000 to make a meaningful improvement, which yields something like 1349325 bytes (~9.6B/spot).
"
"How to calculate horizontal median for numerical columns?
df = pl.DataFrame({&quot;ABC&quot;:[&quot;foo&quot;, &quot;bar&quot;, &quot;foo&quot;], &quot;A&quot;:[1,2,3], &quot;B&quot;:[2,1,None], &quot;C&quot;:[1,2,3]})
print(df)

shape: (3, 4)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ ABC â”† A   â”† B    â”† C   â”‚
â”‚ --- â”† --- â”† ---  â”† --- â”‚
â”‚ str â”† i64 â”† i64  â”† i64 â”‚
â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•¡
â”‚ foo â”† 1   â”† 2    â”† 1   â”‚
â”‚ bar â”† 2   â”† 1    â”† 2   â”‚
â”‚ foo â”† 3   â”† null â”† 3   â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

I want to achieve the same as with the below pl.mean_horizontal, but get median instead of the mean. I did not find existing expression for this.
print(df.with_columns(pl.mean_horizontal(pl.col(pl.Int64)).alias(&quot;Horizontal Mean&quot;)))

shape: (3, 5)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ABC â”† A   â”† B    â”† C   â”† Horizontal Mean â”‚
â”‚ --- â”† --- â”† ---  â”† --- â”† ---             â”‚
â”‚ str â”† i64 â”† i64  â”† i64 â”† f64             â”‚
â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ foo â”† 1   â”† 2    â”† 1   â”† 1.333333        â”‚
â”‚ bar â”† 2   â”† 1    â”† 2   â”† 1.666667        â”‚
â”‚ foo â”† 3   â”† null â”† 3   â”† 3.0             â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

","There's no median_horizontal() at the moment, but you could use

pl.concat_list() to create list column out of all pl.Int64 columns.
pl.Expr.list.median() to calculate median.

df.with_columns(
    pl.concat_list(pl.col(pl.Int64)).list.median().alias(&quot;Horizontal Median&quot;)
)

shape: (3, 5)
┌─────┬─────┬──────┬─────┬───────────────────┐
│ ABC ┆ A   ┆ B    ┆ C   ┆ Horizontal Median │
│ --- ┆ --- ┆ ---  ┆ --- ┆ ---               │
│ str ┆ i64 ┆ i64  ┆ i64 ┆ f64               │
╞═════╪═════╪══════╪═════╪═══════════════════╡
│ foo ┆ 1   ┆ 2    ┆ 1   ┆ 1.0               │
│ bar ┆ 2   ┆ 1    ┆ 2   ┆ 2.0               │
│ foo ┆ 3   ┆ null ┆ 3   ┆ 3.0               │
└─────┴─────┴──────┴─────┴───────────────────┘

Or you can use numpy integration (but this will probably be slower):
import numpy as np

df.with_columns(
    pl.Series(&quot;Horizontal Median&quot;, np.nanmedian(df.select(pl.col(pl.Int64)), axis=1))
)

shape: (3, 5)
┌─────┬─────┬──────┬─────┬───────────────────┐
│ ABC ┆ A   ┆ B    ┆ C   ┆ Horizontal Median │
│ --- ┆ --- ┆ ---  ┆ --- ┆ ---               │
│ str ┆ i64 ┆ i64  ┆ i64 ┆ f64               │
╞═════╪═════╪══════╪═════╪═══════════════════╡
│ foo ┆ 1   ┆ 2    ┆ 1   ┆ 1.0               │
│ bar ┆ 2   ┆ 1    ┆ 2   ┆ 2.0               │
│ foo ┆ 3   ┆ null ┆ 3   ┆ 3.0               │
└─────┴─────┴──────┴─────┴───────────────────┘

"
"#AttributeError: 'FigureCanvasInterAgg' object has no attribute 'tostring_rgb'. Did you mean: 'tostring_argb'?
#import matplotlib.pyplot as plt

#========================
# This can be work
# import matplotlib
# matplotlib.use('TkAgg')
# import matplotlib.pyplot as plt
#=========================


with open('notebook.txt', encoding='utf-8') as file:
    # contents = file.read()
    # print(contents)
    # for line in file:
    #     print('line:', line)
    contents = file.readlines()
    print(contents)

newList = []
for content in contents:
    newContent = content.replace('\n', '')
    money = newContent.split(':')[-1]
    newList.append(int(money))
    # 6æœˆ: 9000
    # contents = content.replace('\n', '')
print(newList)
x = [1, 2, 3, 4, 5, 6]
y = newList
plt.plot(x, y, 'r')
plt.xlabel('month')
plt.ylabel('money')
plt.legend()
plt.show()

1æœˆ: 7000
2æœˆ: 10000
3æœˆ: 15000
4æœˆ: 12000
5æœˆ: 13000
6æœˆ: 9000

I am learning to draw graphs with matplotlib, but import matplolib.plylot as plt does not recognize the data. I have pip installed matplotlib, but I suspect it is not installed in the right path. Is there any way to solve this problem?
","The following code runs successfully on my computer, and my maplotlib verson is 3.7.1
if you don't know your matplotlib verson,you can press &quot;ctrl&quot; and 'r',then input &quot;cmd&quot; to open the terminal,and input &quot;pip list&quot;,then you can find your matlotlib version
import matplotlib.pyplot as plt
from matplotlib import rcParams
# 设置支持中文字体
rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体
rcParams['axes.unicode_minus'] = False   # 正常显示负号

with open('notebook.txt', encoding='utf-8') as file:
    contents = file.readlines()  # 按行读取文件内容

newList = []
for content in contents:
    newContent = content.replace('\n', '')  # 去掉换行符
    money = newContent.split(':')[-1].strip()  # 提取“:”后面的金额部分并去掉空格
    newList.append(int(money))

x = [1, 2, 3, 4, 5, 6]

plt.plot(x, newList, 'r', label='收入')  # 绘制红色折线图，并设置图例标签
plt.xlabel('月份')  # 设置 x 轴标签
plt.ylabel('金额')  # 设置 y 轴标签
plt.legend()  # 显示图例
plt.show()  # 展示图形


"
"I need to simulate DB connection without actual connection. All answers I found are trying to mock methods in different ways, connect to docker db, connect to actual PostgreSQL running locally. I believe I need mocking variant but I cannot formulate in my head how should I mock. Am I missing something? Am I moving into wrong direction?
I use PostgreSQL and psycopg2. Package psycopg2-binary
Database connection:
import os

import psycopg2
from loguru import logger
from psycopg2.extensions import parse_dsn


def init_currency_history_table(cursor):
    create_users_table_query = &quot;&quot;&quot;
        CREATE TABLE IF NOT EXISTS history(
          id BIGINT PRIMARY KEY NOT NULL,
          event TEXT,
          creation_date TIMESTAMPTZ DEFAULT NOW()
        );
    &quot;&quot;&quot;
    cursor.execute(create_users_table_query)


def load_db(db_url):
    db = psycopg2.connect(**db_url)
    db.autocommit = True
    return db


class PostgresqlApi(object):

    def __init__(self, load=load_db):
        logger.info(os.environ.get('DATABASE_URL'))
        db_url = parse_dsn(os.environ.get('DATABASE_URL'))
        db_url['sslmode'] = 'require'
        logger.info('HOST: {0}'.format(db_url.get('host')))
        self.db = load_db(db_url)
        self.cursor = self.db.cursor()

        init_currency_history_table(self.cursor)
        self.db.commit()

    def add_event(self, *, event):
        insert_event_table = &quot;&quot;&quot;
            INSERT INTO history (event) VALUES (%s);
        &quot;&quot;&quot;
        self.cursor.execute(insert_event_table, (event))

    def events(self):
        select_event_table = &quot;&quot;&quot;SELECT * FROM event;&quot;&quot;&quot;
        self.cursor.execute(select_event_table)
        return self.cursor.fetchall()

    def close(self):
        self.cursor.close()
        self.db.close()


I use DB for Falcon API.
from fastapi import Depends, FastAPI, HTTPException, status
from fastapi.security import HTTPBasic, HTTPBasicCredentials
from decimal import Decimal, getcontext

from db import PostgresqlApi

app = FastAPI()
security = HTTPBasic()
database = None


def db_connection():
    global database
    if not database:
        database = PostgresqlApi()
    return database

def check_basic_auth_creds(credentials: HTTPBasicCredentials = Depends(security)):
    correct_username = secrets.compare_digest(credentials.username, os.environ.get('APP_USERNAME'))
    correct_password = secrets.compare_digest(credentials.password, os.environ.get('APP_PASSWORD'))
    if not (correct_username and correct_password):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail=&quot;Incorrect username and password&quot;,
            headers={'WWW-Authenticate': 'Basic'}
        )
    return credentials

@app.get(&quot;/currencies&quot;)
def read_currencies(credentials: HTTPBasicCredentials = Depends(check_basic_auth_creds)):
    db = db_connection()
    return {'get events': 'ok'}

I have tried different methods and plugins. Among others arepytest-pgsql, pytest-postgresql.
","The solution I landed at is below.

Created fake class that has exactly structure of PostgresqlApi. (see implementation below)
Created fixture for db_connection method. (see implementation below)

Fake class implementation
class FakePostgresqlApi(PostgresqlApi):

    event_list = []

    def __init__(self):
        pass

    def add_event(self, *, event):
        self.event_list.append([1, 'magic trick', 1653630607])

    def events(self):
        return self.event_list

    def close(self):
        self.event_list.clear()

Fixture
from unittest.mock import MagicMock

@pytest.fixture
def mock_db_connection(mocker):
    mocker.patch('src.main.db_connection', MagicMock(return_value=FakePostgresqlApi()))

The test itself was:
def test_read_events(mock_db_connection):
   # Do whatever I need here, in my case call Falcon API test client

"
"Currently, I am trying to create a pydantic model for a pandas dataframe. I would like to check if a column is unique by the following
import pandas as pd
from typing import List
from pydantic import BaseModel

class CustomerRecord(BaseModel):
    
    id: int
    name: str
    address: str

class CustomerRecordDF(BaseModel):
    
    __root__: List[CustomerRecord]


df = pd.DataFrame({'id':[1,2,3], 
                   'name':['Bob','Joe','Justin'], 
                   'address': ['123 Fake St', '125 Fake St', '123 Fake St']})

df_dict = df.to_dict(orient='records')

CustomerRecordDF.parse_obj(df_dict)

I would now like to run a validation here and have it fail since address is not unique.
The following returns what I need
from pydantic import root_validator

class CustomerRecordDF(BaseModel):
    
    __root__: List[CustomerRecord]
    
    @root_validator(pre=True)
    def unique_values(cls, values):
        root_values = values.get('__root__')
        value_set = set()
        for value in root_values:
            print(value['address'])
            
            
            if value['address'] in value_set:
                raise ValueError('Duplicate Address')
            else:
                value_set.add(value['address'])
        return values

CustomerRecordDF.parse_obj(df_dict)
&gt;&gt;&gt; ValidationError: 1 validation error for CustomerRecordDF
  __root__
  Duplicate Address (type=value_error)

but i want to be able to reuse this validator for other other dataframes I create and to also pass in this unique check on multiple columns. Not just address.
Ideally something like the following
from pydantic import root_validator

class CustomerRecordDF(BaseModel):

    __root__: List[CustomerRecord]
    
    _validate_unique_name = root_unique_validator('name')
    _validate_unique_address = root_unique_validator('address')

","You could use an inner function and the allow_reuse argument:
def root_unique_validator(field):
    def validator(cls, values):
        # Use the field arg to validate a specific field
        ...

    return root_validator(pre=True, allow_reuse=True)(validator)

Full example:
import pandas as pd
from typing import List
from pydantic import BaseModel, root_validator


class CustomerRecord(BaseModel):
    id: int
    name: str
    address: str


def root_unique_validator(field):
    def validator(cls, values):
        root_values = values.get(&quot;__root__&quot;)
        value_set = set()
        for value in root_values:
            if value[field] in value_set:
                raise ValueError(f&quot;Duplicate {field}&quot;)
            else:
                value_set.add(value[field])
        return values

    return root_validator(pre=True, allow_reuse=True)(validator)


class CustomerRecordDF(BaseModel):
    __root__: List[CustomerRecord]

    _validate_unique_name = root_unique_validator(&quot;name&quot;)
    _validate_unique_address = root_unique_validator(&quot;address&quot;)


df = pd.DataFrame(
    {
        &quot;id&quot;: [1, 2, 3],
        &quot;name&quot;: [&quot;Bob&quot;, &quot;Joe&quot;, &quot;Justin&quot;],
        &quot;address&quot;: [&quot;123 Fake St&quot;, &quot;125 Fake St&quot;, &quot;123 Fake St&quot;],
    }
)

df_dict = df.to_dict(orient=&quot;records&quot;)

CustomerRecordDF.parse_obj(df_dict)

# Output:
# pydantic.error_wrappers.ValidationError: 1 validation error for CustomerRecordDF
# __root__
#   Duplicate address (type=value_error)

And if you use a duplicated name:
# Here goes the most part of the full example above

df = pd.DataFrame(
    {
        &quot;id&quot;: [1, 2, 3],
        &quot;name&quot;: [&quot;Bob&quot;, &quot;Joe&quot;, &quot;Bob&quot;],
        &quot;address&quot;: [&quot;123 Fake St&quot;, &quot;125 Fake St&quot;, &quot;127 Fake St&quot;],
    }
)

df_dict = df.to_dict(orient=&quot;records&quot;)

CustomerRecordDF.parse_obj(df_dict)

# Output:
# pydantic.error_wrappers.ValidationError: 1 validation error for CustomerRecordDF
# __root__
#   Duplicate name (type=value_error)

You could also receive more than one field and have a single root validator that validates all the fields. That will probably make the allow_reuse argument unnecessary.
"
"I would like to have a plot where the font are in &quot;computer modern&quot; (i.e. Latex style) but   with x-ticks and y-ticks in bold.
Due to the recent upgrade of matplotlib my previous procedure does not work anymore.
This is my old procedure:
plt.rc('font', family='serif',size=24)
matplotlib.rc('text', usetex=True)
matplotlib.rc('legend', fontsize=24) 
matplotlib.rcParams['text.latex.preamble'] = [r'\boldmath']

This is the output message:
test_font.py:26: MatplotlibDeprecationWarning: Support for setting an rcParam that expects a str value to a non-str value is deprecated since 3.5 and support will be removed two minor releases later.
  matplotlib.rcParams['text.latex.preamble'] = [r'\boldmath']

I have decide that a possible solution could be to use the &quot;computer modern&quot; as font. This is my example:
import matplotlib
import matplotlib.pyplot as plt
import numpy as np


font = {'family' : 'serif',
        'weight' : 'bold',
        'size'   : 12
        }

matplotlib.rc('font', **font)


# Data for plotting
t = np.arange(0.0, 2.0, 0.01)
s = 1 + np.sin(2 * np.pi * t)

fig, ax = plt.subplots(1,figsize=(9,6))

ax.plot(t, s)

ax.set(xlabel='time (s)  $a_1$', ylabel='voltage (mV)',
       title='About as simple as it gets, folks')
ax.grid()

fig.savefig(&quot;test.png&quot;)
plt.show()

This is the result:

I am not able, however, to set-up in font the font style.
I have tried to set the font family as &quot;cmr10&quot;. This the code:
font = {'family' : 'serif',
         'weight' : 'bold',
         'size'   : 12,
         'serif':  'cmr10'
         }

matplotlib.rc('font', **font)

It seems that the &quot;cmr10&quot; makes disappear the bold option.
Have I made some errors?
Do you have in mind other possible solution?
Thanks
","You still can use your old procedure, but with a slight change. The MatplotlibDeprecationWarning that you get states that the parameter expects a str value but it's getting something else. In this case what is happening is that you are passing it as a list. Removing the brackets will do the trick:
import matplotlib
import matplotlib.pyplot as plt
import numpy as np

plt.rc('font', family='serif',size=24)
matplotlib.rc('text', usetex=True)
matplotlib.rc('legend', fontsize=24)
matplotlib.rcParams['text.latex.preamble'] = r'\boldmath'


# Data for plotting
t = np.arange(0.0, 2.0, 0.01)
s = 1 + np.sin(2 * np.pi * t)

fig, ax = plt.subplots(1,figsize=(9,6))

ax.plot(t, s)

ax.set(xlabel='time (s)  $a_1$', ylabel='voltage (mV)',
       title='About as simple as it gets, folks')
ax.grid()

fig.savefig(&quot;test.png&quot;)
plt.show()

The code above produces this plot without any errors:

"
"I'm attempting to determine the time based on the timezone specified in each row using Polars. Consider the following code snippet:
import polars as pl
from datetime import datetime
from polars import col as c

df = pl.DataFrame({
    &quot;time&quot;: [datetime(2023, 4, 3, 2), datetime(2023, 4, 4, 3), datetime(2023, 4, 5, 4)],
    &quot;tzone&quot;: [&quot;Asia/Tokyo&quot;, &quot;America/Chicago&quot;, &quot;Europe/Paris&quot;]
}).with_columns(c.time.dt.replace_time_zone(&quot;UTC&quot;))

df.with_columns(
    tokyo=c.time.dt.convert_time_zone(&quot;Asia/Tokyo&quot;).dt.hour(),
    chicago=c.time.dt.convert_time_zone(&quot;America/Chicago&quot;).dt.hour(),
    paris=c.time.dt.convert_time_zone(&quot;Europe/Paris&quot;).dt.hour()
)

In this example, I've computed the time separately for each timezone to achieve the desired outcome, which is [11, 22, 6], corresponding to the hour of the time column according to the tzone timezone. Even then it is difficult to collect the information from the correct column.
Unfortunately, the following simple attempt to dynamically pass the timezone from the tzone column directly into the convert_time_zone function does not work:
df.with_columns(c.time.dt.convert_time_zone(c.tzone).dt.hour())
# TypeError: argument 'time_zone': 'Expr' object cannot be converted to 'PyString'

What would be the most elegant approach to accomplish this task?
","The only way to do this which fully works with lazy execution is to use the polars-xdt plugin:
df = pl.DataFrame(
    {
        &quot;time&quot;: [
            datetime(2023, 4, 3, 2),
            datetime(2023, 4, 4, 3),
            datetime(2023, 4, 5, 4),
        ],
        &quot;tzone&quot;: [&quot;Asia/Tokyo&quot;, &quot;America/Chicago&quot;, &quot;Europe/Paris&quot;],
    }
).with_columns(pl.col(&quot;time&quot;).dt.replace_time_zone(&quot;UTC&quot;))

df.with_columns(
    result=xdt.to_local_datetime(&quot;time&quot;, pl.col(&quot;tzone&quot;)).dt.hour(),
)

Result:
Out[6]:
shape: (3, 3)
┌─────────────────────────┬─────────────────┬────────┐
│ time                    ┆ tzone           ┆ result │
│ ---                     ┆ ---             ┆ ---    │
│ datetime[μs, UTC]       ┆ str             ┆ i8     │
╞═════════════════════════╪═════════════════╪════════╡
│ 2023-04-03 02:00:00 UTC ┆ Asia/Tokyo      ┆ 11     │
│ 2023-04-04 03:00:00 UTC ┆ America/Chicago ┆ 22     │
│ 2023-04-05 04:00:00 UTC ┆ Europe/Paris    ┆ 6      │
└─────────────────────────┴─────────────────┴────────┘

https://github.com/pola-rs/polars-xdt
If you don't need lazy execution, then as other answers have suggested, you can iterate over the unique values of your 'time_zone' column
"
"I need some help with polars:
I have a dataframe with a categorical values column
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ session_id        â”† elapsed_time â”† fqid   â”‚
â”‚ ---               â”† ---          â”† ---    â”‚
â”‚ i64               â”† i32          â”† cat    â”‚
â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•¡
â”‚ 20090312431273200 â”† 0            â”† intro  â”‚
â”‚ 20090312431273200 â”† 1323         â”† gramps â”‚
â”‚ 20090312431273200 â”† 831          â”† gramps â”‚
â”‚ 20090312431273200 â”† 1147         â”† gramps â”‚
â”‚ â€¦                 â”† â€¦            â”† â€¦      â”‚
â”‚ 20090312431273200 â”† 5197         â”† teddy  â”‚
â”‚ 20090312431273200 â”† 6180         â”† teddy  â”‚
â”‚ 20090312431273200 â”† 7014         â”† teddy  â”‚
â”‚ 20090312431273200 â”† 7946         â”† teddy  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

And I want to transform the fqid-column to look like this:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ session_id        â”† fqid_gramps â”† fqid_intro â”† fqid_teddy â”‚
â”‚ ---               â”† ---         â”† ---        â”† ---        â”‚
â”‚ i64               â”† i32         â”† i32        â”† i32        â”‚
â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ 20090312431273200 â”† 1           â”† 1          â”† 4          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

That is, I would like to:

Group_by over session_id,
Make a value_counts() over fqid,
Rename columns so that it would be 'fqid_' + category,
Turn them into columns (transpose),
Add them to the result.

Technically, I could achieve this without groupby by using something like
column_values = train['fqid'].value_counts().with_columns(pl.concat_str(pl.lit('fqid' + '_').alias('fqid'), pl.col('fqid').cast(pl.String))).transpose()
column_values = column_values.rename(column_values.head(1).to_dicts().pop()).slice(1)

But when I am trying to make an aggregating function from this replacing train['fqid'] with pl.col('fqid') and making a group_by('session_id').aggregate(func('fqid')) it gives me nothing but errors like AttributeError: 'Expr' object has no attribute 'with_columns'.
Could you kindly suggest a proper way of making this operation?
","Starting from
train=pl.from_repr(
     &quot;&quot;&quot;┌───────────────────┬──────────────┬────────┐
        │ session_id        ┆ elapsed_time ┆ fqid   │
        │ ---               ┆ ---          ┆ ---    │
        │ i64               ┆ i32          ┆ cat    │
        ╞═══════════════════╪══════════════╪════════╡
        │ 20090312431273200 ┆ 0            ┆ intro  │
        │ 20090312431273200 ┆ 1323         ┆ gramps │
        │ 20090312431273200 ┆ 831          ┆ gramps │
        │ 20090312431273200 ┆ 1147         ┆ gramps │
        │ 20090312431273200 ┆ 5197         ┆ teddy  │
        │ 20090312431273200 ┆ 6180         ┆ teddy  │
        │ 20090312431273200 ┆ 7014         ┆ teddy  │
        │ 20090312431273200 ┆ 7946         ┆ teddy  │
        └───────────────────┴──────────────┴────────┘&quot;&quot;&quot;)

we can do
(
    train
        .group_by(
            (piv_idx:='session_id'),
            (len_id:='fqid'),
            maintain_order=True) 
        .len()
        .pivot(on=len_id,
               index=piv_idx,
               values='len',
               aggregate_function='first')
        .select(
            piv_idx,
            pl.exclude(piv_idx).name.prefix(f&quot;{len_id}_&quot;)
            )
)

shape: (1, 4)
┌───────────────────┬────────────┬─────────────┬────────────┐
│ session_id        ┆ fqid_intro ┆ fqid_gramps ┆ fqid_teddy │
│ ---               ┆ ---        ┆ ---         ┆ ---        │
│ i64               ┆ u32        ┆ u32         ┆ u32        │
╞═══════════════════╪════════════╪═════════════╪════════════╡
│ 20090312431273200 ┆ 1          ┆ 3           ┆ 4          │
└───────────────────┴────────────┴─────────────┴────────────┘

Since you want the count (now len) of the fqids, you need to include that in the group_by. Next, we do a pivot to make the results wide. The output of pivot doesn't keep the original column name so we have to add that back manually. We do that in a select by first taking the session_id and then adding to that every column except session_id with the prefix 'fqid_' to get the final desired result.
Incidentally, I'm not using value_counts because it returns a list of structs so we can't do, for example, train.select(pl.col('fqid').value_counts().over('session_id'))
I used the walrus operator to assign the column names to variables in the group_by so that you only need to change the columns in one place without repeating yourself. in the pivot and select.
"
"So I have a custom middleware like this:
Its objective is to add some meta_data fields to every response from all endpoints of my FastAPI app.

@app.middelware(&quot;http&quot;)
async def add_metadata_to_response_payload(request: Request, call_next):

    response = await call_next(request)

    body = b&quot;&quot;
    async for chunk in response.body_iterator:
        body+=chunk


    data = {}
    data[&quot;data&quot;] = json.loads(body.decode())
    data[&quot;metadata&quot;] = {
        &quot;some_data_key_1&quot;: &quot;some_data_value_1&quot;,
        &quot;some_data_key_2&quot;: &quot;some_data_value_2&quot;,
        &quot;some_data_key_3&quot;: &quot;some_data_value_3&quot;
    }

    body = json.dumps(data, indent=2, default=str).encode(&quot;utf-8&quot;)

    return Response(
        content=body,
        status_code=response.status_code,
        media_type=response.media_type
    )


However, when I served my app using uvicorn, and launched the swagger URL, here is what I see:

Unable to render this definition

The provided definition does not specify a valid version field.

Please indicate a valid Swagger or OpenAPI version field. Supported version fields are
Swagger: &quot;2.0&quot; and those that match openapi: 3.0.n (for example, openapi: 3.0.0)


With a lot of debugging, I found that this error was due to the custom middleware and specifically this line:
body = json.dumps(data, indent=2, default=str).encode(&quot;utf-8&quot;)

If I simply comment out this line, swagger renders just fine for me. However, I need this line for passing the content argument in Response from Middleware. How to sort this out?
UPDATE:
I tried the following:
body = json.dumps(data, indent=2).encode(&quot;utf-8&quot;)
by removing default arg, the swagger did successfully load. But now when I hit any of the APIs, here is what swagger tells me along with response payload on screen:
Unrecognised response type; displaying content as text
More Updates (6th April 2022):
Got a solution to fix 1 part of the problem by Chris, but the swagger wasn't still loading. The code was hung up in the middleware level indefinitely and the page was not still loading.
So, I found in all these places:

https://github.com/encode/starlette/issues/919
Blocked code while using middleware and dependency injections to log requests in FastAPI(Python)
https://github.com/tiangolo/fastapi/issues/394

that this way of adding custom middleware works by inheriting from BaseHTTPMiddleware in Starlette and has its own issues (something to do with awaiting inside middleware, streamingresponse and normal response, and the way it is called). I don't understand it yet.
","Here's how you could do that (inspired by this). Make sure to check the Content-Type of the response (as shown below), so that you can modify it by adding the metadata, only if it is of application/json type.
For the OpenAPI (Swagger UI) to render (both /docs and /redoc), make sure to check whether openapi key is not present in the response, so that you can proceed modifying the response only in that case. If you happen to have a key with such a name in your response data, then you could have additional checks using further keys that are present in the response for the OpenAPI, e.g., info, version, paths, and, if needed, you can check against their values too.
from fastapi import FastAPI, Request, Response
import json

app = FastAPI()

@app.middleware(&quot;http&quot;)
async def add_metadata_to_response_payload(request: Request, call_next):
    response = await call_next(request)
    content_type = response.headers.get('Content-Type')
    if content_type == &quot;application/json&quot;:
        response_body = [section async for section in response.body_iterator]
        resp_str = response_body[0].decode()  # converts &quot;response_body&quot; bytes into string
        resp_dict = json.loads(resp_str)  # converts resp_str into dict 
        #print(resp_dict)
        if &quot;openapi&quot; not in resp_dict:
            data = {}
            data[&quot;data&quot;] = resp_dict  # adds the &quot;resp_dict&quot; to the &quot;data&quot; dictionary
            data[&quot;metadata&quot;] = {
                &quot;some_data_key_1&quot;: &quot;some_data_value_1&quot;,
                &quot;some_data_key_2&quot;: &quot;some_data_value_2&quot;,
                &quot;some_data_key_3&quot;: &quot;some_data_value_3&quot;}
            resp_str = json.dumps(data, indent=2)  # converts dict into JSON string
        
        return Response(content=resp_str, status_code=response.status_code, media_type=response.media_type)
        
    return response


@app.get(&quot;/&quot;)
def foo(request: Request):
    return {&quot;hello&quot;: &quot;world!&quot;}

Update 1
Alternatively, a likely better approach would be to check for the request's url path at the start of the middleware function (against a pre-defined list of paths/routes that you would like to add metadata to their responses), and proceed accordingly. Example is given below.
from fastapi import FastAPI, Request, Response, Query
from pydantic import constr
from fastapi.responses import JSONResponse
import re
import uvicorn
import json

app = FastAPI()
routes_with_middleware = [&quot;/&quot;]
rx = re.compile(r'^(/items/\d+|/courses/[a-zA-Z0-9]+)$')  # support routes with path parameters
my_constr = constr(regex=&quot;^[a-zA-Z0-9]+$&quot;)

@app.middleware(&quot;http&quot;)
async def add_metadata_to_response_payload(request: Request, call_next):
    response = await call_next(request)
    if request.url.path not in routes_with_middleware and not rx.match(request.url.path):
        return response
    else:
        content_type = response.headers.get('Content-Type')
        if content_type == &quot;application/json&quot;:
            response_body = [section async for section in response.body_iterator]
            resp_str = response_body[0].decode()  # converts &quot;response_body&quot; bytes into string
            resp_dict = json.loads(resp_str)  # converts resp_str into dict 
            data = {}
            data[&quot;data&quot;] = resp_dict  # adds &quot;resp_dict&quot; to the &quot;data&quot; dictionary
            data[&quot;metadata&quot;] = {
                &quot;some_data_key_1&quot;: &quot;some_data_value_1&quot;,
                &quot;some_data_key_2&quot;: &quot;some_data_value_2&quot;,
                &quot;some_data_key_3&quot;: &quot;some_data_value_3&quot;}
            resp_str = json.dumps(data, indent=2)  # converts dict into JSON string
            return Response(content=resp_str, status_code=response.status_code, media_type=&quot;application/json&quot;)


@app.get(&quot;/&quot;)
def root():
    return {&quot;hello&quot;: &quot;world!&quot;}

@app.get(&quot;/items/{id}&quot;)
def get_item(id: int):
    return {&quot;Item&quot;: id}

@app.get(&quot;/courses/{code}&quot;)
def get_course(code: my_constr):
    return {&quot;course_code&quot;: code, &quot;course_title&quot;: &quot;Deep Learning&quot;}

Update 2
Another solution would be to use a custom APIRoute class, as demonstrated here and here, which would allow you to apply the changes to the response body only for routes that you have specified—which would solve the issue with Swaager UI in a more easy way.
Alternatively, you could still use the middleware option if you wish, but instead of adding the middleware to the main app, you could add it to a sub application—as shown in this answer and this answer—that includes again only the routes for which you need to modify the response, in order to add some additional data in the body.
"
"Update: pl.cut was removed from Polars. Expression equivalents were added instead:
.cut() .qcut()

How can I use it in select context, such as df.with_columns?
To be more specific, if I have a polars dataframe with a lot of columns and one of them is called x, how can I do pl.cut on x and append the grouping result into the original dataframe?
Below is what I tried but it does not work:
df = pl.DataFrame({&quot;a&quot;: [1, 2, 3, 4, 5], &quot;b&quot;: [2, 3, 4, 5, 6], &quot;x&quot;: [1, 3, 5, 7, 9]})
df.with_columns(pl.cut(pl.col(&quot;x&quot;), bins=[2, 4, 6]))

Thanks so much for your help.
","From the docs, as of 2023-01-25, cut takes a Series and returns a DataFrame.  Unlike many/most methods and functions, it doesn't take an expression so you can't use it in a select or with_column(s).  To get your desired result you'd have to join it to your original df.
Additionally, it appears that cut doesn't necessarily maintain the same dtypes as the parent series.  (This is most certainly a bug) As such you have to cast it back to, in this case, int.
You'd have:
df=df.join(
    pl.cut(df.get_column('x'),bins=[2,4,6]).with_columns(pl.col('x').cast(pl.Int64())),
    on='x'
)

shape: (5, 5)
┌─────┬─────┬─────┬─────────────┬─────────────┐
│ a   ┆ b   ┆ x   ┆ break_point ┆ category    │
│ --- ┆ --- ┆ --- ┆ ---         ┆ ---         │
│ i64 ┆ i64 ┆ i64 ┆ f64         ┆ cat         │
╞═════╪═════╪═════╪═════════════╪═════════════╡
│ 1   ┆ 2   ┆ 1   ┆ 2.0         ┆ (-inf, 2.0] │
│ 2   ┆ 3   ┆ 3   ┆ 4.0         ┆ (2.0, 4.0]  │
│ 3   ┆ 4   ┆ 5   ┆ 6.0         ┆ (4.0, 6.0]  │
│ 4   ┆ 5   ┆ 7   ┆ inf         ┆ (6.0, inf]  │
│ 5   ┆ 6   ┆ 9   ┆ inf         ┆ (6.0, inf]  │
└─────┴─────┴─────┴─────────────┴─────────────┘

"
"I have a DataFrame like this:
import polars as pl

df = pl.DataFrame({&quot;x&quot;: [1.2, 1.3, 3.4, 3.5]})
df

# shape: (3, 1)
# â”Œâ”€â”€â”€â”€â”€â”
# â”‚ a   â”‚
# â”‚ --- â”‚
# â”‚ f64 â”‚
# â•žâ•â•â•â•â•â•¡
# â”‚ 1.2 â”‚
# â”‚ 1.3 â”‚
# â”‚ 3.4 â”‚
# â”‚ 3.5 â”‚
# â””â”€â”€â”€â”€â”€â”˜

I would like to make a rolling aggregation using .rolling() so that each row uses a window [-2:1]:
shape: (4, 2)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ x   â”† y                 â”‚
â”‚ --- â”† ---               â”‚
â”‚ f64 â”† list[f64]         â”‚
â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ 1.2 â”† [1.2, 1.3]        â”‚
â”‚ 1.3 â”† [1.2, 1.3, 3.4]   â”‚
â”‚ 3.4 â”† [1.2, 1.3, â€¦ 3.5] â”‚
â”‚ 3.5 â”† [1.3, 3.4, 3.5]   â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

So far, I managed to do this with the following code:
df.with_row_index(&quot;index&quot;).with_columns(
  y = pl.col(&quot;x&quot;).rolling(index_column = &quot;index&quot;, period = &quot;4i&quot;, offset = &quot;-3i&quot;)
).drop(&quot;index&quot;)

However this requires manually creating a column index and then removing it after the operation. Is there a way to achieve the same result in a single with_columns() call?
","Pure expressions approach (apparently slow)
You can use concat_list with shift
(
    df
    .with_columns(
        y=pl.concat_list(
            pl.col('x').shift(x) 
            for x in range(2,-2,-1)
            )
        .list.drop_nulls()
        )
)
shape: (4, 2)
┌─────┬───────────────────┐
│ x   ┆ y                 │
│ --- ┆ ---               │
│ f64 ┆ list[f64]         │
╞═════╪═══════════════════╡
│ 1.2 ┆ [1.2, 1.3]        │
│ 1.3 ┆ [1.2, 1.3, 3.4]   │
│ 3.4 ┆ [1.2, 1.3, … 3.5] │
│ 3.5 ┆ [1.3, 3.4, 3.5]   │
└─────┴───────────────────┘

There are a couple things to note here.

When the input to shift is positive, that means to go backwards which is the opposite of your notation.
range can count backwards with (start, stop, increment) but stop is non-inclusive so when entering that parameter, it needs an extra -1.
At the end of the concat_list you need to manually drop the nulls that it will have for items at the beginning and end of the series.

As always, you can wrap this into a function, including a translation of your preferred notation to what you actually need in range for it to work.
from typing import Sequence


def my_roll(in_column: str | pl.Expr, window: Sequence):
    if isinstance(in_column, str):
        in_column = pl.col(in_column)
    pl_window = range(-window[0], -window[1] - 1, -1)
    return pl.concat_list(in_column.shift(x) for x in pl_window).list.drop_nulls()

which then allows you to do
df.with_columns(y=my_roll(&quot;x&quot;, [-2,1]))

If you don't care about static typing you can even monkey patch it to pl.Expr like this pl.Expr.my_roll = my_roll and then do df.with_columns(y=pl.col(&quot;x&quot;).my_roll([-2,1])) but your pylance/pyright/mypy/etc will complain about it not existing.
Another approach that's kind of cheating if you're an expression purist
You can combine the built in way featuring .with_row_index and .rolling into a .map_batches that just turns your column into a df and spits back the series you care about.
def my_roll(in_column: str | pl.Expr, window):
    if isinstance(in_column, str):
        in_column = pl.col(in_column)
    period = f&quot;{window[1]-window[0]+1}i&quot;
    offset = f&quot;{window[0]-1}i&quot;
    return in_column.map_batches(
        lambda s: (
            s.to_frame()
            .with_row_index()
            .select(
                pl.col(s.name).rolling(
                    index_column=&quot;index&quot;, 
                    period=period, 
                    offset=offset
                )
            )
            .get_column(s.name)
        )
    )

The way this works is that map_batches will turn your column into a Series and then run a function on it where the function returns another Series. If we make the function turn that Series into a DF, then attach the row_index, do the rolling, and get the resultant Series then that gives you exactly what you want all contained in an expression. It should be just as performant as the verbose way, assuming you don't have any other use of the row_index.
then you do
df.with_columns(y=my_roll(&quot;x&quot;, [-2,1]))

"
"Just trying to rewrite this c# code to python.
Server send public key(modulus, exponent), need to encrypt it with pkcs1 padding.
using (TcpClient client = new TcpClient())
{
    await client.ConnectAsync(ip, port);
    using (NetworkStream stream = client.GetStream())
    {
        await App.SendCmdToServer(stream, &quot;auth&quot;, this.Ver.ToString().Split('.', StringSplitOptions.None));
        
        byte[] modulus = new byte[256];
        int num2 = await stream.ReadAsync(modulus, 0, modulus.Length);
        byte[] exponent = new byte[3];
        int num3 = await stream.ReadAsync(exponent, 0, exponent.Length);
        
        this.ServerRsa = RSA.Create();
        this.ServerRsa.ImportParameters(new RSAParameters()
        {
          Modulus = modulus,
          Exponent = exponent
        });

        using (MemoryStream data = new MemoryStream())
        {
          using (BinaryWriter writer = new BinaryWriter((Stream) data))
          {
            writer.Write(string1);
            writer.Write(string2);
            await App.SendDataToServer(stream, this.ServerRsa.Encrypt(data.ToArray(), RSAEncryptionPadding.Pkcs1));
          }
        }
    }
}

Everything works fine, except encrypted result by python.
I've tried with rsa and pycryptodome, no luck at all, server returns reject.
Tried something like this (rsa)
server_rsa = rsa.newkeys(2048)[0]
server_rsa.n = int.from_bytes(modulus, byteorder='big')
server_rsa.e = int.from_bytes(exponent, byteorder='big')
data = (string1 + string2).encode()
encrypted_data = rsa.encrypt(data, server_rsa)

or this (pycryptodome)
pubkey = construct((int.from_bytes(modulus, 'big'), int.from_bytes(exponent, 'big')))
cipher = PKCS1_v1_5.new(pubkey)
encrypted_data = cipher.encrypt(data)

Is there some special python RSA implementation, that just not working with C#, or vice versa?
","The PyCryptodome is a good choice for cryptographic tasks in Python. The problem is with the data formatting, you are concatenating the strings directly in Python and the BinaryWriter in C# write the lengths of the strings as prefixes.This code show how you can do that:
import struct

data = b&quot;&quot;
data += struct.pack(&quot;&gt;I&quot;, len(string1.encode('utf-8')))   # add length as big-endian unsigned int
data += string1.encode('utf-8')
data += struct.pack(&quot;&gt;I&quot;, len(string2.encode('utf-8')))
data += string2.encode('utf-8')



In the code above I encoded the length of the strings as big-endian unsigned int but as was commented by @Topaco the BinaryWriter encodes the length prefix with LEB128. So to replicate BinaryWriter you can do this:
import leb128

data = bytearray()
data += leb128.u.encode(len(string1.encode()))
data += string1.encode()
data += leb128.u.encode(len(string2.encode())) 
data += string2.encode()

I used the leb128 package that can be installed with pip install leb128. But you can create a function to do that encoding
def encode_leb128(number):
    if number == 0:
        return bytearray([0])
    
    result = bytearray()
    while number &gt; 0:
        byte = number &amp; 0x7f
        number &gt;&gt;= 7
        if number &gt; 0:
            byte |= 0x80
        result.append(byte)
    return result


"
"With the move to the new pyproject.toml system, I was wondering whether there was a way to install packages in editable mode while compiling extensions (which pip install -e . does not do).
So I want pip to:

run the build_ext I configured for Cython and generate my .so files
put them in the local folder
do the rest of the normal editable install

I found some mentions of build_wheel_for_editable on the pip documentation but I could not find any actual example of where this hook should be implemented and what it should look like. (to be honest, I'm not even completely sure this is what I'm looking for)
So would anyone know how to do that?
I'd also happy about any additional explanation as to why pip install . runs build_ext but the editable command does not.

Details:
I don't have a setup.py file anymore; the pyproject.toml uses setuptools and contains
[build-system]
requires = [&quot;setuptools&gt;=61.0&quot;, &quot;numpy&gt;=1.17&quot;, &quot;cython&gt;=0.18&quot;]
build-backend = &quot;setuptools.build_meta&quot;

[tool.setuptools]
package-dir = {&quot;&quot; = &quot;.&quot;}

[tool.setuptools.packages]
find = {}

[tool.setuptools.cmdclass]
build_ext = &quot;_custom_build.build_ext&quot;

The custom build_ext looks like
from setuptools import Extension
from setuptools.command.build_ext import build_ext as _build_ext

from Cython.Build import cythonize

class build_ext(_build_ext):

    def initialize_options(self):
        super().initialize_options()
        if self.distribution.ext_modules is None:
            self.distribution.ext_modules = []
        extensions = Extension(...)
        self.distribution.ext_modules.extend(cythonize(extensions))

    def build_extensions(self):
        ...
        super().build_extensions()

It builds a .pyx into .cpp, then adds it with another cpp into a .so.
","I created a module that looks like this:
$  tree .   
.
├── pyproject.toml
├── setup.py
└── test
    └── helloworld.pyx

1 directory, 3 files

My pyproject.toml looks like:
[build-system]
requires = [&quot;setuptools&gt;=61.0&quot;, &quot;numpy&gt;=1.17&quot;, &quot;cython&gt;=0.18&quot;]
build-backend = &quot;setuptools.build_meta&quot;

[tool.setuptools]
py-modules = [&quot;test&quot;]

[project]
name        = &quot;test&quot;
version     = &quot;0.0.1&quot;%    

My setup.py:
from setuptools import setup
from Cython.Build import cythonize

setup(ext_modules=cythonize(&quot;test/helloworld.pyx&quot;))

And helloworld.pyx just contains print(&quot;Hello world&quot;).
When I do pip install -e ., it builds the cython file as expected.
If you really don't want to have a setup.py at all, I think you'll need to override build_py instead of build_ext, but IMO just having the simple setup.py file isn't a big deal.
"
"One can do that with dataclasses like so:
from dataclasses import dataclass
import pandas as pd

@dataclass
class MyDataClass:
    i: int
    s: str


df = pd.DataFrame([MyDataClass(&quot;a&quot;, 1), MyDataClass(&quot;b&quot;, 2)])

that makes the DataFrame df with columns i and s as one would expect.
Is there an easy way to do that with an attrs class?
I can do it by iterating over the the object's properties and constructing an object of a type like dict[str, list] ({&quot;i&quot;: [1, 2], &quot;s&quot;: [&quot;a&quot;, &quot;b&quot;]} in this case) and constructing the DataFrame from that but it would be nice to have support for attrs objects directly.
","You can access the dictionary at the heart of a dataclass like so
a = MyDataClass(&quot;a&quot;, 1)
a.__dict__

this outputs:
{'i': 'a', 's': 1}

Knowing this, if you have an iterable arr of type MyDataClass, you can access the __dict__ attribute and construct a dataframe
arr = [MyDataClass(&quot;a&quot;, 1), MyDataClass(&quot;b&quot;, 2)]
df = pd.DataFrame([x.__dict__ for x in arr])

df outputs:
   i  s
0  a  1
1  b  2

The limitation with this approach that if the slots option is used, then this will not work.
Alternatively, it is possible to convert the data from a dataclass to a tuple or dictionary using dataclasses.astuple and dataclasses.asdict respectively.
The data frame can be also constructed using either of the following:
# using astuple
df = pd.DataFrame(
  [dataclasses.astuple(x) for x in arr], 
  columns=[f.name for f in dataclasses.fields(MyDataClass)]
)

# using asdict
df = pd.DataFrame([dataclasses.asdict(x) for x in arr])

"
"Is there a way in Pydatic to perform the full validation of my classes? And return all the possible errors?
It seems that the standard behaviour blocks the validation at the first encountered error.
As an example:
from pydantic import BaseModel

class Salary(BaseModel):
    gross: int
    net: int
    tax: int

class Employee(BaseModel):
    name: str
    age: int
    salary: Salary

salary = Salary(gross = &quot;hello&quot;, net = 1000, tax = 10)
employee= Employee(name = &quot;Mattia&quot;, age = &quot;hello&quot;, Salary=salary)

This code works fine and returns the validation error:
pydantic.error_wrappers.ValidationError: 1 validation error for Salary
gross
  value is not a valid integer (type=type_error.integer)

However, it is not catching the second validation error on the age field. In a real bugfix scenario, I would need to fix the first validation error, re-run everything again, and only at that point I would discover the second error on age.
Is there a way to perform the full validation in pydantic? So validate everything and return ALL the validation errors? (so basically, do not stop at the first error met)
","What you are describing is not Pydantic-specific behavior. This is how exceptions in Python work. As soon as one is raised (and is not caught somewhere up the stack), execution stops.
Validation is triggered, when you attempt to initialize a Salary instance. Failed validation triggers the ValidationError. The Python interpreter doesn't even begin executing the line, where you want to initialize an Employee.
Pydantic is actually way nicer in this regard than it could be. If you pass more than one invalid value in the same initialization, the ValidationError will contain info about about all of them. Like this examle:
...
salary = Salary(gross=&quot;hello&quot;, net=&quot;foo&quot;, tax=10)

The error message will look like this:
ValidationError: 2 validation errors for Salary
gross
  value is not a valid integer (type=type_error.integer)
net
  value is not a valid integer (type=type_error.integer)

What you'll have to do, if you want to postpone raising errors, is wrap the initialization in a try-block and upon catching an error, you could for example add it to a list to be processed later.
In your example, this will not work because you want to use salary later on. In that case you could just initialize the Employee like this:
...
employee = Employee(
    name=&quot;Mattia&quot;,
    age=&quot;hello&quot;,
    salary={&quot;gross&quot;: &quot;hello&quot;, &quot;net&quot;: 100, &quot;tax&quot;: 10}
)

Which would also give you:
ValidationError: 2 validation errors for Employee
age
  value is not a valid integer (type=type_error.integer)
salary -&gt; gross
  value is not a valid integer (type=type_error.integer)

Hope this helps.
"
